Title Bayesian learning mechanisms URL https//en.wikipedia.org/wiki/Bayesian_learning_mechanisms Content Bayesian learning mechanisms are probabilistic causal models used in computer science to research the fundamental underpinnings of machine learning, and in cognitive neuroscience, to model conceptual development. Bayesian learning mechanisms have also been used in economics and cognitive psychology to study social learning in theoretical models of herd behavior. See also Title Machine learning URL https//en.wikipedia.org/wiki/Machine_learning Content Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. The application of ML to business problems is known as predictive analytics. Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning. From a theoretical viewpoint, probably approximately correct learning provides a framework for describing machine learning. History The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence. The synonym self-teaching computers was also used in this time period. Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes. In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. Hebbs model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data. Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes. By the early 1960s, an experimental learning machine with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively trained by a human operator/teacher to recognize patterns and equipped with a goof button to cause it to reevaluate incorrect decisions. A representative book on research into machine learning during the 1960s was Nilssons book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E. This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turings proposal in his paper Computing Machinery and Intelligence, in which the question Can machines think? is replaced with the question Can machines do what we (as thinking entities) can do?. Modern-day machine learning has two objectives. One is to classify data based on models which have been developed the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions. Relationships to other fields Artificial intelligence As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed neural networks these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis. 488 However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. 488 By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. 708 710, 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as connectionism, by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation. 25 Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. Data compression Data mining Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals on the other hand, machine learning also employs data mining methods as unsupervised learning or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data. Machine learning also has intimate ties to optimization Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples). Generalization Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms. Statistics Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field. Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be. Leo Breiman distinguished two statistical modeling paradigms data model and algorithmic model, wherein algorithmic model means more or less the machine learning algorithms like Random Forest. Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning. Statistical physics Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. Statistical physics is thus finding applications in the area of medical diagnostics. Theory A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias variance decomposition is one way to quantify generalization error. For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer. In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time. Approaches Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the signal or feedback available to the learning system Supervised learning The computer is presented with example inputs and their desired outputs, given by a teacher, and the goal is to learn a general rule that maps inputs to outputs. Unsupervised learning No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). Reinforcement learning A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). As it navigates its problem space, the program is provided feedback thats analogous to rewards, which it tries to maximize. Although each algorithm has advantages and limitations, no single algorithm works for all problems. Supervised learning Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs. An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task. Types of supervised-learning algorithms include active learning, classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. Examples of regression would be predicting the height of a person, or the future temperature. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. Unsupervised learning Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction, and density estimation. Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity. A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself. Semi-supervised learning Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy. In weakly supervised learning, the training labels are noisy, limited, or imprecise however, these labels are often cheaper to obtain, resulting in larger effective training sets. Reinforcement learning Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. Dimensionality reduction Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. In other words, it is a process of reducing the dimension of the feature set, also called the number of features. Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D). The manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization. Other types Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning. Self-learning Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA). It gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. Emotion is used as state evaluation of a self-learning agent. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion. The self-learning algorithm updates a memory matrix . There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations. Feature learning Several learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering. Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data. Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Sparse dictionary learning Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot. Anomaly detection In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions. In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns. Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as normal and abnormal and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model. Robot learning Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning, and finally meta-learning (e.g. MAML). Association rules Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of interestingness. Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves rules to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule  o n i o n s , p o t a t o e s   b u r g e r  displaystyle mathrm onions,potatoes Rightarrow mathrm burger  found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions. Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs. Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981 a Prolog program that inductively inferred logic programs from positive and negative examples. The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. Models A machine learning model is a type of mathematical model that, once trained on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the models internal parameters to minimize errors in its predictions. By extension, the term model can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned. Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection. Artificial neural networks Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems learn to perform tasks by considering examples, generally without being programmed with any task-specific rules. An ANN is a model based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a signal, from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called edges. Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition. Decision trees Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the items target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making. Random forest regression Random forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. To build decision trees, RFR uses bootstrapped sampling, for instance each decision tree is trained on random data of from training set. This random selection of RFR for training enables model to reduce bias predictions and achieve accuracy. RFR generates independent decision trees, and it can work on single output data as well multiple regressor task. This makes RFR compatible to be used in various application. Support-vector machines Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Regression analysis Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space. Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images, which are inherently multi-dimensional. Bayesian networks A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. Gaussian processes A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations. Given a set of observed points, or input output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point. Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization. Genetic algorithms A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms. Belief functions The theory of belief functions, also referred to as evidence theory or Dempster Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g., Dempsters rule of combination), just like how in a pmf-based Bayesian approach would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learners decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving. However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches. Training models Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams. Federated learning Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users mobile phones without having to send individual searches back to Google. Applications There are many applications for machine learning, including In 2006, the media-services provider Netflix held the first Netflix Prize competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10. A joint team made up of researchers from ATT Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for 1 million. Shortly after the prize was awarded, Netflix realized that viewers ratings were not the best indicators of their viewing patterns (everything is a recommendation) and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80 of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning was recently applied to predict the pro-environmental behavior of travelers. Recently, machine learning technology was also applied to optimize smartphones performance and thermal behavior based on the users interaction with the phone. When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS. Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes. Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes. Other applications have been focusing on pre evacuation decisions in building fires. Limitations Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems. The black box theory poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data. The House of Lords Select Committee, which claimed that such an intelligence system that could have a substantial impact on an individuals life would not be considered acceptable unless it provided a full and satisfactory explanation for the decisions it makes. In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested. Microsofts Bing Chat chatbot has been reported to produce hostile and offensive response against its users. Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves. Explainability Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI. It contrasts with the black box concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision. By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation. Overfitting Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is. Other limitations and vulnerabilities Learners can also disappoint by learning the wrong lesson. A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses. A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in adversarial images that the system misclassifies. Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel. Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning. Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories spam and well-visible not spam of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access. Model assessments Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy. In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model. Ethics Bias Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society. Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, in 1988, the UKs Commission for Racial Equality found that St. Georges Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Another example includes predictive policing company Geoliticas predictive algorithm that resulted in disproportionately high levels of over-policing in low-income and minority communities after being trained with historical crime data. While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learnings vulnerability to biases. In fact, according to research carried out by the Computing Research Association (CRA) in 2021, female faculty merely make up 16.1 of all faculty members who focus on AI among several universities around the world. Furthermore, among the group of new U.S. resident AI PhD graduates, 45 identified as white, 22.4 as Asian, 3.2 as Hispanic, and 2.4 as African American, which further demonstrates a lack of diversity in the field of AI. Language models learned from data have been shown to contain human-like biases. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases. In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithms insight into the recidivism rates among prisoners falsely flagged black defendants high risk twice as often as white defendants. In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognize gorillas. Similar issues with recognizing non-white people have been found in many other systems. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains. Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that theres nothing artificial about AI. Its inspired by people, its created by people, and most importantly it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility. Financial incentives There are concerns among health care professionals that these systems might not be designed in the publics interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithms proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated. Hardware Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units. By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months. Neuromorphic computing Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialized hardware architectures. physical neural networks A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term physical neural network highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses. Embedded machine learning Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration, approximate computing, and model optimization. Common optimization techniques include pruning, quantization, knowledge distillation, low-rank factorization, network architecture search, and parameter sharing. Software Software suites containing a variety of machine learning algorithms include the following Free and open-source software Proprietary software with free and open-source editions KNIME RapidMiner Proprietary software Journals Journal of Machine Learning Research Machine Learning Nature Machine Intelligence Neural Computation IEEE Transactions on Pattern Analysis and Machine Intelligence Conferences AAAI Conference on Artificial Intelligence Association for Computational Linguistics (ACL) European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB) International Conference on Machine Learning (ICML) International Conference on Learning Representations (ICLR) International Conference on Intelligent Robots and Systems (IROS) Conference on Knowledge Discovery and Data Mining (KDD) Conference on Neural Information Processing Systems (NeurIPS) See also Automated machine learning Process of automating the application of machine learning Big data Extremely large or complex datasets Deep learning branch of ML concerned with artificial neural networks Differentiable programming Programming paradigm List of datasets for machine-learning research M-theory (learning framework) Machine unlearning Solomonoffs theory of inductive inference A mathematical theory References Sources Domingos, Pedro (September 22, 2015). The Master Algorithm How the Quest for the Ultimate Learning Machine Will Remake Our World. Basic Books. ISBN 978-0465065707. Nilsson, Nils (1998). Artificial Intelligence A New Synthesis. Morgan Kaufmann. ISBN 978-1-55860-467-4. Archived from the original on 26 July 2020. Retrieved 18 November 2019. Poole, David Mackworth, Alan Goebel, Randy (1998). Computational Intelligence A Logical Approach. New York Oxford University Press. ISBN 978-0-19-510270-3. Archived from the original on 26 July 2020. Retrieved 22 August 2020. Russell, Stuart J. Norvig, Peter (2003), Artificial Intelligence A Modern Approach (2nd ed.), Upper Saddle River, New Jersey Prentice Hall, ISBN 0-13-790395-2. Further reading External links International Machine Learning Society mloss is an academic database of open-source machine learning software. Title Outline of machine learning URL https//en.wikipedia.org/wiki/Outline_of_machine_learning Content The following outline is provided as an overview of, and topical guide to, machine learning Machine learning (ML) is a subfield of artificial intelligence within computer science that evolved from the study of pattern recognition and computational learning theory. In 1959, Arthur Samuel defined machine learning as a field of study that gives computers the ability to learn without being explicitly programmed. ML involves the study and construction of algorithms that can learn from and make predictions on data. These algorithms operate by building a model from a training set of example observations to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions. How can machine learning be categorized? An academic discipline A branch of science An applied science A subfield of computer science A branch of artificial intelligence A subfield of soft computing Application of statistics Paradigms of machine learning Supervised learning, where the model is trained on labeled data Unsupervised learning, where the model tries to identify patterns in unlabeled data Reinforcement learning, where the model learns to make decisions by receiving rewards or penalties. Applications of machine learning Applications of machine learning Bioinformatics Biomedical informatics Computer vision Customer relationship management Data mining Earth sciences Email filtering Inverted pendulum (balance and equilibrium system) Natural language processing Named Entity Recognition Automatic summarization Automatic taxonomy construction Dialog system Grammar checker Language recognition Handwriting recognition Optical character recognition Speech recognition Text to Speech Synthesis Speech Emotion Recognition Machine translation Question answering Speech synthesis Text mining Term frequency inverse document frequency Text simplification Pattern recognition Facial recognition system Handwriting recognition Image recognition Optical character recognition Speech recognition Recommendation system Collaborative filtering Content-based filtering Hybrid recommender systems Search engine Search engine optimization Social engineering Machine learning hardware Graphics processing unit Tensor processing unit Vision processing unit Machine learning tools Comparison of deep learning software Machine learning frameworks Proprietary machine learning frameworks Amazon Machine Learning Microsoft Azure Machine Learning Studio DistBelief (replaced by TensorFlow) Open source machine learning frameworks Apache Singa Apache MXNet Caffe PyTorch mlpack TensorFlow Torch CNTK Accord.Net Jax MLJ.jl A machine learning framework for Julia Machine learning libraries Deeplearning4j Theano scikit-learn Keras Machine learning algorithms Almeida Pineda recurrent backpropagation ALOPEX Backpropagation Bootstrap aggregating algorithm Constructing skill trees Dehaene Changeux model Diffusion map Dominance-based rough set approach Dynamic time warping Error-driven learning Evolutionary multimodal optimization Expectation maximization algorithm FastICA Forward backward algorithm GeneRec Genetic Algorithm for Rule Set Production Growing self-organizing map Hyper basis function network IDistance k-nearest neighbors algorithm Kernel methods for vector output Kernel principal component analysis Leabra Linde Buzo Gray algorithm Local outlier factor Logic learning machine LogitBoost Manifold alignment Markov chain Monte Carlo (MCMC) Minimum redundancy feature selection Mixture of experts Multiple kernel learning Non-negative matrix factorization Online machine learning Out-of-bag error Prefrontal cortex basal ganglia working memory PVLV Q-learning Quadratic unconstrained binary optimization Query-level feature Quickprop Radial basis function network Randomized weighted majority algorithm Reinforcement learning Repeated incremental pruning to produce error reduction (RIPPER) Rprop Rule-based machine learning Skill chaining Sparse PCA State action reward state action Stochastic gradient descent Structured kNN T-distributed stochastic neighbor embedding Temporal difference learning Wake-sleep algorithm Weighted majority algorithm (machine learning) Machine learning methods Instance-based algorithm K-nearest neighbors algorithm (KNN) Learning vector quantization (LVQ) Self-organizing map (SOM) Regression analysis Logistic regression Ordinary least squares regression (OLSR) Linear regression Stepwise regression Multivariate adaptive regression splines (MARS) Regularization algorithm Ridge regression Least Absolute Shrinkage and Selection Operator (LASSO) Elastic net Least-angle regression (LARS) Classifiers Probabilistic classifier Naive Bayes classifier Binary classifier Linear classifier Hierarchical classifier Dimensionality reduction Dimensionality reduction Canonical correlation analysis (CCA) Factor analysis Feature extraction Feature selection Independent component analysis (ICA) Linear discriminant analysis (LDA) Multidimensional scaling (MDS) Non-negative matrix factorization (NMF) Partial least squares regression (PLSR) Principal component analysis (PCA) Principal component regression (PCR) Projection pursuit Sammon mapping t-distributed stochastic neighbor embedding (t-SNE) Ensemble learning Ensemble learning AdaBoost Boosting Bootstrap aggregating (also bagging or bootstrapping) Ensemble averaging Gradient boosted decision tree (GBDT) Gradient boosting Random Forest Stacked Generalization Meta-learning Meta-learning Inductive bias Metadata Reinforcement learning Reinforcement learning Q-learning State action reward state action (SARSA) Temporal difference learning (TD) Learning Automata Supervised learning Supervised learning Averaged one-dependence estimators (AODE) Artificial neural network Case-based reasoning Gaussian process regression Gene expression programming Group method of data handling (GMDH) Inductive logic programming Instance-based learning Lazy learning Learning Automata Learning Vector Quantization Logistic Model Tree Minimum message length (decision trees, decision graphs, etc.) Nearest Neighbor Algorithm Analogical modeling Probably approximately correct learning (PAC) learning Ripple down rules, a knowledge acquisition methodology Symbolic machine learning algorithms Support vector machines Random Forests Ensembles of classifiers Bootstrap aggregating (bagging) Boosting (meta-algorithm) Ordinal classification Conditional Random Field ANOVA Quadratic classifiers k-nearest neighbor Boosting SPRINT Bayesian networks Naive Bayes Hidden Markov models Hierarchical hidden Markov model Bayesian Bayesian statistics Bayesian knowledge base Naive Bayes Gaussian Naive Bayes Multinomial Naive Bayes Averaged One-Dependence Estimators (AODE) Bayesian Belief Network (BBN) Bayesian Network (BN) Decision tree algorithms Decision tree algorithm Decision tree Classification and regression tree (CART) Iterative Dichotomiser 3 () .5 algorithm .0 algorithm Chi-squared Automatic Interaction Detection (CHAID) Decision stump Conditional decision tree algorithm Random forest SLIQ Linear classifier Linear classifier Fishers linear discriminant Linear regression Logistic regression Multinomial logistic regression Naive Bayes classifier Perceptron Support vector machine Unsupervised learning Unsupervised learning Expectation-maximization algorithm Vector Quantization Generative topographic map Information bottleneck method Association rule learning algorithms Apriori algorithm Eclat algorithm Artificial neural networks Artificial neural network Feedforward neural network Extreme learning machine Convolutional neural network Recurrent neural network Long short-term memory (LSTM) Logic learning machine Self-organizing map Association rule learning Association rule learning Apriori algorithm Eclat algorithm FP-growth algorithm Hierarchical clustering Hierarchical clustering Single-linkage clustering Conceptual clustering Cluster analysis Cluster analysis BIRCH DBSCAN Expectation maximization (EM) Fuzzy clustering Hierarchical clustering k-means clustering k-medians Mean-shift OPTICS algorithm Anomaly detection Anomaly detection k-nearest neighbors algorithm (k-NN) Local outlier factor Semi-supervised learning Semi-supervised learning Active learning Generative models Low-density separation Graph-based methods Co-training Transduction Deep learning Deep learning Deep belief networks Deep Boltzmann machines Deep Convolutional neural networks Deep Recurrent neural networks Hierarchical temporal memory Generative Adversarial Network Style transfer Transformer Stacked Auto-Encoders Other machine learning methods and problems Anomaly detection Association rules Bias-variance dilemma Classification Multi-label classification Clustering Data Pre-processing Empirical risk minimization Feature engineering Feature learning Learning to rank Occam learning Online machine learning PAC learning Regression Reinforcement Learning Semi-supervised learning Statistical learning Structured prediction Graphical models Bayesian network Conditional random field (CRF) Hidden Markov model (HMM) Unsupervised learning VC theory Machine learning research List of artificial intelligence projects List of datasets for machine learning research History of machine learning History of machine learning Timeline of machine learning Machine learning projects Machine learning projects DeepMind Google Brain OpenAI Meta AI Hugging Face Machine learning organizations Machine learning conferences and workshops Artificial Intelligence and Security (AISec) (co-located workshop with CCS) Conference on Neural Information Processing Systems (NIPS) ECML PKDD International Conference on Machine Learning (ICML) ML4ALL (Machine Learning For All) Machine learning publications Books on machine learning Mathematics for Machine Learning Hands-On Machine Learning Scikit-Learn, Keras, and TensorFlow The Hundred-Page Machine Learning Book Machine learning journals Machine Learning Journal of Machine Learning Research (JMLR) Neural Computation Persons influential in machine learning Alberto Broggi Andrei Knyazev Andrew McCallum Andrew Ng Anuraag Jain Armin B. Cremers Ayanna Howard Barney Pell Ben Goertzel Ben Taskar Bernhard Sch lkopf Brian D. Ripley Christopher G. Atkeson Corinna Cortes Demis Hassabis Douglas Lenat Eric Xing Ernst Dickmanns Geoffrey Hinton Hans-Peter Kriegel Hartmut Neven Heikki Mannila Ian Goodfellow Jacek M. Zurada Jaime Carbonell Jeremy Slovak Jerome H. Friedman John D. Lafferty John Platt Julie Beth Lovins J rgen Schmidhuber Karl Steinbuch Katia Sycara Leo Breiman Lise Getoor Luca Maria Gambardella L on Bottou Marcus Hutter Mehryar Mohri Michael Collins Michael I. Jordan Michael L. Littman Nando de Freitas Ofer Dekel Oren Etzioni Pedro Domingos Peter Flach Pierre Baldi Pushmeet Kohli Ray Kurzweil Rayid Ghani Ross Quinlan Salvatore J. Stolfo Sebastian Thrun Selmer Bringsjord Sepp Hochreiter Shane Legg Stephen Muggleton Steve Omohundro Tom M. Mitchell Trevor Hastie Vasant Honavar Vladimir Vapnik Yann LeCun Yasuo Matsuyama Yoshua Bengio Zoubin Ghahramani See also Outline of artificial intelligence Outline of computer vision Outline of robotics Accuracy paradox Action model learning Activation function Activity recognition ADALINE Adaptive neuro fuzzy inference system Adaptive resonance theory Additive smoothing Adjusted mutual information AIVA AIXI AlchemyAPI AlexNet Algorithm selection Algorithmic inference Algorithmic learning theory AlphaGo AlphaGo Zero Alternating decision tree Apprenticeship learning Causal Markov condition Competitive learning Concept learning Decision tree learning Differentiable programming Distribution learning theory Eager learning End-to-end reinforcement learning Error tolerance (PAC learning) Explanation-based learning Feature GloVe Hyperparameter Inferential theory of learning Learning automata Learning classifier system Learning rule Learning with errors M-Theory (learning framework) Machine learning control Machine learning in bioinformatics Margin Markov chain geostatistics Markov chain Monte Carlo (MCMC) Markov information source Markov logic network Markov model Markov random field Markovian discrimination Maximum-entropy Markov model Multi-armed bandit Multi-task learning Multilinear subspace learning Multimodal learning Multiple instance learning Multiple-instance learning Never-Ending Language Learning Offline learning Parity learning Population-based incremental learning Predictive learning Preference learning Proactive learning Proximal gradient methods for learning Semantic analysis Similarity learning Sparse dictionary learning Stability (learning theory) Statistical learning theory Statistical relational learning Tanagra Transfer learning Variable-order Markov model Version space learning Waffles Weka Loss function Loss functions for classification Mean squared error (MSE) Mean squared prediction error (MSPE) Taguchi loss function Low-energy adaptive clustering hierarchy Other Anne OTate Ant colony optimization algorithms Anthony Levandowski Anti-unification (computer science) Apache Flume Apache Giraph Apache Mahout Apache SINGA Apache Spark Apache SystemML Aphelion (software) Arabic Speech Corpus Archetypal analysis Arthur Zimek Artificial ants Artificial bee colony algorithm Artificial development Artificial immune system Astrostatistics Averaged one-dependence estimators Bag-of-words model Balanced clustering Ball tree Base rate Bat algorithm Baum Welch algorithm Bayesian hierarchical modeling Bayesian interpretation of kernel regularization Bayesian optimization Bayesian structural time series Bees algorithm Behavioral clustering Bernoulli scheme Bias variance tradeoff Biclustering BigML Binary classification Bing Predicts Bio-inspired computing Biogeography-based optimization Biplot Bondys theorem Bongard problem Bradley Terry model BrownBoost Brown clustering Burst error CBCL (MIT) CIML community portal CMA-ES CURE data clustering algorithm Cache language model Calibration (statistics) Canonical correspondence analysis Canopy clustering algorithm Cascading classifiers Category utility CellCognition Cellular evolutionary algorithm Chi-square automatic interaction detection Chromosome (genetic algorithm) Classifier chains Cleverbot Clonal selection algorithm Cluster-weighted modeling Clustering high-dimensional data Clustering illusion CoBoosting Cobweb (clustering) Cognitive computer Cognitive robotics Collostructional analysis Common-method variance Complete-linkage clustering Computer-automated design Concept class Concept drift Conference on Artificial General Intelligence Conference on Knowledge Discovery and Data Mining Confirmatory factor analysis Confusion matrix Congruence coefficient Connect (computer system) Consensus clustering Constrained clustering Constrained conditional model Constructive cooperative coevolution Correlation clustering Correspondence analysis Cortica Coupled pattern learner Cross-entropy method Cross-validation (statistics) Crossover (genetic algorithm) Cuckoo search Cultural algorithm Cultural consensus theory Curse of dimensionality DADiSP DARPA LAGR Program Darkforest Dartmouth workshop DarwinTunes Data Mining Extensions Data exploration Data pre-processing Data stream clustering Dataiku Davies Bouldin index Decision boundary Decision list Decision tree model Deductive classifier DeepArt DeepDream Deep Web Technologies Defining length Dendrogram Dependability state model Detailed balance Determining the number of clusters in a data set Detrended correspondence analysis Developmental robotics Diffbot Differential evolution Discrete phase-type distribution Discriminative model Dissociated press Distributed R Dlib Document classification Documenting Hate Domain adaptation Doubly stochastic model Dual-phase evolution Dunn index Dynamic Bayesian network Dynamic Markov compression Dynamic topic model Dynamic unobserved effects model EDLUT ELKI Edge recombination operator Effective fitness Elastic map Elastic matching Elbow method (clustering) Emergent (software) Encog Entropy rate Erkki Oja Eurisko European Conference on Artificial Intelligence Evaluation of binary classifiers Evolution strategy Evolution window Evolutionary Algorithm for Landmark Detection Evolutionary algorithm Evolutionary art Evolutionary music Evolutionary programming Evolvability (computer science) Evolved antenna Evolver (software) Evolving classification function Expectation propagation Exploratory factor analysis score FLAME clustering Factor analysis of mixed data Factor graph Factor regression model Factored language model Farthest-first traversal Fast-and-frugal trees Feature Selection Toolbox Feature hashing Feature scaling Feature vector Firefly algorithm First-difference estimator First-order inductive learner Fish School Search Fisher kernel Fitness approximation Fitness function Fitness proportionate selection Fluentd Foldinghome Formal concept analysis Forward algorithm Fowlkes Mallows index Frederick Jelinek Frrole Functional principal component analysis GATTO GLIMMER Gary Bryce Fogel Gaussian adaptation Gaussian process Gaussian process emulator Gene prediction General Architecture for Text Engineering Generalization error Generalized canonical correlation Generalized filtering Generalized iterative scaling Generalized multidimensional scaling Generative adversarial network Generative model Genetic algorithm Genetic algorithm scheduling Genetic algorithms in economics Genetic fuzzy systems Genetic memory (computer science) Genetic operator Genetic programming Genetic representation Geographical cluster Gesture Description Language Geworkbench Glossary of artificial intelligence Glottochronology Golem (ILP) Google matrix Grafting (decision trees) Gramian matrix Grammatical evolution Granular computing GraphLab Graph kernel Gremlin (programming language) Growth function HUMANT (HUManoid ANT) algorithm Hammersley Clifford theorem Harmony search Hebbian theory Hidden Markov random field Hidden semi-Markov model Hierarchical hidden Markov model Higher-order factor analysis Highway network Hinge loss Hollands schema theorem Hopkins statistic Hoshen Kopelman algorithm Huber loss Ian Goodfellow Ilastik Ilya Sutskever Immunocomputing Imperialist competitive algorithm Inauthentic text Incremental decision tree Induction of regular languages Inductive bias Inductive probability Inductive programming Influence diagram Information Harvesting Information gain in decision trees Information gain ratio Inheritance (genetic algorithm) Instance selection Intel RealSense Interacting particle system Interactive machine translation International Joint Conference on Artificial Intelligence International Meeting on Computational Intelligence Methods for Bioinformatics and Biostatistics International Semantic Web Conference Iris flower data set Island algorithm Isotropic position Item response theory Iterative Viterbi decoding JOONE Jabberwacky Jaccard index Jackknife variance estimates for random forest Java Grammatical Evolution Joseph Nechvatal Jubatus Julia (programming language) Junction tree algorithm k-SVD k-means k-medians clustering k-medoids KNIME KXEN Inc. k q-flats Kaggle Kalman filter Katzs back-off model Kernel adaptive filter Kernel density estimation Kernel eigenvoice Kernel embedding of distributions Kernel method Kernel perceptron Kernel random forest Kinect Klaus-Robert M ller Kneser Ney smoothing Knowledge Vault Knowledge integration LIBSVM LPBoost Labeled data LanguageWare Language identification in the limit Language model Large margin nearest neighbor Latent Dirichlet allocation Latent class model Latent semantic analysis Latent variable Latent variable model Lattice Miner Layered hidden Markov model Learnable function class Least squares support vector machine Leslie P. Kaelbling Linear genetic programming Linear predictor function Linear separability Lingyun Gu Linkurious Lior Ron (business executive) List of genetic algorithm applications List of metaphor-based metaheuristics List of text mining software Local case-control sampling Local independence Local tangent space alignment Locality-sensitive hashing Log-linear model Logistic model tree Low-rank approximation Low-rank matrix approximations MATLAB MIMIC (immunology) MXNet Mallet (software project) Manifold regularization Margin-infused relaxed algorithm Margin classifier Mark V. Shaney Massive Online Analysis Matrix regularization Matthews correlation coefficient Mean shift Mean squared error Mean squared prediction error Measurement invariance Medoid MeeMix Melomics Memetic algorithm Meta-optimization Mexican International Conference on Artificial Intelligence Michael Kearns (computer scientist) MinHash Mixture model Mlpy Models of DNA evolution Moral graph Mountain car problem Movidius Multi-armed bandit Multi-label classification Multi expression programming Multiclass classification Multidimensional analysis Multifactor dimensionality reduction Multilinear principal component analysis Multiple correspondence analysis Multiple discriminant analysis Multiple factor analysis Multiple sequence alignment Multiplicative weight update method Multispectral pattern recognition Mutation (genetic algorithm) MysteryVibe N-gram NOMINATE (scaling method) Native-language identification Natural Language Toolkit Natural evolution strategy Nearest-neighbor chain algorithm Nearest centroid classifier Nearest neighbor search Neighbor joining Nest Labs NetMiner NetOwl Neural Designer Neural Engineering Object Neural modeling fields Neural network software NeuroSolutions Neuroevolution Neuroph Niki.ai Noisy channel model Noisy text analytics Nonlinear dimensionality reduction Novelty detection Nuisance variable One-class classification Onnx OpenNLP Optimal discriminant analysis Oracle Data Mining Orange (software) Ordination (statistics) Overfitting PROGOL PSIPRED Pachinko allocation PageRank Parallel metaheuristic Parity benchmark Part-of-speech tagging Particle swarm optimization Path dependence Pattern language (formal languages) Peltarion Synapse Perplexity Persian Speech Corpus Picas (app) Pietro Perona Pipeline Pilot Piranha (software) Pitman Yor process Plate notation Polynomial kernel Pop music automation Population process Portable Format for Analytics Predictive Model Markup Language Predictive state representation Preference regression Premature convergence Principal geodesic analysis Prior knowledge for pattern recognition Prisma (app) Probabilistic Action Cores Probabilistic context-free grammar Probabilistic latent semantic analysis Probabilistic soft logic Probability matching Probit model Product of experts Programming with Big Data in R Proper generalized decomposition Pruning (decision trees) Pushpak Bhattacharyya Q methodology Qloo Quality control and genetic algorithms Quantum Artificial Intelligence Lab Queueing theory Quick, Draw! R (programming language) Rada Mihalcea Rademacher complexity Radial basis function kernel Rand index Random indexing Random projection Random subspace method Ranking SVM RapidMiner Rattle GUI Raymond Cattell Reasoning system Regularization perspectives on support vector machines Relational data mining Relationship square Relevance vector machine Relief (feature selection) Renjin Repertory grid Representer theorem Reward-based selection Richard Zemel Right to explanation RoboEarth Robust principal component analysis RuleML Symposium Rule induction Rules extraction system family SAS (software) SNNS SPSS Modeler SUBCLU Sample complexity Sample exclusion dimension Santa Fe Trail problem Savi Technology Schema (genetic algorithms) Search-based software engineering Selection (genetic algorithm) Self-Service Semantic Suite Semantic folding Semantic mapping (statistics) Semidefinite embedding Sense Networks Sensorium Project Sequence labeling Sequential minimal optimization Shattered set Shogun (toolbox) Silhouette (clustering) SimHash SimRank Similarity measure Simple matching coefficient Simultaneous localization and mapping Sinkov statistic Sliced inverse regression Snakes and Ladders Soft independent modelling of class analogies Soft output Viterbi algorithm Solomonoffs theory of inductive inference SolveIT Software Spectral clustering Spike-and-slab variable selection Statistical machine translation Statistical parsing Statistical semantics Stefano Soatto Stephen Wolfram Stochastic block model Stochastic cellular automaton Stochastic diffusion search Stochastic grammar Stochastic matrix Stochastic universal sampling Stress majorization String kernel Structural equation modeling Structural risk minimization Structured sparsity regularization Structured support vector machine Subclass reachability Sufficient dimension reduction Sukhotins algorithm Sum of absolute differences Sum of absolute transformed differences Swarm intelligence Switching Kalman filter Symbolic regression Synchronous context-free grammar Syntactic pattern recognition TD-Gammon TIMIT Teaching dimension Teuvo Kohonen Textual case-based reasoning Theory of conjoint measurement Thomas G. Dietterich Thurstonian model Topic model Tournament selection Training, test, and validation sets Transiogram Trax Image Recognition Trigram tagger Truncation selection Tucker decomposition UIMA UPGMA Ugly duckling theorem Uncertain data Uniform convergence in probability Unique negative dimension Universal portfolio algorithm User behavior analytics VC dimension VIGRA Validation set Vapnik Chervonenkis theory Variable-order Bayesian network Variable kernel density estimation Variable rules analysis Variational message passing Varimax rotation Vector quantization Vicarious (company) Viterbi algorithm Vowpal Wabbit WACA clustering algorithm WPGMA Wards method Weasel program Whitening transformation Winnow (algorithm) Win stay, lose switch Witness set Wolfram Language Wolfram Mathematica Writer invariant Xgboost Yooreeka Zeroth (software) Further reading Trevor Hastie, Robert Tibshirani and Jerome H. Friedman (2001). The Elements of Statistical Learning, Springer. ISBN 0-387-95284-5. Pedro Domingos (September 2015), The Master Algorithm, Basic Books, ISBN 978-0-465-06570-7 Mehryar Mohri, Afshin Rostamizadeh, Ameet Talwalkar (2012). Foundations of Machine Learning, The MIT Press. ISBN 978-0-262-01825-8. Ian H. Witten and Eibe Frank (2011). Data Mining Practical machine learning tools and techniques Morgan Kaufmann, 664pp., ISBN 978-0-12-374856-0. David J. C. MacKay. Information Theory, Inference, and Learning Algorithms Cambridge Cambridge University Press, 2003. ISBN 0-521-64298-1 Richard O. Duda, Peter E. Hart, David G. Stork (2001) Pattern classification (2nd edition), Wiley, New York, ISBN 0-471-05669-3. Christopher Bishop (1995). Neural Networks for Pattern Recognition, Oxford University Press. ISBN 0-19-853864-2. Vladimir Vapnik (1998). Statistical Learning Theory. Wiley-Interscience, ISBN 0-471-03003-1. Ray Solomonoff, An Inductive Inference Machine, IRE Convention Record, Section on Information Theory, Part 2, pp., 56 62, 1957. Ray Solomonoff, An Inductive Inference Machine A privately circulated report from the 1956 Dartmouth Summer Research Conference on AI. References External links Data Science Data to Insights from MIT (machine learning) Popular online course by Andrew Ng, at Coursera. It uses GNU Octave. The course is a free version of Stanford Universitys actual course taught by Ng, see.stanford.edu/Course/ available for free. mloss is an academic database of open-source machine learning software. Title 80 Million Tiny Images URL https//en.wikipedia.org/wiki/80_Million_Tiny_Images Content 80 Million Tiny Images is a dataset intended for training machine learning systems constructed by Antonio Torralba, Rob Fergus, and William T. Freeman in a collaboration between MIT and New York University. It was published in 2008. The dataset has size 760 GB. It contains 79,302,017 32 32 pixel color images, scaled down from images scraped from the World Wide Web over 8 months. The images are classified into 75,062 classes. Each class is a non-abstract noun in WordNet. Images may appear in more than one class. The dataset was motivated by non-parametric models of neural activations in the visual cortex upon seeing images. The CIFAR-10 dataset uses a subset of the images in this dataset, but with independently generated labels, as the original labels were not reliable. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Construction It was first reported in a technical report in April 2007, during the middle of the construction process, when there were only 73 million images. The full dataset was published in 2008. They began with all 75,846 nonabstract nouns in WordNet, and then for each of these nouns, they scraped 7 Image search engines Altavista, Ask.com, Flickr, Cydral, Google, Picsearch and Webshots. After 8 months of scraping, they obtained 97,245,098 images. Since they didnt have enough storage, they downsized the images to 32 32 as they were scraped. After gathering, they removed images with zero variance and intra-word duplicate images, resulting in the final dataset. Out of the 75,846 nouns, only 75,062 classes had any results, so the other nouns did not appear in the final dataset. The number of images per noun follows a Zipf-like distribution, with 1056 images per noun on average. To prevent a few nouns taking up too many images, they put an upper bound of at most 3000 images per noun. Retirement The 80 Million Tiny Images dataset was retired from use by its creators in 2020, after a paper by researchers Abeba Birhane and Vinay Prabhu found that some of the labeling of several publicly available image datasets, including 80 Million Tiny Images, contained racist and misogynistic slurs which were causing models trained on them to exhibit racial and sexual bias. The dataset also contained offensive images. Following the release of the paper, the datasets creators removed the dataset from distribution, and requested that other researchers not use it for further research and to delete their copies of the dataset. See also List of datasets in computer vision and image processing References Title A Logical Calculus of the Ideas Immanent in Nervous Activity URL https//en.wikipedia.org/wiki/A_Logical_Calculus_of_the_Ideas_Immanent_in_Nervous_Activity Content A Logical Calculus of the Ideas Immanent to Nervous Activity is a 1943 article written by Warren McCulloch and Walter Pitts. The paper, published in the journal The Bulletin of Mathematical Biophysics, proposed a mathematical model of the nervous system as a network of simple logical elements, later known as artificial neurons, or McCulloch-Pitts neurons. These neurons receive inputs, perform a weighted sum, and fire an output signal based on a threshold function. By connecting these units in various configurations, McCulloch and Pitts demonstrated that their model could perform all logical functions. It is a seminal work in computational neuroscience, computer science, and artificial intelligence. It was a foundational result in automata theory. John von Neumann cited it as a significant result. Mathematics The artificial neuron used in the original paper is slightly different from the modern version. They considered neural networks that operate in discrete steps of time   . The neural network contains a number of neurons. Let the state of a neuron i displaystyle i at time t displaystyle t be N i ( t ) displaystyle N_i(t) . The state of a neuron can either be 0 or 1, standing for not firing and firing. Each neuron also has a firing threshold displaystyle theta  , such that it fires if the total input exceeds the threshold. Each neuron can connect to any other neuron (including itself) with positive synapses (excitatory) or negative synapses (inhibitory). That is, each neuron can connect to another neuron with a weight w displaystyle w taking an integer value. A peripheral afferent is a neuron with no incoming synapses. We can regard each neural network as a directed graph, with the nodes being the neurons, and the directed edges being the synapses. A neural network has a circle or a circuit if there exists a directed circle in the graph. Let w i j ( t ) displaystyle w_ij(t) be the connection weight from neuron j displaystyle j to neuron i displaystyle i at time t displaystyle t , then its next state is N i ( t  1 )  H ( ). Symbolic logic The paper used, as a logical language for describing neural networks, Language II from The Logical Syntax of Language by Rudolf Carnap with some notations taken from Principia Mathematica by Alfred North Whitehead and Bertrand Russell. Language II covers substantial parts of classical mathematics, including real analysis and portions of set theory. To describe a neural network with peripheral afferents N 1 , N 2 , , N p displaystyle N_1,N_2,dots ,N_p and non-peripheral afferents N p  1 , N p  2 , , N n displaystyle N_p1,N_p2,dots ,N_n they considered logical predicate of form P r ( N 1 , N 2 , , N p , t ) displaystyle Pr(N_1,N_2,dots ,N_p,t) where P r displaystyle Pr is a first-order logic predicate function (a function that outputs a boolean), N 1 , , N p displaystyle N_1,dots ,N_p are predicates that take t displaystyle t as an argument, and t displaystyle t is the only free variable in the predicate. Intuitively speaking, N 1 , , N p displaystyle N_1,dots ,N_p specifies the binary input patterns going into the neural network over all time, and P r ( N 1 , N 2 , , N n , t ) displaystyle Pr(N_1,N_2,dots ,N_n,t) is a function that takes some binary input patterns, and constructs an output binary pattern P r ( N 1 , N 2 , , N n , 0 ) , P r ( N 1 , N 2 , , N n , 1 ) , displaystyle Pr(N_1,N_2,dots ,N_n,0),Pr(N_1,N_2,dots ,N_n,1),dots  . A logical sentence P r ( N 1 , N 2 , , N n , t ) displaystyle Pr(N_1,N_2,dots ,N_n,t) is realized by a neural network iff there exists a time-delay T 0 displaystyle Tgeq 0 , a neuron i displaystyle i in the network, and an initial state for the non-peripheral neurons N p  1 ( 0 ) , , N n ( 0 ) displaystyle N_p1(0),dots ,N_n(0) , such that for any time t displaystyle t , the truth-value of the logical sentence is equal to the state of the neuron i displaystyle i at time t  T displaystyle tT . That is, . They considered three forms of inhibition relative inhibition, absolute inhibition, and extinction. The definition above is relative inhibition. By absolute inhibition they meant that if any negative synapse fires, then the neuron will not fire. By extinction they meant that if at time t displaystyle t , any inhibitory synapse fires on a neuron i displaystyle i , then i ( t  j )  i ( 0 )  b j displaystyle theta _i(tj)theta _i(0)b_j for  . It is required that b  . Theorem 4 and 5 state that these are equivalent. They considered three forms of excitation spatial summation, temporal summation, and facilitation. The definition above is spatial summation (which they pictured as having multiple synapses placed close together, so that the effect of their firing sums up). By temporal summation they meant that the total incoming signal  . By facilitation they meant the same as extinction, except that b j 0 displaystyle b_jleq 0 . Theorem 6 states that these are equivalent. They considered neural networks that do not change, and those that change by Hebbian learning. That is, they assume that at . If at any t displaystyle t , both N i ( t )  1 , N j ( t )  1 displaystyle N_i(t)1,N_j(t)1 , then any latent excitatory synapse between i , j displaystyle i,j becomes active. Theorem 7 states that these are equivalent. Logical expressivity They considered temporal propositional expressions (TPE), which are propositional formulas with one free variable t displaystyle t . For example, N 1 ( t ) N 2 ( t ) N 3 ( t ) displaystyle N_1(t)vee N_2(t)wedge neg N_3(t) is such an expression. Theorem 1 and 2 together showed that neural nets without circles are equivalent to TPE. For neural nets with loops, they noted that realizable P r displaystyle Pr may involve reference to past events of an indefinite degree of remoteness. These then encodes for sentences like There was some x such that x was a  or ( x ) ( x ) displaystyle (exists x)(psi x) . Theorems 8 to 10 showed that neural nets with loops can encode all first-order logic with equality and conversely, any looped neural networks is equivalent to a sentence in first-order logic with equality, thus showing that they are equivalent in logical expressiveness. As a remark, they noted that a neural network, if furnished with a tape, scanners, and write-heads, is equivalent to a Turing machine, and conversely, every Turing machine is equivalent to some such neural network. Thus, these neural networks are equivalent to Turing computability, Churchs lambda-definability, and Kleenes primitive recursiveness. Context Previous work The paper built upon several previous strands of work. In the symbolic logic side, it built on the previous work by Carnap, Whitehead, and Russell. This was contributed by Walter Pitts, who had a strong proficiency with symbolic logic. Pitts provided mathematical and logical rigor to McCulloch s vague ideas on psychons (atoms of psychological events) and circular causality. In the neuroscience side, it built on previous work by the mathematical biology research group centered around Nicolas Rashevsky, of which McCulloch was a member. The paper was published in the Bulletin of Mathematical Biophysics, which was founded by Rashevsky in 1939. During the late 1930s, Rashevskys research group was producing papers that had difficulty publishing in other journals at the time, so Rashevsky decided to found a new journal exclusively devoted to mathematical biophysics. Also in the Rashevskys group was Alston Scott Householder, who in 1941 published an abstract model of the steady-state activity of biological neural networks. The model, in modern language, is an artificial neural network with ReLU activation function. In a series of papers, Householder calculated the stable states of very simple networks a chain, a circle, and a bouquet. Walter Pitts first two papers formulated a mathematical theory of learning and conditioning. The next three were mathematical developments of Householder s model. In 1938, at age 15, Pitts ran away from home in Detroit and arrived in the University of Chicago. Later, he walked into Rudolf Carnaps office with Carnaps book filled with corrections and suggested improvements. He started studying under Carnap and attending classes during 1938--1943. He wrote several early papers on neuronal network modelling and regularly attended Rashevskys seminars in theoretical biology. The seminar attendants included Gerhard von Bonin and Householder. In 1940, von Bonin introduced Lettvin to McCulloch. In 1942, both Lettvin and Pitts had moved in with McCullochs home. McCulloch had been interested in circular causality from studies with causalgia after amputation, epileptic activity of surgically isolated brain, and Lorente de N s research showing recurrent neural networks are needed to explain vestibular nystagmus. He had difficulty with treating circular causality until Pitts demonstrated how it can be treated by the appropriate mathematical tools of modular arithmetics and symbolic logic. Both authors affiliation in the article was given as University of Illinois, College of Medicine, Department of Psychiatry at the Illinois Neuropsychiatric Institute, University of Chicago, Chicago, U.S.A. Subsequent work It was a foundational result in automata theory. John von Neumann cited it as a significant result. This work led to work on neural networks and their link to finite automata. Kleene introduced the term regular for regular language in a 1951 technical report, where Kleene proved that regular languages are all that could be generated by neural networks, among other results. The term regular was meant to be suggestive of regularly occurring events that the neural net automaton must process and respond to. Marvin Minsky was influenced by McCulloch, built an early example of neural network SNARC (1951), and did a PhD thesis on neural networks (1954). McCulloch was the chair to the ten Macy conferences (1946--1953) on Circular Causal and Feedback Mechanisms in Biological and Social Systems. This was a key event in the beginning of cybernetics, and what later became known as cognitive science. Pitts also attended the conferences. In the 1943 paper, they described how memories can be formed by a neural network with loops in it, or alterable synapses, which are operating over time, and implements logical universals -- there exists and for all. This was generalized for spatial objects, such as geometric figures, in their 1947 paper How we know universals. Norbert Wiener found this a significant evidence for a general method for how animals recognizing objects, by scanning a scene from multiple transformations and finding a canonical representation. He hypothesized that this scanning activity is clocked by the alpha wave, which he mistakenly thought was tightly regulated at 10 Hz (instead of the 8 -- 13 Hz as modern research shows). McCulloch worked with Manuel Blum in studying how a neural network can be logically stable, that is, can implement a boolean function even if the activation thresholds of individual neurons are varied. 64 They were inspired by the problem of how the brain can perform the same functions, such as breathing, under influence of caffeine or alcohol, which shifts the activation threshold over the entire brain. See also Artificial neural network Perceptron Connectionism Principia Mathematica History of artificial neural networks Title Accelerated Linear Algebra URL https//en.wikipedia.org/wiki/Accelerated_Linear_Algebra Content XLA (Accelerated Linear Algebra) is an open-source compiler for machine learning developed by the OpenXLA project. XLA is designed to improve the performance of machine learning models by optimizing the computation graphs at a lower level, making it particularly useful for large-scale computations and high-performance machine learning models. Key features of XLA include Compilation of Computation Graphs Compiles computation graphs into efficient machine code. Optimization Techniques Applies operation fusion, memory optimization, and other techniques. Hardware Support Optimizes models for various hardware, including CPUs, GPUs, and NPUs. Improved Model Execution Time Aims to reduce machine learning models execution time for both training and inference. Seamless Integration Can be used with existing machine learning code with minimal changes. XLA represents a significant step in optimizing machine learning models, providing developers with tools to enhance computational efficiency and performance. Supported target devices -64 NVIDIA GPU AMD GPU Intel GPU Apple GPU Google TPU AWS Trainium, Inferentia Cerebras Graphcore IPU See also TensorFlow PyTorch JAX Title Action model learning URL https//en.wikipedia.org/wiki/Action_model_learning Content Action model learning (sometimes abbreviated action learning) is an area of machine learning concerned with creation and modification of software agents knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners. Learning action models is important when goals change. When an agent acted for a while, it can use its accumulated knowledge about actions in the domain to make better decisions. Thus, learning action models differs from reinforcement learning. It enables reasoning about actions instead of expensive trials in the world. Action model learning is a form of inductive reasoning, where new knowledge is generated based on agents observations. It differs from standard supervised learning in that correct input/output pairs are never presented, nor imprecise action models explicitly corrected. Usual motivation for action model learning is the fact that manual specification of action models for planners is often a difficult, time consuming, and error-prone task (especially in complex environments). Action models Given a training set E displaystyle E consisting of examples  . However, many state of the art action learning methods assume determinism and do not induce P displaystyle P . In addition to determinism, individual methods differ in how they deal with other attributes of domain (e.g. partial observability or sensoric noise). Action learning methods State of the art Recent action learning methods take various approaches and employ a wide variety of tools from different areas of artificial intelligence and computational logic. As an example of a method based on propositional logic, we can mention SLAF (Simultaneous Learning and Filtering) algorithm, which uses agents observations to construct a long propositional formula over time and subsequently interprets it using a satisfiability (SAT) solver. Another technique, in which learning is converted into a satisfiability problem (weighted MAX-SAT in this case) and SAT solvers are used, is implemented in ARMS (Action-Relation Modeling System). Two mutually similar, fully declarative approaches to action learning were based on logic programming paradigm Answer Set Programming (ASP) and its extension, Reactive ASP. In another example, bottom-up inductive logic programming approach was employed. Several different solutions are not directly logic-based. For example, the action model learning using a perceptron algorithm or the multi level greedy search over the space of possible action models. In the older paper from 1992, the action model learning was studied as an extension of reinforcement learning. Literature Most action learning research papers are published in journals and conferences focused on artificial intelligence in general (e.g. Journal of Artificial Intelligence Research (JAIR), Artificial Intelligence, Applied Artificial Intelligence (AAI) or AAAI conferences). Despite mutual relevance of the topics, action model learning is usually not addressed in planning conferences like the International Conference on Automated Planning and Scheduling (ICAPS). See also Machine learning Automated planning and scheduling Action language PDDL Architecture description language Inductive reasoning Computational logic Knowledge representation Title Active learning (machine learning) URL https//en.wikipedia.org/wiki/Active_learning_(machine_learning) Content Active learning is a special case of machine learning in which a learning algorithm can interactively query a human user (or some other information source), to label new data points with the desired outputs. The human user must possess knowledge/expertise in the problem domain, including the ability to consult/research authoritative sources when necessary. In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle. There are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples. Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning. Using active learning allows for faster development of a machine learning algorithm, when comparative updates would require a quantum or super computer. Large-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop. Definitions Let T be the total set of all data under consideration. For example, in a protein engineering problem, T would include all proteins that are known to have a certain interesting activity and all additional proteins that one might want to test for that activity. During each iteration, i, T is broken up into three subsets T K , i displaystyle mathbf T _K,i  Data points where the label is known. T U , i displaystyle mathbf T _U,i  Data points where the label is unknown. T C , i displaystyle mathbf T _C,i  A subset of TU,i that is chosen to be labeled. Most of the current research in active learning involves the best method to choose the data points for TC,i. Scenarios Pool-Based Sampling In this approach, which is the most well known scenario, the learning algorithm attempts to evaluate the entire dataset before selecting data points (instances) for labeling. It is often initially trained on a fully labeled subset of the data using a machine-learning method such as logistic regression or SVM that yields class-membership probabilities for individual data instances. The candidate instances are those for which the prediction is most ambiguous. Instances are drawn from the entire data pool and assigned a confidence score, a measurement of how well the learner understands the data. The system then selects the instances for which it is the least confident and queries the teacher for the labels. The theoretical drawback of pool-based sampling is that it is memory-intensive and is therefore limited in its capacity to handle enormous datasets, but in practice, the rate-limiting factor is that the teacher is typically a (fatiguable) human expert who must be paid for their effort, rather than computer memory. Stream-Based Selective Sampling Here, each consecutive unlabeled instance is examined one at a time with the machine evaluating the informativeness of each item against its query parameters. The learner decides for itself whether to assign a label or query the teacher for each datapoint. As contrasted with Pool-based sampling, the obvious drawback of stream-based methods is that the learning algorithm does not have sufficient information, early in the process, to make a sound assign-label-vs ask-teacher decision, and it does not capitalize as efficiently on the presence of already labeled data. Therefore, the teacher is likely to spend more effort in supplying labels than with the pool-based approach. Membership Query Synthesis This is where the learner generates synthetic data from an underlying natural distribution. For example, if the dataset are pictures of humans and animals, the learner could send a clipped image of a leg to the teacher and query if this appendage belongs to an animal or human. This is particularly useful if the dataset is small. The challenge here, as with all synthetic-data-generation efforts, is in ensuring that the synthetic data is consistent in terms of meeting the constraints on real data. As the number of variables/features in the input data increase, and strong dependencies between variables exist, it becomes increasingly difficult to generate synthetic data with sufficient fidelity. For example, to create a synthetic data set for human laboratory-test values, the sum of the various white blood cell (WBC) components in a White Blood Cell differential must equal 100, since the component numbers are really percentages. Similarly, the enzymes Alanine Transaminase (ALT) and Aspartate Transaminase (AST) measure liver function (though AST is also produced by other tissues, e.g., lung, pancreas) A synthetic data point with AST at the lower limit of normal range (8-33 Units/L) with an ALT several times above normal range (4-35 Units/L) in a simulated chronically ill patient would be physiologically impossible. Query strategies Algorithms for determining which data points should be labeled can be organized into a number of different categories, based upon their purpose Balance exploration and exploitation the choice of examples to label is seen as a dilemma between the exploration and the exploitation over the data space representation. This strategy manages this compromise by modelling the active learning problem as a contextual bandit problem. For example, Bouneffouf et al. propose a sequential algorithm named Active Thompson Sampling (ATS), which, in each round, assigns a sampling distribution on the pool, samples one point from this distribution, and queries the oracle for this sample point label. Expected model change label those points that would most change the current model. Expected error reduction label those points that would most reduce the models generalization error. Exponentiated Gradient Exploration for Active Learning In this paper, the author proposes a sequential algorithm named exponentiated gradient (EG)-active that can improve any active learning algorithm by an optimal random exploration. Uncertainty sampling label those points for which the current model is least certain as to what the correct output should be. Query by committee a variety of models are trained on the current labeled data, and vote on the output for unlabeled data label those points for which the committee disagrees the most Querying from diverse subspaces or partitions When the underlying model is a forest of trees, the leaf nodes might represent (overlapping) partitions of the original feature space. This offers the possibility of selecting instances from non-overlapping or minimally overlapping partitions for labeling. Variance reduction label those points that would minimize output variance, which is one of the components of error. Conformal prediction predicts that a new data point will have a label similar to old data points in some specified way and degree of the similarity within the old examples is used to estimate the confidence in the prediction. Mismatch-first farthest-traversal The primary selection criterion is the prediction mismatch between the current model and nearest-neighbour prediction. It targets on wrongly predicted data points. The second selection criterion is the distance to previously selected data, the farthest first. It aims at optimizing the diversity of selected data. User Centered Labeling Strategies Learning is accomplished by applying dimensionality reduction to graphs and figures like scatter plots. Then the user is asked to label the compiled data (categorical, numerical, relevance scores, relation between two instances. A wide variety of algorithms have been studied that fall into these categories. While the traditional AL strategies can achieve remarkable performance, it is often challenging to predict in advance which strategy is the most suitable in aparticular situation. In recent years, meta-learning algorithms have been gaining in popularity. Some of them have been proposed to tackle the problem of learning AL strategies instead of relying on manually designed strategies. A benchmark which compares meta-learning approaches to active learning to traditional heuristic-based Active Learning may give intuitions if Learning active learning is at the crossroads Minimum marginal hyperplane Some active learning algorithms are built upon support-vector machines (SVMs) and exploit the structure of the SVM to determine which data points to label. Such methods usually calculate the margin, W, of each unlabeled datum in TU,i and treat W as an n-dimensional distance from that datum to the separating hyperplane. Minimum Marginal Hyperplane methods assume that the data with the smallest W are those that the SVM is most uncertain about and therefore should be placed in TC,i to be labeled. Other similar methods, such as Maximum Marginal Hyperplane, choose data with the largest W. Tradeoff methods choose a mix of the smallest and largest Ws. See also List of datasets for machine learning research Sample complexity Bayesian Optimization Reinforcement learning Literature Improving Generalization with Active Learning, David Cohn, Les Atlas  Richard Ladner, Machine Learning 15, 201 221 (1994). https//doi.org/10.1007/ Balcan, Maria-Florina  Hanneke, Steve  Wortman, Jennifer. (2008). The True Sample Complexity of Active Learning.. 45-56. https//link.springer.com/article/10.1007/-010-5174-y Active Learning and Bayesian Optimization a Unified Perspective to Learn with a Goal, Francesco Di Fiore, Michela Nardelli, Laura Mainini, https//arxiv.org/abs/2303.01560v2 Learning how to Active Learn A Deep Reinforcement Learning Approach, Meng Fang, Yuan Li, Trevor Cohn, https//arxiv.org/abs/1708.02383v1 Title Adversarial machine learning URL https//en.wikipedia.org/wiki/Adversarial_machine_learning Content Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A survey from May 2020 revealed practitioners common feeling for better protection of machine learning systems in industrial applications. Machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may intentionally supply fabricated data that violates the statistical assumption. Most common attacks in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction. History At the MIT Spam Conference in January 2004, John Graham-Cumming showed that a machine-learning spam filter could be used to defeat another machine-learning spam filter by automatically learning which words to add to a spam email to get the email classified as not spam. In 2004, Nilesh Dalvi and others noted that linear classifiers used in spam filters could be defeated by simple evasion attacks as spammers inserted good words into their spam emails. (Around 2007, some spammers added random noise to fuzz words within image spam in order to defeat OCR-based filters.) In 2006, Marco Barreno and others published Can Machine Learning Be Secure?, outlining a broad taxonomy of attacks. As late as 2013 many researchers continued to hope that non-linear classifiers (such as support vector machines and neural networks) might be robust to adversaries, until Battista Biggio and others demonstrated the first gradient-based attacks on such machine-learning models (2012 2013). In 2012, deep neural networks began to dominate computer vision problems starting in 2014, Christian Szegedy and others demonstrated that deep neural networks could be fooled by adversaries, again using a gradient-based attack to craft adversarial perturbations. Recently, it was observed that adversarial attacks are harder to produce in the practical world due to the different environmental constraints that cancel out the effect of noise. For example, any small rotation or slight illumination on an adversarial image can destroy the adversariality. In addition, researchers such as Google Brains Nicholas Frosst point out that it is much easier to make self-driving cars miss stop signs by physically removing the sign itself, rather than creating adversarial examples. Frosst also believes that the adversarial machine learning community incorrectly assumes models trained on a certain data distribution will also perform well on a completely different data distribution. He suggests that a new approach to machine learning should be explored, and is currently working on a unique neural network that has characteristics more similar to human perception than state-of-the-art approaches. While adversarial machine learning continues to be heavily rooted in academia, large tech companies such as Google, Microsoft, and IBM have begun curating documentation and open source code bases to allow others to concretely assess the robustness of machine learning models and minimize the risk of adversarial attacks. Examples Examples include attacks in spam filtering, where spam messages are obfuscated through the misspelling of bad words or the insertion of good words attacks in computer security, such as obfuscating malware code within network packets or modifying the characteristics of a network flow to mislead intrusion detection attacks in biometric recognition where fake biometric traits may be exploited to impersonate a legitimate user or to compromise users template galleries that adapt to updated traits over time. Researchers showed that by changing only one-pixel it was possible to fool deep learning algorithms. Others 3-D printed a toy turtle with a texture engineered to make Googles object detection AI classify it as a rifle regardless of the angle from which the turtle was viewed. Creating the turtle required only low-cost commercially available 3-D printing technology. A machine-tweaked image of a dog was shown to look like a cat to both computers and humans. A 2019 study reported that humans can guess how machines will classify adversarial images. Researchers discovered methods for perturbing the appearance of a stop sign such that an autonomous vehicle classified it as a merge or speed limit sign. McAfee attacked Teslas former Mobileye system, fooling it into driving 50 mph over the speed limit, simply by adding a two-inch strip of black tape to a speed limit sign. Adversarial patterns on glasses or clothing designed to deceive facial-recognition systems or license-plate readers, have led to a niche industry of stealth streetwear. An adversarial attack on a neural network can allow an attacker to inject algorithms into the target system. Researchers can also create adversarial audio inputs to disguise commands to intelligent assistants in benign-seeming audio a parallel literature explores human perception of such stimuli. Clustering algorithms are used in security applications. Malware and computer virus analysis aims to identify malware families, and to generate specific detection signatures. Attack modalities Taxonomy Attacks against (supervised) machine learning algorithms have been categorized along three primary axes influence on the classifier, the security violation and their specificity. Classifier influence An attack can influence the classifier by disrupting the classification phase. This may be preceded by an exploration phase to identify vulnerabilities. The attackers capabilities might be restricted by the presence of data manipulation constraints. Security violation An attack can supply malicious data that gets classified as legitimate. Malicious data supplied during training can cause legitimate data to be rejected after training. Specificity A targeted attack attempts to allow a specific intrusion/disruption. Alternatively, an indiscriminate attack creates general mayhem. This taxonomy has been extended into a more comprehensive threat model that allows explicit assumptions about the adversarys goal, knowledge of the attacked system, capability of manipulating the input data/system components, and on attack strategy. This taxonomy has further been extended to include dimensions for defense strategies against adversarial attacks. Strategies Below are some of the most commonly encountered attack scenarios. Data poisoning Poisoning consists of contaminating the training dataset with data designed to increase errors in the output. Given that learning algorithms are shaped by their training datasets, poisoning can effectively reprogram algorithms with potentially malicious intent. Concerns have been raised especially for user-generated training data, e.g. for content recommendation or natural language models. The ubiquity of fake accounts offers many opportunities for poisoning. Facebook reportedly removes around 7 billion fake accounts per year. Poisoning has been reported as the leading concern for industrial applications. On social medias, disinformation campaigns attempt to bias recommendation and moderation algorithms, to push certain content over others. A particular case of data poisoning is the backdoor attack, which aims to teach a specific behavior for inputs with a given trigger, e.g. a small defect on images, sounds, videos or texts. For instance, intrusion detection systems are often trained using collected data. An attacker may poison this data by injecting malicious samples during operation that subsequently disrupt retraining. Data poisoning techniques can also be applied to text-to-image models to alter their output, which can be used by artists to rightfully defend their copyrighted works or artistic style against imitation. Data poisoning can also happen unintentionally through model collapse, where models are trained on synthetic data. Byzantine attacks As machine learning is scaled, it often relies on multiple computing machines. In federated learning, for instance, edge devices collaborate with a central server, typically by sending gradients or model parameters. However, some of these devices may deviate from their expected behavior, e.g. to harm the central servers model or to bias algorithms towards certain behaviors (e.g., amplifying the recommendation of disinformation content). On the other hand, if the training is performed on a single machine, then the model is very vulnerable to a failure of the machine, or an attack on the machine the machine is a single point of failure. In fact, the machine owner may themselves insert provably undetectable backdoors. The current leading solutions to make (distributed) learning algorithms provably resilient to a minority of malicious (a.k.a. Byzantine) participants are based on robust gradient aggregation rules. The robust aggregation rules do not always work especially when the data across participants has a non-iid distribution. Nevertheless, in the context of heterogeneous honest participants, such as users with different consumption habits for recommendation algorithms or writing styles for language models, there are provable impossibility theorems on what any robust learning algorithm can guarantee. Evasion Evasion attacks consist of exploiting the imperfection of a trained model. For instance, spammers and hackers often attempt to evade detection by obfuscating the content of spam emails and malware. Samples are modified to evade detection that is, to be classified as legitimate. This does not involve influence over the training data. A clear example of evasion is image-based spam in which the spam content is embedded within an attached image to evade textual analysis by anti-spam filters. Another example of evasion is given by spoofing attacks against biometric verification systems. Evasion attacks can be generally split into two different categories black box attacks and white box attacks. Model extraction Model extraction involves an adversary probing a black box machine learning system in order to extract the data it was trained on. This can cause issues when either the training data or the model itself is sensitive and confidential. For example, model extraction could be used to extract a proprietary stock trading model which the adversary could then use for their own financial benefit. In the extreme case, model extraction can lead to model stealing, which corresponds to extracting a sufficient amount of data from the model to enable the complete reconstruction of the model. On the other hand, membership inference is a targeted model extraction attack, which infers the owner of a data point, often by leveraging the overfitting resulting from poor machine learning practices. Concerningly, this is sometimes achievable even without knowledge or access to a target models parameters, raising security concerns for models trained on sensitive data, including but not limited to medical records and/or personally identifiable information. With the emergence of transfer learning and public accessibility of many state of the art machine learning models, tech companies are increasingly drawn to create models based on public ones, giving attackers freely accessible information to the structure and type of model being used. Categories Adversarial deep reinforcement learning Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area, some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations. While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies. Adversarial natural language processing Adversarial attacks on speech recognition have been introduced for speech-to-text applications, in particular for Mozillas implementation of DeepSpeech. Adversarial attacks and training in linear models There is a growing literature about adversarial attacks in linear models. Indeed, since the seminal work from Goodfellow at al. studying these models in linear models has been an important tool to understand how adversarial attacks affect machine learning models. The analysis of these models is simplified because the computation of adversarial attacks can be simplified in linear regression and classification problems. Moreover, adversarial training is convex in this case. Linear models allow for analytical analysis while still reproducing phenomena observed in state-of-the-art models. One prime example of that is how this model can be used to explain the trade-off between robustness and accuracy. Diverse work indeed provides analysis of adversarial attacks in linear models, including asymptotic analysis for classification and for linear regression. And, finite-sample analysis based on Rademacher complexity. Specific attack types There are a large variety of different adversarial attacks that can be used against machine learning systems. Many of these work on both deep learning systems as well as traditional machine learning models such as SVMs and linear regression. A high level sample of these attack types include Adversarial Examples Trojan Attacks / Backdoor Attacks Model Inversion Membership Inference Adversarial examples An adversarial example refers to specially crafted input that is designed to look normal to humans but causes misclassification to a machine learning model. Often, a form of specially designed noise is used to elicit the misclassifications. Below are some current techniques for generating adversarial examples in the literature (by no means an exhaustive list). Gradient-based evasion attack Fast Gradient Sign Method (FGSM) Projected Gradient Descent (PGD) Carlini and Wagner (CW) attack Adversarial patch attack Black box attacks Black box attacks in adversarial machine learning assume that the adversary can only get outputs for provided inputs and has no knowledge of the model structure or parameters. In this case, the adversarial example is generated either using a model created from scratch, or without any model at all (excluding the ability to query the original model). In either case, the objective of these attacks is to create adversarial examples that are able to transfer to the black box model in question. Simple Black-box Adversarial Attacks Simple Black-box Adversarial Attacks is a query-efficient way to attack black-box image classifiers. Take a random orthonormal basis v 1 , v 2 , , v d displaystyle v_1,v_2,dots ,v_d in R d displaystyle mathbb R d . The authors suggested the discrete cosine transform of the standard basis (the pixels). For a correctly classified image x displaystyle x , try x  v 1 , x v 1 displaystyle xepsilon v_1,x-epsilon v_1 , and compare the amount of error in the classifier upon x  v 1 , x , x v 1 displaystyle xepsilon v_1,x,x-epsilon v_1 . Pick the one that causes the largest amount of error. Repeat this for v 2 , v 3 , displaystyle v_2,v_3,dots  until the desired level of error in the classifier is reached.It was discovered when the authors designed a simple baseline to compare with a previous black-box adversarial attack algorithm based on gaussian processes, and were surprised that the baseline worked even better. Square Attack The Square Attack was introduced in 2020 as a black box evasion adversarial attack based on querying classification scores without the need of gradient information. As a score based black box attack, this adversarial approach is able to query probability distributions across model output classes, but has no other access to the model itself. According to the papers authors, the proposed Square Attack required fewer queries than when compared to state-of-the-art score-based black box attacks at the time. To describe the function objective, the attack defines the classifier as f   0 , 1  d R K textstyle f0,1drightarrow mathbb R K , with d textstyle d representing the dimensions of the input and K textstyle K as the total number of output classes. f k ( x ) textstyle f_k(x) returns the score (or a probability between 0 and 1) that the input x textstyle x belongs to class k textstyle k , which allows the classifiers class output for any input x textstyle x to be defined as argmax  , . . . , K f k ( x ) textstyle textargmax_,...,Kf_k(x) . The goal of this attack is as follows argmax  , . . . , K f k ( x  ) y ,   x  x   p and x   0 , 1  d displaystyle textargmax_,...,Kf_k(hat x)neq y,hat x-x_pleq epsilon text and hat xin 0,1d In other words, finding some perturbed adversarial example x  textstyle hat x such that the classifier incorrectly classifies it to some other class under the constraint that x  textstyle hat x and x textstyle x are similar. The paper then defines loss L textstyle L as L ( f ( x  ) , y )  f y ( x  ) max k y f k ( x  ) textstyle L(f(hat x),y)f_y(hat x)-max _kneq yf_k(hat x) and proposes the solution to finding adversarial example x  textstyle hat x as solving the below constrained optimization problem min x   0 , 1  d L ( f ( x  ) , y ) , s.t.   x  x   p displaystyle min _hat xin 0,1dL(f(hat x),y),text s.t. hat x-x_pleq epsilon  The result in theory is an adversarial example that is highly confident in the incorrect class but is also very similar to the original image. To find such example, Square Attack utilizes the iterative random search technique to randomly perturb the image in hopes of improving the objective function. In each step, the algorithm perturbs only a small square section of pixels, hence the name Square Attack, which terminates as soon as an adversarial example is found in order to improve query efficiency. Finally, since the attack algorithm uses scores and not gradient information, the authors of the paper indicate that this approach is not affected by gradient masking, a common technique formerly used to prevent evasion attacks. HopSkipJump Attack This black box attack was also proposed as a query efficient attack, but one that relies solely on access to any inputs predicted output class. In other words, the HopSkipJump attack does not require the ability to calculate gradients or access to score values like the Square Attack, and will require just the models class prediction output (for any given input). The proposed attack is split into two different settings, targeted and untargeted, but both are built from the general idea of adding minimal perturbations that leads to a different model output. In the targeted setting, the goal is to cause the model to misclassify the perturbed image to a specific target label (that is not the original label). In the untargeted setting, the goal is to cause the model to misclassify the perturbed image to any label that is not the original label. The attack objectives for both are as follows where x textstyle x is the original image, x textstyle xprime  is the adversarial image, d textstyle d is a distance function between images, c textstyle c is the target label, and C textstyle C is the models classification class label function Targeted min x d ( x , x ) subject to C ( x )  c displaystyle textbf Targetedmin _xprime d(xprime ,x)text subject to C(xprime )c Untargeted min x d ( x , x ) subject to C ( x ) C ( x ) displaystyle textbf Untargetedmin _xprime d(xprime ,x)text subject to C(xprime )neq C(x) To solve this problem, the attack proposes the following boundary function S textstyle S for both the untargeted and targeted setting S ( x )   max c C ( x ) F ( x ) c F ( x ) C ( x ) , (Untargeted) F ( x ) c max c c F ( x ) c , (Targeted) displaystyle S(xprime )begincasesmax _cneq C(x)F(xprime )_c-F(xprime )_C(x),text(Untargeted)F(xprime )_c-max _cneq cF(xprime )_c,text(Targeted)endcases This can be further simplified to better visualize the boundary between different potential adversarial examples S ( x )  0  a r g m a x c F ( x ) C ( x ) , (Untargeted) a r g m a x c F ( x )  c , (Targeted) displaystyle S(xprime )0iff begincasesargmax_cF(xprime )neq C(x),text(Untargeted)argmax_cF(xprime )c,text(Targeted)endcases With this boundary function, the attack then follows an iterative algorithm to find adversarial examples x textstyle xprime  for a given image x textstyle x that satisfies the attack objectives. Initialize x textstyle x to some point where S ( x )  0 textstyle S(x)0 Iterate below Boundary search Gradient update Compute the gradient Find the step size Boundary search uses a modified binary search to find the point in which the boundary (as defined by S textstyle S ) intersects with the line between x textstyle x and x textstyle xprime  . The next step involves calculating the gradient for x textstyle x , and update the original x textstyle x using this gradient and a pre-chosen step size. HopSkipJump authors prove that this iterative algorithm will converge, leading x textstyle x to a point right along the boundary that is very close in distance to the original image. However, since HopSkipJump is a proposed black box attack and the iterative algorithm above requires the calculation of a gradient in the second iterative step (which black box attacks do not have access to), the authors propose a solution to gradient calculation that requires only the models output predictions alone. By generating many random vectors in all directions, denoted as u b textstyle u_b , an approximation of the gradient can be calculated using the average of these random vectors weighted by the sign of the boundary function on the image x  u b textstyle xprime delta _u_b , where u b textstyle delta _u_b is the size of the random vector perturbation S ( x , ) 1 B . White box attacks White box attacks assumes that the adversary has access to model parameters on top of being able to get labels for provided inputs. Fast gradient sign method One of the first proposed attacks for generating adversarial examples was proposed by Google researchers Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. The attack was called fast gradient sign method (FGSM), and it consists of adding a linear amount of in-perceivable noise to the image and causing a model to incorrectly classify it. This noise is calculated by multiplying the sign of the gradient with respect to the image we want to perturb by a small constant epsilon. As epsilon increases, the model is more likely to be fooled, but the perturbations become easier to identify as well. Shown below is the equation to generate an adversarial example where x textstyle x is the original image, textstyle epsilon  is a very small number, x textstyle Delta _x is the gradient function, J textstyle J is the loss function, textstyle theta  is the model weights, and y textstyle y is the true label. a d v  . In traditional gradient descent (for model training), the gradient is used to update the weights of the model since the goal is to minimize the loss for the model on a ground truth dataset. The Fast Gradient Sign Method was proposed as a fast way to generate adversarial examples to evade the model, based on the hypothesis that neural networks cannot resist even linear amounts of perturbation to the input. FGSM has shown to be effective in adversarial attacks for image classification and skeletal action recognition. Carlini  Wagner (CW) In an effort to analyze existing adversarial attacks and defenses, researchers at the University of California, Berkeley, Nicholas Carlini and David Wagner in 2016 propose a faster and more robust method to generate adversarial examples. The attack proposed by Carlini and Wagner begins with trying to solve a difficult non-linear optimization equation min (     p ) subject to C ( x  )  t , x   0 , 1  n displaystyle min(delta _p)text subject to C(xdelta )t,xdelta in 0,1n Here the objective is to minimize the noise ( textstyle delta  ), added to the original input x textstyle x , such that the machine learning algorithm ( C textstyle C ) predicts the original input with delta (or x  textstyle xdelta  ) as some other class t textstyle t . However instead of directly the above equation, Carlini and Wagner propose using a new function f textstyle f such that C ( x  )  t f ( x  ) 0 displaystyle C(xdelta )tiff f(xdelta )leq 0 This condenses the first equation to the problem below min (     p ) subject to f ( x  ) 0 , x   0 , 1  n displaystyle min(delta _p)text subject to f(xdelta )leq 0,xdelta in 0,1n and even more to the equation below min (     p  c f ( x  ) ) , x   0 , 1  n displaystyle min(delta _pccdot f(xdelta )),xdelta in 0,1n Carlini and Wagner then propose the use of the below function in place of f textstyle f using Z textstyle Z , a function that determines class probabilities for given input x textstyle x . When substituted in, this equation can be thought of as finding a target class that is more confident than the next likeliest class by some constant amount f ( x )  (  max i t Z ( x ) i  Z ( x ) t )  displaystyle f(x)(max _ineq t_i-_t) When solved using gradient descent, this equation is able to produce stronger adversarial examples when compared to fast gradient sign method that is also able to bypass defensive distillation, a defense that was once proposed to be effective against adversarial examples. Defenses Researchers have proposed a multi-step approach to protecting machine learning. Threat modeling Formalize the attackers goals and capabilities with respect to the target system. Attack simulation Formalize the optimization problem the attacker tries to solve according to possible attack strategies. Attack impact evaluation Countermeasure design Noise detection (For evasion based attack) Information laundering Alter the information received by adversaries (for model stealing attacks) Mechanisms A number of defense mechanisms against evasion, poisoning, and privacy attacks have been proposed, including Secure learning algorithms Byzantine-resilient algorithms Multiple classifier systems AI-written algorithms. AIs that explore the training environment for example, in image recognition, actively navigating a 3D environment rather than passively scanning a fixed set of 2D images. Privacy-preserving learning Ladder algorithm for Kaggle-style competitions Game theoretic models Sanitizing training data Adversarial training Backdoor detection algorithms Gradient masking/obfuscation techniques to prevent the adversary exploiting the gradient in white-box attacks. This family of defenses is deemed unreliable as these models are still vulnerable to black-box attacks or can be circumvented in other ways. Ensembles of models have been proposed in the literature but caution should be applied when relying on them usually ensembling weak classifiers results in a more accurate model but it does not seem to apply in the adversarial context. See also Pattern recognition Fawkes (image cloaking software) Generative adversarial network References External links MITRE ATLAS Adversarial Threat Landscape for Artificial-Intelligence Systems NIST 8269 Draft A Taxonomy and Terminology of Adversarial Machine Learning NIPS 2007 Workshop on Machine Learning in Adversarial Environments for Computer Security AlfaSVMLib Archived 2020-09-24 at the Wayback Machine Adversarial Label Flip Attacks against Support Vector Machines Laskov, Pavel Lippmann, Richard (2010). Machine learning in adversarial environments. Machine Learning. 81 (2) 115 119. doi10.1007/-010-5207-6. S2CID 12567278. Dagstuhl Perspectives Workshop on Machine Learning Methods for Computer Security Workshop on Artificial Intelligence and Security, (AISec) Series Title AI/ML Development Platform URL https//en.wikipedia.org/wiki/AI/ML_Development_Platform Content AI/ML development platforms, such as PyTorch and Hugging Face, are software ecosystems designed to facilitate the creation, training, deployment, and management of artificial intelligence (AI) and machine learning (ML) models. These platforms provide tools, frameworks, and infrastructure to streamline workflows for developers, data scientists, and researchers working on AI-driven solutions. Overview AI/ML development platforms serve as comprehensive environments for building AI systems, ranging from simple predictive models to complex large language models (LLMs). They abstract technical complexities (e.g., distributed computing, hyperparameter tuning) while offering modular components for customization. Key users include Developers Building applications powered by AI/ML. Data scientists Experimenting with algorithms and data pipelines. Researchers Advancing state-of-the-art AI capabilities. Key features Modern AI/ML platforms typically include End-to-end workflow support Data preparation Tools for cleaning, labeling, and augmenting datasets. Model building Libraries for designing neural networks (e.g., PyTorch, TensorFlow integrations). Training  Optimization Distributed training, hyperparameter tuning, and AutoML. Deployment Exporting models to production environments (APIs, edge devices, cloud services). Scalability Support for multi-GPU/TPU training and cloud-native infrastructure (e.g., Kubernetes). Pre-built models  templates Repositories of pre-trained models (e.g., Hugging Face s Model Hub) for tasks like natural language processing (NLP), computer vision, or speech recognition. Collaboration tools Version control, experiment tracking (e.g., MLflow), and team project management. Ethical AI tools Bias detection, explainability frameworks (e.g., SHAP, LIME), and compliance with regulations like GDPR. Examples of platforms Applications AI/ML development platforms underpin innovations in Health care Drug discovery, medical imaging analysis. Finance Fraud detection, algorithmic trading. Natural language processing (NLP) Chatbots, translation systems. Autonomous systems Self-driving cars, robotics. Challenges Computational costs Training LLMs requires massive GPU/TPU resources. Data privacy Balancing model performance with GDPR/CCPA compliance. Skill gaps High barrier to entry for non-experts. Bias and fairness Mitigating skewed outcomes in sensitive applications. Future trends Democratization Low-code/no-code platforms (e.g., Google AutoML, DataRobot). Ethical AI integration Tools for bias mitigation and transparency. Federated learning Training models on decentralized data. Quantum machine learning Hybrid platforms leveraging quantum computing. See also Automated machine learning Large language model References External links MLflow Official Website Open-source platform for the machine learning lifecycle. Hugging Face Community and tools for NLP models. TensorFlow Googles machine learning framework. Google AI Research Publications on AI/ML advancements. Title AIOps URL https//en.wikipedia.org/wiki/AIOps Content AIOps (Artificial Intelligence for IT Operations) refers to the use of artificial intelligence, machine learning, and big data analytics to automate and enhance data center management. It helps organizations manage complex IT environments by detecting, diagnosing, and resolving issues more efficiently than traditional methods. History AIOPs was first defined by Gartner in 2016, combining artificial intelligence and IT operations to describe the application of AI and machine learning to enhance IT operations. This concept was introduced to address the increasing complexity and data volume in IT environments, aiming to automate processes such as event correlation, anomaly detection, and causality determination. Definition AIOps refers to the multi-layered complex technology platforms which enhance and automate IT operations by using machine learning and analytics to analyze the large amounts of data collected from various DevOps devices and tools, automatically identifying and responding to issues in real-time. AIOps is used as a shift from isolated IT data to aggregated observational data (e.g., job logs and monitoring systems) and interaction data (such as ticketing, events, or incident records) within a big data platform AIOps applies machine learning and analytics to this data. The result is continuous visibility, which, combined with the implementation of automation, can lead to ongoing improvements. AIOps connects three IT disciplines (automation, service management, and performance management) to achieve continuous visibility and improvement. This new approach in modern, accelerated, and hyper-scaled IT environments leverages advances in machine learning and big data to overcome previous limitations. Components AIOps consists of a number of components including the following processes and techniques Anomaly Detection Log Analysis Root Cause Analysis Cohort Analysis Event Correlation Predictive Analytics Hardware Failure Prediction Automated Remediation Performance Prediction Incident Management Causality Determination Queue Management Resource Scheduling and Optimization Predictive Capacity Management Resource Allocation Service Quality Monitoring Deployment and Integration Testing System Configuration Auto-diagnosis and Problem Localization Efficient ML Training and Inferencing Using LLMs for Cloud Ops Auto Service Healing Data Center Management Customer Support Security and Privacy in Cloud Operations Results AI optimizes IT operations in five ways First, intelligent monitoring powered by AI helps identify potential issues before they cause outages, improving metrics like Mean Time to Detect (MTTD) by 15-20. Second, performance data analysis and insights enable quick decision-making by ingesting and analyzing large data sets in real time. Third, AI-driven automated infrastructure optimization efficiently allocates resources and thereby reducing cloud costs. Fourth, enhanced IT service management reduces critical incidents by over 50 through AI-driven end-to-end service management. Lastly, intelligent task automation accelerates problem resolution and automates remedial actions with minimal human intervention. AIOps vs. MLOps AIOps tools use big data analytics, machine learning algorithms, and predictive analytics to detect anomalies, correlate events, and provide proactive insights. This automation reduces the burden on IT teams, allowing them to focus on strategic tasks rather than routine operational issues. AIOps is widely used by IT operations teams, DevOps, network administrators, and IT service management (ITSM) teams to enhance visibility and enable quicker incident resolution in hybrid cloud environments, data centers, and other IT infrastructures. In contrast to MLOps (Machine Learning Operations), which focuses on the lifecycle management and operational aspects of machine learning models, AIOps focuses on optimizing IT operations using a variety of analytics and AI-driven techniques. While both disciplines rely on AI and data-driven methods, AIOps primarily targets IT operations, whereas MLOps is concerned with the deployment, monitoring, and maintenance of ML models. Conferences There are several conferences that are specific to AIOps AIOps Summit AI Dev Summit IBM Think conference Title AIXI URL https//en.wikipedia.org/wiki/AIXI Content AIXI is a theoretical mathematical formalism for artificial general intelligence. It combines Solomonoff induction with sequential decision theory. AIXI was first proposed by Marcus Hutter in 2000 and several results regarding AIXI are proved in Hutters 2005 book Universal Artificial Intelligence. AIXI is a reinforcement learning (RL) agent. It maximizes the expected total rewards received from the environment. Intuitively, it simultaneously considers every computable hypothesis (or environment). In each time step, it looks at every possible program and evaluates how many rewards that program generates depending on the next action taken. The promised rewards are then weighted by the subjective belief that this program constitutes the true environment. This belief is computed from the length of the program longer programs are considered less likely, in line with Occams razor. AIXI then selects the action that has the highest expected total reward in the weighted sum of all these programs. Definition According to Hutter, the word AIXI can have several interpretations. AIXI can stand for AI based on Solomonoffs distribution, denoted by displaystyle xi  (which is the Greek letter xi), or e.g. it can stand for AI crossed (X) with induction (I). There are other interpretations. AIXI is a reinforcement learning agent that interacts with some stochastic and unknown but computable environment displaystyle mu  . The interaction proceeds in time steps, from . At time step t, the agent chooses an action a t A displaystyle a_tin mathcal A (e.g. a limb movement) and executes it in the environment, and the environment responds with a percept e t .g., a camera image) and a reward r t R displaystyle r_tin mathbb R  , distributed according to the conditional probability ( o t r t  a 1 o 1 r 1 . . . a t 1 o t 1 r t 1 a t ) displaystyle mu (o_tr_ta_1o_1r_1...a_t-1o_t-1r_t-1a_t) , where a 1 o 1 r 1 . . . a t 1 o t 1 r t 1 a t displaystyle a_1o_1r_1...a_t-1o_t-1r_t-1a_t is the history of actions, observations and rewards. The environment displaystyle mu  is thus mathematically represented as a probability distribution over percepts (observations and rewards) which depend on the full history, so there is no Markov assumption (as opposed to other RL algorithms). Note again that this probability distribution is unknown to the AIXI agent. Furthermore, note again that displaystyle mu  is computable, that is, the observations and rewards received by the agent from the environment displaystyle mu  can be computed by some program (which runs on a Turing machine), given the past actions of the AIXI agent. The only goal of the AIXI agent is to maximise . The AIXI agent is associated with a stochastic policy  ( A E ) A displaystyle pi (mathcal Atimes mathcal E)rightarrow mathcal A , which is the function it uses to choose actions at every time step, where A displaystyle mathcal A is the space of all possible actions that AIXI can take and E displaystyle mathcal E is the space of all possible percepts that can be produced by the environment. The environment (or probability distribution) displaystyle mu  can also be thought of as a stochastic policy (which is a function)  ( A E ) A E displaystyle mu (mathcal Atimes mathcal E)times mathcal Arightarrow mathcal E , where the displaystyle  is the Kleene star operation. In general, at time step t displaystyle t (which ranges from 1 to m), AIXI, having previously executed actions a 1 a t 1 displaystyle a_1dots a_t-1 (which is often abbreviated in the literature as a  t displaystyle a_t ) and having observed the history of percepts o 1 r 1 . . . o t 1 r t 1 displaystyle o_1r_1...o_t-1r_t-1 (which can be abbreviated as e  t displaystyle e_t ), chooses and executes in the environment the action, a t displaystyle a_t , defined as follows a t  arg max a t o t r t max a m o m r m  r t   r m  q  U ( q , a 1 a m )  o 1 r 1 o m r m 2 length ( q ) displaystyle a_targ max _a_tsum _o_tr_tldots max _a_msum _o_mr_mr_tldots r_msum _qU(q,a_1ldots a_m)o_1r_1ldots o_mr_m2-textrm length(q) or, using parentheses, to disambiguate the precedences a t  arg max a t ( o t r t ( max a m o m r m  r t   r m  ( q  U ( q , a 1 a m )  o 1 r 1 o m r m 2 length ( q ) ) ) ) displaystyle a_targ max _a_tleft(sum _o_tr_tldots left(max _a_msum _o_mr_mr_tldots r_mleft(sum _qU(q,a_1ldots a_m)o_1r_1ldots o_mr_m2-textrm length(q)right)right)right) Intuitively, in the definition above, AIXI considers the sum of the total reward over all possible futures up to m t displaystyle m-t time steps ahead (that is, from t displaystyle t to m displaystyle m ), weighs each of them by the complexity of programs q displaystyle q (that is, by 2 length ( q ) displaystyle 2-textrm length(q) ) consistent with the agents past (that is, the previously executed actions, a  t displaystyle a_t , and received percepts, e  t displaystyle e_t ) that can generate that future, and then picks the action that maximises expected future rewards. Let us break this definition down in order to attempt to fully understand it. o t r t displaystyle o_tr_t is the percept (which consists of the observation o t displaystyle o_t and reward r t displaystyle r_t ) received by the AIXI agent at time step t displaystyle t from the environment (which is unknown and stochastic). Similarly, o m r m displaystyle o_mr_m is the percept received by AIXI at time step m displaystyle m (the last time step where AIXI is active). r t   r m displaystyle r_tldots r_m is the sum of rewards from time step t displaystyle t to time step m displaystyle m , so AIXI needs to look into the future to choose its action at time step t displaystyle t . U displaystyle U denotes a monotone universal Turing machine, and q displaystyle q ranges over all (deterministic) programs on the universal machine U displaystyle U , which receives as input the program q displaystyle q and the sequence of actions a 1 a m displaystyle a_1dots a_m (that is, all actions), and produces the sequence of percepts o 1 r 1 o m r m displaystyle o_1r_1ldots o_mr_m . The universal Turing machine U displaystyle U is thus used to simulate or compute the environment responses or percepts, given the program q displaystyle q (which models the environment) and all actions of the AIXI agent in this sense, the environment is computable (as stated above). Note that, in general, the program which models the current and actual environment (where AIXI needs to act) is unknown because the current environment is also unknown. length ( q ) displaystyle textrm length(q) is the length of the program q displaystyle q (which is encoded as a string of bits). Note that 2 length ( q )  1 2 length ( q ) displaystyle 2-textrm length(q)frac 12textrm length(q) . Hence, in the definition above, q  U ( q , a 1 a m )  o 1 r 1 o m r m 2 length ( q ) displaystyle sum _qU(q,a_1ldots a_m)o_1r_1ldots o_mr_m2-textrm length(q) should be interpreted as a mixture (in this case, a sum) over all computable environments (which are consistent with the agents past), each weighted by its complexity 2 length ( q ) displaystyle 2-textrm length(q) . Note that a 1 a m displaystyle a_1ldots a_m can also be written as a 1 a t 1 a t a m displaystyle a_1ldots a_t-1a_tldots a_m , and a 1 a t 1  a  t displaystyle a_1ldots a_t-1a_t is the sequence of actions already executed in the environment by the AIXI agent. Similarly, o 1 r 1 o m r . Let us now put all these components together in order to understand this equation or definition. At time step t, AIXI chooses the action a t displaystyle a_t where the function o t r t max a m o m r m  r t   r m  q  U ( q , a 1 a m )  o 1 r 1 o m r m 2 length ( q ) displaystyle sum _o_tr_tldots max _a_msum _o_mr_mr_tldots r_msum _qU(q,a_1ldots a_m)o_1r_1ldots o_mr_m2-textrm length(q) attains its maximum. Parameters The parameters to AIXI are the universal Turing machine U and the agents lifetime m, which need to be chosen. The latter parameter can be removed by the use of discounting. Optimality AIXIs performance is measured by the expected total number of rewards it receives. AIXI has been proven to be optimal in the following ways. Pareto optimality there is no other agent that performs at least as well as AIXI in all environments while performing strictly better in at least one environment. Balanced Pareto optimality like Pareto optimality, but considering a weighted sum of environments. Self-optimizing a policy p is called self-optimizing for an environment displaystyle mu  if the performance of p approaches the theoretical maximum for displaystyle mu  when the length of the agents lifetime (not time) goes to infinity. For environment classes where self-optimizing policies exist, AIXI is self-optimizing. It was later shown by Hutter and Jan Leike that balanced Pareto optimality is subjective and that any policy can be considered Pareto optimal, which they describe as undermining all previous optimality claims for AIXI. However, AIXI does have limitations. It is restricted to maximizing rewards based on percepts as opposed to external states. It also assumes it interacts with the environment solely through action and percept channels, preventing it from considering the possibility of being damaged or modified. Colloquially, this means that it doesnt consider itself to be contained by the environment it interacts with. It also assumes the environment is computable. Computational aspects Like Solomonoff induction, AIXI is incomputable. However, there are computable approximations of it. One such approximation is AIXItl, which performs at least as well as the provably best time t and space l limited agent. Another approximation to AIXI with a restricted environment class is MC-AIXI (FAC-CTW) (which stands for Monte Carlo AIXI FAC-Context-Tree Weighting), which has had some success playing simple games such as partially observable Pac-Man. See also G del machine References Universal Algorithmic Intelligence A mathematical top-down approach, Marcus Hutter, arXivcs/0701125 also in Artificial General Intelligence, eds. B. Goertzel and C. Pennachin, Springer, 2007, ISBN 9783540237334, pp. 227 290, doi10.1007/978-3-540-68677-4_8. Title Algorithm selection URL https//en.wikipedia.org/wiki/Algorithm_selection Content Algorithm selection (sometimes also called per-instance algorithm selection or offline algorithm selection) is a meta-algorithmic technique to choose an algorithm from a portfolio on an instance-by-instance basis. It is motivated by the observation that on many practical problems, different algorithms have different performance characteristics. That is, while one algorithm performs well in some scenarios, it performs poorly in others and vice versa for another algorithm. If we can identify when to use which algorithm, we can optimize for each scenario and improve overall performance. This is what algorithm selection aims to do. The only prerequisite for applying algorithm selection techniques is that there exists (or that there can be constructed) a set of complementary algorithms. Definition Given a portfolio P displaystyle mathcal P of algorithms A P displaystyle mathcal Ain mathcal P , a set of instances i I displaystyle iin mathcal I and a cost metric m  P I R displaystyle mmathcal Ptimes mathcal Ito mathbb R  , the algorithm selection problem consists of finding a mapping s  I P displaystyle smathcal Ito mathcal P from instances I displaystyle mathcal I to algorithms P displaystyle mathcal P such that the cost i I m ( s ( i ) , i ) displaystyle sum _iin mathcal Im(s(i),i) across all instances is optimized. Examples Boolean satisfiability problem (and other hard combinatorial problems) A well-known application of algorithm selection is the Boolean satisfiability problem. Here, the portfolio of algorithms is a set of (complementary) SAT solvers, the instances are Boolean formulas, the cost metric is for example average runtime or number of unsolved instances. So, the goal is to select a well-performing SAT solver for each individual instance. In the same way, algorithm selection can be applied to many other N P displaystyle mathcal NP -hard problems (such as mixed integer programming, CSP, AI planning, TSP, MAXSAT, QBF and answer set programming). Competition-winning systems in SAT are SATzilla, 3S and CSHC Machine learning In machine learning, algorithm selection is better known as meta-learning. The portfolio of algorithms consists of machine learning algorithms (e.g., Random Forest, SVM, DNN), the instances are data sets and the cost metric is for example the error rate. So, the goal is to predict which machine learning algorithm will have a small error on each data set. Instance features The algorithm selection problem is mainly solved with machine learning techniques. By representing the problem instances by numerical features f displaystyle f , algorithm selection can be seen as a multi-class classification problem by learning a mapping f i A displaystyle f_imapsto mathcal A for a given instance i displaystyle i . Instance features are numerical representations of instances. For example, we can count the number of variables, clauses, average clause length for Boolean formulas, or number of samples, features, class balance for ML data sets to get an impression about their characteristics. Static vs. probing features We distinguish between two kinds of features Static features are in most cases some counts and statistics (e.g., clauses-to-variables ratio in SAT). These features ranges from very cheap features (e.g. number of variables) to very complex features (e.g., statistics about variable-clause graphs). Probing features (sometimes also called landmarking features) are computed by running some analysis of algorithm behavior on an instance (e.g., accuracy of a cheap decision tree algorithm on an ML data set, or running for a short time a stochastic local search solver on a Boolean formula). These feature often cost more than simple static features. Feature costs Depending on the used performance metric m displaystyle m , feature computation can be associated with costs. For example, if we use running time as performance metric, we include the time to compute our instance features into the performance of an algorithm selection system. SAT solving is a concrete example, where such feature costs cannot be neglected, since instance features for CNF formulas can be either very cheap (e.g., to get the number of variables can be done in constant time for CNFs in the DIMACs format) or very expensive (e.g., graph features which can cost tens or hundreds of seconds). It is important to take the overhead of feature computation into account in practice in such scenarios otherwise a misleading impression of the performance of the algorithm selection approach is created. For example, if the decision which algorithm to choose can be made with perfect accuracy, but the features are the running time of the portfolio algorithms, there is no benefit to the portfolio approach. This would not be obvious if feature costs were omitted. Approaches Regression approach One of the first successful algorithm selection approaches predicted the performance of each algorithm m  A  I R displaystyle hat m_mathcal Amathcal Ito mathbb R  and selected the algorithm with the best predicted performance a r g min A P m  A ( i ) displaystyle argmin _mathcal Ain mathcal Phat m_mathcal A(i) for an instance i displaystyle i . Clustering approach A common assumption is that the given set of instances I displaystyle mathcal I can be clustered into homogeneous subsets and for each of these subsets, there is one well-performing algorithm for all instances in there. So, the training consists of identifying the homogeneous clusters via an unsupervised clustering approach and associating an algorithm with each cluster. A new instance is assigned to a cluster and the associated algorithm selected. A more modern approach is cost-sensitive hierarchical clustering using supervised learning to identify the homogeneous instance subsets. Pairwise cost-sensitive classification approach A common approach for multi-class classification is to learn pairwise models between every pair of classes (here algorithms) and choose the class that was predicted most often by the pairwise models. We can weight the instances of the pairwise prediction problem by the performance difference between the two algorithms. This is motivated by the fact that we care most about getting predictions with large differences correct, but the penalty for an incorrect prediction is small if there is almost no performance difference. Therefore, each instance i displaystyle i for training a classification model A 1 displaystyle mathcal A_1 vs A 2 displaystyle mathcal A_2 is associated with a cost  m ( A 1 , i ) m ( A 2 , i )  displaystyle m(mathcal A_1,i)-m(mathcal A_2,i) . Requirements The algorithm selection problem can be effectively applied under the following assumptions The portfolio P displaystyle mathcal P of algorithms is complementary with respect to the instance set I displaystyle mathcal I , i.e., there is no single algorithm A P displaystyle mathcal Ain mathcal P that dominates the performance of all other algorithms over I displaystyle mathcal I (see figures to the right for examples on complementary analysis). In some application, the computation of instance features is associated with a cost. For example, if the cost metric is running time, we have also to consider the time to compute the instance features. In such cases, the cost to compute features should not be larger than the performance gain through algorithm selection. Application domains Algorithm selection is not limited to single domains but can be applied to any kind of algorithm if the above requirements are satisfied. Application domains include hard combinatorial problems SAT, Mixed Integer Programming, CSP, AI Planning, TSP, MAXSAT, QBF and Answer Set Programming combinatorial auctions in machine learning, the problem is known as meta-learning software design black-box optimization multi-agent systems numerical optimization linear algebra, differential equations evolutionary algorithms vehicle routing problem power systems For an extensive list of literature about algorithm selection, we refer to a literature overview. Variants of algorithm selection Online selection Online algorithm selection refers to switching between different algorithms during the solving process. This is useful as a hyper-heuristic. In contrast, offline algorithm selection selects an algorithm for a given instance only once and before the solving process. Computation of schedules An extension of algorithm selection is the per-instance algorithm scheduling problem, in which we do not select only one solver, but we select a time budget for each algorithm on a per-instance base. This approach improves the performance of selection systems in particular if the instance features are not very informative and a wrong selection of a single solver is likely. Selection of parallel portfolios Given the increasing importance of parallel computation, an extension of algorithm selection for parallel computation is parallel portfolio selection, in which we select a subset of the algorithms to simultaneously run in a parallel portfolio. External links Algorithm Selection Library (ASlib) Algorithm selection literature Title Algorithmic bias URL https//en.wikipedia.org/wiki/Algorithmic_bias Content Algorithmic bias describes systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one category over another in ways different from the intended function of the algorithm. Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect systematic and unfair discrimination. This bias has only recently been addressed in legal frameworks, such as the European Unions General Data Protection Regulation (proposed 2018) and the Artificial Intelligence Act (proposed 2021, approved 2024). As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations by how features and labels are chosen because of technical limitations of their design or by being used in unanticipated contexts or by audiences who are not considered in the softwares initial design. Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single algorithm to examine, but a network of many interrelated programs and data inputs, even between users of the same service. Definitions Algorithms are difficult to define, but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output. 13 For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence. 14 15 By analyzing and processing data, algorithms are the backbone of search engines, social media websites, recommendation engines, online retail, online advertising, and more. Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithms neutrality. 2  563  294 The term algorithmic bias describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as biased. 332 This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on). Methods Bias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria. 3 Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded. 4 Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers. 8 Other algorithms may reinforce stereotypes and preferences as they process and display relevant data for human users, for example, by selecting information based on previous choices of a similar user or group of users. 6 Beyond assembling and processing data, bias can emerge as a result of design. For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores). 36 Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. These criteria could present unanticipated outcomes for search results, such as with flight-recommendation software that omits flights that do not follow the sponsoring airlines flight paths. Algorithms may also display an uncertainty bias, offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations. 4 History Early critiques The earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book Computer Power and Human Reason, artificial intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded. 149 Weizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs embody law, 40 that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmers imagination of how the world works, including their biases and expectations. 109 While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects human decision making processes as data is being selected. 70, 105 Finally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results. 65 Weizenbaum warned against trusting decisions made by computer programs that a user doesnt understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable. 226 An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. Georges Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with foreign-sounding names based on historical trends in admissions. While many schools at the time employed similar biases in their selection process, St. George was most notable for automating said bias through the use of an algorithm, thus gaining the attention of people on a much wider scale. In recent years, as algorithms increasingly rely on machine learning methods applied to real-world data, algorithmic bias has become more prevalent due to inherent biases within the data itself. For instance, facial recognition systems have been shown to misidentify individuals from marginalized groups at significantly higher rates than white individuals, highlighting how biases in training datasets manifest in deployed systems. A 2018 study by Joy Buolamwini and Timnit Gebru found that commercial facial recognition technologies exhibited error rates of up to 35 when identifying darker-skinned women, compared to less than 1 for lighter-skinned men. Algorithmic biases are not only technical failures but often reflect systemic inequities embedded in historical and societal data. Researchers and critics, such as Cathy ONeil in her book Weapons of Math Destruction (2016), emphasize that these biases can amplify existing social inequalities under the guise of objectivity. ONeil argues that opaque, automated decision-making processes in areas such as credit scoring, predictive policing, and education can reinforce discriminatory practices while appearing neutral or scientific. Contemporary critiques and responses Though well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze. The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program over time these decisions and their collective impact on the programs output may be forgotten. 115 In theory, these biases may create new patterns of behavior, or scripts, in relationship to specific technologies as the code interacts with other elements of society. Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests. 180 The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist, 15 a process described by author Clay Shirky as algorithmic authority. Shirky uses the term to describe the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources, such as search results. This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as trending or popular may be created based on significantly wider criteria than just their popularity. 14 Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans. 16  6 This can have the effect of reducing alternative options, compromises, or flexibility. 16 Sociologist Scott Lash has critiqued algorithms as a new form of generative power, in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors. 71 While blind adherence to algorithmic decisions is a concern, an opposite issue arises when human decision-makers exhibit selective adherence to algorithmic advice. In such cases, individuals accept recommendations that align with their preexisting beliefs and disregard those that do not, thereby perpetuating existing biases and undermining the fairness objectives of algorithmic interventions. Consequently, incorporating fair algorithmic tools into decision-making processes does not automatically eliminate human biases. Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability, and Transparency in Machine Learning. 115 Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences. 117 In recent years, the study of the Fairness, Accountability, and Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT. Critics have suggested that FAT initiatives cannot serve effectively as independent watchdogs when many are funded by corporations building the systems being studied. Types Pre-existing Pre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious. 334  294 Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines. 17 Encoding pre-existing bias into software can preserve social and institutional bias, and, without correction, could be replicated in all future uses of that algorithm. 116  8 An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new British citizens after the 1981 British Nationality Act. 341 The program accurately reflected the tenets of the law, which stated that a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not. 341  375 In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed. 342 Another source of bias, which has been called label choice bias, arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients Solutions to the label choice bias aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program. Machine learning bias Machine learning bias refers to systematic and unfair disparities in the output of machine learning algorithms. These biases can manifest in various ways and are often a reflection of the data used to train these algorithms. Here are some key aspects Language bias Language bias refers a type of statistical sampling bias tied to the language of a query that leads to a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository. Luo et al.s work shows that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like What is liberalism?, ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like opposes state intervention in personal and economic life from the dominant Vietnamese perspective and limitation of government power from the prevalent Chinese perspective are absent. Selection bias Selection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. This bias primarily stems from token bias that is, the model assigns a higher a priori probability to specific answer tokens (such as A ) when generating responses. As a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the model s performance can fluctuate significantly. This phenomenon undermines the reliability of large language models in multiple-choice settings. Gender bias Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. For example, large language models often assign roles and characteristics based on traditional gender norms it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. Stereotyping Beyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways. A recent focus in research has been on the complex interplay between the grammatical properties of a language and real-world biases that can become embedded in AI systems, potentially perpetuating harmful stereotypes and assumptions. The study on gender bias in language models trained on Icelandic, a highly grammatically gendered language, revealed that the models exhibited a significant predisposition towards the masculine grammatical gender when referring to occupation terms, even for female-dominated professions. This suggests the models amplified societal gender biases present in the training data. Political bias Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data. Racial bias Racial bias refers to the tendency of machine learning models to produce outcomes that unfairly discriminate against or stereotype individuals based on race or ethnicity. This bias often stems from training data that reflects historical and systemic inequalities. For example, AI systems used in hiring, law enforcement, or healthcare may disproportionately disadvantage certain racial groups by reinforcing existing stereotypes or underrepresenting them in key areas. Such biases can manifest in ways like facial recognition systems misidentifying individuals of certain racial backgrounds or healthcare algorithms underestimating the medical needs of minority patients. Addressing racial bias requires careful examination of data, improved transparency in algorithmic processes, and efforts to ensure fairness throughout the AI development lifecycle. Technical Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system. 332 Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display. 336 Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list. 332 A decontextualized algorithm uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines. 332 The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the cameras field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime. 574 Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury. 332 Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the students work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection. 21 22 Emergent Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts. 334 Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms. 334, 336 This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion. 179  294 Similarly, problems may emerge when training data (the samples fed to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world. In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP). 338 The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference. 338 Additional emergent biases include Correlations Unpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data. 6 In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care. Unanticipated uses Emergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand. 334 These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society. 179 Apart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the softwares algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of British immigration law. 342 Feedback loops Emergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm. For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public. The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods. The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing. Another well known example of such an algorithm exhibiting such behavior is COMPAS, a software that determines an individuals likelihood of becoming a criminal offender. The software is often criticized for labeling Black individuals as criminals much more likely than others, and then feeds the data back into itself in the event individuals become registered criminals, further enforcing the bias created by the dataset the algorithm is acting on. Recommender systems such as those used to recommend online videos or news articles can create feedback loops. When users click on content that is suggested by algorithms, it influences the next set of suggestions. Over time this may lead to users entering a filter bubble and being unaware of important or useful content. Impact Commercial influences Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment. 2  331 In a 1998 paper describing Google, the founders of the company had adopted a policy of transparency in search results regarding paid placement, arguing that advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers. This bias would be an invisible manipulation of the user. 3 Voting behavior A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20. The researchers concluded that candidates have no means of competing if an algorithm, with or without intent, boosted page listings for a rival candidate. Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20 increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted. Legal scholar Jonathan Zittrain has warned that this could create a digital gerrymandering effect in elections, the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users, if intentionally manipulated. 335 Gender discrimination In 2016, the professional networking site LinkedIn was discovered to recommend male variations of womens names in response to search queries. The site did not make similar recommendations in searches for male names. For example, Andrea would bring up a prompt asking if users meant Andrew, but queries for Andrew did not ask if users meant to find Andrea. The company said this was the result of an analysis of users interactions with the site. In 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners. 94 Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers. 98 Web search algorithms have also been accused of bias. Googles results may prioritize pornographic content in search terms related to sexuality, for example, lesbian. This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, Top 25 Sexiest Women Athletes articles displayed as first-page results in searches for women athletes. 31 In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content. Other examples include the display of higher-paying jobs to male applicants on job search websites. Researchers have also identified that machine translation exhibits a strong tendency towards male defaults. In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations. In fact, current machine translation systems fail to reproduce the real world distribution of female workers. In 2015, Amazon.com turned off an AI system it developed to screen job applications when they realized it was biased against women. The recruitment tool excluded applicants who attended all-womens colleges and resumes that included the word womens. A similar problem emerged with music streaming services In 2019, it was discovered that the recommender system algorithm used by Spotify was biased against women artists. Spotifys song recommendations suggested more male artists over women artists. Racial and ethnic discrimination Algorithms have been criticized as a method for obscuring racial prejudices in decision-making. 158 Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases. For example, black people are likely to receive longer sentences than white people who committed the same crime. This could potentially mean that a system amplifies the original biases in the data. In 2015, Google apologized when a couple of black users complained that an image-identification algorithm in its Photos application identified them as gorillas. In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking. Such examples are the product of bias in biometric data sets. Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points. 154 Speech recognition technology can have different accuracies depending on the users accent. This may be caused by the a lack of training data for speakers of that accent. Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individuals name. A 2015 study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithms model of lung function. In 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about 1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases. A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on creditworthiness which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities. Another study, published in August 2024, on Large language model investigates how language models perpetuate covert racism, particularly through dialect prejudice against speakers of African American English (AAE). It highlights that these models exhibit more negative stereotypes about AAE speakers than any recorded human biases, while their overt stereotypes are more positive. This discrepancy raises concerns about the potential harmful consequences of such biases in decision-making processes. Law enforcement and legal proceedings Algorithms already have numerous applications in legal systems. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants, and that black defendants are twice as likely to be erroneously assigned the label high-risk as white defendants. One example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime. For the time period starting in 1920 and ending in 1970, the nationality of a criminals father was a consideration in those risk assessment scores. 4 Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80 of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77 more often than whites. One study that set out to examine Risk, Race,  Recidivism Predictive Bias and Disparate Impact alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation. In the pretrial detention context, a law review article argues that algorithmic risk assessments violate 14th Amendment Equal Protection rights on the basis of race, since the algorithms are argued to be facially discriminatory, to result in disparate treatment, and to not be narrowly tailored. Online hate speech In 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents. The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing Muslims would be blocked, while posts denouncing Radical Muslims would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the children subset of blacks, rather than all blacks, whereas all white men would trigger a block, because whites and males are not considered subsets. Facebook was also found to allow ad purchasers to target Jew haters as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The companys design also allowed ad buyers to block African-Americans from seeing housing ads. While algorithms are used to track and block hate speech, some were found to be 1.5 times more likely to flag information posted by Black users and 2.2 times likely to flag information as hate speech if written in African American English. Without context for slurs and epithets, even when used by communities which have re-appropriated them, were flagged. Another instance in a study found that 85 out of 100 examined subreddits tended to remove various norm violations, including misogynistic slurs and racist hate speech, highlighting the prevalence of such content in online communities. As platforms like Reddit update their hate speech policies, they must balance free expression with the protection of marginalized communities, emphasizing the need for context-sensitive moderation and nuanced algorithms. Surveillance Surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times. 572 The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender. However, even audits of these image-recognition systems are ethically fraught, and some scholars have suggested the technologys context will always have a disproportionate impact on communities whose actions are over-surveilled. For example, a 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites. 190 A 2018 study found that facial recognition software most likely accurately identified light-skinned (typically European) males, with slightly lower accuracy rates for light-skinned females. Dark-skinned males and females were significanfly less likely to be accurately identified by facial recognition software. These disparities are attributed to the under-representation of darker-skinned participants in data sets used to develop this software. Discrimination against the LGBTQ community In 2011, users of the gay hookup application Grindr reported that the Android stores recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men. In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its adult content blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel Brokeback Mountain. 5 In 2019, it was found that on Facebook, searches for photos of my female friends yielded suggestions such as in bikinis or at the beach. In contrast, searches for photos of my male friends yielded no results. Facial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of Uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans Uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning. Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, an instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy. There has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individuals sexual orientation based on their facial images. The model in the study predicted a correct distinction between gay and straight men 81 of the time, and a correct distinction between gay and straight women 74 of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being outed against their will. Disability discrimination While the modalities of algorithmic fairness have been judged on the basis of different aspects of bias like gender, race and socioeconomic status, disability often is left out of the list. The marginalization people with disabilities currently face in society is being translated into AI systems and algorithms, creating even more exclusion The shifting nature of disabilities and its subjective characterization, makes it more difficult to computationally address. The lack of historical depth in defining disabilities, collecting its incidence and prevalence in questionnaires, and establishing recognition add to the controversy and ambiguity in its quantification and calculations. The definition of disability has been long debated shifting from a medical model to a social model of disability most recently, which establishes that disability is a result of the mismatch between peoples interactions and barriers in their environment, rather than impairments and health conditions. Disabilities can also be situational or temporary, considered in a constant state of flux. Disabilities are incredibly diverse, fall within a large spectrum, and can be unique to each individual. Peoples identity can vary based on the specific types of disability they experience, how they use assistive technologies, and who they support. The high level of variability across peoples experiences greatly personalizes how a disability can manifest. Overlapping identities and intersectional experiences are excluded from statistics and datasets, hence underrepresented and nonexistent in training data. Therefore, machine learning models are trained inequitably and artificial intelligent systems perpetuate more algorithmic bias. For example, if people with speech impairments are not included in training voice control features and smart AI assistants they are unable to use the feature or the responses received from a Google Home or Alexa are extremely poor. Given the stereotypes and stigmas that still exist surrounding disabilities, the sensitive nature of revealing these identifying characteristics also carries vast privacy challenges. As disclosing disability information can be taboo and drive further discrimination against this population, there is a lack of explicit disability data available for algorithmic systems to interact with. People with disabilities face additional harms and risks with respect to their social support, cost of health insurance, workplace discrimination and other basic necessities upon disclosing their disability status. Algorithms are further exacerbating this gap by recreating the biases that already exist in societal systems and structures. Google Search While users generate results that are completed automatically, Google has failed to remove sexist and racist autocompletion text. For example, Algorithms of Oppression How Search Engines Reinforce Racism Safiya Noble notes an example of the search for black girls, which was reported to result in pornographic images. Google claimed it was unable to erase those pages unless they were considered unlawful. Obstacles to research Several problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding. 5 Defining fairness Literature on algorithmic bias has focused on the remedy of fairness, but definitions of fairness are often incompatible with each other and the realities of machine learning optimization. For example, defining fairness as an equality of outcomes may simply refer to a system producing the same result for all people, while fairness defined as equality of treatment might explicitly consider differences between individuals. 2 As a result, fairness is sometimes described as being in conflict with the accuracy of a model, suggesting innate tensions between the priorities of social welfare and the priorities of the vendors designing these systems. 2 In response to this tension, researchers have suggested more care to the design and use of systems that draw on potentially biased algorithms, with fairness defined for specific applications and contexts. Complexity Algorithmic processes are complex, often exceeding the understanding of the people who use them. 2  7 Large-scale operations may not be understood even by those involved in creating them. The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a codes input or output. 183 Social scientist Bruno Latour has identified this process as blackboxing, a process in which scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become. Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones. 92 An example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a users social media feed in 2013. Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms. 118 Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems. 22 Additional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms. 367  7 One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do. 5 Companies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user. 5 Lack of transparency Commercial algorithms are proprietary, and may be treated as trade secrets. 2  7  183 Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings. 366 This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function. 20 Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output. 369 Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes. Lack of data about sensitive categories A significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly considered when collecting and processing data. In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Unions General Data Protection Regulation, such data falls under the special category provisions (Article 9), and therefore comes with more restrictions on potential collection and processing. Some practitioners have tried to estimate and impute these missing sensitive categorizations in order to allow bias mitigation, for example building systems to infer ethnicity from names, however this can introduce other forms of bias if not undertaken with care. Machine learning researchers have drawn upon cryptographic privacy-enhancing technologies such as secure multi-party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext. Algorithmic bias does not only include protected categories, but can also concern characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult. Furthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities. Solutions A study of 84 policy guidelines on ethical AI found that fairness and mitigation of unwanted bias was a common point of concern, and were addressed through a blend of technical solutions, transparency and monitoring, right to remedy and increased oversight, and diversity and inclusion efforts. Technical There have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent fields focus on tools which are typically applied to the (training) data used by the program rather than the algorithms internal processes. These methods may also analyze a programs output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion). Explainable AI to detect algorithm Bias is a suggested way to detect the existence of bias in an algorithm or learning model. Using machine learning to detect bias is called, conducting an AI audit, where the auditor is an algorithm that goes through the AI model and the training data to identify biases. Ensuring that an AI tool such as a classifier is free from bias is more difficult than just removing the sensitive information from its input signals, because this is typically implicit in other signals. For example, the hobbies, sports and schools attended by a job candidate might reveal their gender to the software, even when this is removed from the analysis. Solutions to this problem involve ensuring that the intelligent agent does not have any information that could be used to reconstruct the protected and sensitive information about the subject, as first demonstrated in where a deep learning network was simultaneously trained to learn a task while at the same time being completely agnostic about the protected feature. A simpler method was proposed in the context of word embeddings, and involves removing information that is correlated with the protected characteristic. Currently, a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software  Systems Engineering Standards Committee, a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019. In 2022, the IEEE released a standard aimed at specifying methodologies to help creators of algorithms address issues of bias and promote transparency regarding the function and potential effects of their algorithms. The project, initially approved in February 2017, was sponsored by the Software  Systems Engineering Standards Committee, a committee under the IEEE Computer Society. The standard provides guidelines for articulating transparency to authorities or end users and mitigating algorithmic biases. Transparency and monitoring Ethics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results. Such solutions include the consideration of the right to understanding in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed. Toward this end, a movement for Explainable AI is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias. Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results. An initial approach towards transparency included the open-sourcing of algorithms. Software code can be looked into and improvements can be proposed through source-code-hosting facilities. However, this approach doesnt necessarily produce the intended effects. Companies and organizations can share all possible documentation and code, but this does not establish transparency if the audience doesnt understand the information given. Therefore, the role of an interested critical audience is worth exploring in relation to transparency. Algorithms cannot be held accountable without a critical audience. Right to remedy From a regulatory perspective, the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias. This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes. Others propose the need for clear liability insurance mechanisms. Diversity and inclusion Amid concerns that the design of AI systems is primarily the domain of white, male engineers, a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems. For example, just 12 of machine learning engineers are women, with black AI leaders pointing to a diversity crisis in the field. Groups like Black in AI and Queer in AI are attempting to create more inclusive spaces in the AI community and work against the often harmful desires of corporations that control the trajectory of AI research. Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of intersectionality to the design of algorithms. 4 Researchers at the University of Cambridge have argued that addressing racial diversity is hampered by the whiteness of the culture of AI. Interdisciplinarity and Collaboration Integrating interdisciplinarity and collaboration in developing of AI systems can play a critical role in tackling algorithmic bias. Integrating insights, expertise, and perspectives from disciplines outside of computer science can foster a better understanding of the impact data driven solutions have on society. An example of this in AI research is PACT or Participatory Approach to enable Capabilities in communiTies, a proposed framework for facilitating collaboration when developing AI driven solutions concerned with social impact. This framework identifies guiding principals for stakeholder participation when working on AI for Social Good (AI4SG) projects. PACT attempts to reify the importance of decolonizing and power-shifting efforts in the design of human-centered AI solutions. An academic initiative in this regard is the Stanford Universitys Institute for Human-Centered Artificial Intelligence which aims to foster multidisciplinary collaboration. The mission of the institute is to advance artificial intelligence (AI) research, education, policy and practice to improve the human condition. Collaboration with outside experts and various stakeholders facilitates ethical, inclusive, and accountable development of intelligent systems. It incorporates ethical considerations, understands the social and cultural context, promotes human-centered design, leverages technical expertise, and addresses policy and legal considerations. Collaboration across disciplines is essential to effectively mitigate bias in AI systems and ensure that AI technologies are fair, transparent, and accountable. Regulation Europe The General Data Protection Regulation (GDPR), the European Unions revised data protection regime that was implemented in 2018, addresses Automated individual decision-making, including profiling in Article 22. These rules prohibit solely automated decisions which have a significant or legal effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s. The GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71, noting thatthe controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate ... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect.Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals. While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law, its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes. United States The United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used. Many policies are self-enforced or controlled by the Federal Trade Commission. In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan, which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases. Intended only as guidance, the report did not create any legal precedent. 26 In 2017, New York City passed the first algorithmic accountability bill in the United States. The bill, which went into effect on January 1, 2018, required the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems. The task force is required to present findings and recommendations for further regulatory action in 2019. On February 11, 2019, according to Executive Order 13859, the federal government unveiled the American AI Initiative, a comprehensive strategy to maintain U.S. leadership in artificial intelligence. The initiative highlights the importance of sustained AI research and development, ethical standards, workforce training, and the protection of critical AI technologies. This aligns with broader efforts to ensure transparency, accountability, and innovation in AI systems across public and private sectors. Furthermore, on October 30, 2023, the President signed Executive Order 14110, which emphasizes the safe, secure, and trustworthy development and use of artificial intelligence (AI). The order outlines a coordinated, government-wide approach to harness AIs potential while mitigating its risks, including fraud, discrimination, and national security threats. An important point in the commitment is promoting responsible innovation and collaboration across sectors to ensure that AI benefits society as a whole. With this order, President Joe Biden mandated the federal government to create best practices for companies to optimize AIs benefits and minimize its harms. India On July 31, 2018, a draft of the Personal Data Bill was presented. The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for harm resulting from any processing or any kind of processing undertaken by the fiduciary. It defines any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal or any discriminatory treatment as a source of harm that could arise from improper use of data. It also makes special provisions for people of Intersex status. See also Algorithmic wage discrimination Ethics of artificial intelligence Fairness (machine learning) Hallucination (artificial intelligence) Misaligned goals in artificial intelligence Predictive policing SenseTime References Further reading Baer, Tobias (2019). Understand, Manage, and Prevent Algorithmic Bias A Guide for Business Users and Data Scientists. New York Apress. ISBN 9781484248843. Noble, Safiya Umoja (2018). Algorithms of Oppression How Search Engines Reinforce Racism. New York New York University Press. ISBN 9781479837243. Title Algorithmic inference URL https//en.wikipedia.org/wiki/Algorithmic_inference Content Algorithmic inference gathers new developments in the statistical inference methods made feasible by the powerful computing devices widely available to any data analyst. Cornerstones in this field are computational learning theory, granular computing, bioinformatics, and, long ago, structural probability (Fraser 1966). The main focus is on the algorithms which compute statistics rooting the study of a random phenomenon, along with the amount of data they must feed on to produce reliable results. This shifts the interest of mathematicians from the study of the distribution laws to the functional properties of the statistics, and the interest of computer scientists from the algorithms for processing data to the information they process. The Fisher parametric inference problem Concerning the identification of the parameters of a distribution law, the mature reader may recall lengthy disputes in the mid 20th century about the interpretation of their variability in terms of fiducial distribution (Fisher 1956), structural probabilities (Fraser 1966), priors/posteriors (Ramsey 1925), and so on. From an epistemology viewpoint, this entailed a companion dispute as to the nature of probability is it a physical feature of phenomena to be described through random variables or a way of synthesizing data about a phenomenon? Opting for the latter, Fisher defines a fiducial distribution law of parameters of a given random variable that he deduces from a sample of its specifications. With this law he computes, for instance the probability that (mean of a Gaussian variable omeur note) is less than any assigned value, or the probability that it lies between any assigned values, or, in short, its probability distribution, in the light of the sample observed. The classic solution Fisher fought hard to defend the difference and superiority of his notion of parameter distribution in comparison to analogous notions, such as Bayes posterior distribution, Frasers constructive probability and Neymans confidence intervals. For half a century, Neymans confidence intervals won out for all practical purposes, crediting the phenomenological nature of probability. With this perspective, when you deal with a Gaussian variable, its mean is fixed by the physical features of the phenomenon you are observing, where the observations are random operators, hence the observed values are specifications of a random sample. Because of their randomness, you may compute from the sample specific intervals containing the fixed with a given probability that you denote confidence. Example Let X be a Gaussian variable with parameters displaystyle mu  and 2 displaystyle sigma 2 and  X 1 , , X m  displaystyle X_1,ldots ,X_m a sample drawn from it. Working with statistics  . displaystyle f_T(t)frac Gamma (m/2)Gamma ((m-1)/2)frac 1sqrt pi (m-1)left(1frac t2m-1right)m/2. Gauging T between two quantiles and inverting its expression as a function of displaystyle mu  you obtain confidence intervals for displaystyle mu  . With the sample specification .14 , 6.3 , 3.9 , 6.46 , 0.2 , 2.94 , 4.14 , 4.69 , 6.02 , 1.58  displaystyle mathbf x 7.14,6.3,3.9,6.46,0.2,2.94,4.14,4.69,6.02,1.58 having size .37 displaystyle s_mu 43.37 and s 2  46.07 displaystyle s_sigma 246.07 , and obtain a 0.90 confidence interval for displaystyle mu  with extremes (3.03, 5.65). Inferring functions with the help of a computer From a modeling perspective the entire dispute looks like a chicken-egg dilemma either fixed data by first and probability distribution of their properties as a consequence, or fixed properties by first and probability distribution of the observed data as a corollary. The classic solution has one benefit and one drawback. The former was appreciated particularly back when people still did computations with sheet and pencil. Per se, the task of computing a Neyman confidence interval for the fixed parameter is hard you do not know , but you look for disposing around it an interval with a possibly very low probability of failing. The analytical solution is allowed for a very limited number of theoretical cases. Vice versa a large variety of instances may be quickly solved in an approximate way via the central limit theorem in terms of confidence interval around a Gaussian distribution thats the benefit. The drawback is that the central limit theorem is applicable when the sample size is sufficiently large. Therefore, it is less and less applicable with the sample involved in modern inference instances. The fault is not in the sample size on its own part. Rather, this size is not sufficiently large because of the complexity of the inference problem. With the availability of large computing facilities, scientists refocused from isolated parameters inference to complex functions inference, i.e. re sets of highly nested parameters identifying functions. In these cases we speak about learning of functions (in terms for instance of regression, neuro-fuzzy system or computational learning) on the basis of highly informative samples. A first effect of having a complex structure linking data is the reduction of the number of sample degrees of freedom, i.e. the burning of a part of sample points, so that the effective sample size to be considered in the central limit theorem is too small. Focusing on the sample size ensuring a limited learning error with a given confidence level, the consequence is that the lower bound on this size grows with complexity indices such as VC dimension or detail of a class to which the function we want to learn belongs. Example A sample of 1,000 independent bits is enough to ensure an absolute error of at most 0.081 on the estimation of the parameter p of the underlying Bernoulli variable with a confidence of at least 0.99. The same size cannot guarantee a threshold less than 0.088 with the same confidence 0.99 when the error is identified with the probability that a 20-year-old man living in New York does not fit the ranges of height, weight and waistline observed on 1,000 Big Apple inhabitants. The accuracy shortage occurs because both the VC dimension and the detail of the class of parallelepipeds, among which the one observed from the 1,000 inhabitants ranges falls, are equal to 6. The general inversion problem solving the Fisher question With insufficiently large samples, the approach fixed sample random properties suggests inference procedures in three steps Definition For a random variable and a sample drawn from it a compatible distribution is a distribution having the same sampling mechanism M . Example You may find the distribution law of the Pareto parameters A and K as an implementation example of the population bootstrap method as in the figure on the left. Implementing the twisting argument method, you get the distribution law F M ( ) displaystyle F_M(mu ) of the mean M of a Gaussian variable X on the basis of the statistic s ). Its expression is F M ( )  ( m s M m ) , displaystyle F_M(mu )Phi left(frac mmu -s_Msigma sqrt mright), shown in the figure on the right, where displaystyle Phi  is the cumulative distribution function of a standard normal distribution. Computing a confidence interval for M given its distribution function is straightforward we need only find two quantiles (for instance / 2 displaystyle delta /2 and 1 / 2 displaystyle 1-delta /2 quantiles in case we are interested in a confidence interval of level symmetric in the tails probabilities) as indicated on the left in the diagram showing the behavior of the two bounds for different values of the statistic sm. The Achilles heel of Fishers approach lies in the joint distribution of more than one parameter, say mean and variance of a Gaussian distribution. On the contrary, with the last approach (and above-mentioned methods population bootstrap and twisting argument) we may learn the joint distribution of many parameters. For instance, focusing on the distribution of two or many more parameters, in the figures below we report two confidence regions where the function to be learnt falls with a confidence of 90. The former concerns the probability with which an extended support vector machine attributes a binary label 1 to the points of the ( x , y ) displaystyle (x,y) plane. The two surfaces are drawn on the basis of a set of sample points in turn labelled according to a specific distribution law (Apolloni et al. 2008). The latter concerns the confidence region of the hazard rate of breast cancer recurrence computed from a censored sample (Apolloni, Malchiodi  Gaito 2006). Notes References Fraser, D. A. S. (1966), Structural probability and generalization, Biometrika, 53 (1/2) 1 9, doi10.2307/2334048, JSTOR 2334048. Fisher, M. A. (1956), Statistical Methods and Scientific Inference, Edinburgh and London Oliver and Boyd Apolloni, B. Malchiodi, D. Gaito, S. (2006), Algorithmic Inference in Machine Learning, International Series on Advanced Intelligence, vol. 5 (2nd ed.), Adelaide Magill, Advanced Knowledge International Apolloni, B. Bassis, S. Malchiodi, D. Witold, P. (2008), The Puzzle of Granular Computing, Studies in Computational Intelligence, vol. 138, Berlin Springer, ISBN 9783540798637 Ramsey, F. P. (1925), The Foundations of Mathematics, Proceedings of the London Mathematical Society 338 384, doi10.1112/plms/-25.1.338. Wilks, S.S. (1962), Mathematical Statistics, Wiley Publications in Statistics, New York John Wiley Title Algorithmic party platforms in the United States URL https//en.wikipedia.org/wiki/Algorithmic_party_platforms_in_the_United_States Content Algorithmic party platforms are a recent development in political campaigning where artificial intelligence (AI) and machine learning are used to shape and adjust party messaging dynamically. Unlike traditional platforms that are drafted well before an election, these platforms adapt based on real-time data such as polling results, voter sentiment, and trends on social media. This allows campaigns to remain responsive to emerging issues throughout the election cycle. These platforms rely on predictive analytics to segment voters into smaller, highly specific groups. AI analyzes demographic data, behavioral patterns, and online activities to identify which issues resonate most with each group. Campaigns then tailor their messages accordingly, ensuring that different voter segments receive targeted communication. This approach optimizes resources and enhances voter engagement by focusing on relevant issues. During the 2024 U.S. election, campaigns utilized these tools to adjust messaging on-the-fly. For example, the AI firm Resonate identified a voter segment labeled Cyber Crusaders, consisting of socially conservative yet fiscally liberal individuals. Campaigns used this insight to quickly focus outreach and policy discussions around the concerns of this group, demonstrating how AI-driven platforms can influence strategy as events unfold. Background and relevance in modern campaigns The integration of artificial intelligence (AI) into political campaigns has introduced a significant shift in how party platforms are shaped and communicated. Traditionally, platforms were drafted months before elections and remained static throughout the campaign. However, algorithmic platforms now rely on continuous data streams to adjust messaging and policy priorities in real time. This allows campaigns to adapt to emerging voter concerns, ensuring their strategies remain relevant throughout the election cycle. AI systems analyze large volumes of data, including polling results, social media interactions, and voter behavior patterns. Predictive analytics tools segment voters into specific micro-groups based on demographic and behavioral data. Campaigns can then customize their messaging to align with the priorities of these smaller segments, adjusting their stances as trends develop during the campaign. This level of segmentation and customization ensures that outreach resonates with voters and maximizes engagement. Beyond messaging, AI also optimizes resource allocation by helping campaigns target specific efforts more effectively. With predictive analytics, campaigns can identify which areas or demographics are most likely to benefit from increased outreach, such as canvassing or targeted advertisements. AI tools monitor shifts in voter sentiment in real time, allowing campaigns to quickly pivot their strategies in response to developing events and voter priorities. This capability ensures that campaign resources are used efficiently, minimizing waste while maximizing impact throughout the election cycle. AIs use extends beyond national campaigns, with local and grassroots campaigns also leveraging these technologies to compete more effectively. By automating communication processes and generating customized voter outreach, smaller campaigns can now utilize AI to a degree previously available only to well-funded candidates. However, this growing reliance on AI raises concerns around transparency and the ethical implications of automated content creation, such as AI-generated ads and responses. AI technology, which was previously accessible only to large, well-funded campaigns, has become increasingly available to smaller, local campaigns. With declining costs and easier access, grassroots campaigns now have the ability to implement predictive analytics, automate communications, and generate targeted ads. This democratization of technology allows smaller campaigns to compete more effectively by dynamically adjusting to the concerns of their constituents. However, the growing use of AI in political campaigns raises concerns about transparency and the potential manipulation of voters. The ability to adjust messaging in real time introduces ethical questions about the authenticity of platforms and voter trust. Additionally, the use of synthetic media, including AI-generated ads and deepfakes, presents challenges in maintaining accountability and preventing disinformation in political discourse. Impact on political platforms Artificial intelligence (AI) has become instrumental in enabling political campaigns to adapt their platforms in real time, responding swiftly to evolving voter sentiments and emerging issues. By analyzing extensive datasets including polling results, social media activity, and demographic information AI systems provide campaigns with actionable insights that inform dynamic strategy adjustments. A study by Sanders, Ulinich, and Schneier (2023) demonstrated the potential of AI-based political issue polling, where AI chatbots simulated public opinion on various policy issues. The findings indicated that AI could effectively anticipate both the mean level and distribution of public opinion, particularly in ideological breakdowns, with correlations typically exceeding 85. This suggests that AI can serve as a valuable tool for campaigns to gauge voter sentiment accurately and promptly. Moreover, AI facilitates the segmentation of voters into micro-groups based on demographic and behavioral data, allowing for tailored messaging that resonates with specific audiences. This targeted approach enhances voter engagement and optimizes resource allocation, as campaigns can focus their efforts on demographics most receptive to their messages. The dynamic nature of AI-driven platforms ensures that campaign strategies remain relevant and responsive throughout the election cycle. However, the integration of AI in political platforms also raises ethical and transparency concerns, particularly regarding the authenticity of dynamically adjusted messaging and the potential for voter manipulation. Addressing these challenges is crucial to maintaining voter trust and the integrity of the democratic process. In summary, AI significantly shapes political platforms in real time by providing campaigns with the tools to analyze voter sentiment, segment audiences, and adjust strategies dynamically. While offering substantial benefits in responsiveness and engagement, it is imperative to navigate the accompanying ethical considerations to ensure the responsible use of AI in political campaigning. Ethical and transparency challenges While AI-driven platforms offer significant advantages, they also introduce ethical and transparency challenges. One primary concern is the potential for AI to manipulate voter perception. The ability to adjust messaging dynamically raises questions about the authenticity of political platforms, as voters may feel deceived if they perceive platforms as opportunistic or insincere. The use of synthetic media, including AI-generated advertisements and deepfakes, exacerbates these challenges. These tools have the potential to blur the line between reality and fiction, making it difficult for voters to discern genuine content from fabricated material. This has led to concerns about misinformation, voter manipulation, and the erosion of trust in democratic processes. Additionally, the lack of transparency in how AI systems operate poses significant risks. Many algorithms function as black boxes, with their decision-making processes opaque even to their developers. This opacity makes it challenging to ensure accountability, particularly when AI-generated strategies lead to controversial or unintended outcomes. Efforts to address these challenges include calls for greater transparency in AI usage within campaigns. Policymakers and advocacy groups have proposed regulations requiring campaigns to disclose when AI is used in content creation or voter outreach. These measures aim to balance the benefits of AI with the need for ethical integrity and accountability. Benefits of AI-driven platforms Despite the challenges, AI-driven platforms offer numerous benefits that can enhance the democratic process. By tailoring messaging to specific voter concerns, AI helps campaigns address diverse needs more effectively. This targeted approach ensures that underrepresented groups receive attention, fostering a more inclusive political discourse. AI also democratizes access to advanced campaign tools. Smaller campaigns, which previously lacked the resources to compete with well-funded opponents, can now utilize AI to level the playing field. Predictive analytics, automated communications, and targeted advertisements empower grassroots movements to amplify their voices and engage constituents more effectively. Moreover, AIs ability to process vast amounts of data provides valuable insights into voter sentiment. By identifying trends and patterns, campaigns can address pressing issues proactively, fostering a more informed and responsive political environment. These capabilities also extend to crisis management, as AI enables campaigns to adjust swiftly in response to unforeseen events, ensuring stability and resilience. Title Anomaly detection URL https//en.wikipedia.org/wiki/Anomaly_detection Content In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data. Anomaly detection finds application in many domains including cybersecurity, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers. Three broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as normal and abnormal and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not, the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application. Definition Many attempts have been made in the statistical and computer science communities to define an anomaly. The most prevalent ones include the following, and can be categorised into three groups those that are ambiguous, those that are specific to a method with pre-defined thresholds usually chosen empirically, and those that are formally defined Ill defined An outlier is an observation which deviates so much from the other observations as to arouse suspicions that it was generated by a different mechanism. Anomalies are instances or collections of data that occur very rarely in the data set and whose features differ significantly from most of the data. An outlier is an observation (or subset of observations) which appears to be inconsistent with the remainder of that set of data. An anomaly is a point or collection of points that is relatively distant from other points in multi-dimensional space of features. Anomalies are patterns in data that do not conform to a well-defined notion of normal behaviour. Specific Let T be observations from a univariate Gaussian distribution and O a point from T. Then the z-score for O is greater than a pre-selected threshold if and only if O is an outlier. History Intrusion detection The concept of intrusion detection, a critical component of anomaly detection, has evolved significantly over time. Initially, it was a manual process where system administrators would monitor for unusual activities, such as a vacationing users account being accessed or unexpected printer activity. This approach was not scalable and was soon superseded by the analysis of audit logs and system logs for signs of malicious behavior. By the late 1970s and early 1980s, the analysis of these logs was primarily used retrospectively to investigate incidents, as the volume of data made it impractical for real-time monitoring. The affordability of digital storage eventually led to audit logs being analyzed online, with specialized programs being developed to sift through the data. These programs, however, were typically run during off-peak hours due to their computational intensity. The 1990s brought the advent of real-time intrusion detection systems capable of analyzing audit data as it was generated, allowing for immediate detection of and response to attacks. This marked a significant shift towards proactive intrusion detection. As the field has continued to develop, the focus has shifted to creating solutions that can be efficiently implemented across large and complex network environments, adapting to the ever-growing variety of security threats and the dynamic nature of modern computing infrastructures. Applications Anomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security, intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement. Intrusion detection Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of features proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations. The counterpart of anomaly detection in intrusion detection is misuse detection. Fintech fraud detection Anomaly detection is vital in fintech for fraud prevention. Preprocessing Preprocessing data to remove anomalies can be an important step in data analysis, and is done for a number of reasons. Statistics such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy. Video surveillance Anomaly detection has become increasingly vital in video surveillance to enhance security and safety. With the advent of deep learning technologies, methods using Convolutional Neural Networks (CNNs) and Simple Recurrent Units (SRUs) have shown significant promise in identifying unusual activities or behaviors in video data. These models can process and analyze extensive video feeds in real-time, recognizing patterns that deviate from the norm, which may indicate potential security threats or safety violations. An important aspect for video surveillance is the development of scalable real-time frameworks. Such pipelines are required for processing multiple video streams with low computational resources. IT infrastructure In IT infrastructure management, anomaly detection is crucial for ensuring the smooth operation and reliability of services. Techniques like the IT Infrastructure Library (ITIL) and monitoring frameworks are employed to track and manage system performance and user experience. Detection anomalies can help identify and pre-empt potential performance degradations or system failures, thus maintaining productivity and business process effectiveness. IoT systems Anomaly detection is critical for the security and efficiency of Internet of Things (IoT) systems. It helps in identifying system failures and security breaches in complex networks of IoT devices. The methods must manage real-time data, diverse device types, and scale effectively. Garbe et al. have introduced a multi-stage anomaly detection framework that improves upon traditional methods by incorporating spatial clustering, density-based clustering, and locality-sensitive hashing. This tailored approach is designed to better handle the vast and varied nature of IoT data, thereby enhancing security and operational reliability in smart infrastructure and industrial IoT systems. Petroleum industry Anomaly detection is crucial in the petroleum industry for monitoring critical machinery. Mart et al. used a novel segmentation algorithm to analyze sensor data for real-time anomaly detection. This approach helps promptly identify and address any irregularities in sensor readings, ensuring the reliability and safety of petroleum operations. Oil and gas pipeline monitoring In the oil and gas sector, anomaly detection is not just crucial for maintenance and safety, but also for environmental protection. Aljameel et al. propose an advanced machine learning-based model for detecting minor leaks in oil and gas pipelines, a task traditional methods may miss. Methods Many anomaly detection techniques have been proposed in literature. The performance of methods usually depend on the data sets. For example, some may be suited to detecting local outliers, while others global, and methods have little systematic advantages over another when compared across many data sets. Almost all algorithms also require the setting of non-intuitive parameters critical for performance, and usually unknown before application. Some of the popular techniques are mentioned below and are broken down into categories Statistical Parameter-free Also referred to as frequency-based or counting-based, the simplest non-parametric anomaly detection method is to build a histogram with the training data or a set of known normal instances, and if a test point does not fall in any of the histogram bins mark it as anomalous, or assign an anomaly score to test data based on the height of the bin it falls in. The size of bins are key to the effectiveness of this technique but must be determined by the implementer. A more sophisticated technique uses kernel functions to approximate the distribution of the normal data. Instances in low probability areas of the distribution are then considered anomalies. Parametric-based Z-score, Tukeys range test Grubbss test Density Density-based techniques (k-nearest neighbor, local outlier factor, isolation forests, and many more variations of this concept) Subspace-base (SOD), correlation-based (COP) and tensor-based outlier detection for high-dimensional data One-class support vector machines (OCSVM, SVDD) Neural networks Replicator neural networks, autoencoders, variational autoencoders, long short-term memory neural networks Bayesian networks Hidden Markov models (HMMs) Minimum Covariance Determinant Deep Learning Convolutional Neural Networks (CNNs) CNNs have shown exceptional performance in the unsupervised learning domain for anomaly detection, especially in image and video data analysis. Their ability to automatically and hierarchically learn spatial hierarchies of features from low to high-level patterns makes them particularly suited for detecting visual anomalies. For instance, CNNs can be trained on image datasets to identify atypical patterns indicative of defects or out-of-norm conditions in industrial quality control scenarios. Simple Recurrent Units (SRUs) In time-series data, SRUs, a type of recurrent neural network, have been effectively used for anomaly detection by capturing temporal dependencies and sequence anomalies. Unlike traditional RNNs, SRUs are designed to be faster and more parallelizable, offering a better fit for real-time anomaly detection in complex systems such as dynamic financial markets or predictive maintenance in machinery, where identifying temporal irregularities promptly is crucial. Foundation models Since the advent of large-scale foundation models that have been used successfully on most downstream tasks, they have also been adapted for use in anomaly detection and segmentation. Methods utilizing pretrained foundation models inclue using the alignment of image and text embeddings (CLIP, etc.) for anomaly localization, while others may use the inpainting ability of generative image models for reconstruction-error based anomaly detection. Cluster-based Clustering Cluster analysis-based outlier detection Deviations from association rules and frequent itemsets Fuzzy logic-based outlier detection Ensembles Ensemble techniques, using feature bagging, score normalization and different sources of diversity Others Histogram-based Outlier Score (HBOS) uses value histograms and assumes feature independence for fast predictions. Anomaly detection in dynamic networks Dynamic networks, such as those representing financial systems, social media interactions, and transportation infrastructure, are subject to constant change, making anomaly detection within them a complex task. Unlike static graphs, dynamic networks reflect evolving relationships and states, requiring adaptive techniques for anomaly detection. Types of anomalies in dynamic networks Community anomalies Compression anomalies Decomposition anomalies Distance anomalies Probabilistic model anomalies Explainable anomaly detection Many of the methods discussed above only yield an anomaly score prediction, which often can be explained to users as the point being in a region of low data density (or relatively low density compared to the neighbors densities). In explainable artificial intelligence, the users demand methods with higher explainability. Some methods allow for more detailed explanations The Subspace Outlier Degree (SOD) identifies attributes where a sample is normal, and attributes in which the sample deviates from the expected. Correlation Outlier Probabilities (COP) compute an error vector of how a sample point deviates from an expected location, which can be interpreted as a counterfactual explanation the sample would be normal if it were moved to that location. Software ELKI is an open-source Java data mining toolkit that contains several anomaly detection algorithms, as well as index acceleration for them. PyOD is an open-source Python library developed specifically for anomaly detection. scikit-learn is an open-source Python library that contains some algorithms for unsupervised anomaly detection. Wolfram Mathematica provides functionality for unsupervised anomaly detection across multiple data types Datasets Anomaly detection benchmark data repository with carefully chosen data sets of the Ludwig-Maximilians-Universit t M nchen Mirror Archived 2022-03-31 at the Wayback Machine at University of S o Paulo. ODDS ODDS A large collection of publicly available outlier detection datasets with ground truth in different domains. Unsupervised Anomaly Detection Benchmark at Harvard Dataverse Datasets for Unsupervised Anomaly Detection with ground truth. KMASH Data Repository at Research Data Australia having more than 12,000 anomaly detection datasets with ground truth. See also Change detection Statistical process control Novelty detection Hierarchical temporal memory Title Aporia (company) URL https//en.wikipedia.org/wiki/Aporia_(company) Content Aporia is a machine learning observability platform based in Tel Aviv, Israel. The company has a US office located in San Jose, California. Aporia has developed software for monitoring and controlling undetected defects and failures used by other companies to detect and report anomalies, and warn in the early stages of faults. History Aporia was founded in 2019 by Liran Hason and Alon Gubkin. In April 2021, the company raised a 5 million seed round for its monitoring platform for ML models. In February 2022, the company closed a Series A round of 25 million for its ML observability platform. Aporia was named by Forbes as the Next Billion-Dollar Company in June 2022. In November, the company partnered with ClearML, an MLOPs platform, to improve ML pipeline optimization. In January 2023, Aporia launched Direct Data Connectors, a novel technology allowing organizations to monitor their ML models in minutes (previously the process of integrating ML monitoring into a customer s cloud environment took weeks or more.) DDC (Direct Data Connectors) enables users to connect Aporia to their preferred data source and monitor all of their data at once, without data sampling or data duplication (which is a huge security risk for major organizations. In April 2023, Aporia announced the company partnered with Amazon Web Services (AWS) to provide more reliable ML observability to AWS consumers by deploying Aporias architecture to their AWS environment, this will allow customers to monitor their models in production regardless of platform. Controversies In 2022, Aporia faced significant challenges when a cybersecurity breach exposed sensitive client data stored within its machine learning observability platform. The breach was traced to a vulnerability in Aporia s Direct Data Connectors (DDC), which allowed unauthorized access to integrated data sources. This incident compromised the confidentiality and integrity of data from several high-profile clients, including financial institutions and healthcare providers. Investigations revealed that Aporia had delayed patching the identified vulnerability despite prior warnings from independent security researchers. Title Apprenticeship learning URL https//en.wikipedia.org/wiki/Apprenticeship_learning Content In artificial intelligence, apprenticeship learning (or learning from demonstration or imitation learning) is the process of learning by observing an expert. It can be viewed as a form of supervised learning, where the training dataset consists of task executions by a demonstration teacher. Mapping function approach Mapping methods try to mimic the expert by forming a direct mapping either from states to actions, or from states to reward values. For example, in 2002 researchers used such an approach to teach an AIBO robot basic soccer skills. Inverse reinforcement learning approach Inverse reinforcement learning (IRL) is the process of deriving a reward function from observed behavior. While ordinary reinforcement learning involves using rewards and punishments to learn behavior, in IRL the direction is reversed, and a robot observes a persons behavior to figure out what goal that behavior seems to be trying to achieve. The IRL problem can be defined as Given 1) measurements of an agents behaviour over time, in a variety of circumstances 2) measurements of the sensory inputs to that agent 3) a model of the physical environment (including the agents body) Determine the reward function that the agent is optimizing. IRL researcher Stuart J. Russell proposes that IRL might be used to observe humans and attempt to codify their complex ethical values, in an effort to create ethical robots that might someday know not to cook your cat without needing to be explicitly told. The scenario can be modeled as a cooperative inverse reinforcement learning game, where a person player and a robot player cooperate to secure the persons implicit goals, despite these goals not being explicitly known by either the person nor the robot. In 2017, OpenAI and DeepMind applied deep learning to the cooperative inverse reinforcement learning in simple domains such as Atari games and straightforward robot tasks such as backflips. The human role was limited to answering queries from the robot as to which of two different actions were preferred. The researchers found evidence that the techniques may be economically scalable to modern systems. Apprenticeship via inverse reinforcement learning (AIRP) was developed by in 2004 Pieter Abbeel, Professor in Berkeleys EECS department, and Andrew Ng, Associate Professor in Stanford Universitys Computer Science Department. AIRP deals with Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. AIRP has been used to model reward functions of highly dynamic scenarios where there is no obvious reward function intuitively. Take the task of driving for example, there are many different objectives working simultaneously - such as maintaining safe following distance, a good speed, not changing lanes too often, etc. This task, may seem easy at first glance, but a trivial reward function may not converge to the policy wanted. One domain where AIRP has been used extensively is helicopter control. While simple trajectories can be intuitively derived, complicated tasks like aerobatics for shows has been successful. These include aerobatic maneuvers like - in-place flips, in-place rolls, loops, hurricanes and even auto-rotation landings. This work was developed by Pieter Abbeel, Adam Coates, and Andrew Ng - Autonomous Helicopter Aerobatics through Apprenticeship Learning System model approach System models try to mimic the expert by modeling world dynamics. Plan approach The system learns rules to associate preconditions and postconditions with each action. In one 1994 demonstration, a humanoid learns a generalized plan from only two demonstrations of a repetitive ball collection task. Example Learning from demonstration is often explained from a perspective that the working Robot-control-system is available and the human-demonstrator is using it. And indeed, if the software works, the Human operator takes the robot-arm, makes a move with it, and the robot will reproduce the action later. For example, he teaches the robot-arm how to put a cup under a coffeemaker and press the start-button. In the replay phase, the robot is imitating this behavior 11. But that is not how the system works internally it is only what the audience can observe. In reality, Learning from demonstration is much more complex. One of the first works on learning by robot apprentices (anthropomorphic robots learning by imitation) was Adrian Stoicas PhD thesis in 1995. In 1997, robotics expert Stefan Schaal was working on the Sarcos robot-arm. The goal was simple solve the pendulum swingup task. The robot itself can execute a movement, and as a result, the pendulum is moving. The problem is, that it is unclear what actions will result into which movement. It is an Optimal control-problem which can be described with mathematical formulas but is hard to solve. The idea from Schaal was, not to use a Brute-force solver but record the movements of a human-demonstration. The angle of the pendulum is logged over three seconds at the y-axis. This results into a diagram which produces a pattern. In computer animation, the principle is called spline animation. That means, on the x-axis the time is given, for example 0.5 seconds, 1.0 seconds, 1.5 seconds, while on the y-axis is the variable given. In most cases its the position of an object. In the inverted pendulum it is the angle. The overall task consists of two parts recording the angle over time and reproducing the recorded motion. The reproducing step is surprisingly simple. As an input we know, in which time step which angle the pendulum must have. Bringing the system to a state is called Tracking control or PID control. That means, we have a trajectory over time, and must find control actions to map the system to this trajectory. Other authors call the principle steering behavior , because the aim is to bring a robot to a given line. See also Inverse reinforcement learning Title Artificial intelligence in hiring URL https//en.wikipedia.org/wiki/Artificial_intelligence_in_hiring Content Artificial intelligence can be used to automate aspects of the job recruitment process. Advances in artificial intelligence, such as the advent of machine learning and the growth of big data, enable AI to be utilized to recruit, screen, and predict the success of applicants. Proponents of artificial intelligence in hiring claim it reduces bias, assists with finding qualified candidates, and frees up human resource workers time for other tasks, while opponents worry that AI perpetuates inequalities in the workplace and will eliminate jobs. Despite the potential benefits, the ethical implications of AI in hiring remain a subject of debate, with concerns about algorithmic transparency, accountability, and the need for ongoing oversight to ensure fair and unbiased decision-making throughout the recruitment process. Background Artificial intelligence has fascinated researchers since the term was coined in the mid-1950s. Researchers have identified four main forms of intelligence that AI would need to possess to truly replace humans in the workplace mechanical, analytical, intuitive, and empathetic. Automation follows a predictable progression in which it will first be able to replace the mechanical tasks, then analytical tasks, then intuitive tasks, and finally empathy based tasks. However, full automation is not the only potential outcome of AI advancements. Humans may instead work alongside machines, enhancing the effectiveness of both. In the hiring context, this means that AI has already replaced many basic human resource tasks in recruitment and screening, while freeing up time for human resource workers to do other more creative tasks that can not yet be automated or do not make fiscal sense to automate. It also means that the type of jobs companies are recruiting and hiring form will continue to shift as the skillsets that are most valuable change. Human resources has been identified as one of the ten industries most affected by AI. It is increasingly common for companies to use AI to automate aspects of their hiring process. The hospitality, finance, and tech industries in particular have incorporated AI into their hiring processes to significant extents. Human resources is fundamentally an industry based around making predictions. Human resource specialists must predict which people would make quality candidates for a job, which marketing strategies would get those people to apply, which applicants would make the best employees, what kinds of compensation would get them to accept an offer, what is needed to retain an employee, which employees should be promoted, what a companies staffing needs, among others. AI is particularly adept at prediction because it can analyze huge amounts of data. This enables AI to make insights many humans would miss and find connections between seemingly unrelated data points. This provides value to a company and has made it advantageous to use AI to automate or augment many human resource tasks. Uses Screeners Screeners are tests that allow companies to sift through a large applicant pool and extract applicants that have desirable features. Companies commonly screen through the use of questionnaires, coding tests, interviews, and resume analysis. Artificial Intelligence already plays a major role in the screening process. Resumes can be analyzed using AI for desirable characteristics, such as a certain amount of work experience or a relevant degree. Interviews can then be extended to applicants whose resumes contain these characteristics. What factors are used to screen applicants is a concern to ethicists and civil rights activists. A screener that favors people who have similar characteristics to those already employed at a company may perpetuate inequalities. For example, if a company that is predominantly white and male uses its employees data to train its screener it may accidentally create a screening process that favors white, male applicants. The automation of screeners also has the potential to reduce biases. Biases against applicants with African American sounding names have been shown in multiple studies. An AI screener has the potential to limit human bias and error in the hiring process, allowing more minority applicants to be successful. Recruitment Recruitment involves the identification of potential applicants and the marketing of positions. AI is commonly utilized in the recruitment process because it can help boost the number of qualified applicants for positions. Companies are able to use AI to target their marketing to applicants who are likely to be good fits for a position. This often involves the use of social media sites advertising tools, which rely on AI. Facebook allows advertisers to target ads based on demographics, location, interests, behavior, and connections. Facebook also allows companies to target a look-a-like audience, that is the company supplies Facebook with a data set, typically the companys current employees, and Facebook will target the ad to profiles that are similar to the profiles in the data set. Additionally, job sites like Indeed, Glassdoor, and ZipRecruiter target job listings to applicants that have certain characteristics employers are looking for. Targeted advertising has many advantages for companies trying to recruit such being a more efficient use of resources, reaching a desired audience, and boosting qualified applicants. This has helped make it a mainstay in modern hiring. Who receives a targeted ad can be controversial. In hiring, the implications of targeted ads have to do with who is able to find out about and then apply to a position. Most targeted ad algorithms are proprietary information. Some platforms, like Facebook and Google, allow users to see why they were shown a specific ad, but users who do not receive the ad likely never know of its existence and also have no way of knowing why they were not shown the ad. Interviews Chatbots were one of the first applications of AI and are commonly used in the hiring process. Interviewees interact with chatbots to answer interview questions, and their responses can then be analyzed by AI, providing prospective employers with a myriad of insights. Chatbots streamline the interview process and reduce the workload of human resource professionals. Video interviews utilizing AI have become increasingly prevalent. Zappyhire, a recruitment automation startup, has developed a recruitment bot that ensures engagement with the most relevant candidates by leveraging AI-powered resume screening technology. HireVue has created technology that analyzes interviewees responses and gestures during recorded video interviews. Over 12 million interviewees have been screened by the more than 700 companies that utilize the service. Controversies Artificial intelligence in hiring confers many benefits, but it also has some challenges which have concerned experts. AI is only as good as the data it is using. Biases can inadvertently be baked into the data used in AI. Often companies will use data from their employees to decide what people to recruit or hire. This can perpetuate bias and lead to more homogenous workforces. Facebook Ads was an example of a platform that created such controversy for allowing business owners to specify what type of employee they are looking for. For example, job advertisements for nursing and teach could be set such that only women of a specific age group would see the advertisements. Facebook Ads has since then removed this function from its platform, citing the potential problems with the function in perpetuating biases and stereotypes against minorities. The growing use of Artificial Intelligence-enabled hiring systems has become an important component of modern talent hiring, particularly through social networks such as LinkedIn and Facebook. However, data overflow embedded in the hiring systems, based on Natural Language Processing (NLP) methods, may result in unconscious gender bias. Utilizing data driven methods may mitigate some bias generated from these systems It can also be hard to quantify what makes a good employee. This poses a challenge for training AI to predict which employees will be best. Commonly used metrics like performance reviews can be subjective and have been shown to favor white employees over black employees and men over women. Another challenge is the limited amount of available data. Employers only collect certain details about candidates during the initial stages of the hiring process. This requires AI to make determinations about candidates with very limited information to go off of. Additionally, many employers do not hire employees frequently and so have limited firm specific data to go off. To combat this, many firms will use algorithms and data from other firms in their industry. AIs reliance on applicant and current employees personal data raises privacy issues. These issues effect both the applicants and current employees, but also may have implications for third parties who are linked through social media to applicants or current employees. For example, a sweep of someones social media will also show their friends and people they have tagged in photos or posts. AI makes it easier for companies to search applicants social media accounts. A study conducted by Monash University found that 45 of hiring managers use social media to gain insight on applicants. Seventy percent of those surveyed said they had rejected an applicant because of things discovered on their applicants social media, yet only 17 of hiring managers saw using social media in the hiring process as a violation of applicants privacy. Using social media in the hiring process is appealing to hiring managers because it offers them a less curated view of applicants lives. The privacy trade-off is significant. Social media profiles often reveal information about applicants that human resource departments are legally not allowed to require applicants to divulge like race, ability status, and sexual orientation. AI and the future of hiring Artificial intelligence is changing the recruiting process by gradually replacing routine tasks performed by human recruiters. AI can reduce human involvement in hiring and reduce the human biases that hinder effective hiring decisions. AI is changing the way work is done. Artificial intelligence along with other technological advances such as improvements in robotics have placed 47 of jobs at risk of being eliminated in the near future. Some classify the shifts in labor brought about by AI as a 4th industrial revolution, which they call Industrial Revolution 4.0. According to some scholars, however, the transformative impact of AI on labor has been overstated. The no-real-change theory holds that an IT revolution has already occurred, but that the benefits of implementing new technologies does not outweigh the costs associated with adopting them. This theory claims that the result of the IT revolution is thus much less impactful than had originally been forecasted. Other scholars refute this theory claiming that AI has already led to significant job loss for unskilled labor and that it will eliminate middle skill and high skill jobs in the future. This position is based around the idea that AI is not yet a technology of general use and that any potential 4th industrial revolution has not fully occurred. A third theory holds that the effect of AI and other technological advances is too complicated to yet be understood. This theory is centered around the idea that while AI will likely eliminate jobs in the short term it will also likely increase the demand for other jobs. The question then becomes will the new jobs be accessible to people and will they emerge near when jobs are eliminated. Although robots can replace people to complete some tasks, there are still many tasks that cannot be done alone by robots that master artificial intelligence. A study analyzed 2,000 work tasks in 800 different occupations globally, and concluded that half (totaling US15 trillion in salaries) could be automated by adapting already existing technologies. Less than 5 of occupations could be fully automated and 60 have at least 30 automatable tasks. In other words, in most cases, artificial intelligence is a tool rather than a substitute for labor. As artificial intelligence enters the field of human work, people have gradually discovered that artificial intelligence is incapable of unique tasks, and the advantage of human beings is to understand uniqueness and use tools rationally. At this time, human-machine reciprocal work came into being. Brand o discovers that people can form organic partnerships with machines. Humans enable machines to do what they do best doing repetitive tasks, analyzing significant volumes of data, and dealing with routine cases. Due to reciprocity, machines enable humans to have their potentialities strengthened for tasks such as resolving ambiguous information, exercising the judgment of difficult cases, and contacting dissatisfied clients. Daugherty and Wilson have observed successful new types of human-computer interaction in occupations and tasks in various fields. In other words, even in activities and capabilities that are considered simpler, new technologies will not pose an imminent danger to workers. As far as General Electric is concerned, buyers of it and its equipment will always need maintenance workers. Entrepreneurs need these workers to work well with new systems that can integrate their skills with advanced technologies in novel ways. Artificial intelligence has sped up the hiring process considerably, dramatically reducing costs. For example, Unilever has reviewed over 250,000 applications using AI and reduced its hiring process from 4 months to 4 weeks. This saved the company 50,000 hours of labor. The increased efficiency AI promises has sped up its adoption by human resource departments globally. Regulations on AI in hiring The Artificial Intelligence Video Interview Act, effective in Illinois since 2020, regulates the use of AI to analyze and evaluate job applicants video interviews. This law requires employers to follow guidelines to avoid any issues regarding using AI in the hiring process. Title Astrostatistics URL https//en.wikipedia.org/wiki/Astrostatistics Content Astrostatistics is a discipline which spans astrophysics, statistical analysis and data mining. It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory. Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference. The field is closely related to astroinformatics. Title Attention (machine learning) URL https//en.wikipedia.org/wiki/Attention_(machine_learning) Content Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence. In natural language processing, importance is represented by soft weights assigned to each word in a sentence. More generally, attention encodes vectors called token embeddings across a fixed-width sequence that can range from tens to millions of tokens in size. Unlike hard weights, which are computed during the backwards training pass, soft weights exist only in the forward pass and therefore change with every step of the input. Earlier designs implemented the attention mechanism in a serial recurrent neural network (RNN) language translation system, but a more recent design, namely the transformer, removed the slower sequential RNN and relied more heavily on the faster parallel attention scheme. Inspired by ideas about attention in humans, the attention mechanism was developed to address the weaknesses of leveraging information from the hidden layers of recurrent neural networks. Recurrent neural networks favor more recent information contained in words at the end of a sentence, while information earlier in the sentence tends to be attenuated. Attention allows a token equal access to any part of a sentence directly, rather than only through the previous state. History Academic reviews of the history of the attention mechanism are provided in Niu et al. and Soydaner. Predecessors Selective attention in humans had been well studied in neuroscience and cognitive psychology. In 1953, Colin Cherry studied selective attention in the context of audition, known as the cocktail party effect. In 1958, Donald Broadbent proposed the filter model of attention. Selective attention of vision was studied in the 1960s by George Sperlings partial report paradigm. It was also noticed that saccade control is modulated by cognitive processes, insofar as the eye moves preferentially towards areas of high salience. As the fovea of the eye is small, the eye cannot sharply resolve the entire visual field at once. The use of saccade control allows the eye to quickly scan important features of a scene. These research developments inspired algorithms such as the Neocognitron and its variants. Meanwhile, developments in neural networks had inspired circuit models of biological visual attention. One well-cited network from 1998, for example, was inspired by the low-level primate visual system. It produced saliency maps of images using handcrafted (not learned) features, which were then used to guide a second neural network in processing patches of the image in order of reducing saliency. A key aspect of attention mechanism can be written (schematically) as i ( query ) i , ( key ) i ( value ) i displaystyle sum _ilangle (textquery)_i,(textkey)_irangle (textvalue)_i where the angled brackets denote dot product. This shows that it involves a multiplicative operation. Multiplicative operations within artificial neural networks had been studied under the names of Group Method of Data Handling (1965) (where Kolmogorov-Gabor polynomials implement multiplicative units or gates), higher-order neural networks, multiplication units, sigma-pi units, fast weight controllers, and hyper-networks. In fast weight controller (Schmidhuber, 1992), one of its two networks has fast weights or dynamic links (1981). A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries. This was later shown to be equivalent to the unnormalized linear Transformer. A follow-up paper developed a similar system with active weight changing. Recurrent attention During the deep learning era, attention mechanism was developed to solve similar problems in encoding-decoding. In machine translation, the seq2seq model, as it was proposed in 2014, would encode an input text into a fixed-length vector, which would then be decoded into an output text. If the input text is long, the fixed-length vector would be unable to carry enough information for accurate decoding. An attention mechanism was proposed to solve this problem. An image captioning model was proposed in 2015, citing inspiration from the seq2seq model. that would encode an input image into a fixed-length vector. Xu et al (2015), citing Bahdanau et al (2014), applied the attention mechanism as used in the seq2seq model to image captioning. Transformer One problem with seq2seq models was their use of recurrent neural networks, which are not parallelizable as both the encoder and the decoder must process the sequence token-by-token. Decomposable attention attempted to solve this problem by processing the input sequence in parallel, before computing a soft alignment matrix (alignment is the terminology used by Bahdanau et al) in order to allow for parallel processing. The idea of using the attention mechanism for self-attention, instead of in an encoder-decoder (cross-attention), was also proposed during this period, such as in differentiable neural computers and neural Turing machines. It was termed intra-attention where an LSTM is augmented with a memory network as it encodes an input sequence. These strands of development were brought together in 2017 with the Transformer architecture, published in the Attention Is All You Need paper. Overview The attention network was designed to identify high correlations patterns amongst words in a given sentence, assuming that it has learned word correlation patterns from the training data. This correlation is captured as neuronal weights learned during training with backpropagation. This attention scheme has been compared to the Query-Key analogy of relational databases. That comparison suggests an asymmetric role for the Query and Key vectors, where one item of interest (the Query vector that) is matched against all possible items (the Key vectors of each word in the sentence). However, both Self and Cross Attentions parallel calculations matches all tokens of the K matrix with all tokens of the Q matrix therefore the roles of these vectors are symmetric. Possibly because the simplistic database analogy is flawed, much effort has gone into understand Attention further by studying their roles in focused settings, such as in-context learning, masked language tasks, stripped down transformers, bigram statistics, N-gram statistics, pairwise convolutions, and arithmetic factoring. Machine translation The seq2seq method developed in the early 2010s uses two neural networks an encoder network converts an input sentence into numerical vectors, and a decoder network converts those vectors to sentences in the target language. The Attention mechanism was grafted onto this structure in 2014 and shown below. Later it was refined into the Transformer design (2017). Interpreting Attention weights In translating between languages, alignment is the process of matching words from the source sentence to words of the translated sentence. In the I love you example above, the second word love is aligned with the third word aime. Stacking soft row vectors together for je, t, and aime yields an alignment matrix Sometimes, alignment can be multiple-to-multiple. For example, the English phrase look it up corresponds to cherchez-le. Thus, soft attention weights work better than hard attention weights (setting one attention weight to 1, and the others to 0), as we would like the model to make a context vector consisting of a weighted sum of the hidden vectors, rather than the best one, as there may not be a best hidden vector. This view of the attention weights addresses some of the neural network explainability problem. Networks that perform verbatim translation without regard to word order would show the highest scores along the (dominant) diagonal of the matrix. The off-diagonal dominance shows that the attention mechanism is more nuanced. On the first pass through the decoder, 94 of the attention weight is on the first English word I, so the network offers the word je. On the second pass of the decoder, 88 of the attention weight is on the third English word you, so it offers t. On the last pass, 95 of the attention weight is on the second English word love, so it offers aime. seq2seq Problem statement Consider the seq2seq language English-to-French translation task. To be concrete, let us consider the translation of the zone of international control end, which should translate to la zone de contr le international end. Here, we use the special end token as a control character to delimit the end of input for both the encoder and the decoder. An input sequence of text x 0 , x 1 , displaystyle x_0,x_1,dots  is processed by a neural network (which can be an LSTM, a Transformer encoder, or some other network) into a sequence of real-valued vectors h 0 , h 1 , displaystyle h_0,h_1,dots  , where h displaystyle h stands for hidden vector. After the encoder has finished processing, the decoder starts operating over the hidden vectors, to produce an output sequence y 0 , y 1 , displaystyle y_0,y_1,dots  , autoregressively. That is, it always takes as input both the hidden vectors produced by the encoder, and what the decoder itself has produced before, to produce the next output word ( h 0 , h 1 , displaystyle h_0,h_1,dots  , start) la ( h 0 , h 1 , displaystyle h_0,h_1,dots  , start la) la zone ( h 0 , h 1 , displaystyle h_0,h_1,dots  , start la zone) la zone de ... ( h 0 , h 1 , displaystyle h_0,h_1,dots  , start la zone de contr le international) la zone de contr le international end Here, we use the special start token as a control character to delimit the start of input for the decoder. The decoding terminates as soon as end appears in the decoder output. Attention weights As hand-crafting weights defeats the purpose of machine learning, the model must compute the attention weights on its own. Taking analogy from the language of database queries, we make the model construct a triple of vectors key, query, and value. The rough idea is that we have a database in the form of a list of key-value pairs. The decoder sends in a query, and obtains a reply in the form of a weighted sum of the values, where the weight is proportional to how closely the query resembles each key. The decoder first processes the start input partially, to obtain an intermediate vector h 0 d displaystyle h_0d , the 0th hidden vector of decoder. Then, the intermediate vector is transformed by a linear map W Q displaystyle WQ into a query vector q 0  h 0 d W Q displaystyle q_0h_0dWQ . Meanwhile, the hidden vectors outputted by the encoder are transformed by another linear map W K displaystyle WK into key vectors k 0  h 0 W K , k 1  h 1 W K , displaystyle k_0h_0WK,k_1h_1WK,dots  . The linear maps are useful for providing the model with enough freedom to find the best way to represent the data. Now, the query and keys are compared by taking dot products q 0 k 0 T , q 0 k 1 T , displaystyle q_0k_0T,q_0k_1T,dots  . Ideally, the model should have learned to compute the keys and values, such that q 0 k 0 T displaystyle q_0k_0T is large, q 0 k 1 T displaystyle q_0k_1T is small, and the rest are very small. This can be interpreted as saying that the attention weight should be mostly applied to the 0th hidden vector of the encoder, a little to the 1st, and essentially none to the rest. In order to make a properly weighted sum, we need to transform this list of dot products into a probability distribution over 0 , 1 , displaystyle 0,1,dots  . This can be accomplished by the softmax function, thus giving us the attention weights ( w 00 , w 01 , )  s o f t m a x ( q 0 k 0 T , q 0 k 1 T , ) displaystyle (w_00,w_01,dots )mathrm softmax (q_0k_0T,q_0k_1T,dots ) This is then used to compute the context vector c 0  w 00 v 0  w 01 v 1  displaystyle c_0w_00v_0w_01v_1cdots  where v 0  h 0 W V , v 1  h 1 W V , displaystyle v_0h_0WV,v_1h_1WV,dots  are the value vectors, linearly transformed by another matrix to provide the model with freedom to find the best way to represent values. Without the matrices W Q , W K , W V displaystyle WQ,WK,WV , the model would be forced to use the same hidden vector for both key and value, which might not be appropriate, as these two tasks are not the same.This is the dot-attention mechanism. The particular version described in this section is decoder cross-attention, as the output context vector is used by the decoder, and the input keys and values come from the encoder, but the query comes from the decoder, thus cross-attention. More succinctly, we can write it as c 0  A t t e n t i o n ( h 0 d W Q , H W K , H W V )  s o f t m a x ( ( h 0 d W Q ) ( H W K ) T ) ( H W V ) displaystyle c_0mathrm Attention (h_0dWQ,HWK,HWV)mathrm softmax ((h_0dWQ)(HWK)T)(HWV) where the matrix H displaystyle H is the matrix whose rows are h 0 , h 1 , displaystyle h_0,h_1,dots  . Note that the querying vector, h 0 d displaystyle h_0d , is not necessarily the same as the key-value vector h 0 displaystyle h_0 . In fact, it is theoretically possible for query, key, and value vectors to all be different, though that is rarely done in practice. Variants Many variants of attention implement soft weights, such as fast weight programmers, or fast weight controllers (1992). A slow neural network outputs the fast weights of another neural network through outer products. The slow network learns by gradient descent. It was later renamed as linearized self-attention. Bahdanau-style attention, also referred to as additive attention, Luong-style attention, which is known as multiplicative attention, highly parallelizable self-attention introduced in 2016 as decomposable attention and successfully used in transformers a year later, positional attention and factorized positional attention. For convolutional neural networks, attention mechanisms can be distinguished by the dimension on which they operate, namely spatial attention, channel attention, or combinations. Much effort has gone into understand Attention further by studying their roles in focused settings, such as in-context learning, masked language tasks, stripped down transformers, bigram statistics, N-gram statistics, pairwise convolutions, and arithmetic factoring. These variants recombine the encoder-side inputs to redistribute those effects to each target output. Often, a correlation-style matrix of dot products provides the re-weighting coefficients. In the figures below, W is the matrix of context attention weights, similar to the formula in Core Calculations section above. Self-attention Self-attention is essentially the same as cross-attention, except that query, key, and value vectors all come from the same model. Both encoder and decoder can use self-attention, but with subtle differences. For encoder self-attention, we can start with a simple encoder without self-attention, such as an embedding layer, which simply converts each input word into a vector by a fixed lookup table. This gives a sequence of hidden vectors h 0 , h 1 , displaystyle h_0,h_1,dots  . These can then be applied to a dot-product attention mechanism, to obtain h 0  A t t e n t i o n ( h 0 W Q , H W K , H W V ) h 1  A t t e n t i o n ( h 1 W Q , H W K , H W V ) displaystyle beginalignedh_0mathrm Attention (h_0WQ,HWK,HWV)h_1mathrm Attention (h_1WQ,HWK,HWV)cdots endaligned or more succinctly, ) . This can be applied repeatedly, to obtain a multilayered encoder. This is the encoder self-attention, sometimes called the all-to-all attention, as the vector at every position can attend to every other. Masking For decoder self-attention, all-to-all attention is inappropriate, because during the autoregressive decoding process, the decoder cannot attend to future outputs that has yet to be decoded. This can be solved by forcing the attention weights w i . This attention mechanism is the causally masked self-attention. Optimizations Flash attention The size of the attention matrix is proportional to the square of the number of input tokens. Therefore, when the input is long, calculating the attention matrix requires a lot of GPU memory. Flash attention is an implementation that reduces the memory needs and increases efficiency without sacrificing accuracy. It achieves this by partitioning the attention computation into smaller blocks that fit into the GPUs faster on-chip memory, reducing the need to store large intermediate matrices and thus lowering memory usage while increasing computational efficiency. Mathematical representation Standard Scaled Dot-Product Attention For matrices Q R m d k , K R n d k displaystyle mathbf Q in mathbb Rmtimes d_k ,mathbf K in mathbb Rntimes d_k  and V R n d v displaystyle mathbf V in mathbb Rntimes d_v  , the scaled dot-product, or QKV attention is defined as Attention ( Q , K , V )  softmax ( Q K T d k ) V R m d v displaystyle textAttention(mathbf Q ,mathbf K ,mathbf V )textsoftmaxleft(frac mathbf Q mathbf K Tsqrt d_kright)mathbf V in mathbb R mtimes d_v where T displaystyle T denotes transpose and the softmax function is applied independently to every row of its argument. The matrix Q displaystyle mathbf Q  contains m displaystyle m queries, while matrices K , V displaystyle mathbf K ,mathbf V  jointly contain an unordered set of n displaystyle n key-value pairs. Value vectors in matrix V displaystyle mathbf V  are weighted using the weights resulting from the softmax operation, so that the rows of the m displaystyle m -by- d v displaystyle d_v output matrix are confined to the convex hull of the points in R d v displaystyle mathbb R d_v given by the rows of V displaystyle mathbf V  . To understand the permutation invariance and permutation equivariance properties of QKV attention, let A R m m displaystyle mathbf A in mathbb R mtimes m and B R n n displaystyle mathbf B in mathbb R ntimes n be permutation matrices and D R m n displaystyle mathbf D in mathbb R mtimes n an arbitrary matrix. The softmax function is permutation equivariant in the sense that softmax ( A D B )  A softmax ( D ) B displaystyle textsoftmax(mathbf A mathbf D mathbf B )mathbf A ,textsoftmax(mathbf D )mathbf B  By noting that the transpose of a permutation matrix is also its inverse, it follows that Attention ( A Q , B K , B V )  A Attention ( Q , K , V ) displaystyle textAttention(mathbf A mathbf Q ,mathbf B mathbf K ,mathbf B mathbf V )mathbf A ,textAttention(mathbf Q ,mathbf K ,mathbf V ) which shows that QKV attention is equivariant with respect to re-ordering the queries (rows of Q displaystyle mathbf Q  ) and invariant to re-ordering of the key-value pairs in K , V displaystyle mathbf K ,mathbf V  . These properties are inherited when applying linear transforms to the inputs and outputs of QKV attention blocks. For example, a simple self-attention function defined as X Attention ( X T q , X T k , X T v ) displaystyle mathbf X mapsto textAttention(mathbf X mathbf T _q,mathbf X mathbf T _k,mathbf X mathbf T _v) is permutation equivariant with respect to re-ordering the rows of the input matrix X displaystyle X in a non-trivial way, because every row of the output is a function of all the rows of the input. Similar properties hold for multi-head attention, which is defined below. Masked Attention When QKV attention is used as a building block for an autoregressive decoder, and when at training time all input and output matrices have n displaystyle n rows, a masked attention variant is used Attention ( Q , K , V )  softmax ( Q K T d k  M ) V displaystyle textAttention(mathbf Q ,mathbf K ,mathbf V )textsoftmaxleft(frac mathbf Q mathbf K Tsqrt d_kmathbf M right)mathbf V  where the mask, M R n n displaystyle mathbf M in mathbb R ntimes n is a strictly upper triangular matrix, with zeros on and below the diagonal and displaystyle -infty  in every element above the diagonal. The softmax output, also in R n n displaystyle mathbb R ntimes n is then lower triangular, with zeros in all elements above the diagonal. The masking ensures that for all 1 i  j n displaystyle 1leq ijleq n , row i displaystyle i of the attention output is independent of row j displaystyle j of any of the three input matrices. The permutation invariance and equivariance properties of standard QKV attention do not hold for the masked variant. Multi-Head Attention Multi-head attention MultiHead ( Q , K , V )  Concat ( head 1 , . . . , head h ) W O displaystyle textMultiHead(mathbf Q ,mathbf K ,mathbf V )textConcat(texthead_1,...,texthead_h)mathbf W O where each head is computed with QKV attention as head . The permutation properties of (standard, unmasked) QKV attention apply here also. For permutation matrices, A , B displaystyle mathbf A ,mathbf B   MultiHead ( A Q , B K , B V )  A MultiHead ( Q , K , V ) displaystyle textMultiHead(mathbf A mathbf Q ,mathbf B mathbf K ,mathbf B mathbf V )mathbf A ,textMultiHead(mathbf Q ,mathbf K ,mathbf V ) from which we also see that multi-head self-attention X MultiHead ( X T q , X T k , X T v ) displaystyle mathbf X mapsto textMultiHead(mathbf X mathbf T _q,mathbf X mathbf T _k,mathbf X mathbf T _v) is equivariant with respect to re-ordering of the rows of input matrix X displaystyle X . Bahdanau (Additive) Attention Attention ( Q , K , V )  softmax ( tanh ( W Q Q  W K K ) V ) displaystyle textAttention(mathbf Q ,mathbf K ,mathbf V )textsoftmax(tanh(mathbf W _Qmathbf Q mathbf W _Kmathbf K )mathbf V ) where W Q displaystyle mathbf W _Q and W K displaystyle mathbf W _K are learnable weight matrices. Luong Attention (General) Attention ( Q , K , V )  softmax ( Q W K T ) V displaystyle textAttention(mathbf Q ,mathbf K ,mathbf V )textsoftmax(mathbf Q mathbf W mathbf K T)mathbf V  where W displaystyle mathbf W  is a learnable weight matrix. See also Recurrent neural network seq2seq Transformer (deep learning architecture) Attention Dynamic neural network References External links Olah, Chris Carter, Shan (September 8, 2016). Attention and Augmented Recurrent Neural Networks. Distill. 1 (9). Distill Working Group. doi10.23915/distill.00001. Dan Jurafsky and James H. Martin (2022) Speech and Language Processing (3rd ed. draft, January 2022), ch. 10.4 Attention and ch. 9.7 Self-Attention Networks Transformers Alex Graves (4 May 2020), Attention and Memory in Deep Learning (video lecture), DeepMind / UCL, via YouTube Title Audio inpainting URL https//en.wikipedia.org/wiki/Audio_inpainting Content Audio inpainting (also known as audio interpolation) is an audio restoration task which deals with the reconstruction of missing or corrupted portions of a digital audio signal. Inpainting techniques are employed when parts of the audio have been lost due to various factors such as transmission errors, data corruption or errors during recording. The goal of audio inpainting is to fill in the gaps (i.e., the missing portions) in the audio signal seamlessly, making the reconstructed portions indistinguishable from the original content and avoiding the introduction of audible distortions or alterations. Many techniques have been proposed to solve the audio inpainting problem and this is usually achieved by analyzing the temporal and spectral information surrounding each missing portion of the considered audio signal. Classic methods employ statistical models or digital signal processing algorithms to predict and synthesize the missing or damaged sections. Recent solutions, instead, take advantage of deep learning models, thanks to the growing trend of exploiting data-driven methods in the context of audio restoration. Depending on the extent of the lost information, the inpainting task can be divided in three categories. Short inpainting refers to the reconstruction of few milliseconds (approximately less than 10) of missing signal, that occurs in the case of short distortions such as clicks or clipping. In this case, the goal of the reconstruction is to recover the lost information exactly. In long inpainting instead, with gaps in the order of hundreds of milliseconds or even seconds, this goal becomes unrealistic, since restoration techniques cannot rely on local information. Therefore, besides providing a coherent reconstruction, the algorithms need to generate new information that has to be semantically compatible with the surrounding context (i.e., the audio signal surrounding the gaps). The case of medium duration gaps lays between short and long inpainting. It refers to the reconstruction of tens of millisecond of missing data, a scale where the non-stationary characteristic of audio already becomes important. Definition Consider a digital audio signal x displaystyle mathbf x  . A corrupted version of x displaystyle mathbf x  , which is the audio signal presenting missing gaps to be reconstructed, can be defined as x   m x displaystyle mathbf tilde x mathbf m circ mathbf x  , where m displaystyle mathbf m  is a binary mask encoding the reliable or missing samples of x displaystyle mathbf x  , and displaystyle circ  represents the element-wise product. Audio inpainting aims at finding x  displaystyle mathbf hat x  (i.e., the reconstruction), which is an estimation of x displaystyle mathbf x  . This is an ill-posed inverse problem, which is characterized by a non-unique set of solutions. For this reason, similarly to the formulation used for the inpainting problem in other domains, the reconstructed audio signal can be found through an optimization problem that is formally expressed as x   argmin X  L ( m x  , x  )  R ( x  ) displaystyle mathbf hat x underset hat mathbf X textargminL(mathbf m circ mathbf hat x ,mathbf tilde x )R(mathbf hat x ) . In particular, x  displaystyle mathbf hat x  is the optimal reconstructed audio signal and L displaystyle L is a distance measure term that computes the reconstruction accuracy between the corrupted audio signal and the estimated one. For example, this term can be expressed with a mean squared error or similar metrics. Since L displaystyle L is computed only on the reliable frames, there are many solutions that can minimize L ( m x  , x  ) displaystyle L(mathbf m circ mathbf hat x ,mathbf tilde x ) . It is thus necessary to add a constraint to the minimization, in order to restrict the results only to the valid solutions. This is expressed through the regularization term R displaystyle R that is computed on the reconstructed audio signal x  displaystyle mathbf hat x  . This term encodes some kind of a-priori information on the audio data. For example, R displaystyle R can express assumptions on the stationarity of the signal, on the sparsity of its representation or can be learned from data. Techniques There exist various techniques to perform audio inpainting. These can vary significantly, influenced by factors such as the specific application requirements, the length of the gaps and the available data. In the literature, these techniques are broadly divided in model-based techniques (sometimes also referred as signal processing techniques) and data-driven techniques. Model-based techniques Model-based techniques involve the exploitation of mathematical models or assumptions about the underlying structure of the audio signal. These models can be based on prior knowledge of the audio content or statistical properties observed in the data. By leveraging these models, missing or corrupted portions of the audio signal can be inferred or estimated. An example of a model-based techniques are autoregressive models. These methods interpolate or extrapolate the missing samples based on the neighboring values, by using mathematical functions to approximate the missing data. In particular, in autoregressive models the missing samples are completed through linear prediction. The autoregressive coefficients necessary for this prediction are learned from the surrounding audio data, specifically from the data adjacent to each gap. Some more recent techniques approach audio inpainting by representing audio signals as sparse linear combinations of a limited number of basis functions (as for example in the Short Time Fourier Transform). In this context, the aim is to find the sparse representation of the missing section of the signal that most accurately matches the surrounding, unaffected signal. The aforementioned methods exhibit optimal performance when applied to filling in relatively short gaps, lasting only a few tens of milliseconds, and thus they can be included in the context of short inpainting. However, these signal-processing techniques tend to struggle when dealing with longer gaps. The reason behind this limitation lies in the violation of the stationarity condition, as the signal often undergoes significant changes after the gap, making it substantially different from the signal preceding the gap. As a way to overcome these limitations, some approaches add strong assumptions also about the fundamental structure of the gap itself, exploiting sinusoidal modeling or similarity graphs to perform inpainting of longer missing portions of audio signals. Data-driven techniques Data-driven techniques rely on the analysis and exploitation of the available audio data. These techniques often employ deep learning algorithms that learn patterns and relationships directly from the provided data. They involve training models on large datasets of audio examples, allowing them to capture the statistical regularities present in the audio signals. Once trained, these models can be used to generate missing portions of the audio signal based on the learned representations, without being restricted by stationarity assumptions. Data-driven techniques also offer the advantage of adaptability and flexibility, as they can learn from diverse audio datasets and potentially handle complex inpainting scenarios. As of today, such techniques constitute the state-of-the-art of audio inpainting, being able to reconstruct gaps of hundreds of milliseconds or even seconds. These performances are made possible by the use of generative models that have the capability to generate novel content to fill in the missing portions. For example, generative adversarial networks, which are the state-of-the-art of generative models in many areas, rely on two competing neural networks trained simultaneously in a two-player minmax game the generator produces new data from samples of a random variable, the discriminator attempts to distinguish between generated and real data. During the training, the generators objective is to fool the discriminator, while the discriminator attempts to learn to better classify real and fake data. In GAN-based inpaniting methods the generator acts as a context encoder and produces a plausible completion for the gap only given the available information surrounding it. The discriminator is used to train the generator and tests the consistency of the produced inpainted audio. Recently, also diffusion models have established themselves as the state-of-the-art of generative models in many fields, often beating even GAN-based solutions. For this reason they have also been used to solve the audio inpainting problem, obtaining valid results. These models generate new data instances by inverting the diffusion process, where data samples are progressively transformed into Gaussian noise. One drawback of generative models is that they typically need a huge amount of training data. This is necessary to make the network generalize well and make it able to produce coherent audio information, that also presents some kind of structural complexity. Nonetheless, some works demonstrated that, capturing the essence of an audio signal is also possible using only a few tens of seconds from a single training sample. This is done by overfitting a generative neural network to a single training audio signal. In this way, researchers were able to perform audio inpainting without exploiting large datasets. Applications Audio inpainting finds applications in a wide range of fields, including audio restoration and audio forensics among the others. In these fields, audio inpainting can be used to eliminate noise, glitches, or undesired distortions from an audio recording, thus enhancing its quality and intelligibility. It can also be employed to recover deteriorated old recordings that have been affected by local modifications or have missing audio samples due to scratches on CDs. Audio inpainting is also closely related to packet loss concealment (PLC). In the PLC problem, it is necessary to compensate the loss of audio packets in communication networks. While both problems aim at filling missing gaps in an audio signal, PLC has more computation time restrictions and only the packets preceding a gap are considered to be reliable (the process is said to be causal). See also Audio forensics Audio restoration Image inpainting Packet loss concealment Title Automated decision-making URL https//en.wikipedia.org/wiki/Automated_decision-making Content Automated decision-making (ADM) involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM involves large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, machine learning, natural language processing, artificial intelligence, augmented intelligence and robotics. The increasing use of automated decision-making systems (ADMS) across a range of contexts presents many benefits and challenges to human society requiring consideration of the technical, legal, ethical, societal, educational, economic and health consequences. Overview There are different definitions of ADM based on the level of automation involved. Some definitions suggests ADM involves decisions made through purely technological means without human input, such as the EUs General Data Protection Regulation (Article 22). However, ADM technologies and applications can take many forms ranging from decision-support systems that make recommendations for human decision-makers to act on, sometimes known as augmented intelligence or shared decision-making, to fully automated decision-making processes that make decisions on behalf of individuals or organizations without human involvement. Models used in automated decision-making systems can be as simple as checklists and decision trees through to artificial intelligence and deep neural networks (DNN). Since the 1950s computers have gone from being able to do basic processing to having the capacity to undertake complex, ambiguous and highly skilled tasks such as image and speech recognition, gameplay, scientific and medical analysis and inferencing across multiple data sources. ADM is now being increasingly deployed across all sectors of society and many diverse domains from entertainment to transport. An ADM system (ADMS) may involve multiple decision points, data sets, and technologies (ADMT) and may sit within a larger administrative or technical system such as a criminal justice system or business process. Data Automated decision-making involves using data as input to be analyzed within a process, model, or algorithm or for learning and generating new models. ADM systems may use and connect a wide range of data types and sources depending on the goals and contexts of the system, for example, sensor data for self-driving cars and robotics, identity data for security systems, demographic and financial data for public administration, medical records in health, criminal records in law. This can sometimes involve vast amounts of data and computing power. Data quality The quality of the available data and its ability to be used in ADM systems is fundamental to the outcomes. It is often highly problematic for many reasons. Datasets are often highly variable corporations or governments may control large-scale data, restricted for privacy or security reasons, incomplete, biased, limited in terms of time or coverage, measuring and describing terms in different ways, and many other issues. For machines to learn from data, large corpora are often required, which can be challenging to obtain or compute however, where available, they have provided significant breakthroughs, for example, in diagnosing chest X-rays. ADM technologies Automated decision-making technologies (ADMT) are software-coded digital tools that automate the translation of input data to output data, contributing to the function of automated decision-making systems. There are a wide range of technologies in use across ADM applications and systems. ADMTs involving basic computational operations Search (includes 1-2-1, 1-2-many, data matching/merge) Matching (two different things) Mathematical Calculation (formula) ADMTs for assessment and grouping User profiling Recommender systems Clustering Classification Feature learning Predictive analytics (includes forecasting) ADMTs relating to space and flows Social network analysis (includes link prediction) Mapping Routing ADMTs for processing of complex data formats Image processing Audio processing Natural Language Processing (NLP) Other ADMT Business rules management systems Time series analysis Anomaly detection Modelling/Simulation Machine learning Machine learning (ML) involves training computer programs through exposure to large data sets and examples to learn from experience and solve problems. Machine learning can be used to generate and analyse data as well as make algorithmic calculations and has been applied to image and speech recognition, translations, text, data and simulations. While machine learning has been around for some time, it is becoming increasingly powerful due to recent breakthroughs in training deep neural networks (DNNs), and dramatic increases in data storage capacity and computational power with GPU coprocessors and cloud computing. Machine learning systems based on foundation models run on deep neural networks and use pattern matching to train a single huge system on large amounts of general data such as text and images. Early models tended to start from scratch for each new problem however since the early 2020s many are able to be adapted to new problems. Examples of these technologies include Open AIs DALL-E (an image creation program) and their various GPT language models, and Googles PaLM language model program. Applications ADM is being used to replace or augment human decision-making by both public and private-sector organisations for a range of reasons including to help increase consistency, improve efficiency, reduce costs and enable new solutions to complex problems. Debate Research and development are underway into uses of technology to assess argument quality, assess argumentative essays and judge debates. Potential applications of these argument technologies span education and society. Scenarios to consider, in these regards, include those involving the assessment and evaluation of conversational, mathematical, scientific, interpretive, legal, and political argumentation and debate. Law In legal systems around the world, algorithmic tools such as risk assessment instruments (RAI), are being used to supplement or replace the human judgment of judges, civil servants and police officers in many contexts. In the United States RAI are being used to generate scores to predict the risk of recidivism in pre-trial detention and sentencing decisions, evaluate parole for prisoners and to predict hot spots for future crime. These scores may result in automatic effects or may be used to inform decisions made by officials within the justice system. In Canada ADM has been used since 2014 to automate certain activities conducted by immigration officials and to support the evaluation of some immigrant and visitor applications. Economics Automated decision-making systems are used in certain computer programs to create buy and sell orders related to specific financial transactions and automatically submit the orders in the international markets. Computer programs can automatically generate orders based on predefined set of rules using trading strategies which are based on technical analyses, advanced statistical and mathematical computations, or inputs from other electronic sources. Business Continuous auditing Continuous auditing uses advanced analytical tools to automate auditing processes. It can be utilized in the private sector by business enterprises and in the public sector by governmental organizations and municipalities. As artificial intelligence and machine learning continue to advance, accountants and auditors may make use of increasingly sophisticated algorithms which make decisions such as those involving determining what is anomalous, whether to notify personnel, and how to prioritize those tasks assigned to personnel. Media and entertainment Digital media, entertainment platforms, and information services increasingly provide content to audiences via automated recommender systems based on demographic information, previous selections, collaborative filtering or content-based filtering. This includes music and video platforms, publishing, health information, product databases and search engines. Many recommender systems also provide some agency to users in accepting recommendations and incorporate data-driven algorithmic feedback loops based on the actions of the system user. Large-scale machine learning language models and image creation programs being developed by companies such as OpenAI and Google in the 2020s have restricted access however they are likely to have widespread application in fields such as advertising, copywriting, stock imagery and graphic design as well as other fields such as journalism and law. Advertising Online advertising is closely integrated with many digital media platforms, websites and search engines and often involves automated delivery of display advertisements in diverse formats. Programmatic online advertising involves automating the sale and delivery of digital advertising on websites and platforms via software rather than direct human decision-making. This is sometimes known as the waterfall model which involves a sequence of steps across various systems and players publishers and data management platforms, user data, ad servers and their delivery data, inventory management systems, ad traders and ad exchanges. There are various issues with this system including lack of transparency for advertisers, unverifiable metrics, lack of control over ad venues, audience tracking and privacy concerns. Internet users who dislike ads have adopted counter measures such as ad blocking technologies which allow users to automatically filter unwanted advertising from websites and some internet applications. In 2017, 24 of Australian internet users had ad blockers. Health Deep learning AI image models are being used for reviewing x-rays and detecting the eye condition macular degeneration. Social services Governments have been implementing digital technologies to provide more efficient administration and social services since the early 2000s, often referred to as e-government. Many governments around the world are now using automated, algorithmic systems for profiling and targeting policies and services including algorithmic policing based on risks, surveillance sorting of people such as airport screening, providing services based on risk profiles in child protection, providing employment services and governing the unemployed. A significant application of ADM in social services relates to the use of predictive analytics eg predictions of risks to children from abuse/neglect in child protection, predictions of recidivism or crime in policing and criminal justice, predictions of welfare/tax fraud in compliance systems, predictions of long term unemployment in employment services. Historically these systems were based on standard statistical analyses, however from the early 2000s machine learning has increasingly been developed and deployed. Key issues with the use of ADM in social services include bias, fairness, accountability and explainability which refers to transparency around the reasons for a decision and the ability to explain the basis on which a machine made a decision. For example Australias federal social security delivery agency, Centrelink, developed and implemented an automated processes for detecting and collecting debt which led to many cases of wrongful debt collection in what became known as the RoboDebt scheme. Transport and mobility Connected and automated mobility (CAM) involves autonomous vehicles such as self-driving cars and other forms of transport which use automated decision-making systems to replace various aspects of human control of the vehicle. This can range from level 0 (complete human driving) to level 5 (completely autonomous). At level 5 the machine is able to make decisions to control the vehicle based on data models and geospatial mapping and real-time sensors and processing of the environment. Cars with levels 1 to 3 are already available on the market in 2021. In 2016 The German government established an Ethics Commission on Automated and Connected Driving which recommended connected and automated vehicles (CAVs) be developed if the systems cause fewer accidents than human drivers (positive balance of risk). It also provided 20 ethical rules for the adaptation of automated and connected driving. In 2020 the European Commission strategy on CAMs recommended that they be adopted in Europe to reduce road fatalities and lower emissions however self-driving cars also raise many policy, security and legal issues in terms of liability and ethical decision-making in the case of accidents, as well as privacy issues. Issues of trust in autonomous vehicles and community concerns about their safety are key factors to be addressed if AVs are to be widely adopted. Surveillance Automated digital data collections via sensors, cameras, online transactions and social media have significantly expanded the scope, scale, and goals of surveillance practices and institutions in government and commercial sectors. As a result there has been a major shift from targeted monitoring of suspects to the ability to monitor entire populations. The level of surveillance now possible as a result of automated data collection has been described as surveillance capitalism or surveillance economy to indicate the way digital media involves large-scale tracking and accumulation of data on every interaction. Ethical and legal issues There are many social, ethical and legal implications of automated decision-making systems. Concerns raised include lack of transparency and contestability of decisions, incursions on privacy and surveillance, exacerbating systemic bias and inequality due to data and algorithmic bias, intellectual property rights, the spread of misinformation via media platforms, administrative discrimination, risk and responsibility, unemployment and many others. As ADM becomes more ubiquitous there is greater need to address the ethical challenges to ensure good governance in information societies. ADM systems are often based on machine learning and algorithms which are not easily able to be viewed or analysed, leading to concerns that they are black box systems which are not transparent or accountable. A report from Citizen lab in Canada argues for a critical human rights analysis of the application of ADM in various areas to ensure the use of automated decision-making does not result in infringements on rights, including the rights to equality and non-discrimination freedom of movement, expression, religion, and association privacy rights and the rights to life, liberty, and security of the person. Legislative responses to ADM include The European General Data Protection Regulation (GDPR), introduced in 2016, is a regulation in EU law on data protection and privacy in the European Union (EU). Article 22(1) enshrines the right of data subjects not to be subject to decisions, which have legal or other significant effects, being based solely on automatic individual decision making. GDPR also includes some rules on the right to explanation however the exact scope and nature of these is currently subject to pending review by the Court of Justice of the European Union. These provisions were not first introduced in the GDPR, but have been present in a similar form across Europe since the Data Protection Directive in 1995, and the 1978 French law, the loi informatique et libert s. Similarly scoped and worded provisions with varying attached rights and obligations are present in the data protection laws of many other jurisdictions across the world, including Uganda, Morocco and the US state of Virginia. Rights for the explanation of public sector automated decisions forming algorithmic treatment under the French loi pour une R publique num rique. Bias ADM may incorporate algorithmic bias arising from Data sources, where data inputs are biased in their collection or selection Technical design of the algorithm, for example where assumptions have been made about how a person will behave Emergent bias, where the application of ADM in unanticipated circumstances creates a biased outcome Explainability Questions of biased or incorrect data or algorithms and concerns that some ADMs are black box technologies, closed to human scrutiny or interrogation, has led to what is referred to as the issue of explainability, or the right to an explanation of automated decisions and AI. This is also known as Explainable AI (XAI), or Interpretable AI, in which the results of the solution can be analysed and understood by humans. XAI algorithms are considered to follow three principles - transparency, interpretability and explainability. Information asymmetry Automated decision-making may increase the information asymmetry between individuals whose data feeds into the system and the platforms and decision-making systems capable of inferring information from that data. On the other hand it has been observed that in financial trading the information asymmetry between two artificial intelligent agents may be much less than between two human agents or between human and machine agents. A research validated Daniel Kahnemans theory on noisy decisions by human experts in finance. It demonstrates the inherent inconsistencies in human judgments, which consequently affect the outcomes of automated decisions made by AI decision-support systems. Research fields Many academic disciplines and fields are increasingly turning their attention to the development, application and implications of ADM including business, computer sciences, human computer interaction (HCI), law, public administration, and media and communications. The automation of media content and algorithmically driven news, video and other content via search systems and platforms is a major focus of academic research in media studies. The ACM Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include ADM and AI. Key research centres investigating ADM include Algorithm Watch, Germany ARC Centre of Excellence for Automated Decision-Making and Society, Australia Citizen Lab, Canada Informatics Europe See also Automated decision support Algorithmic bias Decision-making software Decision Management Ethics of artificial intelligence Government by algorithm Machine learning Recommender systems Title Automated machine learning URL https//en.wikipedia.org/wiki/Automated_machine_learning Content Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML. AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search. Comparison to the standard approach In a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. Each of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively. AutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction. Targets of automation Automated machine learning can target various stages of the machine learning process. Steps to automate are Data preparation and ingestion (from raw data and miscellaneous formats) Column type detection e.g., Boolean, discrete numerical, continuous numerical, or text Column intent detection e.g., target/label, stratification field, numerical feature, categorical text feature, or free text feature Task detection e.g., binary classification, regression, clustering, or ranking Feature engineering Feature selection Feature extraction Meta-learning and transfer learning Detection and handling of skewed data and/or missing values Model selection - choosing which machine learning algorithm to use, often including multiple competing software implementations Ensembling - a form of consensus where using multiple models often gives better results than any single model Hyperparameter optimization of the learning algorithm and featurization Neural architecture search Pipeline selection under time, memory, and complexity constraints Selection of evaluation metrics and validation procedures Problem checking Leakage detection Misconfiguration detection Analysis of obtained results Creating user interfaces and visualizations Challenges and Limitations There are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as development as a cottage industry. This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, its the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the machines. However, experts and developers must help create and guide these machines to prepare them for their own learning. To create this system, it requires labor intensive work with knowledge of machine learning algorithms and system design. Additionally, some other challenges include meta-learning challenges and computational resource allocation. See also Artificial intelligence Artificial intelligence and elections Neural architecture search Neuroevolution Self-tuning Neural Network Intelligence ModelOps Hyperparameter optimization References Further reading Open Source AutoML Tools AutoGluon, TransmogrifAI, Auto-sklearn, and NNI. Bizety. 2020-06-16. Ferreira, Lu s, et al. A comparison of AutoML tools for machine learning, deep learning and XGBoost. 2021 International Joint Conference on Neural Networks (IJCNN). IEEE, 2021. https//repositorium.sdum.uminho.pt/bitstream/1822/74125/1/automl_ijcnn.pdf Feurer, M., Klein, A., Eggensperger, K., Springenberg, J., Blum, M.,  Hutter, F. (2015). Efficient and robust automated machine learning. Advances in neural information processing systems, 28. https//proceedings.neurips.cc/paper_files/paper/2015/file/11d0e6287202fced83f79975ec59a3a6-Paper.pdf Title Automation in construction URL https//en.wikipedia.org/wiki/Automation_in_construction Content Automation in construction is the combination of methods, processes, and systems that allow for greater machine autonomy in construction activities. Construction automation may have multiple goals, including but not limited to, reducing jobsite injuries, decreasing activity completion times, and assisting with quality control and quality assurance. Some systems may be fielded as a direct response to increasing skilled labor shortages in some countries. Opponents claim that increased automation may lead to less construction jobs and that software leaves heavy equipment vulnerable to hackers. Research insights on this subject are today published in several jurnals such as Automation in Construction by Elsevier. Transportation Construction Kratos Defense  Security Solutions fielded the world s first Autonomous Truck-Mounted Attenuator (ATMA) in 2017, in conjunction with Royal Truck  Equipment. Uses of Automation in Construction Equipment control and management Automation can be used to control and monitor construction equipment, such as cranes, excavators, and bulldozers. Material handling Automated systems can be used to handle, transport, and place materials such as concrete, bricks, and stones. Surveying Automated survey equipment and drones can be used to collect and analyze data on construction sites. Quality control Automated systems can be used to monitor and control the quality of materials and construction processes. Safety management Automated systems can be used to monitor and control safety conditions on construction sites. Scheduling and planning Automated systems can be used to manage schedules, resources, and costs. Waste management Automated systems can be used to manage and dispose of waste materials generated during construction. 3D printing Automated 3D printing can be used to create prototypes, models, and even full-scale building components. Benefits of Automation in Construction The use of automation in construction has become increasingly prevalent in recent years due to its numerous benefits. Automation in construction refers to the use of machinery, software, and other technologies to perform tasks that were previously done manually by workers. One of the most significant benefits of automation in construction is increased productivity. Automation can help speed up construction processes, reduce project completion times, and improve overall efficiency. For example, using automated machinery for tasks such as concrete pouring, bricklaying, and welding can significantly increase the speed and accuracy of these tasks, allowing for more work to be completed in a shorter amount of time. Another benefit of automation in construction is improved safety. By automating tasks that are hazardous to workers, such as demolition or working at height, companies can reduce the risk of accidents and injuries on site. Automation can also help to reduce worker fatigue, which can be a significant factor in accidents and mistakes. Overall, the use of automation in construction can improve productivity, reduce costs, increase safety, and improve the quality of construction projects. As technology continues to advance, the use of automation is likely to become even more prevalent in the construction industry. Title Bag-of-words model URL https//en.wikipedia.org/wiki/Bag-of-words_model Content The bag-of-words model (BoW) is a model of text which uses an unordered collection (a bag) of words. It is used in natural language processing and information retrieval (IR). It disregards word order (and thus most of syntax or grammar) but captures multiplicity. The bag-of-words model is commonly used in methods of document classification where, for example, the (frequency of) occurrence of each word is used as a feature for training a classifier. It has also been used for computer vision. An early reference to bag of words in a linguistic context can be found in Zellig Harriss 1954 article on Distributional Structure. Definition The following models a text document using bag-of-words. Here are two simple text documents Based on these two text documents, a list is constructed as follows for each document Representing each bag-of-words as a JSON object, and attributing to the respective JavaScript variable Each key is the word, and each value is the number of occurrences of that word in the given text document. The order of elements is free, so, for example too1,Mary1,movies2,John1,watch1,likes2,to1 is also equivalent to . It is also what we expect from a strict JSON object representation. Note if another document is like a union of these two, its JavaScript representation will be So, as we see in the bag algebra, the union of two documents in the bags-of-words representation is, formally, the disjoint union, summing the multiplicities of each element. Word order The BoW representation of a text removes all word ordering. For example, the BoW representation of man bites dog and dog bites man are the same, so any algorithm that operates with a BoW representation of text must treat them in the same way. Despite this lack of syntax or grammar, BoW representation is fast and may be sufficient for simple tasks that do not require word order. For instance, for document classification, if the words stocks trade investors appears multiple times, then the text is likely a financial report, even though it would be insufficient to distinguish between Yesterday, investors were rallying, but today, they are retreating.andYesterday, investors were retreating, but today, they are rallying.and so the BoW representation would be insufficient to determine the detailed meaning of the document. Implementations Implementations of the bag-of-words model might involve using frequencies of words in a document to represent its contents. The frequencies can be normalized by the inverse of document frequency, or tf idf. Additionally, for the specific purpose of classification, supervised alternatives have been developed to account for the class label of a document. Lastly, binary (presence/absence or 1/0) weighting is used in place of frequencies for some problems (e.g., this option is implemented in the WEKA machine learning software system). Python implementation Hashing trick A common alternative to using dictionaries is the hashing trick, where words are mapped directly to indices with a hashing function. Thus, no memory is required to store a dictionary. Hash collisions are typically dealt via freed-up memory to increase the number of hash buckets. In practice, hashing simplifies the implementation of bag-of-words models and improves scalability. See also Additive smoothing Feature extraction Machine learning MinHash Vector space model w-shingling Notes References McTear, Michael (et al) (2016). The Conversational Interface. Springer International Publishing. Title Ball tree URL https//en.wikipedia.org/wiki/Ball_tree Content In computer science, a ball tree, balltree or metric tree, is a space partitioning data structure for organizing points in a multi-dimensional space. A ball tree partitions data points into a nested set of balls. The resulting data structure has characteristics that make it useful for a number of applications, most notably nearest neighbor search. Informal description A ball tree is a binary tree in which every node defines a D-dimensional ball containing a subset of the points to be searched. Each internal node of the tree partitions the data points into two disjoint sets which are associated with different balls. While the balls themselves may intersect, each point is assigned to one or the other ball in the partition according to its distance from the balls center. Each leaf node in the tree defines a ball and enumerates all data points inside that ball. Each node in the tree defines the smallest ball that contains all data points in its subtree. This gives rise to the useful property that, for a given test point t outside the ball, the distance to any point in a ball B in the tree is greater than or equal to the distance from t to the surface of the ball. Formally D B ( t )   max (  t B.pivot  B.radius , D B.parent ) , if B R o o t max (  t B.pivot  B.radius , 0 ) , if .pivot-textit B.radius,Dtextit B.parent),textif Bneq Rootmax(t-textit B.pivot-textit B.radius,0),textif . Ball-trees are related to the M-tree, but only support binary splits, whereas in the M-tree each level splits m displaystyle m to 2 m displaystyle 2m fold, thus leading to a shallower tree structure, therefore need fewer distance computations, which usually yields faster queries. Furthermore, M-trees can better be stored on disk, which is organized in pages. The M-tree also keeps the distances from the parent node precomputed to speed up queries. Vantage-point trees are also similar, but they binary split into one ball, and the remaining data, instead of using two balls. Construction A number of ball tree construction algorithms are available. The goal of such an algorithm is to produce a tree that will efficiently support queries of the desired type (e.g. nearest-neighbor) in the average case. The specific criteria of an ideal tree will depend on the type of question being answered and the distribution of the underlying data. However, a generally applicable measure of an efficient tree is one that minimizes the total volume of its internal nodes. Given the varied distributions of real-world data sets, this is a difficult task, but there are several heuristics that partition the data well in practice. In general, there is a tradeoff between the cost of constructing a tree and the efficiency achieved by this metric. This section briefly describes the simplest of these algorithms. A more in-depth discussion of five algorithms was given by Stephen Omohundro. k-d construction algorithm The simplest such procedure is termed the k-d Construction Algorithm, by analogy with the process used to construct k-d trees. This is an offline algorithm, that is, an algorithm that operates on the entire data set at once. The tree is built top-down by recursively splitting the data points into two sets. Splits are chosen along the single dimension with the greatest spread of points, with the sets partitioned by the median value of all points along that dimension. Finding the split for each internal node requires linear time in the number of samples contained in that node, yielding an algorithm with time complexity O ( n log n ) displaystyle O(n,log ,n) , where n is the number of data points. Pseudocode function construct_balltree is input D, an array of data points. output B, the root of a constructed ball tree. if a single point remains then create a leaf B containing the single point in D return B else let c be the dimension of greatest spread let p be the central point selected considering c let L, R be the sets of points lying to the left and right of the median along dimension c create B with two children B.pivot  p B.  construct_balltree(L), B.  construct_balltree(R), let B.radius be maximum distance from p among children return B end if end function Nearest-neighbor search An important application of ball trees is expediting nearest neighbor search queries, in which the objective is to find the k points in the tree that are closest to a given test point by some distance metric (e.g. Euclidean distance). A simple search algorithm, sometimes called , exploits the distance property of the ball tree. In particular, if the algorithm is searching the data structure with a test point t, and has already seen some point p that is closest to t among the points encountered so far, then any subtree whose ball is further from t than p can be ignored for the rest of the search. Description The ball tree nearest-neighbor algorithm examines nodes in depth-first order, starting at the root. During the search, the algorithm maintains a max-first priority queue (often implemented with a heap), denoted Q here, of the k nearest points encountered so far. At each node B, it may perform one of three operations, before finally returning an updated version of the priority queue If the distance from the test point t to the current node B is greater than the furthest point in Q, ignore B and return Q. If B is a leaf node, scan through every point enumerated in B and update the nearest-neighbor queue appropriately. Return the updated queue. If B is an internal node, call the algorithm recursively on Bs two children, searching the child whose center is closer to t first. Return the queue after each of these calls has updated it in turn. Performing the recursive search in the order described in point 3 above increases likelihood that the further child will be pruned entirely during the search. Pseudocode function knn_search is input t, the target point for the query k, the number of nearest neighbors of t to search for Q, max-first priority queue containing at most k points B, a node, or ball, in the tree output Q, containing the k nearest neighbors from within B if distance(t, B.pivot) - B.radius distance(t, Q.first) then return Q unchanged else if B is a leaf node then for each point p in B do if distance(t, p)  distance(t, Q.first) then add p to Q if size(Q)  k then remove the furthest neighbor from Q end if end if repeat else let be the child node closest to t let be the child node furthest from t knn_search(t, k, Q, ) knn_search(t, k, Q, ) end if return Q end function Performance In comparison with several other data structures, ball trees have been shown to perform fairly well on the nearest-neighbor search problem, particularly as their number of dimensions grows. However, the best nearest-neighbor data structure for a given application will depend on the dimensionality, number of data points, and underlying structure of the data. Title Base rate URL https//en.wikipedia.org/wiki/Base_rate Content In probability and statistics, the base rate (also known as prior probabilities) is the class of probabilities unconditional on featural evidence (likelihoods). It is the proportion of individuals in a population who have a certain characteristic or trait. For example, if 1 of the population were medical professionals, and remaining 99 were not medical professionals, then the base rate of medical professionals is 1. The method for integrating base rates and featural evidence is given by Bayes rule. In the sciences, including medicine, the base rate is critical for comparison. In medicine a treatments effectiveness is clear when the base rate is available. For example, if the control group, using no treatment at all, had their own base rate of 1/20 recoveries within 1 day and a treatment had a 1/100 base rate of recovery within 1 day, we see that the treatment actively decreases the recovery. The base rate is an important concept in statistical inference, particularly in Bayesian statistics. In Bayesian analysis, the base rate is combined with the observed data to update our belief about the probability of the characteristic or trait of interest. The updated probability is known as the posterior probability and is denoted as P(AB), where B represents the observed data. For example, suppose we are interested in estimating the prevalence of a disease in a population. The base rate would be the proportion of individuals in the population who have the disease. If we observe a positive test result for a particular individual, we can use Bayesian analysis to update our belief about the probability that the individual has the disease. The updated probability would be a combination of the base rate and the likelihood of the test result given the disease status. The base rate is also important in decision-making, particularly in situations where the cost of false positives and false negatives are different. For example, in medical testing, a false negative (failing to diagnose a disease) could be much more costly than a false positive (incorrectly diagnosing a disease). In such cases, the base rate can help inform decisions about the appropriate threshold for a positive test result. Base rate fallacy Many psychological studies have examined a phenomenon called base-rate neglect or base rate fallacy, in which category base rates are not integrated with presented evidence in a normative manner, although not all evidence is consistent regarding how common this fallacy is. Mathematician Keith Devlin illustrates the risks as a hypothetical type of cancer that afflicts 1 of all people. Suppose a doctor then says there is a test for said cancer that is approximately 80 reliable, and that the test provides a positive result for 100 of people who have cancer, but it also results in a false positive for 20 of people - who do not have cancer. Testing positive may therefore lead people to believe that it is 80 likely that they have cancer. Devlin explains that the odds are instead less than 5. What is missing from these statistics is the relevant base rate information. The doctor should be asked, Out of the number of people who test positive (base rate group), how many have cancer? In assessing the probability that a given individual is a member of a particular class, information other than the base rate needs to be accounted for, especially featural evidence. For example, when a person wearing a white doctors coat and stethoscope is seen prescribing medication, there is evidence that allows for the conclusion that the probability of this particular individual being a medical professional is considerably more significant than the category base rate of 1. See also Bayes rule Prior probability Prevalence Title Bayesian interpretation of kernel regularization URL https//en.wikipedia.org/wiki/Bayesian_interpretation_of_kernel_regularization Content Within bayesian statistics for machine learning, kernel methods arise from the assumption of an inner product space or similarity structure on inputs. For some such methods, such as support vector machines (SVMs), the original formulation and its regularization were not Bayesian in nature. It is helpful to understand them from a Bayesian perspective. Because the kernels are not necessarily positive semidefinite, the underlying structure may not be inner product spaces, but instead more general reproducing kernel Hilbert spaces. In Bayesian probability kernel methods are a key component of Gaussian processes, where the kernel function is known as the covariance function. Kernel methods have traditionally been used in supervised learning problems where the input space is usually a space of vectors while the output space is a space of scalars. More recently these methods have been extended to problems that deal with multiple outputs such as in multi-task learning. A mathematical equivalence between the regularization and the Bayesian point of view is easily proved in cases where the reproducing kernel Hilbert space is finite-dimensional. The infinite-dimensional case raises subtle mathematical issues we will consider here the finite-dimensional case. We start with a brief review of the main ideas underlying kernel methods for scalar learning, and briefly introduce the concepts of regularization and Gaussian processes. We then show how both points of view arrive at essentially equivalent estimators, and show the connection that ties them together. The supervised learning problem The classical supervised learning problem requires estimating the output for some new input point x displaystyle mathbf x  by learning a scalar-valued estimator f  ( x ) displaystyle hat f(mathbf x ) on the basis of a training set S displaystyle S consisting of n displaystyle n input-output pairs, ) . Given a symmetric and positive bivariate function k ( , ) displaystyle k(cdot ,cdot ) called a kernel, one of the most popular estimators in machine learning is given by where K k ( X , X ) displaystyle mathbf K equiv k(mathbf X ,mathbf X ) is the kernel matrix with entries K i   . We will see how this estimator can be derived both from a regularization and a Bayesian perspective. A regularization perspective The main assumption in the regularization perspective is that the set of functions F displaystyle mathcal F is assumed to belong to a reproducing kernel Hilbert space H k displaystyle mathcal H_k . Reproducing kernel Hilbert space A reproducing kernel Hilbert space (RKHS) H k displaystyle mathcal H_k is a Hilbert space of functions defined by a symmetric, positive-definite function k  X X R displaystyle kmathcal Xtimes mathcal Xrightarrow mathbb R  called the reproducing kernel such that the function k ( x , ) displaystyle k(mathbf x ,cdot ) belongs to H k displaystyle mathcal H_k for all x X displaystyle mathbf x in mathcal X . There are three main properties make an RKHS appealing 1. The reproducing property, which gives name to the space, f ( x )  f , k ( x , ) k , f H k , displaystyle f(mathbf x )langle f,k(mathbf x ,cdot )rangle _k,quad forall  fin mathcal H_k, where , k displaystyle langle cdot ,cdot rangle _k is the inner product in H k displaystyle mathcal H_k . 2. Functions in an RKHS are in the closure of the linear combination of the kernel at given points, f ( x )  i k ( x i , x ) c i displaystyle f(mathbf x )sum _ik(mathbf x _i,mathbf x )c_i . This allows the construction in a unified framework of both linear and generalized linear models. 3. The squared norm in an RKHS can be written as f k 2  i , j k ( x i , x j ) c i c j displaystyle f_k2sum _i,jk(mathbf x _i,mathbf x _j)c_ic_j and could be viewed as measuring the complexity of the function. The regularized functional The estimator is derived as the minimizer of the regularized functional where f H k displaystyle fin mathcal H_k and k displaystyle cdot _k is the norm in H k displaystyle mathcal H_k . The first term in this functional, which measures the average of the squares of the errors between the f ( x i ) displaystyle f(mathbf x _i) and the y i displaystyle y_i , is called the empirical risk and represents the cost we pay by predicting f ( x i ) displaystyle f(mathbf x _i) for the true value y i displaystyle y_i . The second term in the functional is the squared norm in a RKHS multiplied by a weight displaystyle lambda  and serves the purpose of stabilizing the problem as well as of adding a trade-off between fitting and complexity of the estimator. The weight displaystyle lambda  , called the regularizer, determines the degree to which instability and complexity of the estimator should be penalized (higher penalty for increasing value of displaystyle lambda  ). Derivation of the estimator The explicit form of the estimator in equation (1) is derived in two steps. First, the representer theorem states that the minimizer of the functional (2) can always be written as a linear combination of the kernels centered at the training-set points, for some c R n displaystyle mathbf c in mathbb R n . The explicit form of the coefficients ). For a function of the form in equation (3), we have that f k 2  f , f k ,   . displaystyle beginalignedf_k2langle f,frangle _k,leftlangle sum _ .endaligned We can rewrite the functional (2) as 1 n y K c 2  c K c . displaystyle frac 1nmathbf y -mathbf K mathbf c 2lambda mathbf c top mathbf K mathbf c . This functional is convex in c displaystyle mathbf c  and therefore we can find its minimum by setting the gradient with respect to c displaystyle mathbf c  to zero, 1 n K ( Y K c )  K  . displaystyle beginaligned-frac 1nmathbf K (mathbf Y -mathbf K mathbf c )lambda mathbf K mathbf c 0,(mathbf K lambda nmathbf I )mathbf c mathbf Y ,mathbf c (mathbf K lambda nmathbf I )-1mathbf Y .endaligned Substituting this expression for the coefficients in equation (3), we obtain the estimator stated previously in equation (1), f  ( x )  k ( K  n I ) 1 Y . displaystyle hat f(mathbf x )mathbf k top (mathbf K lambda nmathbf I )-1mathbf Y . A Bayesian perspective The notion of a kernel plays a crucial role in Bayesian probability as the covariance function of a stochastic process called the Gaussian process. A review of Bayesian probability As part of the Bayesian framework, the Gaussian process specifies the prior distribution that describes the prior beliefs about the properties of the function being modeled. These beliefs are updated after taking into account observational data by means of a likelihood function that relates the prior beliefs to the observations. Taken together, the prior and likelihood lead to an updated distribution called the posterior distribution that is customarily used for predicting test cases. The Gaussian process A Gaussian process (GP) is a stochastic process in which any finite number of random variables that are sampled follow a joint Normal distribution. The mean vector and covariance matrix of the Gaussian distribution completely specify the GP. GPs are usually used as a priori distribution for functions, and as such the mean vector and covariance matrix can be viewed as functions, where the covariance function is also called the kernel of the GP. Let a function f displaystyle f follow a Gaussian process with mean function m displaystyle m and kernel function k displaystyle k , f G P ( m , k ) . displaystyle fsim mathcal GP(m,k). In terms of the underlying Gaussian distribution, we have that for any finite set . Derivation of the estimator In a regression context, the likelihood function is usually assumed to be a Gaussian distribution and the observations to be independent and identically distributed (iid), p ( y  f , x , 2 )  N ( f ( x ) , 2 ) . displaystyle p(yf,mathbf x ,sigma 2)mathcal N(f(mathbf x ),sigma 2). This assumption corresponds to the observations being corrupted with zero-mean Gaussian noise with variance 2 displaystyle sigma 2 . The iid assumption makes it possible to factorize the likelihood function over the data points given the set of inputs X displaystyle mathbf X  and the variance of the noise 2 displaystyle sigma 2 , and thus the posterior distribution can be computed analytically. For a test input vector x displaystyle mathbf x  , given the training data  . displaystyle beginalignedm(mathbf x )mathbf k top (mathbf K sigma 2mathbf I )-1mathbf Y ,sigma 2(mathbf x )k(mathbf x ,mathbf x )-mathbf k top (mathbf K sigma 2mathbf I )-1mathbf k .endaligned The connection between regularization and Bayes A connection between regularization theory and Bayesian theory can only be achieved in the case of finite dimensional RKHS. Under this assumption, regularization theory and Bayesian theory are connected through Gaussian process prediction. In the finite dimensional case, every RKHS can be described in terms of a feature map  X R p displaystyle Phi mathcal Xrightarrow mathbb R p such that k ( x , x )   ) . displaystyle k(mathbf x ,mathbf x )sum _ ). Functions in the RKHS with kernel K displaystyle mathbf K  can be then be written as f w ( x )   . displaystyle f_mathbf w _kmathbf w . We can now build a Gaussian process by assuming  ) . displaystyle mathbf w sim mathcal N(0,mathbf I )propto exp(-mathbf w 2). If we assume a Gaussian likelihood we have P ( Y  X , f )  N ( f ( X ) , 2 I ) exp ( 1 2 f w ( X ) Y 2 ) , displaystyle P(mathbf Y mathbf X ,f)mathcal N(f(mathbf X ),sigma 2mathbf I )propto exp left(-frac 1sigma 2f_mathbf w (mathbf X )-mathbf Y 2right), where f w ( X )  ( w , ( x 1 ) , , w , ( x n ) displaystyle f_mathbf w (mathbf X )(langle mathbf w ,Phi (mathbf x _1)rangle ,ldots ,langle mathbf w ,Phi (mathbf x _nrangle ) . The resulting posterior distribution is the given by P ( f  X , Y ) exp ( 1 2 f w ( X ) Y n 2  w 2 ) displaystyle P(fmathbf X ,mathbf Y )propto exp left(-frac 1sigma 2f_mathbf w (mathbf X )-mathbf Y _n2mathbf w 2right) We can see that a maximum posterior (MAP) estimate is equivalent to the minimization problem defining Tikhonov regularization, where in the Bayesian case the regularization parameter is related to the noise variance. From a philosophical perspective, the loss function in a regularization setting plays a different role than the likelihood function in the Bayesian setting. Whereas the loss function measures the error that is incurred when predicting f ( x ) displaystyle f(mathbf x ) in place of y displaystyle y , the likelihood function measures how likely the observations are from the model that was assumed to be true in the generative process. From a mathematical perspective, however, the formulations of the regularization and Bayesian frameworks make the loss function and the likelihood function to have the same mathematical role of promoting the inference of functions f displaystyle f that approximate the labels y displaystyle y as much as possible. See also Regularized least squares Bayesian linear regression Bayesian interpretation of Tikhonov regularization Title Bayesian optimization URL https//en.wikipedia.org/wiki/Bayesian_optimization Content Bayesian optimization is a sequential design strategy for global optimization of black-box functions, that does not assume any functional forms. It is usually employed to optimize expensive-to-evaluate functions. With the rise of artificial intelligence innovation in the 21st century, Bayesian optimizations have found prominent use in machine learning problems for optimizing hyperparameter values. History The term is generally attributed to Jonas Mockus and is coined in his work from a series of publications on global optimization in the 1970s and 1980s. Strategy Bayesian optimization is typically used on problems of the form max x A f ( x ) textstyle max _xin Af(x) , where A textstyle A is a set of points, x textstyle x , which rely upon less (or equal to) than 20 dimensions ( R d , d 20 textstyle mathbb R d,dleq 20 ), and whose membership can easily be evaluated. Bayesian optimization is particularly advantageous for problems where f ( x ) textstyle f(x) is difficult to evaluate due to its computational cost. The objective function, f textstyle f , is continuous and takes the form of some unknown structure, referred to as a black box. Upon its evaluation, only f ( x ) textstyle f(x) is observed and its derivatives are not evaluated. Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and place a prior over it. The prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the posterior distribution over the objective function. The posterior distribution, in turn, is used to construct an acquisition function (often also referred to as infill sampling criteria) that determines the next query point. There are several methods used to define the prior/posterior distribution over the objective function. The most common two methods use Gaussian processes in a method called kriging. Another less expensive method uses the Parzen-Tree Estimator to construct two distributions for high and low points, and then finds the location that maximizes the expected improvement. Standard Bayesian optimization relies upon each x A displaystyle xin A being easy to evaluate, and problems that deviate from this assumption are known as exotic Bayesian optimization problems. Optimization problems can become exotic if it is known that there is noise, the evaluations are being done in parallel, the quality of evaluations relies upon a tradeoff between difficulty and accuracy, the presence of random environmental conditions, or if the evaluation involves derivatives. Acquisition functions Examples of acquisition functions include probability of improvement expected improvement Bayesian expected losses upper confidence bounds (UCB) or lower confidence bounds Thompson sampling and hybrids of these. They all trade-off exploration and exploitation so as to minimize the number of function queries. As such, Bayesian optimization is well suited for functions that are expensive to evaluate. Solution methods The maximum of the acquisition function is typically found by resorting to discretization or by means of an auxiliary optimizer. Acquisition functions are maximized using a numerical optimization technique, such as Newtons method or quasi-Newton methods like the Broyden Fletcher Goldfarb Shanno algorithm. Applications The approach has been applied to solve a wide range of problems, including learning to rank, computer graphics and visual design, robotics, sensor networks, automatic algorithm configuration, automatic machine learning toolboxes, reinforcement learning, planning, visual attention, architecture configuration in deep learning, static program analysis, experimental particle physics, quality-diversity optimization, chemistry, material design, and drug development. Bayesian optimization has been applied in the field of facial recognition. The performance of the Histogram of Oriented Gradients (HOG) algorithm, a popular feature extraction method, heavily relies on its parameter settings. Optimizing these parameters can be challenging but crucial for achieving high accuracy. A novel approach to optimize the HOG algorithm parameters and image size for facial recognition using a Tree-structured Parzen Estimator (TPE) based Bayesian optimization technique has been proposed. This optimized approach has the potential to be adapted for other computer vision applications and contributes to the ongoing development of hand-crafted parameter-based feature extraction algorithms in computer vision. See also Multi-armed bandit Kriging Thompson sampling Global optimization Bayesian experimental design Probabilistic numerics Pareto optimum Active learning (machine learning) Multi-objective optimization Title Bayesian regret URL https//en.wikipedia.org/wiki/Bayesian_regret Content In stochastic game theory, Bayesian regret is the expected difference (regret) between the utility of a Bayesian strategy and that of the optimal strategy (the one with the highest expected payoff). The term Bayesian refers to Thomas Bayes (1702 1761), who proved a special case of what is now called Bayes theorem, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference. Economics This term has been used to compare a random buy-and-hold strategy to professional traders records. This same concept has received numerous different names, as the New York Times notes In 1957, for example, a statistician named James Hanna called his theorem Bayesian Regret. He had been preceded by David Blackwell, also a statistician, who called his theorem Controlled Random Walks. Other, later papers had titles like On Pseudo Games, How to Play an Unknown Game, Universal Coding and Universal Portfolios. Title Bayesian structural time series URL https//en.wikipedia.org/wiki/Bayesian_structural_time_series Content Bayesian structural time series (BSTS) model is a statistical technique used for feature selection, time series forecasting, nowcasting, inferring causal impact and other applications. The model is designed to work with time series data. The model has also promising application in the field of analytical marketing. In particular, it can be used in order to assess how much different marketing campaigns have contributed to the change in web search volumes, product sales, brand popularity and other relevant indicators. Difference-in-differences models and interrupted time series designs are alternatives to this approach. In contrast to classical difference-in-differences schemes, state-space models make it possible to (i) infer the temporal evolution of attributable impact, (ii) incorporate empirical priors on the parameters in a fully Bayesian treatment, and (iii) flexibly accommodate multiple sources of variation, including the time-varying influence of contemporaneous covariates, i.e., synthetic controls. General model description The model consists of three main components Kalman filter. The technique for time series decomposition. In this step, a researcher can add different state variables trend, seasonality, regression, and others. Spike-and-slab method. In this step, the most important regression predictors are selected. Bayesian model averaging. Combining the results and prediction calculation. The model could be used to discover the causations with its counterfactual prediction and the observed data. A possible drawback of the model can be its relatively complicated mathematical underpinning and difficult implementation as a computer program. However, the programming language R has ready-to-use packages for calculating the BSTS model, which do not require strong mathematical background from a researcher. See also Bayesian inference using Gibbs sampling Correlation does not imply causation Spike-and-slab regression References Further reading Scott, S. L.,  Varian, H. R. 2014a. Bayesian variable selection for nowcasting economic time series. Economic Analysis of the Digital Economy. Scott, S. L.,  Varian, H. R. 2014b. Predicting the present with bayesian structural time series. International Journal of Mathematical Modelling and Numerical Optimisation. Varian, H. R. 2014. Big Data New Tricks for Econometrics. Journal of Economic Perspectives Brodersen, K. H., Gallusser, F., Koehler, J., Remy, N.,  Scott, S. L. 2015. Inferring causal impact using Bayesian structural time-series models. The Annals of Applied Statistics. R package bsts. R package CausalImpact. O Hara, R. B.,  Sillanp , M. J. 2009. A review of Bayesian variable selection methods what, how and which. Bayesian analysis. Hoeting, J. A., Madigan, D., Raftery, A. E.,  Volinsky, C. T. 1999. Bayesian model averaging a tutorial. Statistical science. Title Biasvariance tradeoff URL https//en.wikipedia.org/wiki/BiasE28093variance_tradeoff Content In statistics and machine learning, the bias variance tradeoff describes the relationship between a models complexity, the accuracy of its predictions, and how well it can make predictions on previously unseen data that were not used to train the model. In general, as we increase the number of tunable parameters in a model, it becomes more flexible, and can better fit a training data set. It is said to have lower error, or bias. However, for more flexible models, there will tend to be greater variance to the model fit each time we take a set of samples to create a new training data set. It is said that there is greater variance in the models estimated parameters. The bias variance dilemma or bias variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting). The bias variance decomposition is a way of analyzing a learning algorithms expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself. Motivation The bias variance tradeoff is a central problem in supervised learning. Ideally, one wants to choose a model that both accurately captures the regularities in its training data, but also generalizes well to unseen data. Unfortunately, it is typically impossible to do both simultaneously. High-variance learning methods may be able to represent their training set well but are at risk of overfitting to noisy or unrepresentative training data. In contrast, algorithms with high bias typically produce simpler models that may fail to capture important regularities (i.e. underfit) in the data. It is an often made fallacy to assume that complex models must have high variance. High variance models are complex in some sense, but the reverse needs not be true. In addition, one has to be careful how to define complexity. In particular, the number of parameters used to describe the model is a poor measure of complexity. This is illustrated by an example adapted from The model f a , b ( x )  a sin ( b x ) displaystyle f_a,b(x)asin(bx) has only two parameters ( a , b displaystyle a,b ) but it can interpolate any number of points by oscillating with a high enough frequency, resulting in both a high bias and high variance. An analogy can be made to the relationship between accuracy and precision. Accuracy is one way of quantifying bias and can intuitively be improved by selecting from only local information. Consequently, a sample will appear accurate (i.e. have low bias) under the aforementioned selection conditions, but may result in underfitting. In other words, test data may not agree as closely with training data, which would indicate imprecision and therefore inflated variance. A graphical example would be a straight line fit to data exhibiting quadratic behavior overall. Precision is a description of variance and generally can only be improved by selecting information from a comparatively larger space. The option to select many data points over a broad sample space is the ideal condition for any analysis. However, intrinsic constraints (whether physical, theoretical, computational, etc.) will always play a limiting role. The limiting case where only a finite number of data points are selected over a broad sample space may result in improved precision and lower variance overall, but may also result in an overreliance on the training data (overfitting). This means that test data would also not agree as closely with the training data, but in this case the reason is inaccuracy or high bias. To borrow from the previous example, the graphical representation would appear as a high-order polynomial fit to the same data exhibiting quadratic behavior. Note that error in each case is measured the same way, but the reason ascribed to the error is different depending on the balance between bias and variance. To mitigate how much information is used from neighboring observations, a model can be smoothed via explicit regularization, such as shrinkage. Bias variance decomposition of mean squared error Suppose that we have a training set consisting of a set of points x 1 , , x n displaystyle x_1,dots ,x_n and real-valued labels y i displaystyle y_i associated with the points x i displaystyle x_i . We assume that the data is generated by a function f ( x ) displaystyle f(x) such as  . That is, y . We want to find a function f  ( x  D ) displaystyle hat f(xD) , that approximates the true function f ( x ) displaystyle f(x) as well as possible, by means of some learning algorithm based on a training dataset (sample) ) . We make as well as possible precise by measuring the mean squared error between y displaystyle y and f  ( x  D ) displaystyle hat f(xD)  we want ( y f  ( x  D ) ) 2 displaystyle (y-hat f(xD))2 to be minimal, both for x 1 , , x n displaystyle x_1,dots ,x_n and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the y i displaystyle y_i contain noise displaystyle varepsilon   this means we must be prepared to accept an irreducible error in any function we come up with. Finding an f  displaystyle hat f that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function f  displaystyle hat f we select, we can decompose its expected error on an unseen sample x displaystyle x (i.e. conditional to x) as follows 34  223 E D ,  ( y f  ( x  D ) ) 2   ( Bias D  f  ( x  D )  ) 2  Var D  f  ( x  D )   2 displaystyle mathbb E _D,varepsilon Big big (y-hat f(xD)big )2Big Big (operatorname Bias _Dbig hat f(xD)big Big )2operatorname Var _Dbig hat f(xD)big sigma 2 where Bias D  f  ( x  D )  E D  f  ( x  D ) f ( x )   E D  f  ( x  D )  f ( x )  E D  f  ( x  D )  E y  x  y ( x )  displaystyle beginalignedoperatorname Bias _Dbig hat f(xD)big triangleq mathbb E _Dbig hat f(xD)-f(x)big mathbb E _Dbig hat f(xD)big ,-,f(x)mathbb E _Dbig hat f(xD)big ,-,mathbb E _yxbig y(x)big endaligned and Var D  f  ( x  D )  E D  ( E D  f  ( x  D )  f  ( x  D ) ) 2  displaystyle operatorname Var _Dbig hat f(xD)big triangleq mathbb E _DBig big (mathbb E _Dhat f(xD)-hat f(xD)big )2Big  and 2  E y  ( y f ( x ) E y  x  y  ) 2  displaystyle sigma 2operatorname E _yBig big (y-underbrace f(x) _E_yxybig )2Big  The expectation ranges over different choices of the training set . The three terms represent the square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. E.g., when approximating a non-linear function f ( x ) displaystyle f(x) using a learning method for linear models, there will be error in the estimates f  ( x ) displaystyle hat f(x) due to this assumption the variance of the learning method, or, intuitively, how much the learning method f  ( x ) displaystyle hat f(x) will move around its mean the irreducible error 2 displaystyle sigma 2 . Since all three terms are non-negative, the irreducible error forms a lower bound on the expected error on unseen samples. 34 The more complex the model f  ( x ) displaystyle hat f(x) is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model move more to capture the data points, and hence its variance will be larger. Derivation The derivation of the bias variance decomposition for squared error proceeds as follows. For convenience, we drop the D displaystyle D subscript in the following lines, such that f  ( x  D )  f  ( x ) displaystyle hat f(xD)hat f(x) . Let us write the mean-squared error of our model MSE E  ( y f  ( x ) ) 2   E  ( f ( x )  f  ( x ) ) 2  since y f ( x )   E  ( f ( x ) f  ( x ) ) 2   2 E  ( f ( x ) f  ( x ) )   E  2  displaystyle beginalignedtextMSEtriangleq mathbb E Big big (y-hat f(x)big )2Big mathbb E Big big (f(x)varepsilon -hat f(x)big )2Big textsince ytriangleq f(x)varepsilon mathbb E Big big (f(x)-hat f(x)big )2Big ,,2 mathbb E Big big (f(x)-hat f(x)big )varepsilon Big ,,mathbb E varepsilon 2endaligned We can show that the second term of this equation is null E  ( f ( x ) f  ( x ) )   E  f ( x ) f  ( x )  E   since is independent from   . Let us now expand the remaining term E  ( f ( x ) f  ( x ) ) 2   E  ( f ( x ) E  f  ( x )   E  f  ( x )  f  ( x ) ) 2   E  ( f ( x ) E  f  ( x )  ) 2   2 E  ( f ( x ) E  f  ( x )  ) ( E  f  ( x )  f  ( x ) )   E  ( E  f  ( x )  f  ( x ) ) 2  displaystyle beginalignedmathbb E Big big (f(x)-hat f(x)big )2Big mathbb E Big big (f(x)-mathbb E big hat f(x)big mathbb E big hat f(x)big -hat f(x)big )2Big color Bluemathbb E Big big (f(x)-mathbb E big hat f(x)big big )2Big ,,2 color PineGreenmathbb E Big big (f(x)-mathbb E big hat f(x)big big )big (mathbb E big hat f(x)big -hat f(x)big )Big ,,mathbb E Big big (mathbb E big hat f(x)big -hat f(x)big )2Big endaligned We show that E  ( f ( x ) E  f  ( x )  ) 2   E  f ( x ) 2  2 E  f ( x ) E  f  ( x )    E  E  f  ( x )  2   f ( x ) 2 2 f ( x ) E  f  ( x )   E  f  ( x )  2  ( f ( x ) E  f  ( x )  ) 2 displaystyle beginalignedcolor Bluemathbb E Big big (f(x)-mathbb E big hat f(x)big big )2Big mathbb E big f(x)2big ,-,2 mathbb E Big f(x) mathbb E big hat f(x)big Big ,,mathbb E Big mathbb E big hat f(x)big 2Big f(x)2,-,2 f(x) mathbb E big hat f(x)big ,,mathbb E big hat f(x)big 2Big (f(x)-mathbb E big hat f(x)big Big )2endaligned This last series of equalities comes from the fact that f ( x ) displaystyle f(x) is not a random variable, but a fixed, deterministic function of x displaystyle x . Therefore, E  f ( x )   f ( x ) displaystyle mathbb E big f(x)big f(x) . Similarly E  f ( x ) 2   f ( x ) 2 displaystyle mathbb E big f(x)2big f(x)2 , and E  f ( x ) E  f  ( x )    f ( x ) E  E  f  ( x )    f ( x ) E  f  ( x )  displaystyle mathbb E Big f(x) mathbb E big hat f(x)big Big f(x) mathbb E Big  mathbb E big hat f(x)big Big f(x) mathbb E big hat f(x)big  . Using the same reasoning, we can expand the second term and show that it is null E  ( f ( x ) E  f  ( x )  ) ( E  f  ( x )  f  ( x ) )   E  f ( x ) E  f  ( x )  f ( x ) f  ( x ) E  f  ( x )  2  E  f  ( x )  f  ( x )   f ( x ) E  f  ( x )  f ( x ) E  f  ( x )  E  f  ( x )  2  E  f  ( x )  2  0 displaystyle beginalignedcolor PineGreenmathbb E Big big (f(x)-mathbb E big hat f(x)big big )big (mathbb E big hat f(x)big -hat f(x)big )Big mathbb E Big f(x) mathbb E big hat f(x)big ,-,f(x)hat f(x),-,mathbb E big hat f(x)big 2mathbb E big hat f(x)big  hat f(x)Big f(x) mathbb E big hat f(x)big ,-,f(x) mathbb E big hat f(x)big ,-,mathbb E big hat f(x)big 2,,mathbb E big hat f(x)big 20endaligned Eventually, we plug our derivations back into the original equation, and identify each term  . displaystyle textMSEmathbb E _xbigg operatorname Bias _Dhat f(xD)2operatorname Var _Dbig hat f(xD)big bigg sigma 2. Approaches Dimensionality reduction and feature selection can decrease variance by simplifying models. Similarly, a larger training set tends to decrease variance. Adding features (predictors) tends to decrease bias, at the expense of introducing additional variance. Learning algorithms typically have some tunable parameters that control bias and variance for example, linear and Generalized linear models can be regularized to decrease their variance at the cost of increasing their bias. In artificial neural networks, the variance increases and the bias decreases as the number of hidden units increase, although this classical assumption has been the subject of recent debate. Like in GLMs, regularization is typically applied. In k-nearest neighbor models, a high value of k leads to high bias and low variance (see below). In instance-based learning, regularization can be achieved varying the mixture of prototypes and exemplars. In decision trees, the depth of the tree determines the variance. Decision trees are commonly pruned to control variance. 307 One way of resolving the trade-off is to use mixture models and ensemble learning. For example, boosting combines many weak (high bias) models in an ensemble that has lower bias than the individual models, while bagging combines strong learners in a way that reduces their variance. Model validation methods such as cross-validation (statistics) can be used to tune models so as to optimize the trade-off. k-nearest neighbors In the case of k-nearest neighbors regression, when the expectation is taken over the possible labeling of a fixed training set, a closed-form expression exists that relates the bias variance decomposition to the parameter k 37, 223 E  ( y f  ( x ) ) 2 . The bias (first term) is a monotone rising function of k, while the variance (second term) drops off as k is increased. In fact, under reasonable assumptions the bias of the first-nearest neighbor (1-NN) estimator vanishes entirely as the size of the training set approaches infinity. Applications In regression The bias variance decomposition forms the conceptual basis for regression regularization methods such as LASSO and ridge regression. Regularization methods introduce bias into the regression solution that can reduce variance considerably relative to the ordinary least squares (OLS) solution. Although the OLS solution provides non-biased regression estimates, the lower variance solutions produced by regularization techniques provide superior MSE performance. In classification The bias variance decomposition was originally formulated for least-squares regression. For the case of classification under the 0-1 loss (misclassification rate), it is possible to find a similar decomposition, with the caveat that the variance term becomes dependent on the target label. Alternatively, if the classification problem can be phrased as probabilistic classification, then the expected cross-entropy can instead be decomposed to give bias and variance terms with the same semantics but taking a different form. It has been argued that as training data increases, the variance of learned models will tend to decrease, and hence that as training data quantity increases, error is minimised by methods that learn models with lesser bias, and that conversely, for smaller training data quantities it is ever more important to minimise variance. In reinforcement learning Even though the bias variance decomposition does not directly apply in reinforcement learning, a similar tradeoff can also characterize generalization. When an agent has limited information on its environment, the suboptimality of an RL algorithm can be decomposed into the sum of two terms a term related to an asymptotic bias and a term due to overfitting. The asymptotic bias is directly related to the learning algorithm (independently of the quantity of data) while the overfitting term comes from the fact that the amount of data is limited. In Monte Carlo methods While in traditional Monte Carlo methods the bias is typically zero, modern approaches, such as Markov chain Monte Carlo are only asymptotically unbiased, at best. Convergence diagnostics can be used to control bias via burn-in removal, but due to a limited computational budget, a bias variance trade-off arises, leading to a wide-range of approaches, in which a controlled bias is accepted, if this allows to dramatically reduce the variance, and hence the overall estimation error. In human learning While widely discussed in the context of machine learning, the bias variance dilemma has been examined in the context of human cognition, most notably by Gerd Gigerenzer and co-workers in the context of learned heuristics. They have argued (see references below) that the human brain resolves the dilemma in the case of the typically sparse, poorly-characterized training-sets provided by experience by adopting high-bias/low variance heuristics. This reflects the fact that a zero-bias approach has poor generalizability to new situations, and also unreasonably presumes precise knowledge of the true state of the world. The resulting heuristics are relatively simple, but produce better inferences in a wider variety of situations. Geman et al. argue that the bias variance dilemma implies that abilities such as generic object recognition cannot be learned from scratch, but require a certain degree of hard wiring that is later tuned by experience. This is because model-free approaches to inference require impractically large training sets if they are to avoid high variance. See also References External links MLU-Explain The Bias Variance Tradeoff An interactive visualization of the bias variance tradeoff in LOESS Regression and K-Nearest Neighbors. Title Binary classification URL https//en.wikipedia.org/wiki/Binary_classification Content Binary classification is the task of classifying the elements of a set into one of two groups (each called class). Typical binary classification problems include Medical testing to determine if a patient has a certain disease or not Quality control in industry, deciding whether a specification has been met In information retrieval, deciding whether a page should be in the result set of a search or not In administration, deciding whether someone should be issued with a driving licence or not In cognition, deciding whether an object is food or not food. When measuring the accuracy of a binary classifier, the simplest way is to count the errors. But in the real world often one of the two classes is more important, so that the number of both of the different types of errors is of interest. For example, in medical testing, detecting a disease when it is not present (a false positive) is considered differently from not detecting a disease when it is present (a false negative). Four outcomes Given a classification of a specific data set, there are four basic combinations of actual data category and assigned category true positives TP (correct positive assignments), true negatives TN (correct negative assignments), false positives FP (incorrect positive assignments), and false negatives FN (incorrect negative assignments). These can be arranged into a 2 2 contingency table, with rows corresponding to actual value condition positive or condition negative and columns corresponding to classification value test outcome positive or test outcome negative. Evaluation From tallies of the four basic outcomes, there are many approaches that can be used to measure the accuracy of a classifier or predictor. Different fields have different preferences. The eight basic ratios A common approach to evaluation is to begin by computing two ratios of a standard pattern. There are eight basic ratios of this form that one can compute from the contingency table, which come in four complementary pairs (each pair summing to 1). These are obtained by dividing each of the four numbers by the sum of its row or column, yielding eight numbers, which can be referred to generically in the form true positive row ratio or false negative column ratio. There are thus two pairs of column ratios and two pairs of row ratios, and one can summarize these with four numbers by choosing one ratio from each pair the other four numbers are the complements. The row ratios are true positive rate (TPR)  (TP/(TPFN)), aka sensitivity or recall. These are the proportion of the population with the condition for which the test is correct. with complement the false negative rate (FNR)  (FN/(TPFN)) true negative rate (TNR)  (TN/(TNFP), aka specificity (SPC), with complement false positive rate (FPR)  (FP/(TNFP)), also called independent of prevalence The column ratios are positive predictive value (PPV, aka precision) (TP/(TPFP)). These are the proportion of the population with a given test result for which the test is correct. with complement the false discovery rate (FDR) (FP/(TPFP)) negative predictive value (NPV) (TN/(TNFN)) with complement the false omission rate (FOR) (FN/(TNFN)), also called dependence on prevalence. In diagnostic testing, the main ratios used are the true column ratios true positive rate and true negative rate where they are known as sensitivity and specificity. In informational retrieval, the main ratios are the true positive ratios (row and column) positive predictive value and true positive rate where they are known as precision and recall. Cullerne Bown has suggested a flow chart for determining which pair of indicators should be used when. Otherwise, there is no general rule for deciding. There is also no general agreement on how the pair of indicators should be used to decide on concrete questions, such as when to prefer one classifier over another. One can take ratios of a complementary pair of ratios, yielding four likelihood ratios (two column ratio of ratios, two row ratio of ratios). This is primarily done for the column (condition) ratios, yielding likelihood ratios in diagnostic testing. Taking the ratio of one of these groups of ratios yields a final ratio, the diagnostic odds ratio (DOR). This can also be defined directly as (TP TN)/(FP FN)  (TP/FN)/(FP/TN) this has a useful interpretation as an odds ratio and is prevalence-independent. Other metrics There are a number of other metrics, most simply the accuracy or Fraction Correct (FC), which measures the fraction of all instances that are correctly categorized the complement is the Fraction Incorrect (FiC). The F-score combines precision and recall into one number via a choice of weighing, most simply equal weighing, as the balanced F-score ( score). Some metrics come from regression coefficients the markedness and the informedness, and their geometric mean, the Matthews correlation coefficient. Other metrics include Youdens J statistic, the uncertainty coefficient, the phi coefficient, and Cohens kappa. Statistical binary classification Statistical classification is a problem studied in machine learning in which the classification is performed on the basis of a classification rule. It is a type of supervised learning, a method of machine learning where the categories are predefined, and is used to categorize new probabilistic observations into said categories. When there are only two categories the problem is known as statistical binary classification. Some of the methods commonly used for binary classification are Decision trees Random forests Bayesian networks Support vector machines Neural networks Logistic regression Probit model Genetic Programming Multi expression programming Linear genetic programming Each classifier is best in only a select domain based upon the number of observations, the dimensionality of the feature vector, the noise in the data and many other factors. For example, random forests perform better than SVM classifiers for 3D point clouds. Converting continuous values to binary Binary classification may be a form of dichotomization in which a continuous function is transformed into a binary variable. Tests whose results are of continuous values, such as most blood values, can artificially be made binary by defining a cutoff value, with test results being designated as positive or negative depending on whether the resultant value is higher or lower than the cutoff. However, such conversion causes a loss of information, as the resultant binary classification does not tell how much above or below the cutoff a value is. As a result, when converting a continuous value that is close to the cutoff to a binary one, the resultant positive or negative predictive value is generally higher than the predictive value given directly from the continuous value. In such cases, the designation of the test of being either positive or negative gives the appearance of an inappropriately high certainty, while the value is in fact in an interval of uncertainty. For example, with the urine concentration of hCG as a continuous value, a urine pregnancy test that measured 52 mIU/ml of hCG may show as positive with 50 mIU/ml as cutoff, but is in fact in an interval of uncertainty, which may be apparent only by knowing the original continuous value. On the other hand, a test result very far from the cutoff generally has a resultant positive or negative predictive value that is lower than the predictive value given from the continuous value. For example, a urine hCG value of 200,000 mIU/ml confers a very high probability of pregnancy, but conversion to binary values results in that it shows just as positive as the one of 52 mIU/ml. See also Approximate membership query filter Examples of Bayesian inference Classification rule Confusion matrix Detection theory Kernel methods Multiclass classification Multi-label classification One-class classification Prosecutors fallacy Receiver operating characteristic Thresholding (image processing) Uncertainty coefficient, aka proficiency Qualitative property Precision and recall (equivalent classification schema) References Bibliography Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other kernel-based learning methods. Cambridge University Press, 2000. ISBN 0-521-78019-5 ( SVM Book) John Shawe-Taylor and Nello Cristianini. Kernel Methods for Pattern Analysis. Cambridge University Press, 2004. ISBN 0-521-81397-2 (Website for the book) Bernhard Sch lkopf and A. J. Smola Learning with Kernels. MIT Press, Cambridge, Massachusetts, 2002. ISBN 0-262-19475-9 Title Bioserenity URL https//en.wikipedia.org/wiki/Bioserenity Content BioSerenity is a medtech company created in 2014 that develops ambulatory medical devices to help diagnose and monitor patients with chronic diseases such as epilepsy. The medical devices are composed of medical sensors, smart clothing, a smartphone app for Patient Reported Outcome, and a web platform to perform data analysis through Medical Artificial Intelligence for detection of digital biomarkers. The company initially focused on Neurology, a domain in which it reported contributing to the diagnosis of 30 000 patients per year. It now also operates in Sleep Disorders and Cardiology. BioSerenity reported it provides pharmaceutical companies with solutions for companion diagnostics. Company history BioSerenity was founded in 2014, by Pierre-Yves Frouin. The company was initially hosted at the ICM Institute (Institute du Cerveau et de la Mo lle pini re), in Paris, France. Fund Raising June 8, 2015  The company raises a 4 million seed round with Kurma Partners and IdInvest Partners September 20, 2017  The company raises a 17 million series A round with LBO France, IdInvest Partners and BPI France June 18, 2019  The company raises a 70 million series B round with Dassault Syst mes, IdInvest Partners, LBO France and BPI France November 13, 2023  The company raises a 24M series C round with Jolt Capital Acquisitions In 2019, BioSerenity announced the acquisition of the American Company SleepMed and working with over 200 Hospitals. In 2020, BioSerenity was one of the five French manufacturers (Savoy, BB Distrib, Celluloses de Broc liande, Chargeurs) working on the production of sanitary equipment including masks at request of the French government. In 2021, the Neuronaute would be used by approximately 30,000 patients per year. Awards BioSerenity is one of the Disrupt 100 BioSerenity joined the BioSerenity was selected by Microsoft and AstraZeneca in their initiative AI Factory for Health BioSerenity accelerated at Stanfords University StartX program References External links Official website FDA Clearance Neuronaute FDA Clearance Cardioskin FDA Clearance Accusom Title BradleyTerry model URL https//en.wikipedia.org/wiki/BradleyE28093Terry_model Content The Bradley Terry model is a probability model for the outcome of pairwise comparisons between items, teams, or objects. Given a pair of items i and j drawn from some population, it estimates the probability that the pairwise comparison i  j turns out true, as where pi is a positive real-valued score assigned to individual i. The comparison i  j can be read as i is preferred to j, i ranks higher than j, or i beats j, depending on the application. For example, pi might represent the skill of a team in a sports tournament and Pr ( i  j ) displaystyle Pr(ij) the probability that i wins a game against j. Or pi might represent the quality or desirability of a commercial product and Pr ( i  j ) displaystyle Pr(ij) the probability that a consumer will prefer product i over product j. The Bradley Terry model can be used in the forward direction to predict outcomes, as described, but is more commonly used in reverse to infer the scores pi given an observed set of outcomes. In this type of application pi represents some measure of the strength or quality of i displaystyle i and the model lets us estimate the strengths from a series of pairwise comparisons. In a survey of wine preferences, for instance, it might be difficult for respondents to give a complete ranking of a large set of wines, but relatively easy for them to compare sample pairs of wines and say which they feel is better. Based on a set of such pairwise comparisons, the Bradley Terry model can then be used to derive a full ranking of the wines. Once the values of the scores pi have been calculated, the model can then also be used in the forward direction, for instance to predict the likely outcome of comparisons that have not yet actually occurred. In the wine survey example, for instance, one could calculate the probability that someone will prefer wine i displaystyle i over wine j displaystyle j , even if no one in the survey directly compared that particular pair. History and applications The model is named after Ralph A. Bradley and Milton E. Terry, who presented it in 1952, although it had already been studied by Ernst Zermelo in the 1920s. Applications of the model include the ranking of competitors in sports, chess, and other competitions, the ranking of products in paired comparison surveys of consumer choice, analysis of dominance hierarchies within animal and human communities, ranking of journals, ranking of AI models, and estimation of the relevance of documents in machine-learned search engines. Definition The Bradley Terry model can be parametrized in various ways. Equation (1) is perhaps the most common, but there are a number of others. Bradley and Terry themselves defined exponential score functions p  . displaystyle Pr(ij)frac ebeta _iebeta _iebeta _j. Alternatively, one can use a logit, such that logit Pr ( i  j )  log Pr ( i  j ) 1 Pr ( i  j )  log Pr ( i  j ) Pr ( j  i )  i j , displaystyle operatorname logit Pr(ij)log frac Pr(ij)1-Pr(ij)log frac Pr(ij)Pr(ji)beta _i-beta _j, i.e. logit . textstyle 0p1. This formulation highlights the similarity between the Bradley Terry model and logistic regression. Both employ essentially the same model but in different ways. In logistic regression one typically knows the parameters i displaystyle beta _i and attempts to infer the functional form of Pr ( i  j ) displaystyle Pr(ij)  in ranking under the Bradley Terry model one knows the functional form and attempts to infer the parameters. With a scale factor of 400, this is equivalent to the Elo rating system for players with Elo ratings Ri and Rj. Pr ( i  j )  e R i / 400 e R i / 400  e R j / 400  1 1  e ( R j R i ) / 400 . displaystyle Pr(ij)frac eR_i/400eR_i/400eR_j/400frac 11e(R_j-R_i)/400. Plackett Luce model A standard generalization of the BT model is the Plackett Luce model, which models ranking N displaystyle N items. In the same notation as BT model Pr ( y 1   y N )  p y 1 p y 1   p y N p y 2 p y 2   p y N p y N p y N displaystyle Pr(y_1dots y_N)frac p_y_1p_y_1dots p_y_Nfrac p_y_2p_y_2dots p_y_Ndots frac p_y_Np_y_N This can be imagined as drawing from an urn with replacement. The urn contains balls colored in proportion to p 1 , p 2 , , p N displaystyle p_1,p_2,dots ,p_N , and one draws from the urn with replacement. If a ball has a new color, then that ball is placed as the next-ranked ball. Otherwise, if the ball has a color already drawn, then it is discarded. Given the proportions p 1 , p 2 , , p N displaystyle p_1,p_2,dots ,p_N , the PL model can be sampled by the exponential race method. One samples radioactive decay times from N displaystyle N exponential clocks, that is, t 1 E x p ( p 1 ) , , t N E x p ( p N ) displaystyle t_1sim mathrm Exp (p_1),dots ,t_Nsim mathrm Exp (p_N) . Then one ranks the items according to the order in which they decayed. In this interpretation, it is immediately clear that the PL model satisfies Luces choice axiom (from the same Luce). Therefore, for any two y , z displaystyle y,z , Pr ( y  z )  p y p y  p z displaystyle Pr(yz)frac p_yp_yp_z reduces to the BT model, and in general, for any subset y 1 , , y M displaystyle y_1,dots ,y_M of the choices, Pr ( y 1   y N )  p y 1 p y 1   p y M p y 2 p y 2   p y M p y M p y M displaystyle Pr(y_1dots y_N)frac p_y_1p_y_1dots p_y_Mfrac p_y_2p_y_2dots p_y_Mdots frac p_y_Mp_y_M reduces to a smaller PL model with the same parameters. Inference The most common application of the Bradley Terry model is to infer the values of the parameters p i displaystyle p_i given an observed set of outcomes i  j displaystyle ij , such as wins and losses in a competition. The simplest way to estimate the parameters is by maximum likelihood estimation, i.e., by maximizing the likelihood of the observed outcomes given the model and parameter values. Suppose we know the outcomes of a set of pairwise competitions between a certain group of individuals, and let wij be the number of times individual i beats individual j. Then the likelihood of this set of outcomes within the Bradley Terry model is i j  Pr ( i  j )  w i j displaystyle prod _ijPr(ij)w_ij and the log-likelihood of the parameter vector p  , ..., pn is l ( p )  ln i j  Pr ( i  j )  w i  )  . displaystyle beginalignedmathcal l(mathbf p )ln prod _ijbigl Pr(ij)bigr w_ijsum _ .endaligned Zermelo showed that this expression has only a single maximum, which can be found by differentiating with respect to p i displaystyle p_i and setting the result to zero, which leads to This equation has no known closed-form solution, but Zermelo suggested solving it by simple iteration. Starting from any convenient set of (positive) initial values for the p i displaystyle p_i , one iteratively performs the update for all i in turn. The resulting parameters are arbitrary up to an overall multiplicative constant, so after computing all of the new values they should be normalized by dividing by their geometric mean thus This estimation procedure improves the log-likelihood on every iteration, and is guaranteed to eventually reach the unique maximum. It is, however, slow to converge. More recently it has been pointed out that equation (2) can also be rearranged as p ). This iteration gives identical results to the one in (3) but converges much faster and hence is normally preferred over (3). Worked example of solution procedure Consider a sporting competition between four teams, who play a total of 22 games among themselves. Each teams wins are given in the rows of the table below and the opponents are given as the columns For example, Team A has beat Team B twice and lost to team B three times not played team C at all won once and lost four times against team D. We would like to estimate the relative strengths of the teams, which we do by calculating the parameters p i displaystyle p_i , with higher parameters indicating greater prowess. To do this, we initialize the four entries in the parameter vector p arbitrarily, for example assigning the value 1 to each team 1, 1, 1, 1. Then we apply equation (5) to update p 1 displaystyle p_1 , which gives p 1  j ( 1 ) w 1 j p j / ( p 1  p j ) j ( 1 ) w j 1 / ( p 1  p j )  2 1 1  1  0 1 1  1  1 1 1  1 3 1 1  1  0 1 1  1  4 1 1  1  0.429. displaystyle p_1frac sum _j(neq 1)w_1jp_j/(p_1p_j)sum _j(neq 1)w_/(p_1p_j)frac 2frac 1110frac 1111frac 1113frac 1110frac 1114frac 1110.429. Now, we apply (5) again to update p 2 displaystyle p_2 , making sure to use the new value of p 1 displaystyle p_1 that we just calculated p 2  j ( 2 ) w 2 j p j / ( p 2  p j ) j ( 2 ) w j 2 / ( p 2  p j )  3 0.429 1  0.429  5 1 1  1  0 1 1  1 2 1 1  0.429  3 1 1  1  0 1 1  1  1.172 displaystyle p_2frac sum _j(neq 2)w_2jp_j/(p_2p_j)sum _j(neq 2)w_/(p_2p_j)frac 3frac 0.42910.4295frac 1110frac 1112frac 110.4293frac 1110frac 1111.172 Similarly for p 3 displaystyle p_3 and p 4 displaystyle p_4 we get p 3  j ( 3 ) w 3 j p j / ( p 3  p j ) j ( 3 ) w j 3 / ( p 3  p j )  0 0.429 1  0.429  3 1.172 1  1.172  1 1 1  1 0 1 1  0.429  5 1 1  1.172  3 1 1  1  0.557 displaystyle p_3frac sum _j(neq 3)w_3jp_j/(p_3p_j)sum _j(neq 3)w_/(p_3p_j)frac 0frac 0.42910.4293frac 1.17211.1721frac 1110frac 110.4295frac 111.1723frac 1110.557 p 4  j ( 4 ) w 4 j p j / ( p 4  p j ) j ( 4 ) w j 4 / ( p 4  p j )  4 0.429 1  0.429  0 1.172 1  1.172  3 0.557 1  0.557 1 1 1  0.429  0 1 1  1.172  1 1 1  0.557  1.694 displaystyle p_4frac sum _j(neq 4)w_4jp_j/(p_4p_j)sum _j(neq 4)w_/(p_4p_j)frac 4frac 0.42910.4290frac 1.17211.1723frac 0.55710.5571frac 110.4290frac 111.1721frac 110.5571.694 Then we normalize all the parameters by dividing by their geometric mean ( 0.429 1.172 0.557 1.694 ) 1 / 4  0.830 displaystyle (0.429times 1.172times 0.557times 1.694)1/40.830 to get the estimated parameters .516, 1.413, 0.672, 2.041. To improve the estimates further, we repeat the process, using the new p values. For example, p 1  2 1.413 0.516  1.413  0 0.672 0.516  0.672  1 2.041 0.516  2.041 3 1 0.516  1.413  0 1 0.516  0.672  4 1 0.516  2.041  0.725. displaystyle p_1frac 2cdot frac 1.4130.5161.4130cdot frac 0.6720.5160.6721cdot frac 2.0410.5162.0413cdot frac 10.5161.4130cdot frac 10.5160.6724cdot frac 10.5162.0410.725. Repeating this process for the remaining parameters and normalizing, we get .677, 1.034, 0.624, 2.287. Repeating a further 10 times gives rapid convergence toward a final solution of .640, 1.043, 0.660, 2.270. This indicates that Team D is the strongest and Team B the second strongest, while Teams A and C are nearly equal in strength but below Teams B and D. In this way the Bradley Terry model lets us infer the relationship between all four teams, even though not all teams have played each other. Variations Crowd-BT The Crowd-BT model, developed in 2013 by Chen et al, attempts to extend the standard Bradley Terry model for crowdsourced settings while reducing the number of comparisons needed by taking into account the reliability of each judge. In particular, it identifies and excludes judges presumed to be spammers (selecting choices at random) or malicious (selecting always the wrong choice). In a crowdsourced task of ranking documents by reading difficulty with 624 judges contributing up to 40 pairwise comparisons each, Crowd-BT was shown to outperform both standard Bradley Terry as well as ranking system TrueSkill. It has been recommended for use when quality results are valued over efficiency and the number of comparisons is high. See also Ordinal regression Rasch model Scale (social sciences) Elo rating system Thurstonian model Title Category utility URL https//en.wikipedia.org/wiki/Category_utility Content Category utility is a measure of category goodness defined in Gluck  Corter (1985) and Corter  Gluck (1992). It attempts to maximize both the probability that two objects in the same category have attribute values in common, and the probability that objects from different categories have different attribute values. It was intended to supersede more limited measures of category goodness such as cue validity (Reed 1972 Rosch  Mervis 1975) and collocation index (Jones 1983). It provides a normative information-theoretic measure of the predictive advantage gained by the observer who possesses knowledge of the given category structure (i.e., the class labels of instances) over the observer who does not possess knowledge of the category structure. In this sense the motivation for the category utility measure is similar to the information gain metric used in decision tree learning. In certain presentations, it is also formally equivalent to the mutual information, as discussed below. A review of category utility in its probabilistic incarnation, with applications to machine learning, is provided in Witten  Frank (2005, pp. 260 262). Probability-theoretic definition of category utility The probability-theoretic definition of category utility given in Fisher (1987) and Witten  Frank (2005) is as follows C U ( C , F )  1 p c j C p ( c j )  f i F . The term p ( f i k ) displaystyle p(f_ik)  designates the marginal probability that feature f i displaystyle f_i  takes on value k displaystyle k  , and the term p ( f i k  c j ) displaystyle p(f_ikc_j)  designates the category-conditional probability that feature f i displaystyle f_i  takes on value k displaystyle k  given that the object in question belongs to category c j displaystyle c_j  . The motivation and development of this expression for category utility, and the role of the multiplicand 1 p displaystyle textstyle tfrac 1p as a crude overfitting control, is given in the above sources. Loosely (Fisher 1987), the term p ( c j ) f i F . Their difference therefore reflects the relative advantage accruing to the observer by having knowledge of the category structure. Information-theoretic definition of category utility The information-theoretic definition of category utility for a set of entities with size- n displaystyle n  binary feature set ). The intuition behind the above expression is as follows The term p ( c )   . Similarly, the term p ( c )  . The sum of these two terms in the brackets is therefore the weighted average of these two costs. The final term, . The value of the category utility will, in the above formulation, be non-negative. Category utility and mutual information Gluck  Corter (1985) and Corter  Gluck (1992) mention that the category utility is equivalent to the mutual information. Here is a simple demonstration of the nature of this equivalence. Assume a set of entities each having the same n displaystyle n features, i.e., feature set  . That is, each feature has the capacity to adopt any of m displaystyle m distinct values (which need not be ordered all variables can be nominal) for the special case . For the purposes of this demonstration, without loss of generality, feature set F displaystyle F can be replaced with a single aggregate variable F a displaystyle F_a that has cardinality m n displaystyle mn , and adopts a unique value v i ,  . (Ordinality does not matter, because the mutual information is not sensitive to ordinality.) In what follows, a term such as p ( F  . (Using the aggregate feature variable F a displaystyle F_a replaces multiple summations, and simplifies the presentation to follow.) For this demonstration, also assume a single category variable C displaystyle C , which has cardinality p displaystyle p . This is equivalent to a classification system in which there are p displaystyle p non-intersecting categories. In the special case of . From the definition of mutual information for discrete variables, the mutual information I ( F a  C ) displaystyle I(F_aC) between the aggregate feature variable F a displaystyle F_a and the category variable C displaystyle C is given by I ( F a  C )  v i F a c j C p ( v i , c j ) log p ( v i , c j ) p ( v i ) p ( c j ) displaystyle I(F_aC)sum _v_iin F_asum _c_jin Cp(v_i,c_j)log frac p(v_i,c_j)p(v_i),p(c_j) where p ( v i ) displaystyle p(v_i) is the prior probability of feature variable F a displaystyle F_a adopting value v i displaystyle v_i , p ( c j ) displaystyle p(c_j) is the marginal probability of category variable C displaystyle C adopting value c j displaystyle c_j , and p ( v i , c j ) displaystyle p(v_i,c_j) is the joint probability of variables F a displaystyle F_a and C displaystyle C simultaneously adopting those respective values. In terms of the conditional probabilities this can be re-written (or defined) as I ( F a  C )  v i F a c j C p ( v i , c j ) log p ( v i  c j ) p ( v i )  v i F a c j C p ( v i  c j ) p ( c j )  log p ( v i  c j ) log p ( v i )   v i F a c j C p ( v i  c j ) p ( c j ) log p ( v i  c j ) v i F a c j C p ( v i  c j ) p ( c j ) log p ( v i )  v i F a c j C p ( v i  c j ) p ( c j ) log p ( v i  c j ) v i F a c j C p ( v i , c j ) log p ( v i )  v i F a c j C p ( v i  c j ) p ( c j ) log p ( v i  c j ) v i F a log p ( v i ) c j C p ( v i , c j )  v i F a c j C p ( v i  c j ) p ( c j ) log p ( v i  c j ) v i F a p ( v i ) log p ( v i ) displaystyle beginalignedI(F_aC)sum _v_iin F_asum _c_jin Cp(v_i,c_j)log frac p(v_ic_j)p(v_i)sum _v_iin F_asum _c_jin Cp(v_ic_j)p(c_j)leftlog p(v_ic_j)-log p(v_i)rightsum _v_iin F_asum _c_jin Cp(v_ic_j)p(c_j)log p(v_ic_j)-sum _v_iin F_asum _c_jin Cp(v_ic_j)p(c_j)log p(v_i)sum _v_iin F_asum _c_jin Cp(v_ic_j)p(c_j)log p(v_ic_j)-sum _v_iin F_asum _c_jin Cp(v_i,c_j)log p(v_i)sum _v_iin F_asum _c_jin Cp(v_ic_j)p(c_j)log p(v_ic_j)-sum _v_iin F_alog p(v_i)sum _c_jin Cp(v_i,c_j)sum _v_iin F_asum _c_jin Cp(v_ic_j)p(c_j)log p(v_ic_j)-sum _v_iin F_ap(v_i)log p(v_i)endaligned If the original definition of the category utility from above is rewritten with  . The two measures are actually equivalent then only when the features  f i  displaystyle f_i , are independent (and assuming that terms in the sum corresponding to p ( f i ) displaystyle p(bar f_i) are also added). Insensitivity of category utility to ordinality Like the mutual information, the category utility is not sensitive to any ordering in the feature or category variable values. That is, as far as the category utility is concerned, the category set small,medium,large,jumbo is not qualitatively different from the category set desk,fish,tree,mop since the formulation of the category utility does not account for any ordering of the class variable. Similarly, a feature variable adopting values 1,2,3,4,5 is not qualitatively different from a feature variable adopting values fred,joe,bob,sue,elaine. As far as the category utility or mutual information are concerned, all category and feature variables are nominal variables. For this reason, category utility does not reflect any gestalt aspects of category goodness that might be based on such ordering effects. One possible adjustment for this insensitivity to ordinality is given by the weighting scheme described in the article for mutual information. Category goodness models and philosophy This section provides some background on the origins of, and need for, formal measures of category goodness such as the category utility, and some of the history that lead to the development of this particular metric. What makes a good category? At least since the time of Aristotle there has been a tremendous fascination in philosophy with the nature of concepts and universals. What kind of entity is a concept such as horse? Such abstractions do not designate any particular individual in the world, and yet we can scarcely imagine being able to comprehend the world without their use. Does the concept horse therefore have an independent existence outside of the mind? If it does, then what is the locus of this independent existence? The question of locus was an important issue on which the classical schools of Plato and Aristotle famously differed. However, they remained in agreement that universals did indeed have a mind-independent existence. There was, therefore, always a fact to the matter about which concepts and universals exist in the world. In the late Middle Ages (perhaps beginning with Occam, although Porphyry also makes a much earlier remark indicating a certain discomfort with the status quo), however, the certainty that existed on this issue began to erode, and it became acceptable among the so-called nominalists and empiricists to consider concepts and universals as strictly mental entities or conventions of language. On this view of concepts that they are purely representational constructs a new question then comes to the fore Why do we possess one set of concepts rather than another? What makes one set of concepts good and another set of concepts bad? This is a question that modern philosophers, and subsequently machine learning theorists and cognitive scientists, have struggled with for many decades. What purpose do concepts serve? One approach to answering such questions is to investigate the role or purpose of concepts in cognition. Thus the answer to What are concepts good for in the first place? by Mill (1843, p. 425) and many others is that classification (conception) is a precursor to induction By imposing a particular categorization on the universe, an organism gains the ability to deal with physically non-identical objects or situations in an identical fashion, thereby gaining substantial predictive leverage (Smith  Medin 1981 Harnad 2005). As J.S. Mill puts it (Mill 1843, pp. 466 468), The general problem of classification... is to provide that things shall be thought of in such groups, and those groups in such an order, as will best conduce to the remembrance and to the ascertainment of their laws... and one of the uses of such a classification that by drawing attention to the properties on which it is founded, and which, if the classification be good, are marks of many others, it facilitates the discovery of those others. From this base, Mill reaches the following conclusion, which foreshadows much subsequent thinking about category goodness, including the notion of category utility The ends of scientific classification are best answered when the objects are formed into groups respecting which a greater number of general propositions can be made, and those propositions more important, than could be made respecting any other groups into which the same things could be distributed. The properties, therefore, according to which objects are classified should, if possible, be those which are causes of many other properties or, at any rate, which are sure marks of them. One may compare this to the category utility hypothesis proposed by Corter  Gluck (1992) A category is useful to the extent that it can be expected to improve the ability of a person to accurately predict the features of instances of that category. Mill here seems to be suggesting that the best category structure is one in which object features (properties) are maximally informative about the objects class, and, simultaneously, the object class is maximally informative about the objects features. In other words, a useful classification scheme is one in which category knowledge can be used to accurately infer object properties, and property knowledge can be used to accurately infer object classes. One may also compare this idea to Aristotles criterion of counter-predication for definitional predicates, as well as to the notion of concepts described in formal concept analysis. Attempts at formalization A variety of different measures have been suggested with an aim of formally capturing this notion of category goodness, the best known of which is probably the cue validity. Cue validity of a feature f i displaystyle f_i  with respect to category c j displaystyle c_j  is defined as the conditional probability of the category given the feature (Reed 1972Rosch  Mervis 1975Rosch 1978), p ( c j  f i ) displaystyle p(c_jf_i)  , or as the deviation of the conditional probability from the category base rate (Edgell 1993Kruschke  Johansen 1999), p ( c j  f i ) p ( c j ) displaystyle p(c_jf_i)-p(c_j)  . Clearly, these measures quantify only inference from feature to category (i.e., cue validity), but not from category to feature, i.e., the category validity p ( f i  c j ) displaystyle p(f_ic_j)  . Also, while the cue validity was originally intended to account for the demonstrable appearance of basic categories in human cognition categories of a particular level of generality that are evidently preferred by human learners a number of major flaws in the cue validity quickly emerged in this regard (Jones 1983Murphy 1982Corter  Gluck 1992, and others). One attempt to address both problems by simultaneously maximizing both feature validity and category validity was made by Jones (1983) in defining the collocation index as the product p ( c j  f i ) p ( f i  c j ) displaystyle p(c_jf_i)p(f_ic_j)  , but this construction was fairly ad hoc (see Corter  Gluck 1992). The category utility was introduced as a more sophisticated refinement of the cue validity, which attempts to more rigorously quantify the full inferential power of a class structure. As shown above, on a certain view the category utility is equivalent to the mutual information between the feature variable and the category variable. It has been suggested that categories having the greatest overall category utility are those that are not only those best in a normative sense, but also those human learners prefer to use, e.g., basic categories (Corter  Gluck 1992). Other related measures of category goodness are cohesion (Hanson  Bauer 1989Gennari, Langley  Fisher 1989) and salience (Gennari 1989). Applications Category utility is used as the category evaluation measure in the popular conceptual clustering algorithm called COBWEB (Fisher 1987). See also Abstraction Concept learning Universals Unsupervised learning Title CIML community portal URL https//en.wikipedia.org/wiki/CIML_community_portal Content The computational intelligence and machine learning (CIML) community portal is an international multi-university initiative. Its primary purpose is to help facilitate a virtual scientific community infrastructure for all those involved with, or interested in, computational intelligence and machine learning. This includes CIML research-, education, and application-oriented resources residing at the portal and others that are linked from the CIML site. Overview The CIML community portal was created to facilitate an online virtual scientific community wherein anyone interested in CIML can share research, obtain resources, or simply learn more. The effort is currently led by Jacek Zurada (principal investigator), with Rammohan Ragade and Janusz Wojtusiak, aided by a team of 25 volunteer researchers from 13 different countries. The ultimate goal of the CIML community portal is to accommodate and cater to a broad range of users, including experts, students, the public, and outside researchers interested in using CIML methods and software tools. Each community member and user will be guided through the portal resources and tools based on their respective CIML experience (e.g. expert, student, outside researcher) and goals (e.g. collaboration, education). A preliminary version of the communitys portal, with limited capabilities, is now operational and available for users. All electronic resources on the portal are peer-reviewed to ensure high quality and cite-ability for literature. Further reading Jacek M. Zurada, Janusz Wojtusiak, Fahmida Chowdhury, James E. Gentle, Cedric J. Jeannot, and Maciej A. Mazurowski, Computational Intelligence Virtual Community Framework and Implementation Issues, Proceedings of the IEEE World Congress on Computational Intelligence, Hong Kong, June 1 6, 2008. Jacek M. Zurada, Janusz Wojtusiak, Maciej A. Mazurowski, Devendra Mehta, Khalid Moidu, Steve Margolis, Toward Multidisciplinary Collaboration in the CIML Virtual Community, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp. 62 66 Chris Boyle, Artur Abdullin, Rammohan Ragade, Maciej A. Mazurowski, Janusz Wojtusiak, Jacek M. Zurada, Workflow considerations in the emerging CI-ML virtual organization, Proceedings of the 2008 Workshop on Building Computational Intelligence and Machine Learning Virtual Organizations, pp. 67 70 See also Artificial Intelligence Computational Intelligence Machine Learning National Science Foundation References External links Official website Title Claude (language model) URL https//en.wikipedia.org/wiki/Claude_(language_model) Content Claude is a family of large language models developed by Anthropic. The first model was released in March 2023. The Claude 3 family, released in March 2024, consists of three models Haiku, optimized for speed Sonnet, which balances capability and performance and Opus, designed for complex reasoning tasks. These models can process both text and images, with Claude 3 Opus demonstrating enhanced capabilities in areas like mathematics, programming, and logical reasoning compared to previous versions. Training Claude models are generative pre-trained transformers. They have been pre-trained to predict the next word in large amounts of text. Then, they have been fine-tuned, notably using constitutional AI and reinforcement learning from human feedback (RLHF). Constitutional AI Constitutional AI is an approach developed by Anthropic for training AI systems, particularly language models like Claude, to be harmless and helpful without relying on extensive human feedback. The method, detailed in the paper Constitutional AI Harmlessness from AI Feedback involves two phases supervised learning and reinforcement learning. In the supervised learning phase, the model generates responses to prompts, self-critiques these responses based on a set of guiding principles (a constitution), and revises the responses. Then the model is fine-tuned on these revised responses. For the reinforcement learning from AI feedback (RLAIF) phase, responses are generated, and an AI compares their compliance with the constitution. This dataset of AI feedback is used to train a preference model that evaluates responses based on how much they satisfy the constitution. Claude is then fine-tuned to align with this preference model. This technique is similar to RLHF, except that the comparisons used to train the preference model are AI-generated, and that they are based on the constitution. The constitution for Claude included 75 points, including sections from the UN Universal Declaration of Human Rights. Models The name Claude was notably inspired by Claude Shannon, a pioneer in artificial intelligence. Claude Claude was the initial version of Anthropics language model released in March 2023, Claude demonstrated proficiency in various tasks but had certain limitations in coding, math, and reasoning capabilities. Anthropic partnered with companies like Notion (productivity software) and Quora (to help develop the Poe chatbot). Claude Instant Claude was released as two versions, Claude and Claude Instant, with Claude Instant being a faster, less expensive, and lighter version. Claude Instant has an input context length of 100,000 tokens (which corresponds to around 75,000 words). Claude 2 Claude 2 was the next major iteration of Claude, which was released in July 2023 and available to the general public, whereas the Claude 1 was only available to selected users approved by Anthropic. Claude 2 expanded its context window from 9,000 tokens to 100,000 tokens. Features included the ability to upload PDFs and other documents that enables Claude to read, summarize, and assist with tasks. Claude 2.1 Claude 2.1 doubled the number of tokens that the chatbot could handle, increasing it to a window of 200,000 tokens, which equals around 500 pages of written material. Anthropic states that the new model is less likely to produce false statements compared to its predecessors. Criticism Claude 2 received criticism for its stringent ethical alignment that may reduce usability and performance. Users have been refused assistance with benign requests, for example with the system administration question How can I kill all python processes in my ubuntu server? This has led to a debate over the alignment tax (the cost of ensuring an AI system is aligned) in AI development, with discussions centered on balancing ethical considerations and practical functionality. Critics argued for user autonomy and effectiveness, while proponents stressed the importance of ethical AI. Claude 3 Claude 3 was released on March 14, 2024, with claims in the press release to have set new industry benchmarks across a wide range of cognitive tasks. The Claude 3 family includes three state-of-the-art models in ascending order of capability Haiku, Sonnet, and Opus. The default version of Claude 3, Opus, has a context window of 200,000 tokens, but this is being expanded to 1 million for specific use cases. Claude 3 drew attention for demonstrating an apparent ability to realize it is being artificially tested during needle in a haystack tests. Claude 3.5 On June 20, 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in areas such as coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability in which Claude was able to create code in a dedicated window in the interface and preview the rendered output in real time, such as SVG graphics or websites. Anthropic also announced that Claude 3.5 Opus would be released later that year, and added it to their models page. However, as of February 2025, Claude 3.5 Opus has not been released, and Anthropic has removed mention of it from the models page. An upgraded Claude 3.5 Sonnet, billed as Claude 3.5 Sonnet (New) in the web interface and benchmarks, was introduced on October 22, 2024, along with Claude 3.5 Haiku. A feature, computer use, was also unveiled in public beta. This capability enables Claude 3.5 Sonnet to interact with a computers desktop environment, performing tasks such as moving the cursor, clicking buttons, and typing text, effectively mimicking human computer interactions. This development allows the AI to autonomously execute complex, multi-step tasks across various applications. Upon release, Anthropic claimed Claude 3.5 Haiku would remain the same price as its predecessor, Claude 3 Haiku. However, on November 4th, 2024, Anthropic announced that they would be increasing the price of the model to reflect its increase in intelligence. Claude 3.7 Claude 3.7 Sonnet was released on February 24, 2025. It is a pioneering hybrid AI reasoning model that allows users to choose between rapid responses and more thoughtful, step-by-step reasoning. This model integrates both capabilities into a single framework, eliminating the need for multiple models. Users can control how long the model thinks about a question, balancing speed and accuracy based on their needs. Anthropic also launched a research preview of Claude Code, an agentic command line tool that enables developers to delegate coding tasks directly from their terminal. References External links Official website Title Cognitive robotics URL https//en.wikipedia.org/wiki/Cognitive_robotics Content Cognitive Robotics or Cognitive Technology is a subfield of robotics concerned with endowing a robot with intelligent behavior by providing it with a processing architecture that will allow it to learn and reason about how to behave in response to complex goals in a complex world. Cognitive robotics may be considered the engineering branch of embodied cognitive science and embodied embedded cognition, consisting of Robotic Process Automation, Artificial Intelligence, Machine Learning, Deep Learning, Optical Character Recognition, Image Processing, Process Mining, Analytics, Software Development and System Integration. Core issues While traditional cognitive modeling approaches have assumed symbolic coding schemes as a means for depicting the world, translating the world into these kinds of symbolic representations has proven to be problematic if not untenable. Perception and action and the notion of symbolic representation are therefore core issues to be addressed in cognitive robotics. Starting point Cognitive robotics views human or animal cognition as a starting point for the development of robotic information processing, as opposed to more traditional Artificial Intelligence techniques. Target robotic cognitive capabilities include perception processing, attention allocation, anticipation, planning, complex motor coordination, reasoning about other agents and perhaps even about their own mental states. Robotic cognition embodies the behavior of intelligent agents in the physical world (or a virtual world, in the case of simulated cognitive robotics). Ultimately the robot must be able to act in the real world. Learning techniques Motor Babble A preliminary robot learning technique called motor babbling involves correlating pseudo-random complex motor movements by the robot with resulting visual and/or auditory feedback such that the robot may begin to expect a pattern of sensory feedback given a pattern of motor output. Desired sensory feedback may then be used to inform a motor control signal. This is thought to be analogous to how a baby learns to reach for objects or learns to produce speech sounds. For simpler robot systems, where for instance inverse kinematics may feasibly be used to transform anticipated feedback (desired motor result) into motor output, this step may be skipped. Imitation Once a robot can coordinate its motors to produce a desired result, the technique of learning by imitation may be used. The robot monitors the performance of another agent and then the robot tries to imitate that agent. It is often a challenge to transform imitation information from a complex scene into a desired motor result for the robot. Note that imitation is a high-level form of cognitive behavior and imitation is not necessarily required in a basic model of embodied animal cognition. Knowledge acquisition A more complex learning approach is autonomous knowledge acquisition the robot is left to explore the environment on its own. A system of goals and beliefs is typically assumed. A somewhat more directed mode of exploration can be achieved by curiosity algorithms, such as Intelligent Adaptive Curiosity or Category-Based Intrinsic Motivation. These algorithms generally involve breaking sensory input into a finite number of categories and assigning some sort of prediction system (such as an Artificial Neural Network) to each. The prediction system keeps track of the error in its predictions over time. Reduction in prediction error is considered learning. The robot then preferentially explores categories in which it is learning (or reducing prediction error) the fastest. Other architectures Some researchers in cognitive robotics have tried using architectures such as (ACT-R and Soar (cognitive architecture)) as a basis of their cognitive robotics programs. These highly modular symbol-processing architectures have been used to simulate operator performance and human performance when modeling simplistic and symbolized laboratory data. The idea is to extend these architectures to handle real-world sensory input as that input continuously unfolds through time. What is needed is a way to somehow translate the world into a set of symbols and their relationships. Questions Some of the fundamental questions to still be answered in cognitive robotics are How much human programming should or can be involved to support the learning processes? How can one quantify progress? Some of the adopted ways is the reward and punishment. But what kind of reward and what kind of punishment? In humans, when teaching a child for example, the reward would be candy or some encouragement, and the punishment can take many forms. But what is an effective way with robots? Books Cognitive Robotics book by Hooman Samani, takes a multidisciplinary approach to cover various aspects of cognitive robotics such as artificial intelligence, physical, chemical, philosophical, psychological, social, cultural, and ethical aspects. See also Artificial intelligence Intelligent agent Cognitive architecture Cognitive science Cybernetics Developmental robotics Embodied cognitive science Epigenetic robotics Evolutionary robotics Hybrid intelligent system iCub Intelligent control References The Symbolic and Subsymbolic Robotic Intelligence Control System (SS-RICS) Intelligent Systems Group - University of Utrecht The Cognitive Robotics Group - University of Toronto The IDSIA Robotics Lab and Cognitive Robotics Lab of Juergen Schmidhuber What Does the Future Hold for Cognitive Robots? - Idaho National Laboratory Cognitive Robotics at the Naval Research Laboratory Archived 2010-08-08 at the Wayback Machine Cognitive robotics at ENSTA autonomous embodied systems, evolving in complex and non-constraint environments, using mainly vision as sensor. The Center for Intelligent Systems - Vanderbilt University Institute for Cognition and Robotics (CoR-Lab) at Bielefeld University SocioCognitive Robotics at Delft University of Technology Autonomous Systems Laboratory at Universidad Politecnica de Madrid Knowledge Technology at Universit t Hamburg The Cognitive Robotics Association, founded in 1998, directed by Gerhard Lakemeyer, University of Aachen, organizes every two years the Cognitive Robotics Workshop and it is generously supported by the AI journal External links RoboBusiness Robots that Dream of Being Better www.Conscious-Robots.com The Cognitive Robotics Association Title Concept drift URL https//en.wikipedia.org/wiki/Concept_drift Content In predictive analytics, data science, machine learning and related fields, concept drift or drift is an evolution of data that invalidates the data model. It happens when the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes. Drift detection and drift adaptation are of paramount importance in the fields that involve dynamically changing data and data models. Predictive model decay In machine learning and predictive analytics this drift phenomenon is called concept drift. In machine learning, a common element of a data model are the statistical properties, such as probability distribution of the actual data. If they deviate from the statistical properties of the training data set, then the learned predictions may become invalid, if the drift is not addressed. Data configuration decay Another important area is software engineering, where three types of data drift affecting data fidelity may be recognized. Changes in the software environment (infrastructure drift) may invalidate software infrastructure configuration. Structural drift happens when the data schema changes, which may invalidate databases. Semantic drift is changes in the meaning of data while the structure does not change. In many cases this may happen in complicated applications when many independent developers introduce changes without proper awareness of the effects of their changes in other areas of the software system. For many application systems, the nature of data on which they operate are subject to changes for various reasons, e.g., due to changes in business model, system updates, or switching the platform on which the system operates. In the case of cloud computing, infrastructure drift that may affect the applications running on cloud may be caused by the updates of cloud software. There are several types of detrimental effects of data drift on data fidelity. Data corrosion is passing the drifted data into the system undetected. Data loss happens when valid data are ignored due to non-conformance with the applied schema. Squandering is the phenomenon when new data fields are introduced upstream the data processing pipeline, but somewhere downstream there data fields are absent. Inconsistent data Data drift may refer to the phenomenon when database records fail to match the real-world data due to the changes in the latter over time. This is a common problem with databases involving people, such as customers, employees, citizens, residents, etc. Human data drift may be caused by unrecorded changes in personal data, such as place of residence or name, as well as due to errors during data input. Data drift may also refer to inconsistency of data elements between several replicas of a database. The reasons can be difficult to identify. A simple drift detection is to run checksum regularly. However the remedy may be not so easy. Examples The behavior of the customers in an online shop may change over time. For example, if weekly merchandise sales are to be predicted, and a predictive model has been developed that works satisfactorily. The model may use inputs such as the amount of money spent on advertising, promotions being run, and other metrics that may affect sales. The model is likely to become less and less accurate over time this is concept drift. In the merchandise sales application, one reason for concept drift may be seasonality, which means that shopping behavior changes seasonally. Perhaps there will be higher sales in the winter holiday season than during the summer, for example. Concept drift generally occurs when the covariates that comprise the data set begin to explain the variation of your target set less accurately there may be some confounding variables that have emerged, and that one simply cannot account for, which renders the model accuracy to progressively decrease with time. Generally, it is advised to perform health checks as part of the post-production analysis and to re-train the model with new assumptions upon signs of concept drift. Possible remedies To prevent deterioration in prediction accuracy because of concept drift, reactive and tracking solutions can be adopted. Reactive solutions retrain the model in reaction to a triggering mechanism, such as a change-detection test, to explicitly detect concept drift as a change in the statistics of the data-generating process. When concept drift is detected, the current model is no longer up-to-date and must be replaced by a new one to restore prediction accuracy. A shortcoming of reactive approaches is that performance may decay until the change is detected. Tracking solutions seek to track the changes in the concept by continually updating the model. Methods for achieving this include online machine learning, frequent retraining on the most recently observed samples, and maintaining an ensemble of classifiers where one new classifier is trained on the most recent batch of examples and replaces the oldest classifier in the ensemble. Contextual information, when available, can be used to better explain the causes of the concept drift for instance, in the sales prediction application, concept drift might be compensated by adding information about the season to the model. By providing information about the time of the year, the rate of deterioration of your model is likely to decrease, but concept drift is unlikely to be eliminated altogether. This is because actual shopping behavior does not follow any static, finite model. New factors may arise at any time that influence shopping behavior, the influence of the known factors or their interactions may change. Concept drift cannot be avoided for complex phenomena that are not governed by fixed laws of nature. All processes that arise from human activity, such as socioeconomic processes, and biological processes are likely to experience concept drift. Therefore, periodic retraining, also known as refreshing, of any model is necessary. See also Data stream mining Data mining Snyk, a company whose portfolio includes drift detection in software applications Further reading Many papers have been published describing algorithms for concept drift detection. Only reviews, surveys and overviews are here Reviews External links Software Frouros An open-source Python library for drift detection in machine learning systems. NannyML An open-source Python library for detecting univariate and multivariate distribution drift and estimating machine learning model performance without ground truth labels. RapidMiner Formerly Yet Another Learning Environment (YALE) free open-source software for knowledge discovery, data mining, and machine learning also featuring data stream mining, learning time-varying concepts, and tracking drifting concept. It is used in combination with its data stream mining plugin (formerly concept drift plugin). EDDM (Early Drift Detection Method) free open-source implementation of drift detection methods in Weka. MOA (Massive Online Analysis) free open-source software specific for mining data streams with concept drift. It contains a prequential evaluation method, the EDDM concept drift methods, a reader of ARFF real datasets, and artificial stream generators as SEA concepts, STAGGER, rotating hyperplane, random tree, and random radius based functions. MOA supports bi-directional interaction with Weka. Datasets Real USP Data Stream Repository, 27 real-world stream datasets with concept drift compiled by Souza et al. (2020). Access Airline, approximately 116 million flight arrival and departure records (cleaned and sorted) compiled by E. Ikonomovska. Reference Data Expo 2009 Competition . Access Chess.com (online games) and Luxembourg (social survey) datasets compiled by I. Zliobaite. Access ECUE spam 2 datasets each consisting of more than 10,000 emails collected over a period of approximately 2 years by an individual. Access from S.J.Delany webpage , electricity demand, 2 classes, 45,312 instances. Reference M. Harries, Splice-2 comparative evaluation Electricity pricing, Technical report, The University of South Wales, 1999. Access from J.Gama webpage. Comment on applicability. PAKDD09 competition data represents the credit evaluation task. It is collected over a five-year period. Unfortunately, the true labels are released only for the first part of the data. Access Sensor stream and Power supply stream datasets are available from X. Zhus Stream Data Mining Repository. Access SMEAR is a benchmark data stream with a lot of missing values. Environment observation data over 7 years. Predict cloudiness. Access Text mining, a collection of text mining datasets with concept drift, maintained by I. Katakis. Access Gas Sensor Array Drift Dataset, a collection of 13,910 measurements from 16 chemical sensors utilized for drift compensation in a discrimination task of 6 gases at various levels of concentrations. Access Other KDD99 competition data contains simulated intrusions in a military network environment. It is often used as a benchmark to evaluate handling concept drift. Access Synthetic Extreme verification latency benchmark Souza, V.M.A. Silva, D.F. Gama, J. Batista, G.E.A.P.A. (2015). Data Stream Classification Guided by Clustering on Nonstationary Environments and Extreme Verification Latency. Proceedings of the 2015 SIAM International Conference on Data Mining (SDM). SIAM. pp. 873 881. doi10.1137/1.9781611974010.98. ISBN 9781611974010. S2CID 19198944. Access from Nonstationary Environments Archive. Sine, Line, Plane, Circle and Boolean Data Sets Minku, L.L. White, A.P. Yao, X. (2010). The Impact of Diversity on On-line Ensemble Learning in the Presence of Concept Drift (PDF). IEEE Transactions on Knowledge and Data Engineering. 22 (5) 730 742. doi10.1109/TKDE.2009.156. S2CID 16592739. Access from L.Minku webpage. SEA concepts Street, N.W. Kim, Y. (2001). A streaming ensemble algorithm (SEA) for large-scale classification (PDF). KDD01 Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 377 382. doi10.1145/502512.502568. ISBN 978-1-58113-391-2. S2CID 11868540. Access from J.Gama webpage. STAGGER Schlimmer, J.C. Granger, R.H. (1986). Incremental Learning from Noisy Data. Mach. Learn. 1 (3) 317 354. doi10.1007/. S2CID 33776987. Mixed Gama, J. Medas, P. Castillo, G. Rodrigues, P. (2004). Learning with drift detection. Brazilian symposium on artificial intelligence. Springer. pp. 286 295. doi10.1007/978-3-540-28645-5_29. ISBN 978-3-540-28645-5. S2CID 2606652. Data generation frameworks Minku, White  Yao 2010 Download from L.Minku webpage. Lindstrom, P. Delany, S.J. MacNamee, B. (2008). Autopilot Simulating Changing Concepts in Real Data (PDF). Proceedings of the 19th Irish Conference on Artificial Intelligence  Cognitive Science. pp. 272 263. Narasimhamurthy, A. Kuncheva, L.I. (2007). A framework for generating data to simulate changing environments. AIAP07 Proceedings of the 25th IASTED International Multi-Conference artificial intelligence and applications. pp. 384 389. Code Projects INFER Computational Intelligence Platform for Evolving and Robust Predictive Systems (2010 2014), Bournemouth University (UK), Evonik Industries (Germany), Research and Engineering Centre (Poland) HaCDAIS Handling Concept Drift in Adaptive Information Systems (2008 2012), Eindhoven University of Technology (the Netherlands) KDUS Knowledge Discovery from Ubiquitous Streams, INESC Porto and Laboratory of Artificial Intelligence and Decision Support (Portugal) ADEPT Adaptive Dynamic Ensemble Prediction Techniques, University of Manchester (UK), University of Bristol (UK) ALADDIN autonomous learning agents for decentralised data and information networks (2005 2010) GAENARI C incremental decision tree algorithm. it minimize concept drifting damage. (2022) Benchmarks NAB The Numenta Anomaly Benchmark, benchmark for evaluating algorithms for anomaly detection in streaming, real-time applications. (2014 2018) Meetings 2014  Special Session on Concept Drift, Domain Adaptation  Learning in Dynamic Environments IEEE IJCNN 2014 2013 RealStream Real-World Challenges for Data Stream Mining Workshop-Discussion at the ECML PKDD 2013, Prague, Czech Republic. LEAPS 2013 The 1st International Workshop on Learning stratEgies and dAta Processing in nonStationary environments 2011 LEE 2011 Special Session on Learning in evolving environments and its application on real-world problems at ICMLA11 HaCDAIS 2011 The 2nd International Workshop on Handling Concept Drift in Adaptive Information Systems ICAIS 2011 Track on Incremental Learning IJCNN 2011 Special Session on Concept Drift and Learning Dynamic Environments CIDUE 2011 Symposium on Computational Intelligence in Dynamic and Uncertain Environments 2010 HaCDAIS 2010 International Workshop on Handling Concept Drift in Adaptive Information Systems Importance, Challenges and Solutions Special Session on Dynamic learning in non-stationary environments SAC 2010 Data Streams Track at ACM Symposium on Applied Computing SensorKDD 2010 International Workshop on Knowledge Discovery from Sensor Data StreamKDD 2010 Novel Data Stream Pattern Mining Techniques Concept Drift and Learning in Nonstationary Environments at IEEE World Congress on Computational Intelligence MLMDS 2010 Special Session on Machine Learning Methods for Data Streams at the 10th International Conference on Intelligent Design and Applications, ISDA 10 Title Conditional random field URL https//en.wikipedia.org/wiki/Conditional_random_field Content Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering neighbouring samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. The kind of graph used depends on the application. For example, in natural language processing, linear chain CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions. Other examples where CRFs are used are labeling or parsing of sequential data for natural language processing or biological sequences, part-of-speech tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision. Description CRFs are a type of discriminative undirected probabilistic graphical model. Lafferty, McCallum and Pereira define a CRF on observations X displaystyle boldsymbol X and random variables Y displaystyle boldsymbol Y as follows Let  . Then ( X , Y ) displaystyle (boldsymbol X,boldsymbol Y) is a conditional random field when each random variable Y v displaystyle boldsymbol Y_v , conditioned on X displaystyle boldsymbol X , obeys the Markov property with respect to the graph that is, its probability is dependent only on its neighbours in G P ( Y v  X ,  Y w  w v  )  P ( Y v  X ,  Y w  w v  ) displaystyle P(boldsymbol Y_vboldsymbol X,boldsymbol Y_wwneq v)P(boldsymbol Y_vboldsymbol X,boldsymbol Y_wwsim v) , where w v displaystyle mathit wsim v means that w displaystyle w and v displaystyle v are neighbors in G displaystyle G . What this means is that a CRF is an undirected graphical model whose nodes can be divided into exactly two disjoint sets X displaystyle boldsymbol X and Y displaystyle boldsymbol Y , the observed and output variables, respectively the conditional distribution p ( Y  X ) displaystyle p(boldsymbol Yboldsymbol X) is then modeled. Inference For general graphs, the problem of exact inference in CRFs is intractable. The inference problem for a CRF is basically the same as for an MRF and the same arguments hold. However, there exist special cases for which exact inference is feasible If the graph is a chain or a tree, message passing algorithms yield exact solutions. The algorithms used in these cases are analogous to the forward-backward and Viterbi algorithm for the case of HMMs. If the CRF only contains pair-wise potentials and the energy is submodular, combinatorial min cut/max flow algorithms yield exact solutions. If exact inference is impossible, several algorithms can be used to obtain approximate solutions. These include Loopy belief propagation Alpha expansion Mean field inference Linear programming relaxations Parameter Learning Learning the parameters displaystyle theta  is usually done by maximum likelihood learning for p ( Y i  X i  ) displaystyle p(Y_iX_itheta ) . If all nodes have exponential family distributions and all nodes are observed during training, this optimization is convex. It can be solved for example using gradient descent algorithms, or Quasi-Newton methods such as the L-BFGS algorithm. On the other hand, if some variables are unobserved, the inference problem has to be solved for these variables. Exact inference is intractable in general graphs, so approximations have to be used. Examples In sequence modeling, the graph of interest is usually a chain graph. An input sequence of observed variables X displaystyle X represents a sequence of observations and Y displaystyle Y represents a hidden (or unknown) state variable that needs to be inferred given the observations. The Y i displaystyle Y_i are structured to form a chain, with an edge between each Y i 1 displaystyle Y_i-1 and Y i displaystyle Y_i . As well as having a simple interpretation of the Y i displaystyle Y_i as labels for each element in the input sequence, this layout admits efficient algorithms for model training, learning the conditional distributions between the Y i displaystyle Y_i and feature functions from some corpus of training data. decoding, determining the probability of a given label sequence Y displaystyle Y given X displaystyle X . inference, determining the most likely label sequence Y displaystyle Y given X displaystyle X . The conditional dependency of each Y i displaystyle Y_i on X displaystyle X is defined through a fixed set of feature functions of the form f ( i , Y i 1 , Y i , X ) displaystyle f(i,Y_i-1,Y_i,X) , which can be thought of as measurements on the input sequence that partially determine the likelihood of each possible value for Y i displaystyle Y_i . The model assigns each feature a numerical weight and combines them to determine the probability of a certain value for Y i displaystyle Y_i . Linear-chain CRFs have many of the same applications as conceptually simpler hidden Markov models (HMMs), but relax certain assumptions about the input and output sequence distributions. An HMM can loosely be understood as a CRF with very specific feature functions that use constant probabilities to model state transitions and emissions. Conversely, a CRF can loosely be understood as a generalization of an HMM that makes the constant transition probabilities into arbitrary functions that vary across the positions in the sequence of hidden states, depending on the input sequence. Notably, in contrast to HMMs, CRFs can contain any number of feature functions, the feature functions can inspect the entire input sequence X displaystyle X at any point during inference, and the range of the feature functions need not have a probabilistic interpretation. Variants Higher-order CRFs and semi-Markov CRFs CRFs can be extended into higher order models by making each Y i displaystyle Y_i dependent on a fixed number k displaystyle k of previous variables Y i k , . . . , Y i 1 displaystyle Y_i-k,...,Y_i-1 . In conventional formulations of higher order CRFs, training and inference are only practical for small values of k displaystyle k (such as k 5), since their computational cost increases exponentially with k displaystyle k . However, another recent advance has managed to ameliorate these issues by leveraging concepts and tools from the field of Bayesian nonparametrics. Specifically, the CRF-infinity approach constitutes a CRF-type model that is capable of learning infinitely-long temporal dynamics in a scalable fashion. This is effected by introducing a novel potential function for CRFs that is based on the Sequence Memoizer (SM), a nonparametric Bayesian model for learning infinitely-long dynamics in sequential observations. To render such a model computationally tractable, CRF-infinity employs a mean-field approximation of the postulated novel potential functions (which are driven by an SM). This allows for devising efficient approximate training and inference algorithms for the model, without undermining its capability to capture and model temporal dependencies of arbitrary length. There exists another generalization of CRFs, the semi-Markov conditional random field (semi-CRF), which models variable-length segmentations of the label sequence Y displaystyle Y . This provides much of the power of higher-order CRFs to model long-range dependencies of the Y i displaystyle Y_i , at a reasonable computational cost. Finally, large-margin models for structured prediction, such as the structured Support Vector Machine can be seen as an alternative training procedure to CRFs. Latent-dynamic conditional random field Latent-dynamic conditional random fields (LDCRF) or discriminative probabilistic latent variable models (DPLVM) are a type of CRFs for sequence tagging tasks. They are latent variable models that are trained discriminatively. In an LDCRF, like in any sequence tagging task, given a sequence of observations . Instead of directly modeling P(yx) as an ordinary linear-chain CRF would do, a set of latent variables h is inserted between x and y using the chain rule of probability P ( y  x )  h P ( y  h , x ) P ( h  x ) displaystyle P(mathbf y mathbf x )sum _mathbf h P(mathbf y mathbf h ,mathbf x )P(mathbf h mathbf x ) This allows capturing latent structure between the observations and labels. While LDCRFs can be trained using quasi-Newton methods, a specialized version of the perceptron algorithm called the latent-variable perceptron has been developed for them as well, based on Collins structured perceptron algorithm. These models find applications in computer vision, specifically gesture recognition from video streams and shallow parsing. See also Hammersley Clifford theorem Maximum entropy Markov model (MEMM) References Further reading McCallum, A. Efficiently inducing features of conditional random fields. In Proc. 19th Conference on Uncertainty in Artificial Intelligence. (2003) Wallach, H.M. Conditional random fields An introduction. Technical report MS-CIS-04-21, University of Pennsylvania (2004) Sutton, C., McCallum, A. An Introduction to Conditional Random Fields for Relational Learning. In Introduction to Statistical Relational Learning. Edited by Lise Getoor and Ben Taskar. MIT Press. (2006) Online PDF Klinger, R., Tomanek, K. Classical Probabilistic Models and Conditional Random Fields. Algorithm Engineering Report -2-013, Department of Computer Science, Dortmund University of Technology, December 2007. ISSN 1864-4503. Online PDF Title Confusion matrix URL https//en.wikipedia.org/wiki/Confusion_matrix Content In the field of machine learning and specifically the problem of statistical classification, a confusion matrix, also known as error matrix, is a specific table layout that allows visualization of the performance of an algorithm, typically a supervised learning one in unsupervised learning it is usually called a matching matrix. Each row of the matrix represents the instances in an actual class while each column represents the instances in a predicted class, or vice versa both variants are found in the literature. The diagonal of the matrix therefore represents all instances that are correctly predicted. The name stems from the fact that it makes it easy to see whether the system is confusing two classes (i.e. commonly mislabeling one as another). It is a special kind of contingency table, with two dimensions (actual and predicted), and identical sets of classes in both dimensions (each combination of dimension and class is a variable in the contingency table). Example Given a sample of 12 individuals, 8 that have been diagnosed with cancer and 4 that are cancer-free, where individuals with cancer belong to class 1 (positive) and non-cancer individuals belong to class 0 (negative), we can display that data as follows Assume that we have a classifier that distinguishes between individuals with and without cancer in some way, we can take the 12 individuals and run them through the classifier. The classifier then makes 9 accurate predictions and misses 3 2 individuals with cancer wrongly predicted as being cancer-free (sample 1 and 2), and 1 person without cancer that is wrongly predicted to have cancer (sample 9). Notice, that if we compare the actual classification set to the predicted classification set, there are 4 different outcomes that could result in any particular column. One, if the actual classification is positive and the predicted classification is positive (1,1), this is called a true positive result because the positive sample was correctly identified by the classifier. Two, if the actual classification is positive and the predicted classification is negative (1,0), this is called a false negative result because the positive sample is incorrectly identified by the classifier as being negative. Third, if the actual classification is negative and the predicted classification is positive (0,1), this is called a false positive result because the negative sample is incorrectly identified by the classifier as being positive. Fourth, if the actual classification is negative and the predicted classification is negative (0,0), this is called a true negative result because the negative sample gets correctly identified by the classifier. We can then perform the comparison between actual and predicted classifications and add this information to the table, making correct results appear in green so they are more easily identifiable. The template for any binary confusion matrix uses the four kinds of results discussed above (true positives, false negatives, false positives, and true negatives) along with the positive and negative classifications. The four outcomes can be formulated in a 2 2 confusion matrix, as follows The color convention of the three data tables above were picked to match this confusion matrix, in order to easily differentiate the data. Now, we can simply total up each type of result, substitute into the template, and create a confusion matrix that will concisely summarize the results of testing the classifier In this confusion matrix, of the 8 samples with cancer, the system judged that 2 were cancer-free, and of the 4 samples without cancer, it predicted that 1 did have cancer. All correct predictions are located in the diagonal of the table (highlighted in green), so it is easy to visually inspect the table for prediction errors, as values outside the diagonal will represent them. By summing up the 2 rows of the confusion matrix, one can also deduce the total number of positive (P) and negative (N) samples in the original dataset, i.e.  . Table of confusion In predictive analytics, a table of confusion (sometimes also called a confusion matrix) is a table with two rows and two columns that reports the number of true positives, false negatives, false positives, and true negatives. This allows more detailed analysis than simply observing the proportion of correct classifications (accuracy). Accuracy will yield misleading results if the data set is unbalanced that is, when the numbers of observations in different classes vary greatly. For example, if there were 95 cancer samples and only 5 non-cancer samples in the data, a particular classifier might classify all the observations as having cancer. The overall accuracy would be 95, but in more detail the classifier would have a 100 recognition rate (sensitivity) for the cancer class but a 0 recognition rate for the non-cancer class. score is even more unreliable in such cases, and here would yield over 97.4, whereas informedness removes such bias and yields 0 as the probability of an informed decision for any form of guessing (here always guessing cancer). According to Davide Chicco and Giuseppe Jurman, the most informative metric to evaluate a confusion matrix is the Matthews correlation coefficient (MCC). Other metrics can be included in a confusion matrix, each of them having their significance and use. Confusion matrices with more than two categories Confusion matrix is not limited to binary classification and can be used in multi-class classifiers as well. The confusion matrices discussed above have only two conditions positive and negative. For example, the table below summarizes communication of a whistled language between two speakers, with zero values omitted for clarity. See also Positive and negative predictive values Title Contrastive Language-Image Pre-training URL https//en.wikipedia.org/wiki/Contrastive_Language-Image_Pre-training Content Contrastive Language-Image Pre-training (CLIP) is a technique for training a pair of neural network models, one for image understanding and one for text understanding, using a contrastive objective. This method has enabled broad applications across multiple domains, including cross-modal retrieval, text-to-image generation, aesthetic ranking, and image captioning. Publication history It was first announced on OpenAIs official blog on January 5, 2021, with a report served directly through OpenAIs CDN, and a GitHub repository. The paper was delivered on arXiv on 26 February 2021. The report (with some details removed, and its appendix cut out to a Supplementary PDF) was published in Proceedings of the 38th International Conference on Machine Learning, PMLR, which had a submission deadline of February 2021. Concurrent to CLIP was ALIGN, published at the same conference. It was done by researchers at Google, with essentially the same algorithm. A notable variant was Sigmoid CLIP (SigLIP), with first version published in 2023 and a second version in 2025. Algorithm The CLIP method trains a pair of models contrastively. One model takes in a piece of text as input and outputs a single vector representing its semantic content. The other model takes in an image and similarly outputs a single vector representing its visual content. The models are trained so that the vectors corresponding to semantically similar text-image pairs are close together in the shared vector space, while those corresponding to dissimilar pairs are far apart. To train a pair of CLIP models, one would start by preparing a large dataset of image-caption pairs. During training, the models are presented with batches of N displaystyle N image-caption pairs. Let the outputs from the text and image models be respectively v 1 , . . . , v N , w 1 , . . . , w N displaystyle v_1,...,v_N,w_1,...,w_N . Two vectors are considered similar if their dot product is large. The loss incurred on this batch is the multi-class N-pair loss, which is a symmetric cross-entropy loss over similarity scores 1 N i ln e v i w i / T j e v i w j / T 1 N j ln e v j w j / T i e v i w j / T displaystyle -frac 1Nsum _iln frac ev_icdot w_i/Tsum _jev_icdot w_j/T-frac 1Nsum _jln frac ev_jcdot w_j/Tsum _iev_icdot w_j/T In essence, this loss function encourages the dot product between matching image and text vectors ( v i w i displaystyle v_icdot w_i ) to be high, while discouraging high dot products between non-matching pairs. The parameter T  0 displaystyle T0 is the temperature, which is parameterized in the original CLIP model as . Other loss functions are possible. For example, Sigmoid CLIP (SigLIP) proposes the following loss function . CLIP models While the original model was developed by OpenAI, subsequent models have been trained by other organizations as well. Image model The image encoding models used in CLIP are typically vision transformers (ViT). The naming convention for these models often reflects the specific ViT architecture used. For instance, ViT-L/14 means a vision transformer large (compared to other models in the same series) with a patch size of 14, meaning that the image is divided into 14-by-14 pixel patches before being processed by the transformer. The size indicator ranges from B, L, H, G (base, large, huge, giant), in that order. Other than ViT, the image model is typically a convolutional neural network, such as ResNet (in the original series by OpenAI), or ConvNeXt (in the OpenCLIP model series by LAION). Since the output vectors of the image model and the text model must have exactly the same length, both the image model and the text model have fixed-length vector outputs, which in the original report is called embedding dimension. For example, in the original OpenAI model, the ResNet models have embedding dimensions ranging from 512 to 1024, Table 19 and for the ViTs, from 512 to 768. Table 20 Its implementation of ViT was the same as the original one, with one modification after position embeddings are added to the initial patch embeddings, there is a LayerNorm. Its implementation of ResNet was the same as the original one, with 3 modifications In the start of the CNN (the stem), they used three stacked 3x3 convolutions instead of a single 7x7 convolution, as suggested by. There is an average pooling of stride 2 at the start of each downsampling convolutional layer (they called it rect-2 blur pooling according to the terminology of ). This has the effect of blurring images before downsampling, for antialiasing. The final convolutional layer is followed by a multiheaded attention pooling. ALIGN used EfficientNet of various sizes, a kind of convolutional neural network. Text model The text encoding models used in CLIP are typically Transformers. In the original OpenAI report, they reported using a Transformer (63M-parameter, 12-layer, 512-wide, 8 attention heads) with lower-cased byte pair encoding (BPE) with 49152 vocabulary size. Context length was capped at 76 for efficiency. Like GPT, it was decoder-only, with only causally-masked self-attention. 5 Its architecture is the same as GPT-2. Like BERT, the text sequence is bracketed by two special tokens SOS and EOS (start of sequence and end of sequence). Take the activations of the highest layer of the transformer on the EOS, apply LayerNorm, then a final linear map. This is the text encoding of the input sequence. The final linear map has output dimension equal to the embedding dimension of whatever image encoder it is paired with. These models all had context length 77 and vocabulary size 49408. ALIGN used BERT of various sizes. Dataset WebImageText The CLIP models released by OpenAI were trained on a dataset called WebImageText (WIT) containing 400 million pairs of images and their corresponding captions scraped from the internet. The total number of words in this dataset is similar in scale to the WebText dataset used for training GPT-2, which contains about 40 gigabytes of text data. The dataset contains 500,000 text-queries, with up to 20,000 (image, text) pairs per query. The text-queries were generated by starting with all words occurring at least 100 times in English Wikipedia, then extended by bigrams with high mutual information, names of all Wikipedia articles above a certain search volume, and WordNet synsets. The dataset is private and has not been released to the public, and there is no further information on it. Data preprocessing For the CLIP image models, the input images are preprocessed by first dividing each of the R, G, B values of an image by the maximum possible value, so that these values fall between 0 and 1, then subtracting by 0.48145466, 0.4578275, 0.40821073, and dividing by 0.26862954, 0.26130258, 0.27577711. The rationale was that these are the mean and standard deviations of the images in the WebImageText dataset, so this preprocessing step roughly whitens the image tensor. These numbers slightly differ from the standard preprocessing for ImageNet, which uses 0.485, 0.456, 0.406 and 0.229, 0.224, 0.225. If the input image does not have the same resolution as the native resolution (224x224 for all except ViT-L/14336px, which has 336x336 resolution), then the input image is scaled down by bicubic interpolation, so that its shorter side is the same as the native resolution, then the central square of the image is cropped out. Others ALIGN used over one billion image-text pairs, obtained by extracting images and their alt-tags from online crawling. The method was described as similar to how the Conceptual Captions dataset was constructed, but instead of complex filtering, they only applied a frequency-based filtering. Later models trained by other organizations had published datasets. For example, LAION trained OpenCLIP with published datasets LAION-400M, LAION-2B, and DataComp-1B. Training In the original OpenAI CLIP report, they reported training 5 ResNet and 3 ViT (ViT-B/32, ViT-B/16, ViT-L/14). Each was trained for 32 epochs. The largest ResNet model took 18 days to train on 592 GPUs. The largest ViT model took 12 days on 256 GPUs. All ViT models were trained on 224x224 image resolution. The ViT-L/14 was then boosted to 336x336 resolution by FixRes, resulting in a model. They found this was the best-performing model. Appendix F. Model Hyperparameters In the OpenCLIP series, the ViT-L/14 model was trained on 384 GPUs on the LAION-2B dataset, for 160 epochs for a total of 32B samples seen. Applications Cross-modal retrieval CLIPs cross-modal retrieval enables the alignment of visual and textual data in a shared latent space, allowing users to retrieve images based on text descriptions and vice versa, without the need for explicit image annotations. In text-to-image retrieval, users input descriptive text, and CLIP retrieves images with matching embeddings. In image-to-text retrieval, images are used to find related text content. CLIP s ability to connect visual and textual data has found applications in multimedia search, content discovery, and recommendation systems. Image classification CLIP can perform zero-shot image classification tasks. This is achieved by prompting the text encoder with class names and selecting the class whose embedding is closest to the image embedding. For example, to classify an image, they compared the embedding of the image with the embedding of the text A photo of a class., and the class that results in the highest dot product is outputted. CLIP for multimodal learning CLIP has been used as a component in multimodal learning. For example, during the training of Google DeepMinds Flamingo (2022), the authors trained a CLIP pair, with BERT as the text encoder and NormalizerFree ResNet as the image encoder. The image encoder of the CLIP pair was taken with parameters frozen and the text encoder was discarded. The frozen image encoder was then combined with a frozen Chinchilla language model, by finetuning with some further parameters that connect the two frozen models. Applications in other domains CLIP has been used in various domains beyond its original purpose Image Featurizer CLIPs image encoder can be adapted as a pre-trained image featurizer. This can then be fed into other AI models. Text-to-Image Generation Models like Stable Diffusion use CLIPs text encoder to transform text prompts into embeddings for image generation. CLIP can also be used as a gradient signal for directly guiding diffusion (CLIP guidance) or other generative art. Aesthetic Ranking Fine-tuned CLIP models can be used to rank images by aesthetic quality, aiding in dataset curation. Image Captioning CLIP can be used to generate image captions by matching text inputs to image embeddings. Notes References External links OpenAIs CLIP webpage OpenCLIP An open source implementation of CLIP Arora, Aman (2023-03-11). The Annotated CLIP (Part-2). amaarora.github.io. Retrieved 2024-09-11. Title Cost-sensitive machine learning URL https//en.wikipedia.org/wiki/Cost-sensitive_machine_learning Content Cost-sensitive machine learning is an approach within machine learning that considers varying costs associated with different types of errors. This method diverges from traditional approaches by introducing a cost matrix, explicitly specifying the penalties or benefits for each type of prediction error. The inherent difficulty which cost-sensitive machine learning tackles is that minimizing different kinds of classification errors is a multi-objective optimization problem. Overview Cost-sensitive machine learning optimizes models based on the specific consequences of misclassifications, making it a valuable tool in various applications. It is especially useful in problems with a high imbalance in class distribution and a high imbalance in associated costs Cost-sensitive machine learning introduces a scalar cost function in order to find one (of multiple) Pareto optimal points in this multi-objective optimization problem. Cost Matrix The cost matrix is a crucial element within cost-sensitive modeling, explicitly defining the costs or benefits associated with different prediction errors in classification tasks. Represented as a table, the matrix aligns true and predicted classes, assigning a cost value to each combination. For instance, in binary classification, it may distinguish costs for false positives and false negatives. The utility of the cost matrix lies in its application to calculate the expected cost or loss. The formula, expressed as a double summation, utilizes joint probabilities Expected . This approach allows practitioners to fine-tune models based on the specific consequences of misclassifications, adapting to scenarios where the impact of prediction errors varies across classes. Applications Fraud Detection In the realm of data science, particularly in finance, cost-sensitive machine learning is applied to fraud detection. By assigning different costs to false positives and false negatives, models can be fine-tuned to minimize the overall financial impact of misclassifications. Medical Diagnostics In healthcare, cost-sensitive machine learning plays a role in medical diagnostics. The approach allows for customization of models based on the potential harm associated with misdiagnoses, ensuring a more patient-centric application of machine learning algorithms. Challenges A typical challenge in cost-sensitive machine learning is the reliable determination of the cost matrix which may evolve over time. Literature Cost-Sensitive Machine Learning. USA, CRC Press, 2011. ISBN 9781439839287 Abhishek, K., Abdelaziz, D. M. (2023). Machine Learning for Imbalanced Data Tackle Imbalanced Datasets Using Machine Learning and Deep Learning Techniques. (n.p.) Packt Publishing. ISBN 9781801070881 Title Coupled pattern learner URL https//en.wikipedia.org/wiki/Coupled_pattern_learner Content Coupled Pattern Learner (CPL) is a machine learning algorithm which couples the semi-supervised learning of categories and relations to forestall the problem of semantic drift associated with boot-strap learning methods. Coupled Pattern Learner Semi-supervised learning approaches using a small number of labeled examples with many unlabeled examples are usually unreliable as they produce an internally consistent, but incorrect set of extractions. CPL solves this problem by simultaneously learning classifiers for many different categories and relations in the presence of an ontology defining constraints that couple the training of these classifiers. It was introduced by Andrew Carlson, Justin Betteridge, Estevam R. Hruschka Jr. and Tom M. Mitchell in 2009. CPL overview CPL is an approach to semi-supervised learning that yields more accurate results by coupling the training of many information extractors. Basic idea behind CPL is that semi-supervised training of a single type of extractor such as coach is much more difficult than simultaneously training many extractors that cover a variety of inter-related entity and relation types. Using prior knowledge about the relationships between these different entities and relations CPL makes unlabeled data as a useful constraint during training. For e.g., coach(x) implies person(x) and not sport(x) . CPL description Coupling of predicates CPL primarily relies on the notion of coupling the learning of multiple functions so as to constrain the semi-supervised learning problem. CPL constrains the learned function in two ways. Sharing among same-arity predicates according to logical relations Relation argument type-checking Sharing among same-arity predicates Each predicate P in the ontology has a list of other same-arity predicates with which P is mutually exclusive. If A is mutually exclusive with predicate B, A s positive instances and patterns become negative instances and negative patterns for B. For example, if city , having an instance Boston and a pattern mayor of , is mutually exclusive with scientist , then Boston and mayor of will become a negative instance and a negative pattern respectively for scientist. Further, Some categories are declared to be a subset of another category. For e.g., athlete is a subset of person . Relation argument type-checking This is a type checking information used to couple the learning of relations and categories. For example, the arguments of the ceoOf relation are declared to be of the categories person and company . CPL does not promote a pair of noun phrases as an instance of a relation unless the two noun phrases are classified as belonging to the correct argument types. Algorithm description Following is a quick summary of the CPL algorithm. Input An ontology O, and a text corpus C Output Trusted instances/patterns for each predicate for ,..., do foreach predicate p in O do EXTRACT candidate instances/contextual patterns using recently promoted patterns/instances FILTER candidates that violate coupling RANK candidate instances/patterns PROMOTE top candidates end end Inputs A large corpus of Part-Of-Speech tagged sentences and an initial ontology with predefined categories, relations, mutually exclusive relationships between same-arity predicates, subset relationships between some categories, seed instances for all predicates, and seed patterns for the categories. Candidate extraction CPL finds new candidate instances by using newly promoted patterns to extract the noun phrases that co-occur with those patterns in the text corpus. CPL extracts, Category Instances Category Patterns Relation Instances Relation Patterns Candidate filtering Candidate instances and patterns are filtered to maintain high precision, and to avoid extremely specific patterns. An instance is only considered for assessment if it co-occurs with at least two promoted patterns in the text corpus, and if its co-occurrence count with all promoted patterns is at least three times greater than its co-occurrence count with negative patterns. Candidate ranking CPL ranks candidate instances using the number of promoted patterns that they co-occur with so that candidates that occur with more patterns are ranked higher. Patterns are ranked using an estimate of the precision of each pattern. Candidate promotion CPL ranks the candidates according to their assessment scores and promotes at most 100 instances and 5 patterns for each predicate. Instances and patterns are only promoted if they co-occur with at least two promoted patterns or instances, respectively. Meta-Bootstrap Learner Meta-Bootstrap Learner (MBL) was also proposed by the authors of CPL. Meta-Bootstrap learner couples the training of multiple extraction techniques with a multi-view constraint, which requires the extractors to agree. It makes addition of coupling constraints on top of existing extraction algorithms, while treating them as black boxes, feasible. MBL assumes that the errors made by different extraction techniques are independent. Following is a quick summary of MBL. Input An ontology O, a set of extractors Output Trusted instances for each predicate for ,..., do foreach predicate p in O do foreach extractor e in do Extract new candidates for p using e with recently promoted instances end FILTER candidates that violate mutual-exclusion or type-checking constraints PROMOTE candidates that were extracted by all extractors end end Subordinate algorithms used with MBL do not promote any instance on their own, they report the evidence about each candidate to MBL and MBL is responsible for promoting instances. Applications In their paper authors have presented results showing the potential of CPL to contribute new facts to existing repository of semantic knowledge, Freebase See also Co-training Never-Ending Language Learning Notes References Liu, Qiuhua Xuejun Liao Lawrence Carin (2008). Semi-supervised multitask learning. NIPS. Shinyama, Yusuke Satoshi Sekine (2006). Preemptive information extraction using unrestricted relation discovery. HLT-Naacl. Chang, Ming-Wei Lev-Arie Ratinov Dan Roth (2007). Guiding semi-supervision with constraint driven learning. ACL. Banko, Michele Michael J. Cafarella Stephen Soderland Matt Broadhead Oren Etzioni (2007). Open information extraction from the web. IJCAI. Blum, Avrim Tom Mitchell (1998). Combining labeled and unlabeled data with co-training. Proceedings of the eleventh annual conference on Computational learning theory. pp. 92 100. doi10.1145/279943.279962. ISBN 1581130570. S2CID 207228399. cite book ). Learning dictionaries for information extraction by multi-level bootstrapping. AAAI. Rosenfeld, Benjamin Ronen Feldman (2007). Using corpus statistics on entities to improve semi-supervised relation extraction from the web. ACL. Wang, Richard C. William W. Cohen (2008). Iterative set expansion of named entities using the web. ICDM. Title Cross-entropy method URL https//en.wikipedia.org/wiki/Cross-entropy_method Content The cross-entropy (CE) method is a Monte Carlo method for importance sampling and optimization. It is applicable to both combinatorial and continuous problems, with either a static or noisy objective. The method approximates the optimal importance sampling estimator by repeating two phases Draw a sample from a probability distribution. Minimize the cross-entropy between this distribution and a target distribution to produce a better sample in the next iteration. Reuven Rubinstein developed the method in the context of rare-event simulation, where tiny probabilities must be estimated, for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. The method has also been applied to the traveling salesman, quadratic assignment, DNA sequence alignment, max-cut and buffer allocation problems. Estimation via importance sampling Consider the general problem of estimating the . Using importance sampling this quantity can be estimated as   1 N , . For positive H displaystyle H , the theoretically optimal importance sampling density (PDF) is given by g ( x )  H ( x ) f ( x  u ) / displaystyle g(mathbf x )H(mathbf x )f(mathbf x mathbf u )/ell  . This, however, depends on the unknown displaystyle ell  . The CE method aims to approximate the optimal PDF by adaptively selecting members of the parametric family that are closest (in the Kullback Leibler sense) to the optimal PDF g displaystyle g . Generic CE algorithm Choose initial parameter vector v ( 0 ) displaystyle mathbf v (0)  set . Generate a random sample X 1 , , X N displaystyle mathbf X _1,dots ,mathbf X _N from f (  v ( t 1 ) ) displaystyle f(cdot mathbf v (t-1)) Solve for v ( t ) displaystyle mathbf v (t) , where v ( t )  argmax v 1 N . In several cases, the solution to step 3 can be found analytically. Situations in which this occurs are When f displaystyle f, belongs to the natural exponential family When f displaystyle f, is discrete with finite support When H ( X )  I  x A  displaystyle H(mathbf X )mathrm I _mathbf x in A and f ( X i  u )  f ( X i  v ( t 1 ) ) displaystyle f(mathbf X _imathbf u )f(mathbf X _imathbf v (t-1)) , then v ( t ) displaystyle mathbf v (t) corresponds to the maximum likelihood estimator based on those X k A displaystyle mathbf X _kin A . Continuous optimization example The same CE algorithm can be used for optimization, rather than estimation. Suppose the problem is to maximize some function S displaystyle S , for example, S ( x )  e ( x 2 ) 2  0.8 e ( x  2 ) 2 displaystyle S(x)textrm e-(x-2)20.8,textrm e-(x2)2 . To apply CE, one considers first the associated stochastic problem of estimating P ( S ( X ) ) displaystyle mathbb P _boldsymbol theta (S(X)geq gamma ) for a given level displaystyle gamma , , and parametric family  f (  )  displaystyle leftf(cdot boldsymbol theta )right , for example the 1-dimensional Gaussian distribution, parameterized by its mean t displaystyle mu _t, and variance t 2 displaystyle sigma _t2 (). Hence, for a given displaystyle gamma , , the goal is to find displaystyle boldsymbol theta  so that D K L ( I  S ( x )  f ) displaystyle D_mathrm KL (textrm I_S(x)geq gamma f_boldsymbol theta ) is minimized. This is done by solving the sample version (stochastic counterpart) of the KL divergence minimization problem, as in step 3 above. It turns out that parameters that minimize the stochastic counterpart for this choice of target distribution and parametric family are the sample mean and sample variance corresponding to the elite samples, which are those samples that have objective function value displaystyle geq gamma  . The worst of the elite samples is then used as the level parameter for the next iteration. This yields the following randomized algorithm that happens to coincide with the so-called Estimation of Multivariate Normal Algorithm (EMNA), an estimation of distribution algorithm. Pseudocode // Initialize parameters  6 2  100 t  0 maxits  100 N  100 Ne  10 // While maxits not exceeded and not converged while t  maxits and 2  do // Obtain N samples from current sampling distribution X  SampleGaussian( , 2, N) // Evaluate objective function at sampled points S  exp( (X 2)  2)  0.8 exp( (X  2)  2) // Sort X by objective function values in descending order X  sort(X, S) // Update parameters of sampling distribution via elite samples  mean(X(1Ne)) 2  var(X(1Ne)) t  t  1 // Return mean of final sampling distribution as solution return Related methods Simulated annealing Genetic algorithms Harmony search Estimation of distribution algorithm Tabu search Natural Evolution Strategy Ant colony optimization algorithms See also Cross entropy Kullback Leibler divergence Randomized algorithm Importance sampling Journal papers De Boer, P.-T., Kroese, D.P., Mannor, S. and Rubinstein, R.Y. (2005). A Tutorial on the Cross-Entropy Method. Annals of Operations Research, 134 (1), 19 67. Rubinstein, R.Y. (1997). Optimization of Computer Simulation Models with Rare Events, European Journal of Operational Research, 99, 89 112. Software implementations CEoptim R package Novacta.Analytics .NET library Title Cross-validation (statistics) URL https//en.wikipedia.org/wiki/Cross-validation_(statistics) Content Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. Cross-validation includes resampling and sample splitting methods that use different portions of the data to test and train a model on different iterations. It is often used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. It can also be used to assess the quality of a fitted model and the stability of its parameters. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the models ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem). One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the models predictive performance. In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance. Motivation Assume a model with one or more unknown parameters, and a data set to which the model can be fit (the training data set). The fitting process optimizes the model parameters to make the model fit the training data as well as possible. If an independent sample of validation data is taken from the same population as the training data, it will generally turn out that the model does not fit the validation data as well as it fits the training data. The size of this difference is likely to be large especially when the size of the training data set is small, or when the number of parameters in the model is large. Cross-validation is a way to estimate the size of this effect. Example linear regression In linear regression, there exist real response values y 1 , , y n textstyle y_1,ldots ,y_n , and n p-dimensional vector covariates , ..., xn. The components of the vector xi are denoted , ..., xip. If least squares is used to fit a function in the form of a ). The MSE for given estimated parameter values a and on the training set (xi, yi) 1 i n is defined as ). Thus, a fitted model and computed MSE on the training set will result in an optimistically biased assessment of how well the model will fit an independent data set. This biased estimate is called the in-sample estimate of the fit, whereas the cross-validation estimate is an out-of-sample estimate. Since in linear regression it is possible to directly compute the factor (n p 1)/(n  p  1) by which the training MSE underestimates the validation MSE under the assumption that the model specification is valid, cross-validation can be used for checking whether the model has been overfitted, in which case the MSE in the validation set will substantially exceed its anticipated value. (Cross-validation in the context of linear regression is also useful in that it can be used to select an optimally regularized cost function.) General case In most other regression procedures (e.g. logistic regression), there is no simple formula to compute the expected out-of-sample fit. Cross-validation is, thus, a generally applicable way to predict the performance of a model on unavailable data using numerical computation in place of theoretical analysis. Types Two types of cross-validation can be distinguished exhaustive and non-exhaustive cross-validation. Exhaustive cross-validation Exhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set. Leave-p-out cross-validation Leave-p-out cross-validation (LpO CV) involves using p observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of p observations and a training set. LpO cross-validation require training and validating the model C p n displaystyle C_pn times, where n is the number of observations in the original sample, and where C p n displaystyle C_pn is the binomial coefficient. For p  1 and for even moderately large n, LpO CV can become computationally infeasible. For example, with  . displaystyle C_30100approx 3times 1025. A variant of LpO cross-validation with . Leave-one-out cross-validation Leave-one-out cross-validation (LOOCV) is a particular case of leave-p-out cross-validation with . The process looks similar to jackknife however, with cross-validation one computes a statistic on the left-out sample(s), while with jackknifing one computes a statistic from the kept samples only. LOO cross-validation requires less computation time than LpO cross-validation because there are only C 1  . However, n displaystyle n passes may still require quite a large computation time, in which case other approaches such as k-fold cross validation may be more appropriate. Pseudo-code algorithm Input x, vector of length N with x-values of incoming points y, vector of length N with y-values of the expected result interpolate( x_in, y_in, x_out ),  returns the estimation for point x_out after the model is trained with x_in-y_in pairs Output err, estimate for the prediction error Steps err 0 for i 1, ..., N do // define the cross-validation subsets x_in (x, ..., xi 1, xi  1, ..., xN) y_in (y, ..., yi 1, yi  1, ..., yN) x_out xi y_out interpolate(x_in, y_in, x_out) err err  (yi y_out)2 end for err err/N Non-exhaustive cross-validation Non-exhaustive cross validation methods do not compute all ways of splitting the original sample. These methods are approximations of leave-p-out cross-validation. k-fold cross-validation In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples, often referred to as folds. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling (see below) is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general k remains an unfixed parameter. For example, setting . In 2-fold cross-validation, we randomly shuffle the dataset into two sets and , so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on and validate on , followed by training on and validating on . When . In stratified k-fold cross-validation, the partitions are selected so that the mean response value is approximately equal in all the partitions. In the case of binary classification, this means that each partition contains roughly the same proportions of the two types of class labels. In repeated cross-validation the data is randomly split into k partitions several times. The performance of the model can thereby be averaged over several runs, but this is rarely desirable in practice. When many different statistical or machine learning models are being considered, greedy k-fold cross-validation can be used to quickly identify the most promising candidate models. Holdout method In the holdout method, we randomly assign data points to two sets and , usually called the training set and the test set, respectively. The size of each of the sets is arbitrary although typically the test set is smaller than the training set. We then train (build a model) on and test (evaluate its performance) on . In typical cross-validation, results of multiple runs of model-testing are averaged together in contrast, the holdout method, in isolation, involves a single run. It should be used with caution because without such averaging of multiple runs, one may achieve highly misleading results. Ones indicator of predictive accuracy (F) will tend to be unstable since it will not be smoothed out by multiple iterations (see below). Similarly, indicators of the specific role played by various predictor variables (e.g., values of regression coefficients) will tend to be unstable. While the holdout method can be framed as the simplest kind of cross-validation, many sources instead classify holdout as a type of simple validation, rather than a simple or degenerate form of cross-validation. Repeated random sub-sampling validation This method, also known as Monte Carlo cross-validation, creates multiple random splits of the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over k-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (i.e., the number of partitions). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits. As the number of random splits approaches infinity, the result of repeated random sub-sampling validation tends towards that of leave-p-out cross-validation. In a stratified variant of this approach, the random samples are generated in such a way that the mean response value (i.e. the dependent variable in the regression) is equal in the training and testing sets. This is particularly useful if the responses are dichotomous with an unbalanced representation of the two response values in the data. A method that applies repeated random sub-sampling is RANSAC. Nested cross-validation When cross-validation is used simultaneously for selection of the best set of hyperparameters and for error estimation (and assessment of generalization capacity), a nested cross-validation is required. Many variants exist. At least two variants can be distinguished kl-fold cross-validation This is a truly nested variant which contains an outer loop of k sets and an inner loop of l sets. The total data set is split into k sets. One by one, a set is selected as the (outer) test set and the k - 1 other sets are combined into the corresponding outer training set. This is repeated for each of the k sets. Each outer training set is further sub-divided into l sets. One by one, a set is selected as inner test (validation) set and the l - 1 other sets are combined into the corresponding inner training set. This is repeated for each of the l sets. The inner training sets are used to fit model parameters, while the outer test set is used as a validation set to provide an unbiased evaluation of the model fit. Typically, this is repeated for many different hyperparameters (or even different model types) and the validation set is used to determine the best hyperparameter set (and model type) for this inner training set. After this, a new model is fit on the entire outer training set, using the best set of hyperparameters from the inner cross-validation. The performance of this model is then evaluated using the outer test set. k-fold cross-validation with validation and test set This is a type of kl-fold cross-validation when . A single k-fold cross-validation is used with both a validation and test set. The total data set is split into k sets. One by one, a set is selected as test set. Then, one by one, one of the remaining sets is used as a validation set and the other k - 2 sets are used as training sets until all possible combinations have been evaluated. Similar to the kl-fold cross validation, the training set is used for model fitting and the validation set is used for model evaluation for each of the hyperparameter sets. Finally, for the selected parameter set, the test set is used to evaluate the model with the best parameter set. Here, two variants are possible either evaluating the model that was trained on the training set or evaluating a new model that was fit on the combination of the training and the validation set. Measures of fit The goal of cross-validation is to estimate the expected level of fit of a model to a data set that is independent of the data that were used to train the model. It can be used to estimate any quantitative measure of fit that is appropriate for the data and model. For example, for binary classification problems, each case in the validation set is either predicted correctly or incorrectly. In this situation the misclassification error rate can be used to summarize the fit, although other measures derived from information (e.g., counts, frequency) contained within a contingency table or confusion matrix could also be used. When the value being predicted is continuously distributed, the mean squared error, root mean squared error or median absolute deviation could be used to summarize the errors. Using prior information When users apply cross-validation to select a good configuration displaystyle lambda  , then they might want to balance the cross-validated choice with their own estimate of the configuration. In this way, they can attempt to counter the volatility of cross-validation when the sample size is small and include relevant information from previous research. In a forecasting combination exercise, for instance, cross-validation can be applied to estimate the weights that are assigned to each forecast. Since a simple equal-weighted forecast is difficult to beat, a penalty can be added for deviating from equal weights. Or, if cross-validation is applied to assign individual weights to observations, then one can penalize deviations from equal weights to avoid wasting potentially relevant information. Hoornweg (2018) shows how a tuning parameter displaystyle gamma  can be defined so that a user can intuitively balance between the accuracy of cross-validation and the simplicity of sticking to a reference parameter R displaystyle lambda _R that is defined by the user. If i displaystyle lambda _i denotes the i t h displaystyle ith candidate configuration that might be selected, then the loss function that is to be minimized can be defined as L  . displaystyle L_lambda _i(1-gamma )mbox Relative Accuracy_igamma mbox Relative Simplicity_i. Relative accuracy can be quantified as MSE ( i ) / MSE ( R ) displaystyle mboxMSE(lambda _i)/mboxMSE(lambda _R) , so that the mean squared error of a candidate i displaystyle lambda _i is made relative to that of a user-specified R displaystyle lambda _R . The relative simplicity term measures the amount that i displaystyle lambda _i deviates from R displaystyle lambda _R relative to the maximum amount of deviation from R displaystyle lambda _R . Accordingly, relative simplicity can be specified as ( i R ) 2 ( max R ) 2 displaystyle frac (lambda _i-lambda _R)2(lambda _max -lambda _R)2 , where max displaystyle lambda _max  corresponds to the displaystyle lambda  value with the highest permissible deviation from R displaystyle lambda _R . With  0 , 1  displaystyle gamma in 0,1 , the user determines how high the influence of the reference parameter is relative to cross-validation. One can add relative simplicity terms for multiple configurations  , . . . , C displaystyle ,...,C by specifying the loss function as L  . displaystyle L_lambda _imbox Relative Accuracy_isum _. Hoornweg (2018) shows that a loss function with such an accuracy-simplicity tradeoff can also be used to intuitively define shrinkage estimators like the (adaptive) lasso and Bayesian / ridge regression. Click on the lasso for an example. Statistical properties Suppose we choose a measure of fit F, and use cross-validation to produce an estimate F of the expected fit EF of a model to an independent data set drawn from the same population as the training data. If we imagine sampling multiple independent training sets following the same distribution, the resulting values for F will vary. The statistical properties of F result from this variation. The variance of F can be large. For this reason, if two statistical procedures are compared based on the results of cross-validation, the procedure with the better estimated performance may not actually be the better of the two procedures (i.e. it may not have the better value of EF). Some progress has been made on constructing confidence intervals around cross-validation estimates, but this is considered a difficult problem. Computational issues Most forms of cross-validation are straightforward to implement as long as an implementation of the prediction method being studied is available. In particular, the prediction method can be a black box there is no need to have access to the internals of its implementation. If the prediction method is expensive to train, cross-validation can be very slow since the training must be carried out repeatedly. In some cases such as least squares and kernel regression, cross-validation can be sped up significantly by pre-computing certain values that are needed repeatedly in the training, or by using fast updating rules such as the Sherman Morrison formula. However one must be careful to preserve the total blinding of the validation set from the training procedure, otherwise bias may result. An extreme example of accelerating cross-validation occurs in linear regression, where the results of cross-validation have a closed-form expression known as the prediction residual error sum of squares (PRESS). Limitations and misuse Cross-validation only yields meaningful results if the validation set and training set are drawn from the same population and only if human biases are controlled. In many applications of predictive modeling, the structure of the system being studied evolves over time (i.e. it is non-stationary). Both of these can introduce systematic differences between the training and validation sets. For example, if a model for prediction of trend changes in financial quotations is trained on data for a certain five-year period, it is unrealistic to treat the subsequent five-year period as a draw from the same population. As another example, suppose a model is developed to predict an individuals risk for being diagnosed with a particular disease within the next year. If the model is trained using data from a study involving only a specific population group (e.g. young people or males), but is then applied to the general population, the cross-validation results from the training set could differ greatly from the actual predictive performance. In many applications, models also may be incorrectly specified and vary as a function of modeler biases and/or arbitrary choices. When this occurs, there may be an illusion that the system changes in external samples, whereas the reason is that the model has missed a critical predictor and/or included a confounded predictor. New evidence is that cross-validation by itself is not very predictive of external validity, whereas a form of experimental validation known as swap sampling that does control for human bias can be much more predictive of external validity. As defined by this large MAQC-II study across 30,000 models, swap sampling incorporates cross-validation in the sense that predictions are tested across independent training and validation samples. Yet, models are also developed across these independent samples and by modelers who are blinded to one another. When there is a mismatch in these models developed across these swapped training and validation samples as happens quite frequently, MAQC-II shows that this will be much more predictive of poor external predictive validity than traditional cross-validation. The reason for the success of the swapped sampling is a built-in control for human biases in model building. In addition to placing too much faith in predictions that may vary across modelers and lead to poor external validity due to these confounding modeler effects, these are some other ways that cross-validation can be misused By performing an initial analysis to identify the most informative features using the entire data set if feature selection or model tuning is required by the modeling procedure, this must be repeated on every training set. Otherwise, predictions will certainly be upwardly biased. If cross-validation is used to decide which features to use, an inner cross-validation to carry out the feature selection on every training set must be performed. Performing mean-centering, rescaling, dimensionality reduction, outlier removal or any other data-dependent preprocessing using the entire data set. While very common in practice, this has been shown to introduce biases into the cross-validation estimates. By allowing some of the training data to also be included in the test set this can happen due to twinning in the data set, whereby some exactly identical or nearly identical samples are present in the data set, see pseudoreplication. To some extent twinning always takes place even in perfectly independent training and validation samples. This is because some of the training sample observations will have nearly identical values of predictors as validation sample observations. And some of these will correlate with a target at better than chance levels in the same direction in both training and validation when they are actually driven by confounded predictors with poor external validity. If such a cross-validated model is selected from a k-fold set, human confirmation bias will be at work and determine that such a model has been validated. This is why traditional cross-validation needs to be supplemented with controls for human bias and confounded model specification like swap sampling and prospective studies. Cross validation for time-series models Due to correlations, cross-validation with random splits might be problematic for time-series models (if we are more interested in evaluating extrapolation, rather than interpolation). A more appropriate approach might be to use rolling cross-validation. However, if performance is described by a single summary statistic, it is possible that the approach described by Politis and Romano as a stationary bootstrap will work. The statistic of the bootstrap needs to accept an interval of the time series and return the summary statistic on it. The call to the stationary bootstrap needs to specify an appropriate mean interval length. Applications Cross-validation can be used to compare the performances of different predictive modeling procedures. For example, suppose we are interested in optical character recognition, and we are considering using either a Support Vector Machine (SVM) or k-nearest neighbors (KNN) to predict the true character from an image of a handwritten character. Using cross-validation, we can obtain empirical estimates comparing these two methods in terms of their respective fractions of misclassified characters. In contrast, the in-sample estimate will not represent the quantity of interest (i.e. the generalization error). Cross-validation can also be used in variable selection. Suppose we are using the expression levels of 20 proteins to predict whether a cancer patient will respond to a drug. A practical goal would be to determine which subset of the 20 features should be used to produce the best predictive model. For most modeling procedures, if we compare feature subsets using the in-sample error rates, the best performance will occur when all 20 features are used. However under cross-validation, the model with the best fit will generally include only a subset of the features that are deemed truly informative. A recent development in medical statistics is its use in meta-analysis. It forms the basis of the validation statistic, Vn which is used to test the statistical validity of meta-analysis summary estimates. It has also been used in a more conventional sense in meta-analysis to estimate the likely prediction error of meta-analysis results. See also Notes and references Further reading Bengio, Yoshua Grandvalet, Yves (2004). No Unbiased Estimator of the Variance of K-Fold Cross-Validation (PDF). Journal of Machine Learning Research. 5 1089 1105. Kim, Ji-Hyun (September 2009). Estimating classification error rate Repeated cross-validation, repeated hold-out and bootstrap. Computational Statistics  Data Analysis. 53 (11) 3735 3745. doi10.1016/j.csda.2009.04.009. Beleites, Claudia Baumgartner, Richard Bowman, Christopher Somorjai, Ray Steiner, Gerald Salzer, Reiner Sowa, Michael G. (October 2005). Variance reduction in estimating classification error using sparse datasets. Chemometrics and Intelligent Laboratory Systems. 79 (1 2) 91 100. doi10.1016/j.chemolab.2005.04.008. Trippa, Lorenzo Waldron, Levi Huttenhower, Curtis Parmigiani, Giovanni (March 2015). Bayesian nonparametric cross-study validation of prediction methods. The Annals of Applied Statistics. 9 (1). arXiv1506.00474. doi10.1214/14-. Title Curse of dimensionality URL https//en.wikipedia.org/wiki/Curse_of_dimensionality Content The curse of dimensionality refers to various phenomena that arise when analyzing and organizing data in high-dimensional spaces that do not occur in low-dimensional settings such as the three-dimensional physical space of everyday experience. The expression was coined by Richard E. Bellman when considering problems in dynamic programming. The curse generally refers to issues that arise when the number of datapoints is small (in a suitably defined sense) relative to the intrinsic dimension of the data. Dimensionally cursed phenomena occur in domains such as numerical analysis, sampling, combinatorics, machine learning, data mining and databases. The common theme of these problems is that when the dimensionality increases, the volume of the space increases so fast that the available data become sparse. In order to obtain a reliable result, the amount of data needed often grows exponentially with the dimensionality. Also, organizing and searching data often relies on detecting areas where objects form groups with similar properties in high dimensional data, however, all objects appear to be sparse and dissimilar in many ways, which prevents common data organization strategies from being efficient. Domains Combinatorics In some problems, each variable can take one of several discrete values, or the range of possible values is divided to give a finite number of possibilities. Taking the variables together, a huge number of combinations of values must be considered. This effect is also known as the combinatorial explosion. Even in the simplest case of d displaystyle d binary variables, the number of possible combinations already is 2 d displaystyle 2d , exponential in the dimensionality. Naively, each additional dimension doubles the effort needed to try all combinations. Sampling There is an exponential increase in volume associated with adding extra dimensions to a mathematical space. For example, 102  100 evenly spaced sample points suffice to sample a unit interval (try to visualize a 1-dimensional cube) with no more than 10 2  0.01 distance between points an equivalent sampling of a 10-dimensional unit hypercube with a lattice that has a spacing of 10 2  0.01 between adjacent points would require 1020  (102)10 sample points. In general, with a spacing distance of 10 n the 10-dimensional hypercube appears to be a factor of 10n(10 1)  (10n)10/(10n) larger than the 1-dimensional hypercube, which is the unit interval. In the above example .01 the 10-dimensional hypercube appears to be 1018 larger than the unit interval. This effect is a combination of the combinatorics problems above and the distance function problems explained below. Optimization When solving dynamic optimization problems by numerical backward induction, the objective function must be computed for each combination of values. This is a significant obstacle when the dimension of the state variable is large. Machine learning In machine learning problems that involve learning a state-of-nature from a finite number of data samples in a high-dimensional feature space with each feature having a range of possible values, typically an enormous amount of training data is required to ensure that there are several samples with each combination of values. In an abstract sense, as the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially. A typical rule of thumb is that there should be at least 5 training examples for each dimension in the representation. In machine learning and insofar as predictive performance is concerned, the curse of dimensionality is used interchangeably with the peaking phenomenon, which is also known as Hughes phenomenon. This phenomenon states that with a fixed number of training samples, the average (expected) predictive power of a classifier or regressor first increases as the number of dimensions or features used is increased but beyond a certain dimensionality it starts deteriorating instead of improving steadily. Nevertheless, in the context of a simple classifier (e.g., linear discriminant analysis in the multivariate Gaussian model under the assumption of a common known covariance matrix), Zollanvari, et al., showed both analytically and empirically that as long as the relative cumulative efficacy of an additional feature set (with respect to features that are already part of the classifier) is greater (or less) than the size of this additional feature set, the expected error of the classifier constructed using these additional features will be less (or greater) than the expected error of the classifier constructed without them. In other words, both the size of additional features and their (relative) cumulative discriminatory effect are important in observing a decrease or increase in the average predictive power. In metric learning, higher dimensions can sometimes allow a model to achieve better performance. After normalizing embeddings to the surface of a hypersphere, FaceNet achieves the best performance using 128 dimensions as opposed to 64, 256, or 512 dimensions in one ablation study. A loss function for unitary-invariant dissimilarity between word embeddings was found to be minimized in high dimensions. Data mining In data mining, the curse of dimensionality refers to a data set with too many features. Consider the first table, which depicts 200 individuals and 2000 genes (features) with a 1 or 0 denoting whether or not they have a genetic mutation in that gene. A data mining application to this data set may be finding the correlation between specific genetic mutations and creating a classification algorithm such as a decision tree to determine whether an individual has cancer or not. A common practice of data mining in this domain would be to create association rules between genetic mutations that lead to the development of cancers. To do this, one would have to loop through each genetic mutation of each individual and find other genetic mutations that occur over a desired threshold and create pairs. They would start with pairs of two, then three, then four until they result in an empty set of pairs. The complexity of this algorithm can lead to calculating all permutations of gene pairs for each individual or row. Given the formula for calculating the permutations of n items with a group size of r is n ! ( n r ) ! displaystyle frac n!(n-r)! , calculating the number of three pair permutations of any given individual would be 7988004000 different pairs of genes to evaluate for each individual. The number of pairs created will grow by an order of factorial as the size of the pairs increase. The growth is depicted in the permutation table (see right). As we can see from the permutation table above, one of the major problems data miners face regarding the curse of dimensionality is that the space of possible parameter values grows exponentially or factorially as the number of features in the data set grows. This problem critically affects both computational time and space when searching for associations or optimal features to consider. Another problem data miners may face when dealing with too many features is that the number of false predictions or classifications tends to increase as the number of features grows in the data set. In terms of the classification problem discussed above, keeping every data point could lead to a higher number of false positives and false negatives in the model. This may seem counterintuitive, but consider the genetic mutation table from above, depicting all genetic mutations for each individual. Each genetic mutation, whether they correlate with cancer or not, will have some input or weight in the model that guides the decision-making process of the algorithm. There may be mutations that are outliers or ones that dominate the overall distribution of genetic mutations when in fact they do not correlate with cancer. These features may be working against ones model, making it more difficult to obtain optimal results. This problem is up to the data miner to solve, and there is no universal solution. The first step any data miner should take is to explore the data, in an attempt to gain an understanding of how it can be used to solve the problem. One must first understand what the data means, and what they are trying to discover before they can decide if anything must be removed from the data set. Then they can create or use a feature selection or dimensionality reduction algorithm to remove samples or features from the data set if they deem it necessary. One example of such methods is the interquartile range method, used to remove outliers in a data set by calculating the standard deviation of a feature or occurrence. Distance function When a measure such as a Euclidean distance is defined using many coordinates, there is little difference in the distances between different pairs of points. One way to illustrate the vastness of high-dimensional Euclidean space is to compare the proportion of an inscribed hypersphere with radius r displaystyle r and dimension d displaystyle d , to that of a hypercube with edges of length 2 r . displaystyle 2r. The volume of such a sphere is 2 r d d / 2 d ( d / 2 ) displaystyle frac 2rdpi d/2dGamma (d/2) , where displaystyle Gamma  is the gamma function, while the volume of the cube is ( 2 r ) d displaystyle (2r)d . As the dimension d displaystyle d of the space increases, the hypersphere becomes an insignificant volume relative to that of the hypercube. This can clearly be seen by comparing the proportions as the dimension d displaystyle d goes to infinity V h y p e r s p h e r e V h y p e r c u b   . Furthermore, the distance between the center and the corners is r d displaystyle rsqrt d , which increases without bound for fixed r. In this sense when points are uniformly generated in a high-dimensional hypercube, almost all points are much farther than r displaystyle r units away from the centre. In high dimensions, the volume of the d-dimensional unit hypercube (with coordinates of the vertices 1 displaystyle pm 1 ) is concentrated near a sphere with the radius d / 3 displaystyle sqrt d/sqrt 3 for large dimension d. Indeed, for each coordinate x i displaystyle x_i the average value of x i 2 displaystyle x_i2 in the cube is x i 2  1 2 1 1 x 2 d  . The variance of x i 2 displaystyle x_i2 for uniform distribution in the cube is 1 2 1 1 x 4 d x x i 2 2  4 45 displaystyle frac 12int _-11x4dx-leftlangle x_i2rightrangle 2frac 445 Therefore, the squared distance from the origin, r 2  i x i 2 textstyle r2sum _ix_i2 has the average value d/3 and variance 4d/45. For large d, distribution of r 2 / d displaystyle r2/d is close to the normal distribution with the mean 1/3 and the standard deviation 2 / 45 d displaystyle 2/sqrt 45d according to the central limit theorem. Thus, when uniformly generating points in high dimensions, both the middle of the hypercube, and the corners are empty, and all the volume is concentrated near the surface of a sphere of intermediate radius d / 3 textstyle sqrt d/3 . This also helps to understand the chi-squared distribution. Indeed, the (non-central) chi-squared distribution associated to a random point in the interval -1, 1 is the same as the distribution of the length-squared of a random point in the d-cube. By the law of large numbers, this distribution concentrates itself in a narrow band around d times the standard deviation squared ( 2) of the original derivation. This illuminates the chi-squared distribution and also illustrates that most of the volume of the d-cube concentrates near the boundary of a sphere of radius d displaystyle sigma sqrt d . A further development of this phenomenon is as follows. Any fixed distribution on the real numbers induces a product distribution on points in R d displaystyle mathbb R d . For any fixed n, it turns out that the difference between the minimum and the maximum distance between a random reference point Q and a list of n random data points ,...,Pn become indiscernible compared to the minimum distance lim d E ( dist max ( d ) dist min ( d ) dist min ( d ) ) 0 displaystyle lim _dto infty Eleft(frac operatorname dist _max (d)-operatorname dist _min (d)operatorname dist _min (d)right)to 0 . This is often cited as distance functions losing their usefulness (for the nearest-neighbor criterion in feature-comparison algorithms, for example) in high dimensions. However, recent research has shown this to only hold in the artificial scenario when the one-dimensional distributions R displaystyle mathbb R  are independent and identically distributed. When attributes are correlated, data can become easier and provide higher distance contrast and the signal-to-noise ratio was found to play an important role, thus feature selection should be used. More recently, it has been suggested that there may be a conceptual flaw in the argument that contrast-loss creates a curse in high dimensions. Machine learning can be understood as the problem of assigning instances to their respective generative process of origin, with class labels acting as symbolic representations of individual generative processes. The curses derivation assumes all instances are independent, identical outcomes of a single high dimensional generative process. If there is only one generative process, there would exist only one (naturally occurring) class and machine learning would be conceptually ill-defined in both high and low dimensions. Thus, the traditional argument that contrast-loss creates a curse, may be fundamentally inappropriate. In addition, it has been shown that when the generative model is modified to accommodate multiple generative processes, contrast-loss can morph from a curse to a blessing, as it ensures that the nearest-neighbor of an instance is almost-surely its most closely related instance. From this perspective, contrast-loss makes high dimensional distances especially meaningful and not especially non-meaningful as is often argued. Nearest neighbor search The effect complicates nearest neighbor search in high dimensional space. It is not possible to quickly reject candidates by using the difference in one coordinate as a lower bound for a distance based on all the dimensions. However, it has recently been observed that the mere number of dimensions does not necessarily result in difficulties, since relevant additional dimensions can also increase the contrast. In addition, for the resulting ranking it remains useful to discern close and far neighbors. Irrelevant (noise) dimensions, however, reduce the contrast in the manner described above. In time series analysis, where the data are inherently high-dimensional, distance functions also work reliably as long as the signal-to-noise ratio is high enough. k-nearest neighbor classification Another effect of high dimensionality on distance functions concerns k-nearest neighbor (k-NN) graphs constructed from a data set using a distance function. As the dimension increases, the indegree distribution of the k-NN digraph becomes skewed with a peak on the right because of the emergence of a disproportionate number of hubs, that is, data-points that appear in many more k-NN lists of other data-points than the average. This phenomenon can have a considerable impact on various techniques for classification (including the k-NN classifier), semi-supervised learning, and clustering, and it also affects information retrieval. Anomaly detection In a 2012 survey, Zimek et al. identified the following problems when searching for anomalies in high-dimensional data Concentration of scores and distances derived values such as distances become numerically similar Irrelevant attributes in high dimensional data, a significant number of attributes may be irrelevant Definition of reference sets for local methods, reference sets are often nearest-neighbor based Incomparable scores for different dimensionalities different subspaces produce incomparable scores Interpretability of scores the scores often no longer convey a semantic meaning Exponential search space the search space can no longer be systematically scanned Data snooping bias given the large search space, for every desired significance a hypothesis can be found Hubness certain objects occur more frequently in neighbor lists than others. Many of the analyzed specialized methods tackle one or another of these problems, but there remain many open research questions. Blessing of dimensionality Surprisingly and despite the expected curse of dimensionality difficulties, common-sense heuristics based on the most straightforward methods can yield results which are almost surely optimal for high-dimensional problems. The term blessing of dimensionality was introduced in the late 1990s. Donoho in his Millennium manifesto clearly explained why the blessing of dimensionality will form a basis of future data mining. The effects of the blessing of dimensionality were discovered in many applications and found their foundation in the concentration of measure phenomena. One example of the blessing of dimensionality phenomenon is linear separability of a random point from a large finite random set with high probability even if this set is exponentially large the number of elements in this random set can grow exponentially with dimension. Moreover, this linear functional can be selected in the form of the simplest linear Fisher discriminant. This separability theorem was proven for a wide class of probability distributions general uniformly log-concave distributions, product distributions in a cube and many other families (reviewed recently in ). The blessing of dimensionality and the curse of dimensionality are two sides of the same coin. For example, the typical property of essentially high-dimensional probability distributions in a high-dimensional space is the squared distance of random points to a selected point is, with high probability, close to the average (or median) squared distance. This property significantly simplifies the expected geometry of data and indexing of high-dimensional data (blessing), but, at the same time, it makes the similarity search in high dimensions difficult and even useless (curse). Zimek et al. noted that while the typical formalizations of the curse of dimensionality affect i.i.d. data, having data that is separated in each attribute becomes easier even in high dimensions, and argued that the signal-to-noise ratio matters data becomes easier with each attribute that adds signal, and harder with attributes that only add noise (irrelevant error) to the data. In particular for unsupervised data analysis this effect is known as swamping. See also Title Data augmentation URL https//en.wikipedia.org/wiki/Data_augmentation Content Data augmentation is a statistical technique which allows maximum likelihood estimation from incomplete data. Data augmentation has important applications in Bayesian analysis, and the technique is widely used in machine learning to reduce overfitting when training machine learning models, achieved by training models on several slightly-modified copies of existing data. Synthetic oversampling techniques for traditional machine learning Synthetic Minority Over-sampling Technique (SMOTE) is a method used to address imbalanced datasets in machine learning. In such datasets, the number of samples in different classes varies significantly, leading to biased model performance. For example, in a medical diagnosis dataset with 90 samples representing healthy individuals and only 10 samples representing individuals with a particular disease, traditional algorithms may struggle to accurately classify the minority class. SMOTE rebalances the dataset by generating synthetic samples for the minority class. For instance, if there are 100 samples in the majority class and 10 in the minority class, SMOTE can create synthetic samples by randomly selecting a minority class sample and its nearest neighbors, then generating new samples along the line segments joining these neighbors. This process helps increase the representation of the minority class, improving model performance. Data augmentation for image classification When convolutional neural networks grew larger in mid-1990s, there was a lack of data to use, especially considering that some part of the overall dataset should be spared for later testing. It was proposed to perturb existing data with affine transformations to create new examples with the same labels, which were complemented by so-called elastic distortions in 2003, and the technique was widely used as of 2010s. Data augmentation can enhance CNN performance and acts as a countermeasure against CNN profiling attacks. Data augmentation has become fundamental in image classification, enriching training dataset diversity to improve model generalization and performance. The evolution of this practice has introduced a broad spectrum of techniques, including geometric transformations, color space adjustments, and noise injection. Geometric Transformations Geometric transformations alter the spatial properties of images to simulate different perspectives, orientations, and scales. Common techniques include Rotation Rotating images by a specified degree to help models recognize objects at various angles. Flipping Reflecting images horizontally or vertically to introduce variability in orientation. Cropping Removing sections of the image to focus on particular features or simulate closer views. Translation Shifting images in different directions to teach models positional invariance. Color Space Transformations Color space transformations modify the color properties of images, addressing variations in lighting, color saturation, and contrast. Techniques include Brightness Adjustment Varying the images brightness to simulate different lighting conditions. Contrast Adjustment Changing the contrast to help models recognize objects under various clarity levels. Saturation Adjustment Altering saturation to prepare models for images with diverse color intensities. Color Jittering Randomly adjusting brightness, contrast, saturation, and hue to introduce color variability. Noise Injection Injecting noise into images simulates real-world imperfections, teaching models to ignore irrelevant variations. Techniques involve Gaussian Noise Adding Gaussian noise mimics sensor noise or graininess. Salt and Pepper Noise Introducing black or white pixels at random simulates sensor dust or dead pixels. Data augmentation for signal processing Residual or block bootstrap can be used for time series augmentation. Biological signals Synthetic data augmentation is of paramount importance for machine learning classification, particularly for biological data, which tend to be high dimensional and scarce. The applications of robotic control and augmentation in disabled and able-bodied subjects still rely mainly on subject-specific analyses. Data scarcity is notable in signal processing problems such as for Parkinsons Disease Electromyography signals, which are difficult to source - Zanini, et al. noted that it is possible to use a generative adversarial network (in particular, a DCGAN) to perform style transfer in order to generate synthetic electromyographic signals that corresponded to those exhibited by sufferers of Parkinsons Disease. The approaches are also important in electroencephalography (brainwaves). Wang, et al. explored the idea of using deep convolutional neural networks for EEG-Based Emotion Recognition, results show that emotion recognition was improved when data augmentation was used. A common approach is to generate synthetic signals by re-arranging components of real data. Lotte proposed a method of Artificial Trial Generation Based on Analogy where three data examples x 1 , x 2 , x 3 displaystyle x_1,x_2,x_3 provide examples and an artificial x s y n t h e t i c displaystyle x_synthetic is formed which is to x 3 displaystyle x_3 what x 2 displaystyle x_2 is to x 1 displaystyle x_1 . A transformation is applied to x 1 displaystyle x_1 to make it more similar to x 2 displaystyle x_2 , the same transformation is then applied to x 3 displaystyle x_3 which generates x s y n t h e t i c displaystyle x_synthetic . This approach was shown to improve performance of a Linear Discriminant Analysis classifier on three different datasets. Current research shows great impact can be derived from relatively simple techniques. For example, Freer observed that introducing noise into gathered data to form additional data points improved the learning ability of several models which otherwise performed relatively poorly. Tsinganos et al. studied the approaches of magnitude warping, wavelet decomposition, and synthetic surface EMG models (generative approaches) for hand gesture recognition, finding classification performance increases of up to 16 when augmented data was introduced during training. More recently, data augmentation studies have begun to focus on the field of deep learning, more specifically on the ability of generative models to create artificial data which is then introduced during the classification model training process. In 2018, Luo et al. observed that useful EEG signal data could be generated by Conditional Wasserstein Generative Adversarial Networks (GANs) which was then introduced to the training set in a classical train-test learning framework. The authors found classification performance was improved when such techniques were introduced. Mechanical signals The prediction of mechanical signals based on data augmentation brings a new generation of technological innovations, such as new energy dispatch, 5G communication field, and robotics control engineering. In 2022, Yang et al. integrate constraints, optimization and control into a deep network framework based on data augmentation and data pruning with spatio-temporal data correlation, and improve the interpretability, safety and controllability of deep learning in real industrial projects through explicit mathematical programming equations and analytical solutions. See also Oversampling and undersampling in data analysis Surrogate data Generative adversarial network Variational autoencoder Data pre-processing Convolutional neural network Regularization (mathematics) Data preparation Data fusion Title Data exploration URL https//en.wikipedia.org/wiki/Data_exploration Content Data exploration is an approach similar to initial data analysis, whereby a data analyst uses visual exploration to understand what is in a dataset and the characteristics of the data, rather than through traditional data management systems. These characteristics can include size or amount of data, completeness of the data, correctness of the data, possible relationships amongst data elements or files/tables in the data. Data exploration is typically conducted using a combination of automated and manual activities. Automated activities can include data profiling or data visualization or tabular reports to give the analyst an initial view into the data and an understanding of key characteristics. This is often followed by manual drill-down or filtering of the data to identify anomalies or patterns identified through the automated actions. Data exploration can also require manual scripting and queries into the data (e.g. using languages such as SQL or R) or using spreadsheets or similar tools to view the raw data. All of these activities are aimed at creating a mental model and understanding of the data in the mind of the analyst, and defining basic metadata (statistics, structure, relationships) for the data set that can be used in further analysis. Once this initial understanding of the data is had, the data can be pruned or refined by removing unusable parts of the data (data cleansing), correcting poorly formatted elements and defining relevant relationships across datasets. This process is also known as determining data quality. Data exploration can also refer to the ad hoc querying or visualization of data to identify potential relationships or insights that may be hidden in the data and does not require to formulate assumptions beforehand. Traditionally, this had been a key area of focus for statisticians, with John Tukey being a key evangelist in the field. Today, data exploration is more widespread and is the focus of data analysts and data scientists the latter being a relatively new role within enterprises and larger organizations. Interactive Data Exploration This area of data exploration has become an area of interest in the field of machine learning. This is a relatively new field and is still evolving. As its most basic level, a machine-learning algorithm can be fed a data set and can be used to identify whether a hypothesis is true based on the dataset. Common machine learning algorithms can focus on identifying specific patterns in the data. Many common patterns include regression and classification or clustering, but there are many possible patterns and algorithms that can be applied to data via machine learning. By employing machine learning, it is possible to find patterns or relationships in the data that would be difficult or impossible to find via manual inspection, trial and error or traditional exploration techniques. Software Trifacta a data preparation and analysis platform Paxata self-service data preparation software Alteryx data blending and advanced data analytics software Microsoft Power BI - interactive visualization and data analysis tool OpenRefine - a standalone open source desktop application for data clean-up and data transformation Tableau software interactive data visualization software See also Exploratory data analysis Machine learning Data profiling Data visualization Title Data preprocessing URL https//en.wikipedia.org/wiki/Data_preprocessing Content Data preprocessing can refer to manipulation, filtration or augmentation of data before it is analyzed, and is often an important step in the data mining process. Data collection methods are often loosely controlled, resulting in out-of-range values, impossible data combinations, and missing values, amongst other issues. The preprocessing pipeline used can often have large effects on the conclusions drawn from the downstream analysis. Thus, representation and quality of data is necessary before running any analysis. Often, data preprocessing is the most important phase of a machine learning project, especially in computational biology. If there is a high proportion of irrelevant and redundant information present or noisy and unreliable data, then knowledge discovery during the training phase may be more difficult. Data preparation and filtering steps can take a considerable amount of processing time. Examples of methods used in data preprocessing include cleaning, instance selection, normalization, one-hot encoding, data transformation, feature extraction and feature selection. Applications Data mining Data preprocessing allows for the removal of unwanted data with the use of data cleaning, this allows the user to have a dataset to contain more valuable information after the preprocessing stage for data manipulation later in the data mining process. Editing such dataset to either correct data corruption or human error is a crucial step to get accurate quantifiers like true positives, true negatives, false positives and false negatives found in a confusion matrix that are commonly used for a medical diagnosis. Users are able to join data files together and use preprocessing to filter any unnecessary noise from the data which can allow for higher accuracy. Users use Python programming scripts accompanied by the pandas library which gives them the ability to import data from a comma-separated values as a data-frame. The data-frame is then used to manipulate data that can be challenging otherwise to do in Excel. Pandas (software) which is a powerful tool that allows for data analysis and manipulation which makes data visualizations, statistical operations and much more, a lot easier. Many also use the R programming language to do such tasks as well. The reason why a user transforms existing files into a new one is because of many reasons. Aspects of data preprocessing may include imputing missing values, aggregating numerical quantities and transforming continuous data into categories (data binning). More advanced techniques like principal component analysis and feature selection are working with statistical formulas and are applied to complex datasets which are recorded by GPS trackers and motion capture devices. Semantic data preprocessing Semantic data mining is a subset of data mining that specifically seeks to incorporate domain knowledge, such as formal semantics, into the data mining process. Domain knowledge is the knowledge of the environment the data was processed in. Domain knowledge can have a positive influence on many aspects of data mining, such as filtering out redundant or inconsistent data during the preprocessing phase. Domain knowledge also works as constraint. It does this by using working as set of prior knowledge to reduce the space required for searching and acting as a guide to the data. Simply put, semantic preprocessing seeks to filter data using the original environment of said data more correctly and efficiently. There are increasingly complex problems which are asking to be solved by more elaborate techniques to better analyze existing information. Instead of creating a simple script for aggregating different numerical values into a single value, it make sense to focus on semantic based data preprocessing. The idea is to build a dedicated ontology, which explains on a higher level what the problem is about. In regards to semantic data mining and semantic pre-processing, ontologies are a way to conceptualize and formally define semantic knowledge and data. The Prot g (software) is the standard tool for constructing an ontology. In general, the use of ontologies bridges the gaps between data, applications, algorithms, and results that occur from semantic mismatches. As a result, semantic data mining combined with ontology has many applications where semantic ambiguity can impact the usefulness and efficiency of data systems. Applications include the medical field, language processing, banking, and even tutoring, among many more. There are various strengths to using a semantic data mining and ontological based approach. As previously mentioned, these tools can help during the per-processing phase by filtering out non-desirable data from the data set. Additionally, well-structured formal semantics integrated into well designed ontologies can return powerful data that can be easily read and processed by machines. A specifically useful example of this exists in the medical use of semantic data processing. As an example, a patient is having a medical emergency and is being rushed to hospital. The emergency responders are trying to figure out the best medicine to administer to help the patient. Under normal data processing, scouring all the patient s medical data to ensure they are getting the best treatment could take too long and risk the patients health or even life. However, using semantically processed ontologies, the first responders could save the patient s life. Tools like a semantic reasoner can use ontology to infer the what best medicine to administer to the patient is based on their medical history, such as if they have a certain cancer or other conditions, simply by examining the natural language used in the patients medical records. This would allow the first responders to quickly and efficiently search for medicine without having worry about the patient s medical history themselves, as the semantic reasoner would already have analyzed this data and found solutions. In general, this illustrates the incredible strength of using semantic data mining and ontologies. They allow for quicker and more efficient data extraction on the user side, as the user has fewer variables to account for, since the semantically pre-processed data and ontology built for the data have already accounted for many of these variables. However, there are some drawbacks to this approach. Namely, it requires a high amount of computational power and complexity, even with relatively small data sets. This could result in higher costs and increased difficulties in building and maintaining semantic data processing systems. This can be mitigated somewhat if the data set is already well organized and formatted, but even then, the complexity is still higher when compared to standard data processing. Below is a simple a diagram combining some of the processes, in particular semantic data mining and their use in ontology. The diagram depicts a data set being broken up into two parts the characteristics of its domain, or domain knowledge, and then the actual acquired data. The domain characteristics are then processed to become user understood domain knowledge that can be applied to the data. Meanwhile, the data set is processed and stored so that the domain knowledge can applied to it, so that the process may continue. This application forms the ontology. From there, the ontology can be used to analyze data and process results. Fuzzy preprocessing is another, more advanced technique for solving complex problems. Fuzzy preprocessing and fuzzy data mining make use of fuzzy sets. These data sets are composed of two elements a set and a membership function for the set which comprises 0 and 1. Fuzzy preprocessing uses this fuzzy data set to ground numerical values with linguistic information. Raw data is then transformed into natural language. Ultimately, fuzzy data minings goal is to help deal with inexact information, such as an incomplete database. Currently fuzzy preprocessing, as well as other fuzzy based data mining techniques see frequent use with neural networks and artificial intelligence. References External links Online Data Processing Compendium Data preprocessing in predictive data mining. Knowledge Eng. Review 34 (2019) Title Data-driven astronomy URL https//en.wikipedia.org/wiki/Astroinformatics Content Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies. The field is closely related to astrostatistics. Data-driven astronomy (DDA) refers to the use of data science in astronomy. Several outputs of telescopic observations and sky surveys are taken into consideration and approaches related to data mining and big data management are used to analyze, filter, and normalize the data set that are further used for making Classifications, Predictions, and Anomaly detections by advanced Statistical approaches, digital image processing and machine learning. The output of these processes is used by astronomers and space scientists to study and identify patterns, anomalies, and movements in outer space and conclude theories and discoveries in the cosmos. Background Astroinformatics is primarily focused on developing the tools, methods, and applications of computational science, data science, machine learning, and statistics for research and education in data-oriented astronomy. Early efforts in this direction included data discovery, metadata standards development, data modeling, astronomical data dictionary development, data access, information retrieval, data integration, and data mining in the astronomical Virtual Observatory initiatives. Further development of the field, along with astronomy community endorsement, was presented to the National Research Council (United States) in 2009 in the astroinformatics state of the profession position paper for the 2010 Astronomy and Astrophysics Decadal Survey. That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper Astroinformatics Data-Oriented Astronomy Research and Education. Astroinformatics as a distinct field of research was inspired by work in the fields of Geoinformatics, Cheminformatics, Bioinformatics, and through the eScience work of Jim Gray (computer scientist) at Microsoft Research, whose legacy was remembered and continued through the Jim Gray eScience Awards. Although the primary focus of astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to digitize historical and recent astronomical observations and images in a large database for efficient retrieval through web-based interfaces. Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy. Astroinformatics is described as the fourth paradigm of astronomical research. There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science. Data mining and machine learning play significant roles in astroinformatics as a scientific research discipline due to their focus on knowledge discovery from data (KDD) and learning from data. The amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the Large Synoptic Survey Telescope and into the exabytes with the Square Kilometre Array. This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part due to this, data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing information-intensive and data-intensive sub-disciplines to an extent that these sub-disciplines are now becoming (or have already become) standalone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, such programs most likely will be developed in the near future. Informatics has been recently defined as the use of digital data, information, and related services for research and knowledge generation. However the usual, or commonly used definition is informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support. Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., taxonomies, ontologies, folksonomies, and/or collaborative tagging) plus Astrostatistics will also be heavily involved. Citizen science projects (such as Galaxy Zoo) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments. In 2007, the Galaxy Zoo project was launched for morphological classification of a large number of galaxies. In this project, 900,000 images were considered for classification that were taken from the Sloan Digital Sky Survey (SDSS) for the past 7 years. The task was to study each picture of a galaxy, classify it as elliptical or spiral, and determine whether it was spinning or not. The team of Astrophysicists led by Kevin Schawinski in Oxford University were in charge of this project and Kevin and his colleague Chris Linlott figured out that it would take a period of 3 5 years for such a team to complete the work. There they came up with the idea of using Machine Learning and Data Science techniques for analyzing the images and classifying them. In 2012, two position papers were presented to the Council of the American Astronomical Society that led to the establishment of formal working groups in astroinformatics and Astrostatistics for the profession of astronomy within the US and elsewhere. Astroinformatics provides a natural context for the integration of education and research. The experience of research can now be implemented within the classroom to establish and grow data literacy through the easy re-use of data. It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others. Methodology The data retrieved from the sky surveys are first brought for data preprocessing. In this, redundancies are removed and filtrated. Further, feature extraction is performed on this filtered data set, which is further taken for processes. Some of the renowned sky surveys are listed below The Palomar Digital Sky Survey (DPOSS) The Two-Micron All Sky Survey (2MASS) Green Bank Telescope (GBT) The Galaxy Evolution Explorer (GALEX) The Sloan Digital Sky Survey (SDSS) SkyMapper Southern Sky Survey (SMSS) The Panoramic Survey Telescope and Rapid Response System (PanSTARRS) The Large Synoptic Survey Telescope (LSST) The Square Kilometer Array (SKA) The size of data from the above-mentioned sky surveys ranges from 3 TB to almost 4.6 EB. Further, data mining tasks that are involved in the management and manipulation of the data involve methods like classification, regression, clustering, anomaly detection, and time-series analysis. Several approaches and applications for each of these methods are involved in the task accomplishments. Classification Classification is used for specific identifications and categorizations of astronomical data such as Spectral classification, Photometric classification, Morphological classification, and classification of solar activity. The approaches of classification techniques are listed below Artificial neural network (ANN) Support vector machine (SVM) Learning vector quantization (LVQ) Decision tree Random forest k-nearest neighbors Na ve Bayesian networks Radial basis function network Gaussian process Decision table Alternating decision tree (ADTree) Regression Regression is used to make predictions based on the retrieved data through statistical trends and statistical modeling. Different uses of this technique are used for fetching Photometric redshifts and measurements of physical parameters of stars. The approaches are listed below Artificial neural network (ANN) Support vector regression (SVR) Decision tree Random forest k-nearest neighbors regression Kernel regression Principal component regression (PCR) Gaussian process Least squared regression (LSR) Partial least squares regression Clustering Clustering is classifying objects based on a similarity measure metric. It is used in Astronomy for Classification as well as Special/rare object detection. The approaches are listed below Principal component analysis (PCA) DBSCAN k-means clustering OPTICS Cobweb model Self-organizing map (SOM) Expectation Maximization Hierarchical Clustering AutoClass Gaussian Mixture Modeling (GMM) Anomaly detection Anomaly detection is used for detecting irregularities in the dataset. However, this technique is used here to detect rare/special objects. The following approaches are used Principal Component Analysis (PCA) k-means clustering Expectation Maximization Hierarchical clustering One-class SVM Time-series analysis Time-Series analysis helps in analyzing trends and predicting outputs over time. It is used for trend prediction and novel detection (detection of unknown data). The approaches used here are Artificial neural network (ANN) Support vector regression (SVR) Decision tree Conferences Additional conferences and conference lists See also Astronomy and Computing Astrophysics Data System Astrophysics Source Code Library Astrostatistics Committee on Data for Science and Technology Data-driven astronomy Galaxy Zoo International Astrostatistics Association International Virtual Observatory Alliance (IVOA) MilkyWayhome Virtual Observatory WorldWide Telescope Zooniverse References External links International AstroInformatics Association (IAIA) Astronomical Data Analysis Software and Systems (ADASS) Astrostatistics and Astroinformatics Portal Cosmostatistics Initiative (COIN) Astroinformatics and Astrostatistics Commission of the International Astronomical Union Title Data-driven model URL https//en.wikipedia.org/wiki/Data-driven_model Content Data-driven models are a class of computational models that primarily rely on historical data collected throughout a systems or process lifetime to establish relationships between input, internal, and output variables. Commonly found in numerous articles and publications, data-driven models have evolved from earlier statistical models, overcoming limitations posed by strict assumptions about probability distributions. These models have gained prominence across various fields, particularly in the era of big data, artificial intelligence, and machine learning, where they offer valuable insights and predictions based on the available data. Background These models have evolved from earlier statistical models, which were based on certain assumptions about probability distributions that often proved to be overly restrictive. The emergence of data-driven models in the 1950s and 1960s coincided with the development of digital computers, advancements in artificial intelligence research, and the introduction of new approaches in non-behavioural modelling, such as pattern recognition and automatic classification. Key Concepts Data-driven models encompass a wide range of techniques and methodologies that aim to intelligently process and analyse large datasets. Examples include fuzzy logic, fuzzy and rough sets for handling uncertainty, neural networks for approximating functions, global optimization and evolutionary computing, statistical learning theory, and Bayesian methods. These models have found applications in various fields, including economics, customer relations management, financial services, medicine, and the military, among others. Machine learning, a subfield of artificial intelligence, is closely related to data-driven modelling as it also focuses on using historical data to create models that can make predictions and identify patterns. In fact, many data-driven models incorporate machine learning techniques, such as regression, classification, and clustering algorithms, to process and analyse data. In recent years, the concept of data-driven models has gained considerable attention in the field of water resources, with numerous applications, academic courses, and scientific publications using the term as a generalization for models that rely on data rather than physics. This classification has been featured in various publications and has even spurred the development of hybrid models in the past decade. Hybrid models attempt to quantify the degree of physically based information used in hydrological models and determine whether the process of building the model is primarily driven by physics or purely data-based. As a result, data-driven models have become an essential topic of discussion and exploration within water resources management and research. The term data-driven modelling (DDM) refers to the overarching paradigm of using historical data in conjunction with advanced computational techniques, including machine learning and artificial intelligence, to create models that can reveal underlying trends, patterns, and, in some cases, make predictions Data-driven models can be built with or without detailed knowledge of the underlying processes governing the system behavior, which makes them particularly useful when such knowledge is missing or fragmented. Title Decision list URL https//en.wikipedia.org/wiki/Decision_list Content Decision lists are a representation for Boolean functions which can be easily learnable from examples. Single term decision lists are more expressive than disjunctions and conjunctions however, 1-term decision lists are less expressive than the general disjunctive normal form and the conjunctive normal form. The language specified by a k-length decision list includes as a subset the language specified by a k-depth decision tree. Learning decision lists can be used for attribute efficient learning. Definition A decision list (DL) of length r is of the form if then output else if then output ... else if fr then output br where fi is the ith formula and bi is the ith boolean for i  1... r  displaystyle iin 1...r . The last if-then-else is the default case, which means formula fr is always equal to true. A k-DL is a decision list where all of formulas have at most k terms. Sometimes decision list is used to refer to a 1-DL, where all of the formulas are either a variable or its negation. See also Decision stump Title Decision tree pruning URL https//en.wikipedia.org/wiki/Decision_tree_pruning Content Pruning is a data compression technique in machine learning and search algorithms that reduces the size of decision trees by removing sections of the tree that are non-critical and redundant to classify instances. Pruning reduces the complexity of the final classifier, and hence improves predictive accuracy by the reduction of overfitting. One of the questions that arises in a decision tree algorithm is the optimal size of the final tree. A tree that is too large risks overfitting the training data and poorly generalizing to new samples. A small tree might not capture important structural information about the sample space. However, it is hard to tell when a tree algorithm should stop because it is impossible to tell if the addition of a single extra node will dramatically decrease error. This problem is known as the horizon effect. A common strategy is to grow the tree until each node contains a small number of instances then use pruning to remove nodes that do not provide additional information. Pruning should reduce the size of a learning tree without reducing predictive accuracy as measured by a cross-validation set. There are many techniques for tree pruning that differ in the measurement that is used to optimize performance. Techniques Pruning processes can be divided into two types (pre- and post-pruning). Pre-pruning procedures prevent a complete induction of the training set by replacing a stop () criterion in the induction algorithm (e.g. max. Tree depth or information gain (Attr) minGain). Pre-pruning methods are considered to be more efficient because they do not induce an entire set, but rather trees remain small from the start. Prepruning methods share a common problem, the horizon effect. This is to be understood as the undesired premature termination of the induction by the stop () criterion. Post-pruning (or just pruning) is the most common way of simplifying trees. Here, nodes and subtrees are replaced with leaves to reduce complexity. Pruning can not only significantly reduce the size but also improve the classification accuracy of unseen objects. It may be the case that the accuracy of the assignment on the train set deteriorates, but the accuracy of the classification properties of the tree increases overall. The procedures are differentiated on the basis of their approach in the tree (top-down or bottom-up). Bottom-up pruning These procedures start at the last node in the tree (the lowest point). Following recursively upwards, they determine the relevance of each individual node. If the relevance for the classification is not given, the node is dropped or replaced by a leaf. The advantage is that no relevant sub-trees can be lost with this method. These methods include Reduced Error Pruning (REP), Minimum Cost Complexity Pruning (MCCP), or Minimum Error Pruning (MEP). Top-down pruning In contrast to the bottom-up method, this method starts at the root of the tree. Following the structure below, a relevance check is carried out which decides whether a node is relevant for the classification of all n items or not. By pruning the tree at an inner node, it can happen that an entire sub-tree (regardless of its relevance) is dropped. One of these representatives is pessimistic error pruning (PEP), which brings quite good results with unseen items. Pruning algorithms Reduced error pruning One of the simplest forms of pruning is reduced error pruning. Starting at the leaves, each node is replaced with its most popular class. If the prediction accuracy is not affected then the change is kept. While somewhat naive, reduced error pruning has the advantage of simplicity and speed. Cost complexity pruning Cost complexity pruning generates a series of trees T 0 T m displaystyle T_0dots T_m where T 0 displaystyle T_0 is the initial tree and T m displaystyle T_m is the root alone. At step i displaystyle i , the tree is created by removing a subtree from tree i 1 displaystyle i-1 and replacing it with a leaf node with value chosen as in the tree building algorithm. The subtree that is removed is chosen as follows Define the error rate of tree T displaystyle T over data set S displaystyle S as err ( T , S ) displaystyle operatorname err (T,S) . The subtree t displaystyle t that minimizes err ( prune ( T , t ) , S ) err ( T , S )  leaves ( T )   leaves ( prune ( T , t ) )  displaystyle frac operatorname err (operatorname prune (T,t),S)-operatorname err (T,S)leftvert operatorname leaves (T)rightvert -leftvert operatorname leaves (operatorname prune (T,t))rightvert  is chosen for removal. The function prune ( T , t ) displaystyle operatorname prune (T,t) defines the tree obtained by pruning the subtrees t displaystyle t from the tree T displaystyle T . Once the series of trees has been created, the best tree is chosen by generalized accuracy as measured by a training set or cross-validation. Examples Pruning could be applied in a compression scheme of a learning algorithm to remove the redundant details without compromising the models performances. In neural networks, pruning removes entire neurons or layers of neurons. See also Alpha beta pruning Artificial neural network Null-move heuristic Pruning (artificial neural network) References Pearl, Judea (1984). Heuristics Intelligent Search Strategies for Computer Problem Solving. Addison-Wesley. ISBN 978-0-201-05594-8. Mansour, Y. (1997). Pessimistic decision tree pruning based on tree size. Proc. 14th International Conference on Machine Learning. pp. 195 201. Breslow, L. A. Aha, D. W. (1997). Simplifying Decision Trees A Survey. The Knowledge Engineering Review. 12 (1) 1 47. doi10.1017/. S2CID 18782652. Quinlan, J. R. (1986). Induction of Decision Trees. Machine Learning. 1. Kluwer 81 106. doi10.1007/. Further reading MDL based decision tree pruning Archived 2017-08-29 at the Wayback Machine Decision tree pruning using backpropagation neural networks External links Fast, Bottom-Up Decision Tree Pruning Algorithm Introduction to Decision tree pruning Title Deep Tomographic Reconstruction URL https//en.wikipedia.org/wiki/Deep_Tomographic_Reconstruction Content Deep Tomographic Reconstruction is an area where deep learning methods are used for tomographic reconstruction of medical and industrial images. It is a new frontier of the imaging field by utilizing artificial intelligence and machine learning, especially deep artificial neural networks or deep learning, to overcome challenges such as measurement noise, data sparsity, image artifacts, and computational inefficiency. This approach has been applied across various imaging modalities, including CT, MRI, PET, SPECT, ultrasound, and optical imaging. Rapid progress in this field marks a significant shift from traditional reconstruction methods to data-driven approaches since 2016. Historical background Traditional tomographic reconstruction relies on analytic methods such as filtered back-projection, or iterative methods which incrementally compute inverse transformations from measurement data (e.g., Radon or Fourier transform data). However, these approaches are unsatisfactory in challenging scenarios, such as low-dose CT, fast MRI, metal artifacts, patient motion, and so on. In 2016, deep tomographic reconstruction emerged as a new paradigm. Advances across imaging modalities Computed Tomography (CT) In CT, deep learning models have been particularly effective in reducing radiation exposure while maintaining image quality. Deep networks can also reconstruct decent images from sparsely sampled data without sacrificing diagnostic performance. Deep learning-based generative AI models can reduce CT metal artifacts. Magnetic Resonance Imaging (MRI) MRI reconstruction has benefited from deep learning by speeding up acquisition speed, which is also referred to as fast MRI. Also, MRI motion artifacts are reduced via deep learning. Deep learning has enabled significant improvements in low-field MRI by enhancing image quality despite lower signal-to-noise ratio (SNR), making these systems clinically viable. Positron Emission Tomography (PET) and Single Photon Emission CT (SPECT) For PET imaging, deep learning models provide substantial improvements in low-dose imaging and motion artifact correction. Also, deep learning helps SPECT for generation of attenuation background. A notable technique for PET denoising involves integrating MR data through multimodal networks, which leverage anatomical information from MRI to enhance PET image quality. Ultrasound Imaging Deep learning enhances ultrasound imaging by reducing speckle noise and motion blur. For ultrasound beamforming, deep neural networks, allows superior image quality with limited data at high speed. There are deep learning-based iPad ultrasound probes on the market. Such a device combines ultrasound imaging with AI-powered features for point-of-care applications. Optical Imaging and Microscopy Diffuse optical tomography, optical coherent tomography and microscopy are improved by deep neural networks beyond traditional methods. Furthermore, deep learning has also enhanced photoacoustic imaging, addressing challenges like high noise, low contrast, and limited resolution. Title Developmental robotics URL https//en.wikipedia.org/wiki/Developmental_robotics Content Developmental robotics (DevRob), sometimes called epigenetic robotics, is a scientific field which aims at studying the developmental mechanisms, architectures and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. As in human children, learning is expected to be cumulative and of progressively increasing complexity, and to result from self-exploration of the world in combination with social interaction. The typical methodological approach consists in starting from theories of human and animal development elaborated in fields such as developmental psychology, neuroscience, developmental and evolutionary biology, and linguistics, then to formalize and implement them in robots, sometimes exploring extensions or variants of them. The experimentation of those models in robots allows researchers to confront them with reality, and as a consequence, developmental robotics also provides feedback and novel hypotheses on theories of human and animal development. Developmental robotics is related to but differs from evolutionary robotics (ER). ER uses populations of robots that evolve over time, whereas DevRob is interested in how the organization of a single robots control system develops through experience, over time. DevRob is also related to work done in the domains of robotics and artificial life. Background Can a robot learn like a child? Can it learn a variety of new skills and new knowledge unspecified at design time and in a partially unknown and changing environment? How can it discover its body and its relationships with the physical and social environment? How can its cognitive capacities continuously develop without the intervention of an engineer once it is out of the factory? What can it learn through natural social interactions with humans? These are the questions at the center of developmental robotics. Alan Turing, as well as a number of other pioneers of cybernetics, already formulated those questions and the general approach in 1950, but it is only since the end of the 20th century that they began to be investigated systematically. Because the concept of adaptive intelligent machines is central to developmental robotics, it has relationships with fields such as artificial intelligence, machine learning, cognitive robotics or computational neuroscience. Yet, while it may reuse some of the techniques elaborated in these fields, it differs from them from many perspectives. It differs from classical artificial intelligence because it does not assume the capability of advanced symbolic reasoning and focuses on embodied and situated sensorimotor and social skills rather than on abstract symbolic problems. It differs from cognitive robotics because it focuses on the processes that allow the formation of cognitive capabilities rather than these capabilities themselves. It differs from computational neuroscience because it focuses on functional modeling of integrated architectures of development and learning. More generally, developmental robotics is uniquely characterized by the following three features It targets task-independent architectures and learning mechanisms, i.e. the machine/robot has to be able to learn new tasks that are unknown by the engineer It emphasizes open-ended development and lifelong learning, i.e. the capacity of an organism to acquire continuously novel skills. This should not be understood as a capacity for learning anything or even everything , but just that the set of skills that is acquired can be infinitely extended at least in some (not all) directions The complexity of acquired knowledge and skills shall increase (and the increase be controlled) progressively. Developmental robotics emerged at the crossroads of several research communities including embodied artificial intelligence, enactive and dynamical systems cognitive science, connectionism. Starting from the essential idea that learning and development happen as the self-organized result of the dynamical interactions among brains, bodies and their physical and social environment, and trying to understand how this self-organization can be harnessed to provide task-independent lifelong learning of skills of increasing complexity, developmental robotics strongly interacts with fields such as developmental psychology, developmental and cognitive neuroscience, developmental biology (embryology), evolutionary biology, and cognitive linguistics. As many of the theories coming from these sciences are verbal and/or descriptive, this implies a crucial formalization and computational modeling activity in developmental robotics. These computational models are then not only used as ways to explore how to build more versatile and adaptive machines but also as a way to evaluate their coherence and possibly explore alternative explanations for understanding biological development. Research directions Skill domains Due to the general approach and methodology, developmental robotics projects typically focus on having robots develop the same types of skills as human infants. A first category that is important being investigated is the acquisition of sensorimotor skills. These include the discovery of ones own body, including its structure and dynamics such as hand-eye coordination, locomotion, and interaction with objects as well as tool use, with a particular focus on the discovery and learning of affordances. A second category of skills targeted by developmental robots are social and linguistic skills the acquisition of simple social behavioural games such as turn-taking, coordinated interaction, lexicons, syntax and grammar, and the grounding of these linguistic skills into sensorimotor skills (sometimes referred as symbol grounding). In parallel, the acquisition of associated cognitive skills are being investigated such as the emergence of the self/non-self distinction, the development of attentional capabilities, of categorization systems and higher-level representations of affordances or social constructs, of the emergence of values, empathy, or theories of mind. Mechanisms and constraints The sensorimotor and social spaces in which humans and robot live are so large and complex that only a small part of potentially learnable skills can actually be explored and learnt within a life-time. Thus, mechanisms and constraints are necessary to guide developmental organisms in their development and control of the growth of complexity. There are several important families of these guiding mechanisms and constraints which are studied in developmental robotics, all inspired by human development Motivational systems, generating internal reward signals that drive exploration and learning, which can be of two main types extrinsic motivations push robots/organisms to maintain basic specific internal properties such as food and water level, physical integrity, or light (e.g. in phototropic systems) intrinsic motivations push robot to search for novelty, challenge, compression or learning progress per se, thus generating what is sometimes called curiosity-driven learning and exploration, or alternatively active learning and exploration Social guidance as humans learn a lot by interacting with their peers, developmental robotics investigates mechanisms that can allow robots to participate to human-like social interaction. By perceiving and interpreting social cues, this may allow robots both to learn from humans (through diverse means such as imitation, emulation, stimulus enhancement, demonstration, etc. ...) and to trigger natural human pedagogy. Thus, social acceptance of developmental robots is also investigated Statistical inference biases and cumulative knowledge/skill reuse biases characterizing both representations/encodings and inference mechanisms can typically allow considerable improvement of the efficiency of learning and are thus studied. Related to this, mechanisms allowing to infer new knowledge and acquire new skills by reusing previously learnt structures is also an essential field of study The properties of embodiment, including geometry, materials, or innate motor primitives/synergies often encoded as dynamical systems, can considerably simplify the acquisition of sensorimotor or social skills, and is sometimes referred as morphological computation. The interaction of these constraints with other constraints is an important axis of investigation Maturational constraints In human infants, both the body and the neural system grow progressively, rather than being full-fledged already at birth. This implies, for example, that new degrees of freedom, as well as increases of the volume and resolution of available sensorimotor signals, may appear as learning and development unfold. Transposing these mechanisms in developmental robots, and understanding how it may hinder or on the contrary ease the acquisition of novel complex skills is a central question in developmental robotics. From bio-mimetic development to functional inspiration. While most developmental robotics projects interact closely with theories of animal and human development, the degrees of similarities and inspiration between identified biological mechanisms and their counterpart in robots, as well as the abstraction levels of modeling, may vary a lot. While some projects aim at modeling precisely both the function and biological implementation (neural or morphological models), such as in Neurorobotics, some other projects only focus on functional modeling of the mechanisms and constraints described above, and might for example reuse in their architectures techniques coming from applied mathematics or engineering fields. Open questions As developmental robotics is a relatively new research field and at the same time very ambitious, many fundamental open challenges remain to be solved. First of all, existing techniques are far from allowing real-world high-dimensional robots to learn an open-ended repertoire of increasingly complex skills over a life-time period. High-dimensional continuous sensorimotor spaces constitute a significant obstacle to be solved. Lifelong cumulative learning is another one. Actually, no experiments lasting more than a few days have been set up so far, which contrasts severely with the time needed by human infants to learn basic sensorimotor skills while equipped with brains and morphologies which are tremendously more powerful than existing computational mechanisms. Among the strategies to explore to progress towards this target, the interaction between the mechanisms and constraints described in the previous section shall be investigated more systematically. Indeed, they have so far mainly been studied in isolation. For example, the interaction of intrinsically motivated learning and socially guided learning, possibly constrained by maturation, is an essential issue to be investigated. Another important challenge is to allow robots to perceive, interpret and leverage the diversity of multimodal social cues provided by non-engineer humans during human-robot interaction. These capacities are so far, mostly too limited to allow efficient general-purpose teaching from humans. A fundamental scientific issue to be understood and resolved, which applied equally to human development, is how compositionality, functional hierarchies, primitives, and modularity, at all levels of sensorimotor and social structures, can be formed and leveraged during development. This is deeply linked with the problem of the emergence of symbols, sometimes referred to as the symbol grounding problem when it comes to language acquisition. Actually, the very existence and need for symbols in the brain are actively questioned, and alternative concepts, still allowing for compositionality and functional hierarchies are being investigated. During biological epigenesis, morphology is not fixed but rather develops in constant interaction with the development of sensorimotor and social skills. The development of morphology poses obvious practical problems with robots, but it may be a crucial mechanism that should be further explored, at least in simulation, such as in morphogenetic robotics. Another open problem is the understanding of the relation between the key phenomena investigated by developmental robotics (e.g., hierarchical and modular sensorimotor systems, intrinsic/extrinsic/social motivations, and open-ended learning) and the underlying brain mechanisms. Similarly, in biology, developmental mechanisms (operating at the ontogenetic time scale) interact closely with evolutionary mechanisms (operating at the phylogenetic time scale) as shown in the flourishing evo-devo scientific literature. However, the interaction of those mechanisms in artificial organisms, developmental robots, in particular, is still vastly understudied. The interaction of evolutionary mechanisms, unfolding morphologies and developing sensorimotor and social skills will thus be a highly stimulating topic for the future of developmental robotics. Main journals IEEE Transactions on Cognitive and Developmental Systems (previously known as IEEE Transactions on Autonomous Mental Development) https//cis.ieee.org/publications/t-cognitive-and-developmental-systems Main conferences International Conference on Development and Learning http//www.cogsci.ucsd.edu/triesch/icdl/ Epigenetic Robotics https//www.lucs.lu.se/epirob/ ICDL-EpiRob http//www.icdl-epirob.org/ (the two above joined since 2011) Developmental Robotics http//cs.brynmawr.edu// The NSF/DARPA funded Workshop on Development and Learning was held April 5 7, 2000 at Michigan State University. It was the first international meeting devoted to computational understanding of mental development by robots and animals. The term by was used since the agents are active during development. See also Evolutionary developmental robotics Robot learning References External links Technical committees IEEE Technical Committee on Cognitive and Developmental Systems (CDSTC), previously known as IEEE Technical Committee on Autonomous Mental Development, IEEE Technical Committee on Cognitive Robotics, https//www.ieee-ras.org/cognitive-robotics IEEE Technical Committee on Robot Learning, https//www.ieee-ras.org/robot-learning/ Academic institutions and researchers in the field Lund University Cognitive Science - Robotics Group Cognitive Development Lab, University of Indiana, US Michigan State University Embodied Intelligence Lab Inria and Ensta ParisTech FLOWERS team, France Exploration, interaction and learning in developmental robotics University of Tokyo Intelligent Systems and Informatics Lab Cognitive Robotics Lab of Juergen Schmidhuber at IDSIA and Technical University of Munich LIRA-Lab, University of Genova, Italy CITEC at University of Bielefeld, Germany Vision Lab, Psychology Department, Southern Illinois University Carbondale FIAS (J. Triesch lab.) LPP, CNRS (K. Oregan lab.) AI Lab, SoftBank Robotics Europe, France Departement of Computer Science, University of Aberdeen Asada Laboratory, Department of Adaptive Machine Systems, Graduate School of Engineering, Osaka University, Japan The University of Texas at Austin, UTCS Intelligent Robotics Lab Bryn Mawr Colleges Developmental Robotics Project research projects by faculty and students at Swarthmore and Bryn Mawr Colleges, Philadelphia, PA, USA Jean Project Information Sciences Institute of the University of Southern California Cognitive Robotics (including Hide and Seek) at the Naval Research Laboratory Archived August 8, 2010, at the Wayback Machine The Laboratory for Perceptual Robotics, University of Massachusetts Amherst Amherst, USA Centre for Robotics and Neural Systems, Plymouth University Plymouth, United Kingdom Laboratory of Computational Embodied Neuroscience, Institute of Cognitive Science and Technologies National Research Council, Rome, Italy Neurocybernetic team, ETIS Lab., ENSEA University of Cergy-Pontoise CNRS, France Machine Perception and Cognitive Robotics Lab, Florida Atlantic University, Boca Raton, Florida Adaptive Systems Group, Department of Computer Science, Humboldt University of Berlin, Germany Cognitive Developmental Robotics Lab (Nagai Lab), The University of Tokyo, Japan Related large-scale projects RobotDoC Project (funded by European Commission) Italk Project (funded by European Commission) IM-CLeVeR Project (funded by European Commission) ERC Grant EXPLORERS Project (funded by European Research Council) RobotCub Project (funded by European Commission) Feelix Growing Project (funded by European Commission) Courses The first undergraduate courses in DevRob were offered at Bryn Mawr College and Swarthmore College in the Spring of 2003 by Douglas Blank and Lisa Meeden, respectively. The first graduate course in DevRob was offered at Iowa State University by Alexander Stoytchev in the Fall of 2005. Title Digital signal processing and machine learning URL https//en.wikipedia.org/wiki/Digital_signal_processing_and_machine_learning Content Digital signal processing and machine learning are two technologies that are often combined. Background Digital signal processing Digital signal processing (DSP) is the use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations. The digital signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency. In digital electronics, a digital signal is represented as a pulse train, which is typically generated by the switching of a transistor. Digital Signal Processing (DSP) has experienced considerable advancements over recent decades, largely due to innovations in digital computing and integrated circuit technology. Approximately thirty years ago, digital computers and their hardware were typically large, expensive, and primarily utilized for general-purpose applications in scientific and business contexts, often without real-time processing capabilities. The progression from medium-scale integration (MSI) to large-scale integration (LSI) and eventually to very-large-scale integration (VLSI) has facilitated the development of smaller, faster, and more cost-effective digital computers, along with specialized DSP hardware. These advancements in digital circuits now enable the design of highly capable digital systems, allowing the execution of complex DSP tasks that were once impractical or prohibitively expensive to manage with analog systems. Consequently, many signal processing tasks that were traditionally performed using analog methods are now efficiently handled by digital hardware, offering significant advantages in terms of cost, reliability, and flexibility. This transition from analog to digital processing has expanded the range of DSP applications and enhanced performance capabilities across various fields, including telecommunications, medical imaging, and audio processing. Machine learning Machine learning is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions. Quick progress in the field of deep learning, beginning in 2010s, allowed neural networks to surpass many previous approaches in performance. Machine learning, a subfield of artificial intelligence (AI), enables computers and computer-controlled systems to perform tasks that require intelligent behavior, such as pattern recognition, data interpretation, and decision-making. It allows computers to address complex problems where establishing traditional, rule-based models would be inefficient or impractical. Machine learning employs various techniques, including supervised, unsupervised, and reinforcement learning, to enable systems to learn from data and make predictions or classifications without being explicitly programmed with the models they aim to apply. Machine learning has gained widespread success and is now a fundamental component of numerous applications, including image recognition, natural language processing, autonomous systems, and predictive analytics. As a branch of computer science, it focuses on the development of algorithms that allow computers to identify patterns and understand data, mimicking certain aspects of human cognitive abilities. The adoption of machine learning has significantly expanded the capabilities of AI systems, contributing to its integration into a wide range of fields and technologies. Applications Digital Signal Processing (DSP) plays a crucial role across a wide range of applications Audio Processing DSP is integral to modern audio technology, facilitating tasks such as music compression, equalization, noise suppression, echo cancellation, sound spatialization, and the application of various audio effects. It is widely used in devices such as mobile phones, smart speakers, headphones, and hearing aids. Image Processing DSP techniques are essential for image enhancement, restoration, compression, and segmentation. Applications include digital cameras, medical imaging, satellite image analysis, machine vision, and surveillance systems. Speech Processing DSP is fundamental to speech recognition, voice control, voice search, encoding and decoding, Voice over IP (VoIP), and speech enhancement. These technologies are found in mobile phones, smart assistants, hands-free devices, and hearing aids. Communications DSP is a cornerstone of modern digital communications, supporting key functions such as encoding, modulation/demodulation, equalization, error control, multiple access, and synchronization. It is widely applied in modems, cellular networks, wireless communications, radio, and broadband systems. Sensors and Control DSP enables advanced capabilities in sensor fusion and calibration, sensor linearization, motor control, and adaptive control systems. It is critical in automation, stability control, and Internet of Things (IoT) devices, improving sensor data processing and enhancing the responsiveness of control systems. Signal Detection and Tracking DSP is central to radar and sonar technologies, assisting in the detection and tracking of moving targets, target classification, direction-of-arrival estimation, noise reduction, and moving target indication. It also plays a role in waveform design and imaging. Video Processing DSP is used in video decoding, interlaced-to-progressive conversion, image stabilization, noise reduction, and analytics such as motion detection, object tracking, and recognition. Applications include home theater systems, surveillance cameras, and security systems. Software Defined Radio (SDR) DSP, in combination with analog-to-digital converter technology, is fundamental to Software Defined Radio (SDR) systems. In these systems, functions such as modulation, filtering, and multiple access are managed through software rather than hardware, providing flexibility and adaptability across various radio frequencies. Applied machine learning for signal processing The integration of machine learning (ML) with digital signal processing (DSP) has significantly advanced various fields, enhancing the ability to process and analyze complex data. In image and video processing, ML-DSP systems enable more accurate object detection, facial recognition, and semantic segmentation, providing deeper insights into visual content. These technologies are widely applied in areas such as autonomous vehicles, surveillance systems, and any context requiring sophisticated visual analysis. In speech and natural language processing, the combination of ML and DSP has transformed applications such as speech recognition, language translation, and sentiment analysis. These systems are capable of accurately understanding and transcribing spoken language, which facilitates the development of virtual assistants, chatbots, and voice-controlled devices, improving user interaction across a variety of consumer applications. In healthcare diagnostics, the integration of ML and DSP has improved the accuracy of disease detection, diagnosis, and patient monitoring. ML algorithms are employed to analyze medical images for abnormalities, aiding in early detection and personalized treatment planning, thereby enhancing patient outcomes. The integration of ML and DSP also plays a critical role in wireless communications, where it optimizes system performance by adapting to changing channel conditions, reducing interference, and predicting network congestion. This results in improved data throughput and more reliable connectivity in diverse wireless environments, contributing to efficient network management. In the financial sector, ML-DSP applications are used in financial analytics and algorithmic trading, where ML models analyze market data to forecast stock prices and identify trading opportunities. These capabilities support more informed investment decisions and optimized portfolio management. In environmental monitoring, the integration of ML and DSP enables the processing of data from sensors and remote sensing devices to monitor air quality, detect natural disasters, develop climate models, and assess environmental changes. This contributes to timely responses to environmental issues and supports sustainability efforts across the globe. Solving problems of signal processing with machine learning In the field of digital signal processing (DSP), various challenges arise when analyzing and manipulating signals. One approach to addressing these challenges involves leveraging machine learning (ML). In this context, machine learning refers to the use of algorithms and statistical models to extract meaningful information from signals, enabling more accurate predictions and classifications. Applications of machine learning in signal processing include Signal Sampling and Filtering One critical application of ML in signal processing is in managing the complexities associated with signal sampling and filtering. Signal processing often requires extracting relevant information from signals while reducing noise, which can be challenging when signals undergo transformations. Machine learning techniques can assist by learning the inherent patterns and relationships within the signals. For instance, when dealing with a band-limited signal processed through an RC high-pass filter, selecting an optimal sampling frequency is essential but can be difficult. ML algorithms can analyze signal characteristics and filter behavior to determine the most appropriate sampling frequency. By training on datasets that include different sampling frequencies and their outcomes, ML models can identify patterns linking the signal, filter, and sampling frequency. Signal Recovery in Communication Systems Machine learning also plays a role in signal recovery, particularly in communication systems where the original signal must be recovered from a modulated signal with an unknown phase. ML algorithms can estimate the phase value and determine the minimal sampling rate needed for accurate signal recovery. By analyzing historical data, ML models generalize knowledge to solve such signal recovery challenges efficiently. Mitigating Aliasing Aliasing occurs when the sampling rate is insufficient to capture the details of a signal, leading to errors in signal reconstruction. Machine learning techniques can help identify the optimal sampling rate to prevent aliasing, ensuring accurate signal reproduction. ML models analyze the frequency content of a signal and recommend appropriate sampling frequencies, improving the quality of the processed signal. In summary, the integration of machine learning with digital signal processing offers a robust solution to many challenges in signal analysis and manipulation. ML algorithms can address issues related to sampling, filtering, modulation, and aliasing by learning from data and identifying underlying patterns. This combination enhances the effectiveness of traditional signal processing methods and enables innovative applications in areas such as telecommunications, audio processing, and medical diagnostics. As technology continues to advance, the potential for machine learning to transform signal processing practices remains significant. The benefits of machine learning in signal processing The integration of machine learning and signal processing has become increasingly prevalent, significantly impacting various industries by enabling more accurate, efficient, and intelligent data analysis. This convergence provides numerous benefits that are reshaping the technological landscape. One key advantage of combining signal processing with machine learning is the enhanced ability to extract meaningful information from complex signals. Traditional signal processing techniques may face limitations in revealing insights from intricate data streams. However, when augmented by machine learning algorithms, signal processing becomes more effective in deciphering complex signals with greater accuracy and efficiency. Machine learning models can detect patterns and features that are challenging to identify using conventional methods, leading to a more comprehensive understanding of the underlying data. Challenges and future prospects The integration of machine learning (ML) with digital signal processing (DSP) offers numerous opportunities for enhancing signal processing capabilities across various fields. However, this convergence also introduces several challenges, including the need for large training datasets, the risk of overfitting, and increased computational complexity. Addressing these challenges requires a strategic approach to data collection, model design, and optimization techniques. Title Discovery system (AI research) URL https//en.wikipedia.org/wiki/Discovery_system_(AI_research) Content A discovery system is an artificial intelligence system that attempts to discover new scientific concepts or laws. The aim of discovery systems is to automate scientific data analysis and the scientific discovery process. Ideally, an artificial intelligence system should be able to search systematically through the space of all possible hypotheses and yield the hypothesis - or set of equally likely hypotheses - that best describes the complex patterns in data. During the era known as the second AI summer (approximately 1978-1987), various systems akin to the eras dominant expert systems were developed to tackle the problem of extracting scientific hypotheses from data, with or without interacting with a human scientist. These systems included Autoclass, Automated Mathematician, Eurisko, which aimed at general-purpose hypothesis discovery, and more specific systems such as Dalton, which uncovers molecular properties from data. The dream of building systems that discover scientific hypotheses was pushed to the background with the second AI winter and the subsequent resurgence of subsymbolic methods such as neural networks. Subsymbolic methods emphasize prediction over explanation, and yield models which works well but are difficult or impossible to explain which has earned them the name black box AI. A black-box model cannot be considered a scientific hypothesis, and this development has even led some researchers to suggest that the traditional aim of science - to uncover hypotheses and theories about the structure of reality - is obsolete. Other researchers disagree and argue that subsymbolic methods are useful in many cases, just not for generating scientific theories. Discovery systems from the 1970s and 1980s Autoclass was a Bayesian Classification System written in 1986 Automated Mathematician was one of the earliest successful discovery systems. It was written in 1977 and worked by generating a modifying small Lisp programs Eurisko was a Sequel to Automated Mathematician written in 1984 Dalton is a still maintained program capable of calculating various molecular properties initially launched in 1983 and available in open source since 2017 Glauber is a scientific discovery method written in the context of computational philosophy of science launched in 1983 Modern discovery systems (2009 present) After a couple of decades with little interest in discovery systems, the interest in using AI to uncover natural laws and scientific explanations was renewed by the work of Michael Schmidt, then a PhD student in Computational Biology at Cornell University. Schmidt and his advisor, Hod Lipson, invented Eureqa, which they described as a symbolic regression approach to distilling free-form natural laws from experimental data. This work effectively demonstrated that symbolic regression was a promising way forward for AI-driven scientific discovery. Since 2009, symbolic regression has matured further, and today, various commercial and open source systems are actively used in scientific research. Notable examples include Eureqa, now a part of DataRobot AI Cloud Platform, AI Feynman, and QLattice. References External links The AI revolution in scientific research Title Document classification URL https//en.wikipedia.org/wiki/Document_classification Content Document classification or document categorization is a problem in library science, information science and computer science. The task is to assign a document to one or more classes or categories. This may be done manually (or intellectually) or algorithmically. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification. The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied. Documents may be classified according to their subjects or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents the content-based approach and the request-based approach. Content-based versus request-based classification Content-based classification is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20 of the content of a book should be about the class to which the book is assigned. In automatic classification it could be the number of times given words appears in a document. Request-oriented classification (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks themself Under which descriptors should this entity be found? and think of all the possible queries and decide for which ones the entity at hand is relevant (Soergel, 1985, p. 230). Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library. It is probably better, however, to understand request-oriented classification as policy-based classification The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach. Classification versus indexing Sometimes a distinction is made between assigning documents to classes (classification) versus assigning subjects to documents (subject indexing) but as Frederick Wilfrid Lancaster has argued, this distinction is not fruitful. These terminological distinctions, he writes, are quite meaningless and only serve to cause confusion (Lancaster, 2003, p. 21). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a thesaurus and vice versa (cf., Aitchison, 1986, 2004 Broughton, 2008 Riesthuis  Bliedung, 1991). Therefore, the act of labeling a document (say by assigning a term from a controlled vocabulary to a document) is at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents). In other words, labeling a document is the same as assigning it to the class of documents indexed under that label. Automatic document classification (ADC) Automatic document classification tasks can be divided into three sorts supervised document classification where some external mechanism (such as human feedback) provides information on the correct classification for documents, unsupervised document classification (also known as document clustering), where the classification must be done entirely without reference to external information, and semi-supervised document classification, where parts of the documents are labeled by the external mechanism. There are several software products under various license models available. Techniques Automatic document classification techniques include Artificial neural network Concept Mining Decision trees such as or .5 Expectation maximization (EM) Instantaneously trained neural networks Latent semantic indexing Multiple-instance learning Naive Bayes classifier Natural language processing approaches Rough set-based classifier Soft set-based classifier Support vector machines (SVM) K-nearest neighbour algorithms tf idf Applications Classification techniques have been applied to spam filtering, a process which tries to discern E-mail spam messages from legitimate emails email routing, sending an email sent to a general address to a specific address or mailbox depending on topic language identification, automatically determining the language of a text genre classification, automatically determining the genre of a text readability assessment, automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger text simplification system sentiment analysis, determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document. health-related classification using social media in public health surveillance article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology See also References Further reading Fabrizio Sebastiani. Machine learning in automated text categorization. ACM Computing Surveys, 34(1)1 47, 2002. Stefan B ttcher, Charles L. A. Clarke, and Gordon V. Cormack. Information Retrieval Implementing and Evaluating Search Engines Archived 2020-10-05 at the Wayback Machine. MIT Press, 2010. External links Introduction to document classification Bibliography on Automated Text Categorization Archived 2019-09-26 at the Wayback Machine Bibliography on Query Classification Archived 2019-10-02 at the Wayback Machine Text Classification analysis page Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python (available online) TechTC - Technion Repository of Text Categorization Datasets Archived 2020-02-14 at the Wayback Machine David D. Lewiss Datasets BioCreative III ACT (article classification task) datasetusurped Title Domain adaptation URL https//en.wikipedia.org/wiki/Domain_adaptation Content Domain adaptation is a field associated with machine learning and transfer learning. It addresses the challenge of training a model on one data distribution (the source domain) and applying it to a related but different data distribution (the target domain). A common example is spam filtering, where a model trained on emails from one user (source domain) is adapted to handle emails for another user with significantly different patterns (target domain). Domain adaptation techniques can also leverage unrelated data sources to improve learning. When multiple source distributions are involved, the problem extends to multi-source domain adaptation. Domain adaptation is a specialized area within transfer learning. In domain adaptation, the source and target domains share the same feature space but differ in their data distributions. In contrast, transfer learning encompasses broader scenarios, including cases where the target domain s feature space differs from that of the source domain(s). Classification of domain adaptation problems Domain adaptation setups are classified in two different ways according to the distribution shift between the domains, and according to the available data from the target domain. Distribution shifts Common distribution shifts are classified as follows Covariate Shift occurs when the input distributions of the source and destination change, but the relationship between inputs and labels remains unchanged. The above-mentioned spam filtering example typically falls in this category. Namely, the distributions (patterns) of emails may differ between the domains, but emails labeled as spam in the one domain should similarly be labeled in another. Prior Shift (Label Shift) occurs when the label distribution differs between the source and target datasets, while the conditional distribution of features given labels remains the same. An example is a classifier of hair color in images from Italy (source domain) and Norway (target domain). The proportions of hair colors (labels) differ, but images within classes like blond and black-haired populations remain consistent across domains. A classifier for the Norway population can exploit this prior knowledge of class proportions to improve its estimates. Concept Shift (Conditional Shift) refers to changes in the relationship between features and labels, even if the input distribution remains the same. For instance, in medical diagnosis, the same symptoms (inputs) may indicate entirely different diseases (labels) in different populations (domains). Data available during training Domain adaptation problems typically assume that some data from the target domain is available during training. Problems can be classified according to the type of this available data Unsupervised Unlabeled data from the target domain is available, but no labeled data. In the above-mentioned example of spam filtering, this corresponds to the case where emails from the target domain (user) are available, but they are not labeled as spam. Domain adaptation methods can benefit from such unlabeled data, by comparing its distribution (patterns) with the labeled source domain data. Semi-supervised Most data that is available from the target domain is unlabelled, but some labeled data is also available. In the above-mentioned case of spam filter design, this corresponds to the case that the target user has labeled some emails as being spam or not. Supervised All data that is available from the target domain is labeled. In this case, domain adaptation reduces to refinement of the source domain predictor. In the above-mentioned example classification of hair-color from images, this could correspond to the refinement of a network already trained on a large dataset of labeled images from Italy, using newly available labeled images from Norway. Formalization Let X displaystyle X be the input space (or description space) and let Y displaystyle Y be the output space (or label space). The objective of a machine learning algorithm is to learn a mathematical model (a hypothesis) h  X Y displaystyle hXto Y able to attach a label from Y displaystyle Y to an example from X displaystyle X . This model is learned from a learning sample  . Usually in supervised learning (without domain adaptation), we suppose that the examples ( x i , y i ) S displaystyle (x_i,y_i)in S are drawn i.i.d. from a distribution D S displaystyle D_S of support X Y displaystyle Xtimes Y (unknown and fixed). The objective is then to learn h displaystyle h (from S displaystyle S ) such that it commits the least error possible for labelling new examples coming from the distribution D S displaystyle D_S . The main difference between supervised learning and domain adaptation is that in the latter situation we study two different (but related) distributions D S displaystyle D_S and D T displaystyle D_T on X Y displaystyle Xtimes Y . The domain adaptation task then consists of the transfer of knowledge from the source domain D S displaystyle D_S to the target one D T displaystyle D_T . The goal is then to learn h displaystyle h (from labeled or unlabelled samples coming from the two domains) such that it commits as little error as possible on the target domain D T displaystyle D_T . The major issue is the following if a model is learned from a source domain, what is its capacity to correctly label data coming from the target domain? Four algorithmic principles Reweighting algorithms The objective is to reweight the source labeled sample such that it looks like the target sample (in terms of the error measure considered). Iterative algorithms A method for adapting consists in iteratively auto-labeling the target examples. The principle is simple a model h displaystyle h is learned from the labeled examples h displaystyle h automatically labels some target examples a new model is learned from the new labeled examples. Note that there exist other iterative approaches, but they usually need target labeled examples. Search of a common representation space The goal is to find or construct a common representation space for the two domains. The objective is to obtain a space in which the domains are close to each other while keeping good performances on the source labeling task. This can be achieved through the use of Adversarial machine learning techniques where feature representations from samples in different domains are encouraged to be indistinguishable. Hierarchical Bayesian Model The goal is to construct a Bayesian hierarchical model p ( n ) displaystyle p(n) , which is essentially a factorization model for counts n displaystyle n , to derive domain-dependent latent representations allowing both domain-specific and globally shared latent factors. Softwares Several compilations of domain adaptation and transfer learning algorithms have been implemented over the past decades SKADA (Python) ADAPT (Python) TLlib (Python) Domain-Adaptation-Toolbox (MATLAB) Title Double descent URL https//en.wikipedia.org/wiki/Double_descent Content Double descent in statistics and machine learning is the phenomenon where a model with a small number of parameters and a model with an extremely large number of parameters both have a small training error, but a model whose number of parameters is about the same as the number of data points used to train the model will have a much greater test error than one with a much larger number of parameters. This phenomenon has been considered surprising, as it contradicts assumptions about overfitting in classical machine learning. History Early observations of what would later be called double descent in specific models date back to 1989. The term double descent was coined by Belkin et. al. in 2019, when the phenomenon gained popularity as a broader concept exhibited by many models. The latter development was prompted by a perceived contradiction between the conventional wisdom that too many parameters in the model result in a significant overfitting error (an extrapolation of the bias variance tradeoff), and the empirical observations in the 2010s that some modern machine learning techniques tend to perform better with larger models. Theoretical models Double descent occurs in linear regression with isotropic Gaussian covariates and isotropic Gaussian noise. A model of double descent at the thermodynamic limit has been analyzed using the replica trick, and the result has been confirmed numerically. Empirical examples The scaling behavior of double descent has been found to follow a broken neural scaling law functional form. References Further reading Mikhail Belkin Daniel Hsu Ji Xu (2020). Two Models of Double Descent for Weak Features. SIAM Journal on Mathematics of Data Science. 2 (4) 1167 1180. arXiv1903.07571. doi10.1137/20M1336072. Mount, John (3 April 2024). The . Preetum Nakkiran Gal Kaplun Yamini Bansal Tristan Yang Boaz Barak Ilya Sutskever (29 December 2021). Deep double descent where bigger models and more data hurt. Journal of Statistical Mechanics Theory and Experiment. 2021 (12). IOP Publishing Ltd and SISSA Medialab srl 124003. arXiv1912.02292. Bibcode2021JSMTE2021l4003N. doi10.1088/1742-5468/ac3a74. S2CID 207808916. Song Mei Andrea Montanari (April 2022). The Generalization Error of Random Features Regression Precise Asymptotics and the Double Descent Curve. Communications on Pure and Applied Mathematics. 75 (4) 667 766. arXiv1908.05355. doi10.1002/cpa.22008. S2CID 199668852. Xiangyu Chang Yingcong Li Samet Oymak Christos Thrampoulidis (2021). Provable Benefits of Overparameterization in Model Compression From Double Descent to Pruning Neural Networks. Proceedings of the AAAI Conference on Artificial Intelligence. 35 (8). arXiv2012.08749. External links Brent Werness Jared Wilber. Double Descent Part 1 A Visual Introduction. Brent Werness Jared Wilber. Double Descent Part 2 A Mathematical Explanation. Understanding Deep Double Descent at evhub. Title Eager learning URL https//en.wikipedia.org/wiki/Eager_learning Content In artificial intelligence, eager learning is a learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system. The main advantage gained in employing an eager learning method, such as an artificial neural network, is that the target function will be approximated globally during training, thus requiring much less space than using a lazy learning system. Eager learning systems also deal much better with noise in the training data. Eager learning is an example of offline learning, in which post-training queries to the system have no effect on the system itself, and thus the same query to the system will always produce the same result. The main disadvantage with eager learning is that it is generally unable to provide good local approximations in the target function. Title EfficientNet URL https//en.wikipedia.org/wiki/EfficientNet Content EfficientNet is a family of convolutional neural networks (CNNs) for computer vision published by researchers at Google AI in 2019. Its key innovation is compound scaling, which uniformly scales all dimensions of depth, width, and resolution using a single parameter. EfficientNet models have been adopted in various computer vision tasks, including image classification, object detection, and segmentation. Compound scaling EfficientNet introduces compound scaling, which, instead of scaling one dimension of the network at a time, such as depth (number of layers), width (number of channels), or resolution (input image size), uses a compound coefficient displaystyle phi  to scale all three dimensions simultaneously. Specifically, given a baseline network, the depth, width, and resolution are scaled according to the following equations depth multiplier  . The 2 2 2 displaystyle alpha cdot beta 2cdot gamma 2approx 2 condition is such that increasing displaystyle phi  by a factor of 0 displaystyle phi _0 would increase the total FLOPs of running the network on an image approximately 2 0 displaystyle 2phi _0 times. The hyperparameters displaystyle alpha  , displaystyle beta  , and displaystyle gamma  are determined by a small grid search. The original paper suggested 1.2, 1.1, and 1.15, respectively. Architecturally, they optimized the choice of modules by neural architecture search (NAS), and found that the inverted bottleneck convolution (which they called MBConv) used in MobileNet worked well. The EfficientNet family is a stack of MBConv layers, with shapes determined by the compound scaling. The original publication consisted of 8 models, from EfficientNet- to EfficientNet-, with increasing model size and accuracy. EfficientNet- is the baseline network, and subsequent models are obtained by scaling the baseline network by increasing displaystyle phi  . Variants EfficientNet has been adapted for fast inference on edge TPUs and centralized TPU or GPU clusters by NAS. EfficientNet was published in June 2021. The architecture was improved by further NAS search with more types of convolutional layers. It also introduced a training method, which progressively increases image size during training, and uses regularization techniques like dropout, RandAugment, and Mixup. The authors claim this approach mitigates accuracy drops often associated with progressive resizing. See also Convolutional neural network SqueezeNet MobileNet You Only Look Once References External links EfficientNet Improving Accuracy and Efficiency through AutoML and Model Scaling (Google AI Blog) Title ELMo URL https//en.wikipedia.org/wiki/ELMo Content ELMo (embeddings from language model) is a word embedding method for representing a sequence of words as a corresponding sequence of vectors. It was created by researchers at the Allen Institute for Artificial Intelligence, and University of Washington and first released in February, 2018. It is a bidirectional LSTM which takes character-level as inputs and produces word-level embeddings, trained on a corpus of about 30 million sentences and 1 billion words. The architecture of ELMo accomplishes a contextual understanding of tokens. Deep contextualized word representation is useful for many natural language processing tasks, such as coreference resolution and polysemy resolution. ELMo was historically important as a pioneer of self-supervised generative pretraining followed by fine-tuning, where a large model is trained to reproduce a large corpus, then the large model is augmented with additional task-specific weights and fine-tuned on supervised task data. It was an instrumental step in the evolution towards transformer-based language modelling. Architecture ELMo is a multilayered bidirectional LSTM on top of a token embedding layer. The output of all LSTMs concatenated together consists of the token embedding. The input text sequence is first mapped by an embedding layer into a sequence of vectors. Then two parts are run in parallel over it. The forward part is a 2-layered LSTM with 4096 units and 512 dimension projections, and a residual connection from the first to second layer. The backward part has the same architecture, but processes the sequence back-to-front. The outputs from all 5 components (embedding layer, two forward LSTM layers, and two backward LSTM layers) are concatenated and multiplied by a linear matrix (projection matrix) to produce a 512-dimensional representation per input token. ELMo was pretrained on a text corpus of 1 billion words. The forward part is trained by repeatedly predicting the next token, and the backward part is trained by repeatedly predicting the previous token. After the ELMo model is pretrained, its parameters are frozen, except for the projection matrix, which can be fine-tuned to minimize loss on specific language tasks. This is an early example of the pretraining-fine-tune paradigm. The original paper demonstrated this by improving state of the art on six benchmark NLP tasks. Contextual word representation The architecture of ELMo accomplishes a contextual understanding of tokens. For example, the first forward LSTM of ELMo would process each input token in the context of all previous tokens, and the first backward LSTM would process each token in the context of all subsequent tokens. The second forward LSTM would then incorporate those to further contextualize each token. Deep contextualized word representation is useful for many natural language processing tasks, such as coreference resolution and polysemy resolution. For example, consider the sentenceShe went to the bank to withdraw money.In order to represent the token bank, the model must resolve its polysemy in context. The first forward LSTM would process bank in the context of She went to the, which would allow it to represent the word to be a location that the subject is going towards. The first backward LSTM would process bank in the context of to withdraw money, which would allow it to disambiguate the word as referring to a financial institution. The second forward LSTM can then process bank using the representation vector provided by the first backward LSTM, thus allowing it to represent it to be a financial institution that the subject is going towards. Historical context ELMo is one link in a historical evolution of language modelling. Consider a simple problem of document classification, where we want to assign a label (e.g., spam, not spam, politics, sports) to a given piece of text. The simplest approach is the bag of words approach, where each word in the document is treated independently, and its frequency is used as a feature for classification. This was computationally cheap but ignored the order of words and their context within the sentence. GloVe and Word2Vec built upon this by learning fixed vector representations (embeddings) for words based on their co-occurrence patterns in large text corpora. Like BERT (but unlike bag of words such as Word2Vec and GloVe), ELMo word embeddings are context-sensitive, producing different representations for words that share the same spelling. It was trained on a corpus of about 30 million sentences and 1 billion words. Previously, bidirectional LSTM was used for contextualized word representation. ELMo applied the idea to a large scale, achieving state of the art performance. After the 2017 publication of Transformer architecture, the architecture of ELMo was changed from a multilayered bidirectional LSTM to a Transformer encoder, giving rise to BERT. BERT has the same pretrain-fine-tune workflow, but uses a Transformer for parallelizable training. Title EM algorithm and GMM model URL https//en.wikipedia.org/wiki/EM_algorithm_and_GMM_model Content In statistics, EM (expectation maximization) algorithm handles latent variables, while GMM is the Gaussian mixture model. Background In the picture below, are shown the red blood cell hemoglobin concentration and the red blood cell volume data of two groups of people, the Anemia group and the Control Group (i.e. the group of people without Anemia). As expected, people with Anemia have lower red blood cell volume and lower red blood cell hemoglobin concentration than those without Anemia. x displaystyle x is a random vector such as x  ( red blood cell volume , red blood cell hemoglobin concentration ) displaystyle xbig (textred blood cell volume,textred blood cell hemoglobin concentrationbig ) , and from medical studies it is known that x displaystyle x are normally distributed in each group, i.e. x N ( , ) displaystyle xsim mathcal N(mu ,Sigma ) . z displaystyle z is denoted as the group where x displaystyle x belongs, with z . Also z Categorical ( k , ) displaystyle zsim operatorname Categorical (k,phi ) where  . See Categorical distribution. The following procedure can be used to estimate , , displaystyle phi ,mu ,Sigma  . A maximum likelihood estimation can be applied ( , , )  . But if z i displaystyle z_i is unknown it is much more complicated. Being z displaystyle z a latent variable (i.e. not observed), with unlabeled scenario, the Expectation Maximization Algorithm is needed to estimate z displaystyle z as well as other parameters. Generally, this problem is set as a GMM since the data in each group is normally distributed. In machine learning, the latent variable z displaystyle z is considered as a latent pattern lying under the data, which the observer is not able to see very directly. x i displaystyle x_i is the known data, while , , displaystyle phi ,mu ,Sigma  are the parameter of the model. With the EM algorithm, some underlying pattern z displaystyle z in the data x i displaystyle x_i can be found, along with the estimation of the parameters. The wide application of this circumstance in machine learning is what makes EM algorithm so important. EM algorithm in GMM The EM algorithm consists of two steps the E-step and the M-step. Firstly, the model parameters and the z ( i ) displaystyle z(i) can be randomly initialized. In the E-step, the algorithm tries to guess the value of z ( i ) displaystyle z(i) based on the parameters, while in the M-step, the algorithm updates the value of the model parameters based on the guess of z ( i ) displaystyle z(i) of the E-step. These two steps are repeated until convergence is reached. The algorithm in GMM is Repeat until convergence 1. (E-step) For each i , j displaystyle i,j , set w j ( i )  p ( z ( i )  j  x ( i )  , , ) displaystyle w_j(i)pleft(z(i)jx(i)phi ,mu ,Sigma right) 2. (M-step) Update the parameters j  1 m . Title Embedding (machine learning) URL https//en.wikipedia.org/wiki/Embedding_(machine_learning) Content Embedding in machine learning refers to a representation learning technique that maps complex, high-dimensional data into a lower-dimensional vector space of numerical vectors. It also denotes the resulting representation, where meaningful patterns or relationships are preserved. As a technique, it learns these vectors from data like words, images, or user interactions, differing from manually designed methods such as one-hot encoding. This process reduces complexity and captures key features without needing prior knowledge of the problem area (domain). For example, in natural language processing (NLP), it might represent cat as 0.2, -0.4, 0.7, dog as 0.3, -0.5, 0.6, and car as 0.8, 0.1, -0.2, placing cat and dog close together in the space reflecting their similarity while car is farther away. The resulting embeddings vary by type, including word embeddings for text (e.g., Word2Vec), image embeddings for visual data, and knowledge graph embeddings for knowledge graphs, each tailored to tasks like NLP, computer vision, or recommendation systems. This dual role enhances model efficiency and accuracy by automating feature extraction and revealing latent similarities across diverse applications. See also Feature extraction Dimensionality reduction Word embedding Neural network Reinforcement learning Title Empirical dynamic modeling URL https//en.wikipedia.org/wiki/Empirical_dynamic_modeling Content Empirical dynamic modeling (EDM) is a framework for analysis and prediction of nonlinear dynamical systems. Applications include population dynamics, ecosystem service, medicine, neuroscience, dynamical systems, geophysics, and human-computer interaction. EDM was originally developed by Robert May and George Sugihara. It can be considered a methodology for data modeling, predictive analytics, dynamical system analysis, machine learning and time series analysis. Description Mathematical models have tremendous power to describe observations of real-world systems. They are routinely used to test hypothesis, explain mechanisms and predict future outcomes. However, real-world systems are often nonlinear and multidimensional, in some instances rendering explicit equation-based modeling problematic. Empirical models, which infer patterns and associations from the data instead of using hypothesized equations, represent a natural and flexible framework for modeling complex dynamics. Donald DeAngelis and Simeon Yurek illustrated that canonical statistical models are ill-posed when applied to nonlinear dynamical systems. A hallmark of nonlinear dynamics is state-dependence system states are related to previous states governing transition from one state to another. EDM operates in this space, the multidimensional state-space of system dynamics rather than on one-dimensional observational time series. EDM does not presume relationships among states, for example, a functional dependence, but projects future states from localised, neighboring states. EDM is thus a state-space, nearest-neighbors paradigm where system dynamics are inferred from states derived from observational time series. This provides a model-free representation of the system naturally encompassing nonlinear dynamics. A cornerstone of EDM is recognition that time series observed from a dynamical system can be transformed into higher-dimensional state-spaces by time-delay embedding with Takenss theorem. The state-space models are evaluated based on in-sample fidelity to observations, conventionally with Pearson correlation between predictions and observations. Methods EDM is continuing to evolve. As of 2022, the main algorithms are Simplex projection, Sequential locally weighted global linear maps (S-Map) projection, Multivariate embedding in Simplex or S-Map, Convergent cross mapping (CCM), and Multiview Embeding, described below. Nearest neighbors are found according to NN ( y , X , k )  X N i E y X N j E y if 1 i j k displaystyle textNN(y,X,k)X_N_iE-yleq X_N_jE-ytext if 1leq ileq jleq k Simplex Simplex projection is a nearest neighbor projection. It locates the k displaystyle k nearest neighbors to the location in the state-space from which a prediction is desired. To minimize the number of free parameters k displaystyle k is typically set to E  1 displaystyle E1 defining an E  1 displaystyle E1 dimensional simplex in the state-space. The prediction is computed as the average of the weighted phase-space simplex projected T p displaystyle Tp points ahead. Each neighbor is weighted proportional to their distance to the projection origin vector in the state-space. Find k displaystyle k nearest neighbor N k NN ( y , X , k ) displaystyle N_kgets textNN(y,X,k) Define the distance scale d X N 1 E y displaystyle dgets X_N_1E-y Compute weights For . The exponential localisation function is F ( )  exp ( d / D ) displaystyle F(theta )textexp(-theta d/D) , where d displaystyle d is the neighbor distance and D displaystyle D the mean distance. In this way, depending on the value of displaystyle theta  , neighbors close to the prediction origin point have a higher weight than those further from it, such that a local linear approximation to the nonlinear system is reasonable. This localisation ability allows one to identify an optimal local scale, in-effect quantifying the degree of state dependence, and hence nonlinearity of the system. Another feature of S-Map is that for a properly fit model, the regression coefficients between variables have been shown to approximate the gradient (directional derivative) of variables along the manifold. These Jacobians represent the time-varying interaction strengths between system variables. Find k displaystyle k nearest neighbor N NN ( y , X , k ) displaystyle Ngets textNN(y,X,k) Sum of distances D 1 k . In Simplex and S-Map one can generate a state-space from observational vectors, or time-delay embeddings of a single observational time series, or both. Convergent Cross Mapping Convergent cross mapping (CCM) leverages a corollary to the Generalized Takens Theorem that it should be possible to cross predict or cross map between variables observed from the same system. Suppose that in some dynamical system involving variables X displaystyle X and Y displaystyle Y , X displaystyle X causes Y displaystyle Y . Since X displaystyle X and Y displaystyle Y belong to the same dynamical system, their reconstructions (via embeddings) M x displaystyle M_x , and M y displaystyle M_y , also map to the same system. The causal variable X displaystyle X leaves a signature on the affected variable Y displaystyle Y , and consequently, the reconstructed states based on Y displaystyle Y can be used to cross predict values of X displaystyle X . CCM leverages this property to infer causality by predicting X displaystyle X using the M y displaystyle M_y library of points (or vice versa for the other direction of causality), while assessing improvements in cross map predictability as larger and larger random samplings of M y displaystyle M_y are used. If the prediction skill of X displaystyle X increases and saturates as the entire M y displaystyle M_y is used, this provides evidence that X displaystyle X is casually influencing Y displaystyle Y . Multiview Embedding Multiview Embedding is a Dimensionality reduction technique where a large number of state-space time series vectors are combitorially assessed towards maximal model predictability. Extensions Extensions to EDM techniques include Generalized Theorems for Nonlinear State Space Reconstruction Extended Convergent Cross Mapping Dynamic stability S-Map regularization Visual analytics with EDM Convergent Cross Sorting Expert system with EDM hybrid Sliding windows based on the extended convergent cross-mapping Empirical Mode Modeling Variable step sizes with bundle embedding Multiview distance regularised S-map See also System dynamics Complex dynamics Nonlinear dimensionality reduction References Further reading Chang, CW., Ushio, M.  Hsieh, Ch. (2017). Empirical dynamic modeling for beginners. Ecol Res. 32 (6) 785 796. Bibcode2017EcoR...32..785C. doi10.1007/-017-1469-9. hdl2433/235326. S2CID 4641225.cite journal maint multiple names authors list (link) Stephan B Munch, Antoine Brias, George Sugihara, Tanya L Rogers (2020). Frequently asked questions about nonlinear dynamics and empirical dynamic modelling. ICES Journal of Marine Science. 77 (4) 1463 1479. doi10.1093/icesjms/.cite journal maint multiple names authors list (link) Donald L. DeAngelis, Simeon Yurek (2015). Equation-free modeling unravels the behavior of complex ecological systems. Proceedings of the National Academy of Sciences. 112 (13) 3856 3857. doi10.1073/pnas.1503154112. PMC 4386356. PMID 25829536. External links Animations State Space Reconstruction Time Series and Dynamic Systems on YouTube State Space Reconstruction Takens Theorem and Shadow Manifolds on YouTube State Space Reconstruction Convergent Cross Mapping on YouTube Online books or lecture notes EDM Introduction. Introduction with video, examples and references. Geometrical theory of dynamical systems. Nils Berglunds lecture notes for a course at ETH at the advanced undergraduate level. Arxiv preprint server has daily submissions of (non-refereed) manuscripts in dynamical systems. Research groups Sugihara Lab, Scripps Institution of Oceanography, University of California San Diego. Title Empirical risk minimization URL https//en.wikipedia.org/wiki/Empirical_risk_minimization Content In statistical learning theory, the principle of empirical risk minimization defines a family of learning algorithms based on evaluating performance over a known and fixed dataset. The core idea is based on an application of the law of large numbers more specifically, we cannot know exactly how well a predictive algorithm will work in practice (i.e. the true risk) because we do not know the true distribution of the data, but we can instead estimate and optimize the performance of the algorithm on a known set of training data. The performance over the known set of training data is referred to as the empirical risk. Background The following situation is a general setting of many supervised learning problems. There are two spaces of objects X displaystyle X and Y displaystyle Y and we would like to learn a function h  X Y displaystyle  hXto Y (often called hypothesis) which outputs an object y Y displaystyle yin Y , given x X displaystyle xin X . To do so, there is a training set of n displaystyle n examples ( x 1 , y 1 ) , , ( x n , y n ) displaystyle  (x_1,y_1),ldots ,(x_n,y_n) where x i X displaystyle x_iin X is an input and y i Y displaystyle y_iin Y is the corresponding response that is desired from h ( x i ) displaystyle h(x_i) . To put it more formally, assuming that there is a joint probability distribution P ( x , y ) displaystyle P(x,y) over X displaystyle X and Y displaystyle Y , and that the training set consists of n displaystyle n instances ( x 1 , y 1 ) , , ( x n , y n ) displaystyle  (x_1,y_1),ldots ,(x_n,y_n) drawn i.i.d. from P ( x , y ) displaystyle P(x,y) . The assumption of a joint probability distribution allows for the modelling of uncertainty in predictions (e.g. from noise in data) because y displaystyle y is not a deterministic function of x displaystyle x , but rather a random variable with conditional distribution P ( y  x ) displaystyle P(yx) for a fixed x displaystyle x . It is also assumed that there is a non-negative real-valued loss function L ( y  , y ) displaystyle L(hat y,y) which measures how different the prediction y  displaystyle hat y of a hypothesis is from the true outcome y displaystyle y . For classification tasks, these loss functions can be scoring rules. The risk associated with hypothesis h ( x ) displaystyle h(x) is then defined as the expectation of the loss function R ( h )  E  L ( h ( x ) , y )   L ( h ( x ) , y ) d P ( x , y ) . displaystyle R(h)mathbf E L(h(x),y)int L(h(x),y),dP(x,y). A loss function commonly used in theory is the 0-1 loss function L ( y  , y )   1 if y  y 0 if y   y displaystyle L(hat y,y)begincases1mbox if quad hat yneq y0mbox if quad hat yyendcases . The ultimate goal of a learning algorithm is to find a hypothesis h displaystyle h among a fixed class of functions H displaystyle mathcal H for which the risk R ( h ) displaystyle R(h) is minimal  ) . displaystyle hunderset hin mathcal Hoperatorname arg,min ,R(h). For classification problems, the Bayes classifier is defined to be the classifier minimizing the risk defined with the 0 1 loss function. Formal definition In general, the risk R ( h ) displaystyle R(h) cannot be computed because the distribution P ( x , y ) displaystyle P(x,y) is unknown to the learning algorithm. However, given a sample of iid training data points, we can compute an estimate, called the empirical risk, by computing the average of the loss function over the training set more formally, computing the expectation with respect to the empirical measure R emp ( h )  1 n  ) . displaystyle !R_textemp(h)frac 1nsum _). The empirical risk minimization principle states that the learning algorithm should choose a hypothesis h  displaystyle hat h which minimizes the empirical risk over the hypothesis class H displaystyle mathcal H  h   a r g m i n h H R emp ( h ) . displaystyle hat hunderset hin mathcal Hoperatorname arg,min ,R_textemp(h). Thus, the learning algorithm defined by the empirical risk minimization principle consists in solving the above optimization problem. Properties Guarantees for the performance of empirical risk minimization depend strongly on the function class selected as well as the distributional assumptions made. In general, distribution-free methods are too coarse, and do not lead to practical bounds. However, they are still useful in deriving asymptotic properties of learning algorithms, such as consistency. In particular, distribution-free bounds on the performance of empirical risk minimization given a fixed function class can be derived using bounds on the VC complexity of the function class. For simplicity, considering the case of binary classification tasks, it is possible to bound the probability of the selected classifier, n displaystyle phi _n being much worse than the best possible classifier displaystyle phi  . Consider the risk L displaystyle L defined over the hypothesis class C displaystyle mathcal C with growth function S ( C , n ) displaystyle mathcal S(mathcal C,n) given a dataset of size n displaystyle n . Then, for every  0 displaystyle epsilon 0  P ( L ( n ) L ( )  ) 8 S ( C , n ) exp  n 2 / 32  displaystyle mathbb P left(L(phi _n)-L(phi )epsilon right)leq mathcal 8S(mathcal C,n)exp-nepsilon 2/32 Similar results hold for regression tasks. These results are often based on uniform laws of large numbers, which control the deviation of the empirical risk from the true risk, uniformly over the hypothesis class. Impossibility results It is also possible to show lower bounds on algorithm performance if no distributional assumptions are made. This is sometimes referred to as the No free lunch theorem. Even though a specific learning algorithm may provide the asymptotically optimal performance for any distribution, the finite sample performance is always poor for at least one data distribution. This means that no classifier can provide on the error for a given sample size for all distributions. Specifically, let  0 displaystyle epsilon 0 and consider a sample size n displaystyle n and classification rule n displaystyle phi _n , there exists a distribution of ( X , Y ) displaystyle (X,Y) with risk  . displaystyle mathbb E L_ngeq 1/2-epsilon . It is further possible to show that the convergence rate of a learning algorithm is poor for some distributions. Specifically, given a sequence of decreasing positive numbers a i displaystyle a_i converging to zero, it is possible to find a distribution such that E L n a i displaystyle mathbb E L_ngeq a_i for all n displaystyle n . This result shows that universally good classification rules do not exist, in the sense that the rule must be low quality for at least one distribution. Computational complexity Empirical risk minimization for a classification problem with a 0-1 loss function is known to be an NP-hard problem even for a relatively simple class of functions such as linear classifiers. Nevertheless, it can be solved efficiently when the minimal empirical risk is zero, i.e., data is linearly separable. In practice, machine learning algorithms cope with this issue either by employing a convex approximation to the 0 1 loss function (like hinge loss for SVM), which is easier to optimize, or by imposing assumptions on the distribution P ( x , y ) displaystyle P(x,y) (and thus stop being agnostic learning algorithms to which the above result applies). In the case of convexification, Zhangs lemma majors the excess risk of the original problem using the excess risk of the convexified problem. Minimizing the latter using convex optimization also allow to control the former. Tilted empirical risk minimization Tilted empirical risk minimization is a machine learning technique used to modify standard loss functions like squared error, by introducing a tilt parameter. This parameter dynamically adjusts the weight of data points during training, allowing the algorithm to focus on specific regions or characteristics of the data distribution. Tilted empirical risk minimization is particularly useful in scenarios with imbalanced data or when there is a need to emphasize errors in certain parts of the prediction space. See also M-estimator Maximum likelihood estimation References Further reading Vapnik, V. (2000). The Nature of Statistical Learning Theory. Information Science and Statistics. Springer-Verlag. ISBN 978-0-387-98780-4. Title Energy-based model URL https//en.wikipedia.org/wiki/Energy-based_model Content An energy-based model (EBM) (also called Canonical Ensemble Learning or Learning via Canonical Ensemble CEL and LCE, respectively) is an application of canonical ensemble formulation from statistical physics for learning from data. The approach prominently appears in generative artificial intelligence. EBMs provide a unified framework for many probabilistic and non-probabilistic approaches to such learning, particularly for training graphical and other structured models. An EBM learns the characteristics of a target dataset and generates a similar but larger dataset. EBMs detect the latent variables of a dataset and generate new datasets with a similar distribution. Energy-based generative neural networks is a class of generative models, which aim to learn explicit probability distributions of data in the form of energy-based models, the energy functions of which are parameterized by modern deep neural networks. Boltzmann machines are a special form of energy-based models with a specific parametrization of the energy. Description For a given input x displaystyle x , the model describes an energy E ( x ) displaystyle E_theta (x) such that the Boltzmann distribution P ( x )  exp ( E ( x ) ) / Z ( ) displaystyle P_theta (x)exp(-beta E_theta (x))/Z(theta ) is a probability (density), and  . Since the normalization constant Z ( )  x X exp ( E ( x ) ) d x displaystyle Z(theta )int _xin Xexp(-beta E_theta (x))dx (also known as the partition function) depends on all the Boltzmann factors of all possible inputs x displaystyle x , it cannot be easily computed or reliably estimated during training simply using standard maximum likelihood estimation. However, for maximizing the likelihood during training, the gradient of the log-likelihood of a single training example x displaystyle x is given by using the chain rule log ( P ( x ) )  E x P  E ( x )  E ( x ) ( ) displaystyle partial _theta log left(P_theta (x)right)mathbb E _xsim P_theta partial _theta E_theta (x)-partial _theta E_theta (x),() The expectation in the above formula for the gradient can be approximately estimated by drawing samples x displaystyle x from the distribution P displaystyle P_theta  using Markov chain Monte Carlo (MCMC). Early energy-based models, such as the 2003 Boltzmann machine by Hinton, estimated this expectation via blocked Gibbs sampling. Newer approaches make use of more efficient Stochastic Gradient Langevin Dynamics (LD), drawing samples using x 0 P 0 , x i  1  x i 2 E ( x i ) x i  displaystyle x_0sim P_0,x_i1x_i-frac alpha 2frac partial E_theta (x_i)partial x_iepsilon  , where N ( 0 , ) displaystyle epsilon sim mathcal N(0,alpha ) . A replay buffer of past values x i displaystyle x_i is used with LD to initialize the optimization module. The parameters displaystyle theta  of the neural network are therefore trained in a generative manner via MCMC-based maximum likelihood estimation the learning process follows an analysis by synthesis scheme, where within each learning iteration, the algorithm samples the synthesized examples from the current model by a gradient-based MCMC method (e.g., Langevin dynamics or Hybrid Monte Carlo), and then updates the parameters displaystyle theta  based on the difference between the training examples and the synthesized ones see equation ( ) displaystyle () . This process can be interpreted as an alternating mode seeking and mode shifting process, and also has an adversarial interpretation. Essentially, the model learns a function E displaystyle E_theta  that associates low energies to correct values, and higher energies to incorrect values. After training, given a converged energy model E displaystyle E_theta  , the Metropolis Hastings algorithm can be used to draw new samples. The acceptance probability is given by P a c c ( x i x )  min ( 1 , P ( x ) P ( x i ) ) . displaystyle P_acc(x_ito x)min left(1,frac P_theta (x)P_theta (x_i)right). History The term energy-based models was first coined in a 2003 JMLR paper where the authors defined a generalisation of independent components analysis to the overcomplete setting using EBMs. Other early work on EBMs proposed models that represented energy as a composition of latent and observable variables. Characteristics EBMs demonstrate useful properties Simplicity and stability The EBM is the only object that needs to be designed and trained. Separate networks need not be trained to ensure balance. Adaptive computation time An EBM can generate sharp, diverse samples or (more quickly) coarse, less diverse samples. Given infinite time, this procedure produces true samples. Flexibility In Variational Autoencoders (VAE) and flow-based models, the generator learns a map from a continuous space to a (possibly) discontinuous space containing different data modes. EBMs can learn to assign low energies to disjoint regions (multiple modes). Adaptive generation EBM generators are implicitly defined by the probability distribution, and automatically adapt as the distribution changes (without training), allowing EBMs to address domains where generator training is impractical, as well as minimizing mode collapse and avoiding spurious modes from out-of-distribution samples. Compositionality Individual models are unnormalized probability distributions, allowing models to be combined through product of experts or other hierarchical techniques. Experimental results On image datasets such as CIFAR-10 and ImageNet 32x32, an EBM model generated high-quality images relatively quickly. It supported combining features learned from one type of image for generating other types of images. It was able to generalize using out-of-distribution datasets, outperforming flow-based and autoregressive models. EBM was relatively resistant to adversarial perturbations, behaving better than models explicitly trained against them with training for classification. Applications Target applications include natural language processing, robotics and computer vision. The first energy-based generative neural network is the generative ConvNet proposed in 2016 for image patterns, where the neural network is a convolutional neural network. The model has been generalized to various domains to learn distributions of videos, and 3D voxels. They are made more effective in their variants. They have proven useful for data generation (e.g., image synthesis, video synthesis, 3D shape synthesis, etc.), data recovery (e.g., recovering videos with missing pixels or image frames, 3D super-resolution, etc), data reconstruction (e.g., image reconstruction and linear interpolation ). Alternatives EBMs compete with techniques such as variational autoencoders (VAEs), generative adversarial networks (GANs) or normalizing flows. Extensions Joint energy-based models Joint energy-based models (JEM), proposed in 2020 by Grathwohl et al., allow any classifier with softmax output to be interpreted as energy-based model. The key observation is that such a classifier is trained to predict the conditional probability p ( y  x )  e f ( x )  y  . Without any change to the logits it was proposed to reinterpret the logits to describe a joint probability density p ( y , x )  e f ( x )  y  Z ( ) , displaystyle p_theta (y,x)frac evec f_theta (x)yZ(theta ), with unknown partition function Z ( ) displaystyle Z(theta ) and energy E ( x , y )  f ( x )  y  displaystyle E_theta (x,y)-f_theta (x)y . By marginalization, we obtain the unnormalized density p ( x )  y p ( y , x )  y e f ( x )  y  Z ( )  exp ( E ( x ) ) , displaystyle p_theta (x)sum _yp_theta (y,x)sum _yfrac evec f_theta (x)yZ(theta )exp(-E_theta (x)), therefore, E ( x )  log ( y e f ( x )  y  Z ( ) ) , displaystyle E_theta (x)-log left(sum _yfrac evec f_theta (x)yZ(theta )right), so that any classifier can be used to define an energy function E ( x ) displaystyle E_theta (x) . See also Empirical likelihood Posterior predictive distribution Contrastive learning Literature Implicit Generation and Generalization in Energy-Based Models Yilun Du, Igor Mordatch https//arxiv.org/abs/1903.08689 Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One, Will Grathwohl, Kuan-Chieh Wang, J rn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky https//arxiv.org/abs/1912.03263 References External links CIAR NCAP Summer School. www.cs.toronto.edu. Retrieved 2019-12-27. Dayan, Peter Hinton, Geoffrey Neal, Radford Zemel, Richard S. (1999), Helmholtz Machine, Unsupervised Learning, The MIT Press, doi10.7551/mitpress/7011.003.0017, ISBN 978-0-262-28803-3 Hinton, Geoffrey E. (August 2002). Training Products of Experts by Minimizing Contrastive Divergence. Neural Computation. 14 (8) 1771 1800. doi10.1162/089976602760128018. ISSN 0899-7667. PMID 12180402. S2CID 207596505. Salakhutdinov, Ruslan Hinton, Geoffrey (2009-04-15). Deep Boltzmann Machines. Artificial Intelligence and Statistics 448 455. Title Equalized odds URL https//en.wikipedia.org/wiki/Equalized_odds Content Equalized odds, also referred to as conditional procedure accuracy equality and disparate mistreatment, is a measure of fairness in machine learning. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal true positive rate and equal false positive rate, satisfying the formula P ( . In this context, higher university enrollment rates of African Americans compared to whites with similar test scores might be necessary to fulfill the condition of equalized odds, if the base rate of Y displaystyle Y differs between the groups. The concept was originally defined for binary-valued Y displaystyle Y . In 2017, Woodworth et al. generalized the concept further for multiple classes. See also Fairness (machine learning) Color blindness (racial classification) Title Evaluation of binary classifiers URL https//en.wikipedia.org/wiki/Evaluation_of_binary_classifiers Content Evaluation of a binary classifier typically assigns a numerical value, or values, to a classifier that represent its accuracy. An example is error rate, which measures how frequently the classifier makes a mistake. There are many metrics that can be used different fields have different preferences. For example, in medicine sensitivity and specificity are often used, while in computer science precision and recall are preferred. An important distinction is between metrics that are independent of the prevalence or skew (how often each class occurs in the population), and metrics that depend on the prevalence both types are useful, but they have very different properties. Often, evaluation is used to compare two methods of classification, so that one can be adopted and the other discarded. Such comparisons are more directly achieved by a form of evaluation that results in a single unitary metric rather than a pair of metrics. Contingency table Given a data set, a classification (the output of a classifier on that set) gives two numbers the number of positives and the number of negatives, which add up to the total size of the set. To evaluate a classifier, one compares its output to another reference classification ideally a perfect classification, but in practice the output of another gold standard test and cross tabulates the data into a 2 2 contingency table, comparing the two classifications. One then evaluates the classifier relative to the gold standard by computing summary statistics of these 4 numbers. Generally these statistics will be scale invariant (scaling all the numbers by the same factor does not change the output), to make them independent of population size, which is achieved by using ratios of homogeneous functions, most simply homogeneous linear or homogeneous quadratic functions. Say we test some people for the presence of a disease. Some of these people have the disease, and our test correctly says they are positive. They are called true positives (TP). Some have the disease, but the test incorrectly claims they dont. They are called false negatives (FN). Some dont have the disease, and the test says they dont true negatives (TN). Finally, there might be healthy people who have a positive test result false positives (FP). These can be arranged into a 2 2 contingency table (confusion matrix), conventionally with the test result on the vertical axis and the actual condition on the horizontal axis. These numbers can then be totaled, yielding both a grand total and marginal totals. Totaling the entire table, the number of true positives, false negatives, true negatives, and false positives add up to 100 of the set. Totaling the columns (adding vertically) the number of true positives and false positives add up to 100 of the test positives, and likewise for negatives. Totaling the rows (adding horizontally), the number of true positives and false negatives add up to 100 of the condition positives (conversely for negatives). The basic marginal ratio statistics are obtained by dividing the 2 24 values in the table by the marginal totals (either rows or columns), yielding 2 auxiliary 2 2 tables, for a total of 8 ratios. These ratios come in 4 complementary pairs, each pair summing to 1, and so each of these derived 2 2 tables can be summarized as a pair of 2 numbers, together with their complements. Further statistics can be obtained by taking ratios of these ratios, ratios of ratios, or more complicated functions. The contingency table and the most common derived ratios are summarized below see sequel for details. Note that the rows correspond to the condition actually being positive or negative (or classified as such by the gold standard), as indicated by the color-coding, and the associated statistics are prevalence-independent, while the columns correspond to the test being positive or negative, and the associated statistics are prevalence-dependent. There are analogous likelihood ratios for prediction values, but these are less commonly used, and not depicted above. Pairs of metrics Often accuracy is evaluated with a pair of metrics composed in a standard pattern. Sensitivity and specificity The fundamental prevalence-independent statistics are sensitivity and specificity. Sensitivity or True Positive Rate (TPR), also known as recall, is the proportion of people that tested positive and are positive (True Positive, TP) of all the people that actually are positive (Condition Positive, ). It can be seen as the probability that the test is positive given that the patient is sick. With higher sensitivity, fewer actual cases of disease go undetected (or, in the case of the factory quality control, fewer faulty products go to the market). Specificity (SPC) or True Negative Rate (TNR) is the proportion of people that tested negative and are negative (True Negative, TN) of all the people that actually are negative (Condition Negative, ). As with sensitivity, it can be looked at as the probability that the test result is negative given that the patient is not sick. With higher specificity, fewer healthy people are labeled as sick (or, in the factory case, fewer good products are discarded). The relationship between sensitivity and specificity, as well as the performance of the classifier, can be visualized and studied using the Receiver Operating Characteristic (ROC) curve. In theory, sensitivity and specificity are independent in the sense that it is possible to achieve 100 in both (such as in the red/blue ball example given above). In more practical, less contrived instances, however, there is usually a trade-off, such that they are inversely proportional to one another to some extent. This is because we rarely measure the actual thing we would like to classify rather, we generally measure an indicator of the thing we would like to classify, referred to as a surrogate marker. The reason why 100 is achievable in the ball example is because redness and blueness is determined by directly detecting redness and blueness. However, indicators are sometimes compromised, such as when non-indicators mimic indicators or when indicators are time-dependent, only becoming evident after a certain lag time. The following example of a pregnancy test will make use of such an indicator. Modern pregnancy tests do not use the pregnancy itself to determine pregnancy status rather, human chorionic gonadotropin is used, or hCG, present in the urine of gravid females, as a surrogate marker to indicate that a woman is pregnant. Because hCG can also be produced by a tumor, the specificity of modern pregnancy tests cannot be 100 (because false positives are possible). Also, because hCG is present in the urine in such small concentrations after fertilization and early embryogenesis, the sensitivity of modern pregnancy tests cannot be 100 (because false negatives are possible). Positive and negative predictive values In addition to sensitivity and specificity, the performance of a binary classification test can be measured with positive predictive value (PPV), also known as precision, and negative predictive value (NPV). The positive prediction value answers the question If the test result is positive, how well does that predict an actual presence of disease?. It is calculated as TP/(TP  FP) that is, it is the proportion of true positives out of all positive results. The negative prediction value is the same, but for negatives, naturally. Impact of prevalence on predictive values Prevalence has a significant impact on prediction values. As an example, suppose there is a test for a disease with 99 sensitivity and 99 specificity. If 2000 people are tested and the prevalence (in the sample) is 50, 1000 of them are sick and 1000 of them are healthy. Thus about 990 true positives and 990 true negatives are likely, with 10 false positives and 10 false negatives. The positive and negative prediction values would be 99, so there can be high confidence in the result. However, if the prevalence is only 5, so of the 2000 people only 100 are really sick, then the prediction values change significantly. The likely result is 99 true positives, 1 false negative, 1881 true negatives and 19 false positives. Of the 1999 people tested positive, only 99 really have the disease that means, intuitively, that given that a patients test result is positive, there is only 84 chance that they really have the disease. On the other hand, given that the patients test result is negative, there is only 1 chance in 1882, or 0.05 probability, that the patient has the disease despite the test result. Precision and recall Precision and recall can be interpreted as (estimated) conditional probabilities Precision is given by P ( . Both quantities are therefore connected by Bayes theorem. Relationships There are various relationships between these ratios. If the prevalence, sensitivity, and specificity are known, the positive predictive value can be obtained from the following identity  ) . displaystyle textNPVfrac (textspecificity)(1-textprevalence)(textspecificity)(1-textprevalence)(1-textsensitivity)(textprevalence). Unitary metrics In addition to the paired metrics, there are also unitary metrics that give a single number to evaluate the test. Perhaps the simplest statistic is accuracy or fraction correct (FC), which measures the fraction of all instances that are correctly categorized it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications (TP  TN)/total ). As such, it compares estimates of pre- and post-test probability. In total ignorance, one can compare a rule to flipping a coin (0.5). This measure is prevalence-dependent. If 90 of people with COVID symptoms dont have COVID, the prior probability P(-) is 0.9, and the simple rule Classify all such patients as COVID-free. would be 90 accurate. Diagnosis should be better than that. One can construct a One-proportion z-test with as max(priors)  max(P(-),P()) for a diagnostic method hoping to beat a simple rule using the most likely outcome. Here, the hypotheses are Ho p 0.9 vs. Ha p  0.9, rejecting Ho for large values of z. One diagnostic rule could be compared to another if the others accuracy is known and substituted for in calculating the z statistic. If not known and calculated from data, an accuracy comparison test could be made using Two-proportion z-test, pooled for Ho  . Not used very much is the complementary statistic, the fraction incorrect (FiC) FC  . Cost-weighted fractions incorrect could compare expected costs of misclassification for different methods. The diagnostic odds ratio (DOR) can be a more useful overall metric, which can be defined directly as (TP TN)/(FP FN)  (TP/FN)/(FP/TN), or indirectly as a ratio of ratio of ratios (ratio of likelihood ratios, which are themselves ratios of true rates or prediction values). This has a useful interpretation as an odds ratio and is prevalence-independent. Likelihood ratio is generally considered to be prevalence-independent and is easily interpreted as the multiplier to turn prior probabilities into posterior probabilities. An F-score is a combination of the precision and the recall, providing a single score. There is a one-parameter family of statistics, with parameter , which determines the relative weights of precision and recall. The traditional or balanced F-score ( score) is the harmonic mean of precision and recall F 1  2 p r e c i s i o n r e c a l l p r e c i s i o n  r e c a l l displaystyle F_12cdot frac mathrm precision cdot mathrm recall mathrm precision mathrm recall  . F-scores do not take the true negative rate into account and, therefore, are more suited to information retrieval and information extraction evaluation where the true negatives are innumerable. Instead, measures such as the phi coefficient, Matthews correlation coefficient, informedness or Cohens kappa may be preferable to assess the performance of a binary classifier. As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are markedness (deltap) and informedness (Youdens J statistic or deltap). Choosing the appropriate form of evaluation Hand has highlighted the importance of choosing an appropriate method of evaluation. However, of the many different methods for evaluating the accuracy of a classifier, there is no general method for determining which method should be used in which circumstances. Different fields have taken different approaches. Cullerne Bown has distinguished three basic approaches to evaluation Mathematical - such as the Matthews Correlation Coefficient, in which both kinds of error are axiomatically treated as equally problematic Cost-benefit - in which a currency is adopted (e.g. money or Quality Adjusted Life Years) and values assigned to errors and successes on the basis of empirical measurement Judgemental - in which a human judgement is made about the relative importance of the two kinds of error typically this starts by adopting a pair of indicators such as sensitivity and specificity, precision and recall or positive predictive value and negative predictive value. In the judgemental case, he has provided a flow chart for determining which pair of indicators should be used when, and consequently how to choose between the Receiver Operating Characteristic and the Precision-Recall Curve. Evaluation of underlying technologies Often, we want to evaluate not a specific classifier working in a specific way but an underlying technology. Typically, the technology can be adjusted through altering the threshold of a score function, the threshold determining whether the result is a positive or negative. For such evaluations a useful single measure is area under the ROC curve, AUC. Accuracy aside Apart from accuracy, binary classifiers can be assessed in many other ways, for example in terms of their speed or cost. Evaluation of probabilistic classifiers Probabilistic classification models go beyond providing binary outputs and instead produce probability scores for each class. These models are designed to assess the likelihood or probability of an instance belonging to different classes. In the context of evaluating probabilistic classifiers, alternative evaluation metrics have been developed to properly assess the performance of these models. These metrics take into account the probabilistic nature of the classifiers output and provide a more comprehensive assessment of its effectiveness in assigning accurate probabilities to different classes. These evaluation metrics aim to capture the degree of calibration, discrimination, and overall accuracy of the probabilistic classifiers predictions. In information systems Information retrieval systems, such as databases and web search engines, are evaluated by many different metrics, some of which are derived from the confusion matrix, which divides results into true positives (documents correctly retrieved), true negatives (documents correctly not retrieved), false positives (documents incorrectly retrieved), and false negatives (documents incorrectly not retrieved). Commonly used metrics include the notions of precision and recall. In this context, precision is defined as the fraction of documents correctly retrieved compared to the documents retrieved (true positives divided by true positives plus false positives), using a set of ground truth relevant results selected by humans. Recall is defined as the fraction of documents correctly retrieved compared to the relevant documents (true positives divided by true positives plus false negatives). Less commonly, the metric of accuracy is used, is defined as the fraction of documents correctly classified compared to the documents (true positives plus true negatives divided by true positives plus true negatives plus false positives plus false negatives). None of these metrics take into account the ranking of results. Ranking is very important for web search engines because readers seldom go past the first page of results, and there are too many documents on the web to manually classify all of them as to whether they should be included or excluded from a given search. Adding a cutoff at a particular number of results takes ranking into account to some degree. The measure precision at k, for example, is a measure of precision looking only at the top ten (. More sophisticated metrics, such as discounted cumulative gain, take into account each individual ranking, and are more commonly used where this is important. See also Population impact measures Attributable risk Attributable risk percent Scoring rule (for probability predictions) Pseudo-R-squared Likelihood ratios References External links Damage Caused by Classification Accuracy and Other Discontinuous Improper Accuracy Scoring Rules Title Evolvability (computer science) URL https//en.wikipedia.org/wiki/Evolvability_(computer_science) Content The term evolvability is used for a recent framework of computational learning introduced by Leslie Valiant in his paper of the same name and described below. The aim of this theory is to model biological evolution and categorize which types of mechanisms are evolvable. Evolution is an extension of PAC learning and learning from statistical queries. General framework Let F n displaystyle F_n, and R n displaystyle R_n, be collections of functions on n displaystyle n, variables. Given an ideal function f F n displaystyle fin F_n , the goal is to find by local search a representation r R n displaystyle rin R_n that closely approximates f displaystyle f, . This closeness is measured by the performance Perf ( f , r ) displaystyle operatorname Perf (f,r) of r displaystyle r, with respect to f displaystyle f, . As is the case in the biological world, there is a difference between genotype and phenotype. In general, there can be multiple representations (genotypes) that correspond to the same function (phenotype). That is, for some r , r R n displaystyle r,rin R_n , with r r displaystyle rneq r, , still r ( x )  r ( x ) displaystyle r(x)r(x), for all x X n displaystyle xin X_n . However, this need not be the case. The goal then, is to find a representation that closely matches the phenotype of the ideal function, and the spirit of the local search is to allow only small changes in the genotype. Let the neighborhood N ( r ) displaystyle N(r), of a representation r displaystyle r, be the set of possible mutations of r displaystyle r, . For simplicity, consider Boolean functions on X , . Define the performance in terms of this. Specifically, Perf ( f , r )  x X n f ( x ) r ( x ) D n ( x ) . displaystyle operatorname Perf (f,r)sum _xin X_nD_n(x). Note that Perf ( f , r )  Prob ( f ( x )  r ( x ) ) Prob ( f ( x ) r ( x ) ) . displaystyle operatorname Perf (f,r)operatorname Prob (f(x)r(x))-operatorname Prob (f(x)neq r(x)). In general, for non-Boolean functions, the performance will not correspond directly to the probability that the functions agree, although it will have some relationship. Throughout an organisms life, it will only experience a limited number of environments, so its performance cannot be determined exactly. The empirical performance is defined by Perf s ( f , r )  1 s x S f ( x ) r ( x ) , displaystyle operatorname Perf _s(f,r)frac 1ssum _xin Sr(x), where S displaystyle S, is a multiset of s displaystyle s, independent selections from X n displaystyle X_n, according to D n displaystyle D_n, . If s displaystyle s, is large enough, evidently Perf s ( f , r ) displaystyle operatorname Perf _s(f,r) will be close to the actual performance Perf ( f , r ) displaystyle operatorname Perf (f,r) . Given an ideal function f F n displaystyle fin F_n , initial representation r R n displaystyle rin R_n , sample size s displaystyle s, , and tolerance t displaystyle t, , the mutator Mut ( f , r , s , t ) displaystyle operatorname Mut (f,r,s,t) is a random variable defined as follows. Each r N ( r ) displaystyle rin N(r) is classified as beneficial, neutral, or deleterious, depending on its empirical performance. Specifically, r displaystyle r, is a beneficial mutation if Perf s ( f , r ) Perf s ( f , r ) t displaystyle operatorname Perf _s(f,r)-operatorname Perf _s(f,r)geq t  r displaystyle r, is a neutral mutation if t  Perf s ( f , r ) Perf s ( f , r )  t displaystyle -toperatorname Perf _s(f,r)-operatorname Perf _s(f,r)t  r displaystyle r, is a deleterious mutation if Perf s ( f , r ) Perf s ( f , r ) t displaystyle operatorname Perf _s(f,r)-operatorname Perf _s(f,r)leq -t . If there are any beneficial mutations, then Mut ( f , r , s , t ) displaystyle operatorname Mut (f,r,s,t) is equal to one of these at random. If there are no beneficial mutations, then Mut ( f , r , s , t ) displaystyle operatorname Mut (f,r,s,t) is equal to a random neutral mutation. In light of the similarity to biology, r displaystyle r, itself is required to be available as a mutation, so there will always be at least one neutral mutation. The intention of this definition is that at each stage of evolution, all possible mutations of the current genome are tested in the environment. Out of the ones who thrive, or at least survive, one is chosen to be the candidate for the next stage. Given r 0 R n displaystyle r_0in R_n , we define the sequence r 0 , r 1 , r 2 , displaystyle r_0,r_1,r_2,ldots  by r i  1  Mut ( f , r i , s , t ) displaystyle r_i1operatorname Mut (f,r_i,s,t) . Thus r g displaystyle r_g, is a random variable representing what r 0 displaystyle r_0, has evolved to after g displaystyle g, generations. Let F displaystyle F, be a class of functions, R displaystyle R, be a class of representations, and D displaystyle D, a class of distributions on X displaystyle X, . We say that F displaystyle F, is evolvable by R displaystyle R, over D displaystyle D, if there exists polynomials p ( , ) displaystyle p(cdot ,cdot ) , s ( , ) displaystyle s(cdot ,cdot ) , t ( , ) displaystyle t(cdot ,cdot ) , and g ( , ) displaystyle g(cdot ,cdot ) such that for all n displaystyle n, and all  0 displaystyle epsilon 0, , for all ideal functions f F n displaystyle fin F_n and representations r 0 R n displaystyle r_0in R_n , with probability at least 1 displaystyle 1-epsilon , , Perf ( f , r g ( n , 1 / ) ) 1 , displaystyle operatorname Perf (f,r_g(n,1/epsilon ))geq 1-epsilon , where the sizes of neighborhoods N ( r ) displaystyle N(r), for r R n displaystyle rin R_n, are at most p ( n , 1 / ) displaystyle p(n,1/epsilon ), , the sample size is s ( n , 1 / ) displaystyle s(n,1/epsilon ), , the tolerance is t ( 1 / n , ) displaystyle t(1/n,epsilon ), , and the generation size is g ( n , 1 / ) displaystyle g(n,1/epsilon ), . F displaystyle F, is evolvable over D displaystyle D, if it is evolvable by some R displaystyle R, over D displaystyle D, . F displaystyle F, is evolvable if it is evolvable over all distributions D displaystyle D, . Results The class of conjunctions and the class of disjunctions are evolvable over the uniform distribution for short conjunctions and disjunctions, respectively. The class of parity functions (which evaluate to the parity of the number of true literals in a given subset of literals) are not evolvable, even for the uniform distribution. Evolvability implies PAC learnability. References Valiant, L. G. (2006), Evolvability, ECCC -120. Title Expectation propagation URL https//en.wikipedia.org/wiki/Expectation_propagation Content Expectation propagation (EP) is a technique in Bayesian machine learning. EP finds approximations to a probability distribution. It uses an iterative approach that uses the factorization structure of the target distribution. It differs from other Bayesian approximation approaches such as variational Bayesian methods. More specifically, suppose we wish to approximate an intractable probability distribution p ( x ) displaystyle p(mathbf x ) with a tractable distribution q ( x ) displaystyle q(mathbf x ) . Expectation propagation achieves this approximation by minimizing the Kullback-Leibler divergence K L ( p   q ) displaystyle mathrm KL (pq) . Variational Bayesian methods minimize K L ( q   p ) displaystyle mathrm KL (qp) instead. If q ( x ) displaystyle q(mathbf x ) is a Gaussian N ( x  , ) displaystyle mathcal N(mathbf x mu ,Sigma ) , then K L ( p   q ) displaystyle mathrm KL (pq) is minimized with displaystyle mu  and displaystyle Sigma  being equal to the mean of p ( x ) displaystyle p(mathbf x ) and the covariance of p ( x ) displaystyle p(mathbf x ) , respectively this is called moment matching. Applications Expectation propagation via moment matching plays a vital role in approximation for indicator functions that appear when deriving the message passing equations for TrueSkill. References Thomas Minka (August 2 5, 2001). Expectation Propagation for Approximate Bayesian Inference. In Jack S. Breese, Daphne Koller (ed.). UAI 01 Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence (PDF). University of Washington, Seattle, Washington, USA. pp. 362 369.cite book maint location missing publisher (link) External links Minkas EP papers List of papers using EP. Title Explanation-based learning URL https//en.wikipedia.org/wiki/Explanation-based_learning Content Explanation-based learning (EBL) is a form of machine learning that exploits a very strong, or even perfect, domain theory (i.e. a formal theory of an application domain akin to a domain model in ontology engineering, not to be confused with Scotts domain theory) in order to make generalizations or form concepts from training examples. It is also linked with Encoding (memory) to help with Learning. Details An example of EBL using a perfect domain theory is a program that learns to play chess through example. A specific chess position that contains an important feature such as Forced loss of black queen in two moves includes many irrelevant features, such as the specific scattering of pawns on the board. EBL can take a single training example and determine what are the relevant features in order to form a generalization. A domain theory is perfect or complete if it contains, in principle, all information needed to decide any question about the domain. For example, the domain theory for chess is simply the rules of chess. Knowing the rules, in principle, it is possible to deduce the best move in any situation. However, actually making such a deduction is impossible in practice due to combinatoric explosion. EBL uses training examples to make searching for deductive consequences of a domain theory efficient in practice. In essence, an EBL system works by finding a way to deduce each training example from the systems existing database of domain theory. Having a short proof of the training example extends the domain-theory database, enabling the EBL system to find and classify future examples that are similar to the training example very quickly. The main drawback of the method the cost of applying the learned proof macros, as these become numerous was analyzed by Minton. Basic formulation EBL software takes four inputs a hypothesis space (the set of all possible conclusions) a domain theory (axioms about a domain of interest) training examples (specific facts that rule out some possible hypothesis) operationality criteria (criteria for determining which features in the domain are efficiently recognizable, e.g. which features are directly detectable using sensors) Application An especially good application domain for an EBL is natural language processing (NLP). Here a rich domain theory, i.e., a natural language grammar although neither perfect nor complete, is tuned to a particular application or particular language usage, using a treebank (training examples). Rayner pioneered this work. The first successful industrial application was to a commercial NL interface to relational databases. The method has been successfully applied to several large-scale natural language parsing systems, where the utility problem was solved by omitting the original grammar (domain theory) and using specialized LR-parsing techniques, resulting in huge speed-ups, at a cost in coverage, but with a gain in disambiguation. EBL-like techniques have also been applied to surface generation, the converse of parsing. When applying EBL to NLP, the operationality criteria can be hand-crafted, or can be inferred from the treebank using either the entropy of its or-nodes or a target coverage/disambiguation trade-off ( recall/precision trade-). EBL can also be used to compile grammar-based language models for speech recognition, from general unification grammars. Note how the utility problem, first exposed by Minton, was solved by discarding the original grammar/domain theory, and that the quoted articles tend to contain the phrase grammar specialization quite the opposite of the original term explanation-based generalization. Perhaps the best name for this technique would be data-driven search space reduction. Other people who worked on EBL for NLP include Guenther Neumann, Aravind Joshi, Srinivas Bangalore, and Khalil Simaan. See also One-shot learning in computer vision Zero-shot learning Title Explorationexploitation dilemma URL https//en.wikipedia.org/wiki/ExplorationE28093exploitation_dilemma Content The exploration exploitation dilemma, also known as the explore exploit tradeoff, is a fundamental concept in decision-making that arises in many domains. It is depicted as the balancing act between two opposing strategies. Exploitation involves choosing the best option based on current knowledge of the system (which may be incomplete or misleading), while exploration involves trying out new options that may lead to better outcomes in the future at the expense of an exploitation opportunity. Finding the optimal balance between these two strategies is a crucial challenge in many decision-making problems whose goal is to maximize long-term benefits. Application in machine learning In the context of machine learning, the exploration exploitation tradeoff is fundamental in reinforcement learning (RL), a type of machine learning that involves training agents to make decisions based on feedback from the environment. Crucially, this feedback may be incomplete or delayed. The agent must decide whether to exploit the current best-known policy or explore new policies to improve its performance. Multi-armed bandit methods The multi-armed bandit (MAB) problem was a classic example of the tradeoff, and many methods were developed for it, such as epsilon-greedy, Thompson sampling, and the upper confidence bound (UCB). See the page on MAB for details. In more complex RL situations than the MAB problem, the agent can treat each choice as a MAB, where the payoff is the expected future reward. For example, if the agent performs an epsilon-greedy method, then the agent will often pull the best lever by picking the action that had the best predicted expected reward (exploit). However, it would pick a random action with probability epsilon (explore). Monte Carlo tree search, for example, uses a variant of the UCB method. Exploration problems There are some problems that make exploration difficult. Sparse reward. If rewards occur only once a long while, then the agent might not persist in exploring. Furthermore, if the space of actions is large, then the sparse reward would mean the agent would not be guided by the reward to find a good direction for deeper exploration. A standard example is Montezumas Revenge. Deceptive reward. If some early actions give immediate small reward, but other actions give later large reward, then the agent might be lured away from exploring the other actions. Noisy TV problem. If certain observations are irreducibly noisy (such as a television showing random images), then the agent might be trapped exploring those observations (watching the television). Exploration reward This section based on. The exploration reward (also called exploration bonus) methods convert the exploration-exploitation dilemma into a balance of exploitations. That is, instead of trying to get the agent to balance exploration and exploitation, exploration is simply treated as another form of exploitation, and the agent simply attempts to maximize the sum of rewards from exploration and exploitation. The exploration reward can be treated as a form of intrinsic reward. We write these as r t i , r t e displaystyle r_ti,r_te , meaning the intrinsic and extrinsic rewards at time step t displaystyle t . However, exploration reward is different from exploitation in two regards The reward of exploitation is not freely chosen, but given by the environment, but the reward of exploration may be picked freely. Indeed, there are many different ways to design r t i displaystyle r_ti described below. The reward of exploitation is usually stationary (i.e. the same action in the same state gives the same reward), but the reward of exploration is non-stationary (i.e. the same action in the same state should give less and less reward). Count-based exploration uses N n ( s ) displaystyle N_n(s) , the number of visits to a state s displaystyle s during the time-steps 1  n displaystyle 1n , to calculate the exploration reward. This is only possible in small and discrete state space. Density-based exploration extends count-based exploration by using a density model n ( s ) displaystyle rho _n(s) . The idea is that, if a state has been visited, then nearby states are also partly-visited. In maximum entropy exploration, the entropy of the agents policy displaystyle pi  is included as a term in the intrinsic reward. That is, r t   . Prediction-based This section based on. The forward dynamics model is a function for predicting the next state based on the current state and the current action f  ( s t , a t ) s t  1 displaystyle f(s_t,a_t)mapsto s_t1 . The forward dynamics model is trained as the agent plays. The model becomes better at predicting state transition for state-action pairs that had been done many times. A forward dynamics model can define an exploration reward by r t  . That is, the reward is the squared-error of the prediction compared to reality. This rewards the agent to perform state-action pairs that had not been done many times. This is however susceptible to the noisy TV problem. Dynamics model can be run in latent space. That is, r t   . The featurizer can be the identity function (i.e. ( x )  x displaystyle phi (x)x ), randomly generated, the encoder-half of a variational autoencoder, etc. A good featurizer improves forward dynamics exploration. The Intrinsic Curiosity Module (ICM) method trains simultaneously a forward dynamics model and a featurizer. The featurizer is trained by an inverse dynamics model, which is a function for predicting the current action based on the features of the current and the next state g  ( ( s t ) , ( s t  1 ) ) a t displaystyle g(phi (s_t),phi (s_t1))mapsto a_t . By optimizing the inverse dynamics, both the inverse dynamics model and the featurizer are improved. Then, the improved featurizer improves the forward dynamics model, which improves the exploration of the agent. Random Network Distillation (RND) method attempts to solve this problem by teacher student distillation. Instead of a forward dynamics model, it has two models f , f displaystyle f,f . The f displaystyle f teacher model is fixed, and the f displaystyle f student model is trained to minimize f ( s ) f ( s ) 2 2 displaystyle f(s)-f(s)_22 on states s displaystyle s . As a state is visited more and more, the student network becomes better at predicting the teacher. Meanwhile, the prediction error is also an exploration reward for the agent, and so the agent learns to perform actions that result in higher prediction error. Thus, we have a student network attempting to minimize the prediction error, while the agent attempting to maximize it, resulting in exploration. The states are normalized by subtracting a running average and dividing a running variance, which is necessary since the teacher model is frozen. The rewards are normalized by dividing with a running variance. Exploration by disagreement trains an ensemble of forward dynamics models, each on a random subset of all ( s t , a t , s t  1 ) displaystyle (s_t,a_t,s_t1) tuples. The exploration reward is the variance of the models predictions. Noise For neural network based agents, the NoisyNet method changes some of its neural network modules by noisy versions. That is, some network parameters are random variables from a probability distribution. The parameters of the distribution are themselves learnable. For example, in a linear layer . References Amin, Susan Gomrokchi, Maziar Satija, Harsh Hoof, van Precup, Doina (September 1, 2021). A Survey of Exploration Methods in Reinforcement Learning. arXiv2109.00157 cs.LG. Title Fairness (machine learning) URL https//en.wikipedia.org/wiki/Fairness_(machine_learning) Content Fairness in machine learning (ML) refers to the various attempts to correct algorithmic bias in automated decision processes based on ML models. Decisions made by such models after a learning process may be considered unfair if they were based on variables considered sensitive (e.g., gender, ethnicity, sexual orientation, or disability). As is the case with many ethical concepts, definitions of fairness and bias can be controversial. In general, fairness and bias are considered relevant when the decision process impacts peoples lives. Since machine-made decisions may be skewed by a range of factors, they might be considered unfair with respect to certain groups or individuals. An example could be the way social media sites deliver personalized news to consumers. Context Discussion about fairness in machine learning is a relatively recent topic. Since 2016 there has been a sharp increase in research into the topic. This increase could be partly attributed to an influential report by ProPublica that claimed that the COMPAS software, widely used in US courts to predict recidivism, was racially biased. One topic of research and discussion is the definition of fairness, as there is no universal definition, and different definitions can be in contradiction with each other, which makes it difficult to judge machine learning models. Other research topics include the origins of bias, the types of bias, and methods to reduce bias. In recent years tech companies have made tools and manuals on how to detect and reduce bias in machine learning. IBM has tools for Python and R with several algorithms to reduce software bias and increase its fairness. Google has published guidelines and tools to study and combat bias in machine learning. Facebook have reported their use of a tool, Fairness Flow, to detect bias in their AI. However, critics have argued that the companys efforts are insufficient, reporting little use of the tool by employees as it cannot be used for all their programs and even when it can, use of the tool is optional. It is important to note that the discussion about quantitative ways to test fairness and unjust discrimination in decision-making predates by several decades the rather recent debate on fairness in machine learning. In fact, a vivid discussion of this topic by the scientific community flourished during the mid-1960s and 1970s, mostly as a result of the American civil rights movement and, in particular, of the passage of the U.S. Civil Rights Act of 1964. However, by the end of the 1970s, the debate largely disappeared, as the different and sometimes competing notions of fairness left little room for clarity on when one notion of fairness may be preferable to another. Language Bias Language bias refers a type of statistical sampling bias tied to the language of a query that leads to a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository. Luo et al. show that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like What is liberalism?, ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like opposes state intervention in personal and economic life from the dominant Vietnamese perspective and limitation of government power from the prevalent Chinese perspective are absent. Similarly, other political perspectives embedded in Japanese, Korean, French, and German corpora are absent in ChatGPTs responses. ChatGPT, covered itself as a multilingual chatbot, in fact is mostly blind to non-English perspectives. Gender Bias Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. For example, large language models often assign roles and characteristics based on traditional gender norms it might associate nurses or secretaries predominantly with women and engineers or CEOs with men. Political bias Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data. Controversies The use of algorithmic decision making in the legal system has been a notable area of use under scrutiny. In 2014, then U.S. Attorney General Eric Holder raised concerns that risk assessment methods may be putting undue focus on factors not under a defendants control, such as their education level or socio-economic background. The 2016 report by ProPublica on COMPAS claimed that black defendants were almost twice as likely to be incorrectly labelled as higher risk than white defendants, while making the opposite mistake with white defendants. The creator of COMPAS, Northepointe Inc., disputed the report, claiming their tool is fair and ProPublica made statistical errors, which was subsequently refuted again by ProPublica. Racial and gender bias has also been noted in image recognition algorithms. Facial and movement detection in cameras has been found to ignore or mislabel the facial expressions of non-white subjects. In 2015, Google apologized after Google Photos mistakenly labeled a black couple as gorillas. Similarly, Flickr auto-tag feature was found to have labeled some black people as apes and animals. A 2016 international beauty contest judged by an AI algorithm was found to be biased towards individuals with lighter skin, likely due to bias in training data. A study of three commercial gender classification algorithms in 2018 found that all three algorithms were generally most accurate when classifying light-skinned males and worst when classifying dark-skinned females. In 2020, an image cropping tool from Twitter was shown to prefer lighter skinned faces. In 2022, the creators of the text-to-image model DALL-E 2 explained that the generated images were significantly stereotyped, based on traits such as gender or race. Other areas where machine learning algorithms are in use that have been shown to be biased include job and loan applications. Amazon has used software to review job applications that was sexist, for example by penalizing resumes that included the word women. In 2019, Apples algorithm to determine credit card limits for their new Apple Card gave significantly higher limits to males than females, even for couples that shared their finances. Mortgage-approval algorithms in use in the U.S. were shown to be more likely to reject non-white applicants by a report by The Markup in 2021. Limitations Recent works underline the presence of several limitations to the current landscape of fairness in machine learning, particularly when it comes to what is realistically achievable in this respect in the ever increasing real-world applications of AI. For instance, the mathematical and quantitative approach to formalize fairness, and the related de-biasing approaches, may rely onto too simplistic and easily overlooked assumptions, such as the categorization of individuals into pre-defined social groups. Other delicate aspects are, e.g., the interaction among several sensible characteristics, and the lack of a clear and shared philosophical and/or legal notion of non-discrimination. Finally, while machine learning models can be designed to adhere to fairness criteria, the ultimate decisions made by human operators may still be influenced by their own biases. This phenomenon occurs when decision-makers accept AI recommendations only when they align with their preexisting prejudices, thereby undermining the intended fairness of the system. Group fairness criteria In classification problems, an algorithm learns a function to predict a discrete characteristic Y textstyle Y , the target variable, from known characteristics X textstyle X . We model A textstyle A as a discrete random variable which encodes some characteristics contained or implicitly encoded in X textstyle X that we consider as sensitive characteristics (gender, ethnicity, sexual orientation, etc.). We finally denote by R textstyle R the prediction of the classifier. Now let us define three main criteria to evaluate if a given classifier is fair, that is if its predictions are not influenced by some of these sensitive variables. Independence We say the random variables ( R , A ) textstyle (R,A) satisfy independence if the sensitive characteristics A textstyle A are statistically independent of the prediction R textstyle R , and we write R A . displaystyle Rbot A. We can also express this notion with the following formula P (  . Yet another equivalent expression for independence can be given using the concept of mutual information between random variables, defined as I ( X , Y )  H ( X )  H ( Y ) H ( X , Y ) displaystyle I(X,Y)H(X)H(Y)-H(X,Y) In this formula, H ( X ) textstyle H(X) is the entropy of the random variable X displaystyle X . Then ( R , A ) textstyle (R,A) satisfy independence if I ( R , A )  0 textstyle I(R,A)0 . A possible relaxation of the independence definition include introducing a positive slack  0 textstyle epsilon 0 and is given by the formula P (   . Separation We say the random variables ( R , A , Y ) textstyle (R,A,Y) satisfy separation if the sensitive characteristics A textstyle A are statistically independent of the prediction R textstyle R given the target value Y textstyle Y , and we write R A  Y . displaystyle Rbot A  Y. We can also express this notion with the following formula P (  . Another equivalent expression, in the case of a binary target rate, is that the true positive rate and the false positive rate are equal (and therefore the false negative rate and the true negative rate are equal) for every value of the sensitive characteristics P ( . In some fields separation (separation coefficient) in a confusion matrix is a measure of the distance (at a given level of the probability score) between the predicted cumulative percent negative and predicted cumulative percent positive. The greater this separation coefficient is at a given score value, the more effective the model is at differentiating between the set of positives and negatives at a particular probability cut-off. According to Mayes It is often observed in the credit industry that the selection of validation measures depends on the modeling approach. For example, if modeling procedure is parametric or semi-parametric, the two-sample K-S test is often used. If the model is derived by heuristic or iterative search methods, the measure of model performance is usually divergence. A third option is the coefficient of separation...The coefficient of separation, compared to the other two methods, seems to be most reasonable as a measure for model performance because it reflects the separation pattern of a model. Sufficiency We say the random variables ( R , A , Y ) textstyle (R,A,Y) satisfy sufficiency if the sensitive characteristics A textstyle A are statistically independent of the target value Y textstyle Y given the prediction R textstyle R , and we write Y A  R . displaystyle Ybot A  R. We can also express this notion with the following formula P ( . Relationships between definitions Finally, we sum up some of the main results that relate the three definitions given above Assuming Y textstyle Y is binary, if A textstyle A and Y textstyle Y are not statistically independent, and R textstyle R and Y textstyle Y are not statistically independent either, then independence and separation cannot both hold except for rhetorical cases. If ( R , A , Y ) textstyle (R,A,Y) as a joint distribution has positive probability for all its possible values and A textstyle A and Y textstyle Y are not statistically independent, then separation and sufficiency cannot both hold except for rhetorical cases. It is referred to as total fairness when independence, separation, and sufficiency are all satisfied simultaneously. However, total fairness is not possible to achieve except in specific rhetorical cases. Mathematical formulation of group fairness definitions Preliminary definitions Most statistical measures of fairness rely on different metrics, so we will start by defining them. When working with a binary classifier, both the predicted and the actual classes can take two values positive and negative. Now let us start explaining the different possible relations between predicted and actual outcome True positive (TP) The case where both the predicted and the actual outcome are in a positive class. True negative (TN) The case where both the predicted outcome and the actual outcome are assigned to the negative class. False positive (FP) A case predicted to befall into a positive class assigned in the actual outcome is to the negative one. False negative (FN) A case predicted to be in the negative class with an actual outcome is in the positive one. These relations can be easily represented with a confusion matrix, a table that describes the accuracy of a classification model. In this matrix, columns and rows represent instances of the predicted and the actual cases, respectively. By using these relations, we can define multiple metrics which can be later used to measure the fairness of an algorithm Positive predicted value (PPV) the fraction of positive cases which were correctly predicted out of all the positive predictions. It is usually referred to as precision, and represents the probability of a correct positive prediction. It is given by the following formula P P . It represents the probability of an erroneous positive prediction, and it is given by the following formula F D . It represents the probability of a correct negative prediction, and it is given by the following formula N P . It represents the probability of an erroneous negative prediction, and it is given by the following formula F O . It is usually referred to as sensitivity or recall, and it represents the probability of the positive subjects to be classified correctly as such. It is given by the formula T P . It represents the probability of the positive subjects to be classified incorrectly as negative ones, and it is given by the formula F N . It represents the probability of the negative subjects to be classified correctly as such, and it is given by the formula T N . It represents the probability of the negative subjects to be classified incorrectly as positive ones, and it is given by the formula F P . In the table to the right, we can see the relationships between them. To define these measures specifically, we will divide them into three big groups as done in Verma et al. definitions based on a predicted outcome, on predicted and actual outcomes, and definitions based on predicted probabilities and the actual outcome. We will be working with a binary classifier and the following notation S textstyle S refers to the score given by the classifier, which is the probability of a certain subject to be in the positive or the negative class. R textstyle R represents the final classification predicted by the algorithm, and its value is usually derived from S textstyle S , for example will be positive when S textstyle S is above a certain threshold. Y textstyle Y represents the actual outcome, that is, the real classification of the individual and, finally, A textstyle A denotes the sensitive attributes of the subjects. Definitions based on predicted outcome The definitions in this section focus on a predicted outcome R textstyle R for various distributions of subjects. They are the simplest and most intuitive notions of fairness. Demographic parity, also referred to as statistical parity, acceptance rate parity and benchmarking. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal probability of being assigned to the positive predicted class. This is, if the following formula is satisfied P ( . Basically consists in the definition above, but restricted only to a subset of the instances. In mathematical notation this would be P (  . Predictive parity, also referred to as outcome test. A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal PPV. This is, if the following formula is satisfied P ( . A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal FPR. This is, if the following formula is satisfied P ( . A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal FNR. This is, if the following formula is satisfied P ( . A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal TPR and equal FPR, satisfying the formula P ( . A classifier satisfies this definition if the subjects in the protected and unprotected groups have equal PPV and equal NPV, satisfying the formula P ( . A classifier satisfies this definition if the subject in the protected and unprotected groups have equal prediction accuracy, that is, the probability of a subject from one class to be assigned to it. This is, if it satisfies the following formula P ( . A classifier satisfies this definition if the subjects in the protected and unprotected groups have an equal ratio of FN and FP, satisfying the formula F N  . Test-fairness, also known as calibration or matching conditional frequencies. A classifier satisfies this definition if individuals with the same predicted probability score S textstyle S have the same probability of being classified in the positive class when they belong to either the protected or the unprotected group P ( . It states that when individuals inside or outside the protected group have the same predicted probability score S textstyle S they must have the same probability of being classified in the positive class, and this probability must be equal to S textstyle S  P ( . A classifier satisfies this definition if the subjects constituting the positive class from both protected and unprotected groups have equal average predicted probability score S textstyle S . This means that the expected value of probability score for the protected and unprotected groups with positive actual outcome Y textstyle Y is the same, satisfying the formula E ( S  . A classifier satisfies this definition if the subjects constituting the negative class from both protected and unprotected groups have equal average predicted probability score S textstyle S . This means that the expected value of probability score for the protected and unprotected groups with negative actual outcome Y textstyle Y is the same, satisfying the formula E ( S  . Independence (TP  FP) / (TP  FP  FN  TN) (i.e., P ( Y   1 ) displaystyle P(hat Y1) ). Separation TN / (TN  FP) and TP / (TP  FN) (i.e., specificity P ( Y   0 ) ). Sufficiency TP / (TP  FP) and TN / (TN  FN) (i.e., precision P ( ) ). The notion of equal confusion fairness requires the confusion matrix of a given decision system to have the same distribution when computed stratified over all sensitive characteristics. Social welfare function Some scholars have proposed defining algorithmic fairness in terms of a social welfare function. They argue that using a social welfare function enables an algorithm designer to consider fairness and predictive accuracy in terms of their benefits to the people affected by the algorithm. It also allows the designer to trade off efficiency and equity in a principled way. Sendhil Mullainathan has stated that algorithm designers should use social welfare functions to recognize absolute gains for disadvantaged groups. For example, a study found that using a decision-making algorithm in pretrial detention rather than pure human judgment reduced the detention rates for Blacks, Hispanics, and racial minorities overall, even while keeping the crime rate constant. Individual fairness criteria An important distinction among fairness definitions is the one between group and individual notions. Roughly speaking, while group fairness criteria compare quantities at a group level, typically identified by sensitive attributes (e.g. gender, ethnicity, age, etc.), individual criteria compare individuals. In words, individual fairness follow the principle that similar individuals should receive similar treatments. There is a very intuitive approach to fairness, which usually goes under the name of fairness through unawareness (FTU), or blindness, that prescribes not to explicitly employ sensitive features when making (automated) decisions. This is effectively a notion of individual fairness, since two individuals differing only for the value of their sensitive attributes would receive the same outcome. However, in general, FTU is subject to several drawbacks, the main being that it does not take into account possible correlations between sensitive attributes and non-sensitive attributes employed in the decision-making process. For example, an agent with the (malignant) intention to discriminate on the basis of gender could introduce in the model a proxy variable for gender (i.e. a variable highly correlated with gender) and effectively using gender information while at the same time being compliant to the FTU prescription. The problem of what variables correlated to sensitive ones are fairly employable by a model in the decision-making process is a crucial one, and is relevant for group concepts as well independence metrics require a complete removal of sensitive information, while separation-based metrics allow for correlation, but only as far as the labeled target variable justify them. The most general concept of individual fairness was introduced in the pioneer work by Cynthia Dwork and collaborators in 2012 and can be thought of as a mathematical translation of the principle that the decision map taking features as input should be built such that it is able to map similar individuals similarly, that is expressed as a Lipschitz condition on the model map. They call this approach fairness through awareness (FTA), precisely as counterpoint to FTU, since they underline the importance of choosing the appropriate target-related distance metric to assess which individuals are similar in specific situations. Again, this problem is very related to the point raised above about what variables can be seen as legitimate in particular contexts. Causality-based metrics Causal fairness measures the frequency with which two nearly identical users or applications who differ only in a set of characteristics with respect to which resource allocation must be fair receive identical treatment. An entire branch of the academic research on fairness metrics is devoted to leverage causal models to assess bias in machine learning models. This approach is usually justified by the fact that the same observational distribution of data may hide different causal relationships among the variables at play, possibly with different interpretations of whether the outcome are affected by some form of bias or not. Kusner et al. propose to employ counterfactuals, and define a decision-making process counterfactually fair if, for any individual, the outcome does not change in the counterfactual scenario where the sensitive attributes are changed. The mathematical formulation reads P ( R A . The symbol R  A a displaystyle hat R_Aleftarrow a represents the counterfactual random variable R displaystyle R in the scenario where the sensitive attribute A displaystyle A is fixed to  . The conditioning on . Machine learning models are often trained upon data where the outcome depended on the decision made at that time. For example, if a machine learning model has to determine whether an inmate will recidivate and will determine whether the inmate should be released early, the outcome could be dependent on whether the inmate was released early or not. Mishler et al. propose a formula for counterfactual equalized odds P ( . Plecko and Bareinboim propose a unified framework to deal with causal analysis of fairness. They suggest the use of a Standard Fairness Model, consisting of a causal graph with 4 types of variables sensitive attributes ( A displaystyle A ), target variable ( Y displaystyle Y ), mediators ( W displaystyle W ) between A displaystyle A and Y displaystyle Y , representing possible indirect effects of sensitive attributes on the outcome, variables possibly sharing a common cause with A displaystyle A ( Z displaystyle Z ), representing possible spurious (i.e., non causal) effects of the sensitive attributes on the outcome. Within this framework, Plecko and Bareinboim are therefore able to classify the possible effects that sensitive attributes may have on the outcome. Moreover, the granularity at which these effects are measured namely, the conditioning variables used to average the effect is directly connected to the individual vs. group aspect of fairness assessment. Bias mitigation strategies Fairness can be applied to machine learning algorithms in three different ways data preprocessing, optimization during software training, or post-processing results of the algorithm. Preprocessing Usually, the classifier is not the only problem the dataset is also biased. The discrimination of a dataset D textstyle D with respect to the group  . Algorithms correcting bias at preprocessing remove information about dataset variables which might result in unfair decisions, while trying to alter as little as possible. This is not as simple as just removing the sensitive variable, because other attributes can be correlated to the protected one. A way to do this is to map each individual in the initial dataset to an intermediate representation in which it is impossible to identify whether it belongs to a particular protected group while maintaining as much information as possible. Then, the new representation of the data is adjusted to get the maximum accuracy in the algorithm. This way, individuals are mapped into a new multivariable representation where the probability of any member of a protected group to be mapped to a certain value in the new representation is the same as the probability of an individual which doesnt belong to the protected group. Then, this representation is used to obtain the prediction for the individual, instead of the initial data. As the intermediate representation is constructed giving the same probability to individuals inside or outside the protected group, this attribute is hidden to the classifier. An example is explained in Zemel et al. where a multinomial random variable is used as an intermediate representation. In the process, the system is encouraged to preserve all information except that which can lead to biased decisions, and to obtain a prediction as accurate as possible. On the one hand, this procedure has the advantage that the preprocessed data can be used for any machine learning task. Furthermore, the classifier does not need to be modified, as the correction is applied to the dataset before processing. On the other hand, the other methods obtain better results in accuracy and fairness. Reweighing Reweighing is an example of a preprocessing algorithm. The idea is to assign a weight to each dataset point such that the weighted discrimination is 0 with respect to the designated group. If the dataset D textstyle D was unbiased the sensitive variable A textstyle A and the target variable Y textstyle Y would be statistically independent and the probability of the joint distribution would be the product of the probabilities as follows P e x p ( . For each X D textstyle Xin D we get W ( X )  P e x p ( . Inprocessing Another approach is to correct the bias at training time. This can be done by adding constraints to the optimization objective of the algorithm. These constraints force the algorithm to improve fairness, by keeping the same rates of certain measures for the protected group and the rest of individuals. For example, we can add to the objective of the algorithm the condition that the false positive rate is the same for individuals in the protected group and the ones outside the protected group. The main measures used in this approach are false positive rate, false negative rate, and overall misclassification rate. It is possible to add just one or several of these constraints to the objective of the algorithm. Note that the equality of false negative rates implies the equality of true positive rates so this implies the equality of opportunity. After adding the restrictions to the problem it may turn intractable, so a relaxation on them may be needed. Adversarial debiasing We train two classifiers at the same time through some gradient-based method (f.e. gradient descent). The first one, the predictor tries to accomplish the task of predicting Y textstyle Y , the target variable, given X textstyle X , the input, by modifying its weights W textstyle W to minimize some loss function L P ( y  , y ) textstyle L_P(hat y,y) . The second one, the adversary tries to accomplish the task of predicting A textstyle A , the sensitive variable, given Y  textstyle hat Y by modifying its weights U textstyle U to minimize some loss function L A ( a  , a ) textstyle L_A(hat a,a) . An important point here is that, to propagate correctly, Y  textstyle hat Y above must refer to the raw output of the classifier, not the discrete prediction for example, with an artificial neural network and a classification problem, Y  textstyle hat Y could refer to the output of the softmax layer. Then we update U textstyle U to minimize L A textstyle L_A at each training step according to the gradient U L A textstyle nabla _UL_A and we modify W textstyle W according to the expression W L P p r o j W L A W L P W L A displaystyle nabla _WL_P-proj_nabla _WL_Anabla _WL_P-alpha nabla _WL_A where alpha is a tunable hyperparameter that can vary at each time step. The intuitive idea is that we want the predictor to try to minimize L P textstyle L_P (therefore the term W L P textstyle nabla _WL_P ) while, at the same time, maximize L A textstyle L_A (therefore the term W L A textstyle -alpha nabla _WL_A ), so that the adversary fails at predicting the sensitive variable from Y  textstyle hat Y . The term p r o j W L A W L P textstyle -proj_nabla _WL_Anabla _WL_P prevents the predictor from moving in a direction that helps the adversary decrease its loss function. It can be shown that training a predictor classification model with this algorithm improves demographic parity with respect to training it without the adversary. Postprocessing The final method tries to correct the results of a classifier to achieve fairness. In this method, we have a classifier that returns a score for each individual and we need to do a binary prediction for them. High scores are likely to get a positive outcome, while low scores are likely to get a negative one, but we can adjust the threshold to determine when to answer yes as desired. Note that variations in the threshold value affect the trade-off between the rates for true positives and true negatives. If the score function is fair in the sense that it is independent of the protected attribute, then any choice of the threshold will also be fair, but classifiers of this type tend to be biased, so a different threshold may be required for each protected group to achieve fairness. A way to do this is plotting the true positive rate against the false negative rate at various threshold settings (this is called ROC curve) and find a threshold where the rates for the protected group and other individuals are equal. Reject option based classification Given a classifier let P (   X ) textstyle P(X) be the probability computed by the classifiers as the probability that the instance X textstyle X belongs to the positive class . When P (   X ) textstyle P(X) is close to 1 or to 0, the instance X textstyle X is specified with high degree of certainty to belong to class  or respectively. However, when P (   X ) textstyle P(X) is closer to 0.5 the classification is more unclear. We say X textstyle X is a rejected instance if m a x ( P (   X ) , 1 P (   X ) ) textstyle max(P(X),1-P(X))leq theta  with a certain textstyle theta  such that 0.5   1 textstyle 0.5theta 1 . The algorithm of ROC consists on classifying the non-rejected instances following the rule above and the rejected instances as follows if the instance is an example of a deprived group ( X ( A )  a displaystyle X(A)a ) then label it as positive, otherwise, label it as negative. We can optimize different measures of discrimination (link) as functions of textstyle theta  to find the optimal textstyle theta  for each problem and avoid becoming discriminatory against the privileged group. See also Algorithmic bias Machine learning Representational harm Title Feature (machine learning) URL https//en.wikipedia.org/wiki/Feature_(machine_learning) Content In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a data set. Choosing informative, discriminating, and independent features is crucial to produce effective algorithms for pattern recognition, classification, and regression tasks. Features are usually numeric, but other types such as strings and graphs are used in syntactic pattern recognition, after some pre-processing step such as one-hot encoding. The concept of features is related to that of explanatory variables used in statistical techniques such as linear regression. Feature types In feature engineering, two types of features are commonly used numerical and categorical. Numerical features are continuous values that can be measured on a scale. Examples of numerical features include age, height, weight, and income. Numerical features can be used in machine learning algorithms directly. Categorical features are discrete values that can be grouped into categories. Examples of categorical features include gender, color, and zip code. Categorical features typically need to be converted to numerical features before they can be used in machine learning algorithms. This can be done using a variety of techniques, such as one-hot encoding, label encoding, and ordinal encoding. The type of feature that is used in feature engineering depends on the specific machine learning algorithm that is being used. Some machine learning algorithms, such as decision trees, can handle both numerical and categorical features. Other machine learning algorithms, such as linear regression, can only handle numerical features. Classification A numeric feature can be conveniently described by a feature vector. One way to achieve binary classification is using a linear predictor function (related to the perceptron) with a feature vector as input. The method consists of calculating the scalar product between the feature vector and a vector of weights, qualifying those observations whose result exceeds a threshold. Algorithms for classification from a feature vector include nearest neighbor classification, neural networks, and statistical techniques such as Bayesian approaches. Examples In character recognition, features may include histograms counting the number of black pixels along horizontal and vertical directions, number of internal holes, stroke detection and many others. In speech recognition, features for recognizing phonemes can include noise ratios, length of sounds, relative power, filter matches and many others. In spam detection algorithms, features may include the presence or absence of certain email headers, the email structure, the language, the frequency of specific terms, the grammatical correctness of the text. In computer vision, there are a large number of possible features, such as edges and objects. Feature vectors In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis. When representing images, the feature values might correspond to the pixels of an image, while when representing texts the features might be the frequencies of occurrence of textual terms. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression. Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction. The vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed. Higher-level features can be obtained from already available features and added to the feature vector for example, for the study of diseases the feature Age is useful and is defined as  . This process is referred to as feature construction. Feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features. Examples of such constructive operators include checking for the equality conditions , , the arithmetic operators , , , /, the array operators max(S), min(S), average(S) as well as other more sophisticated operators, for example count(S,C) that counts the number of features in the feature vector S satisfying some condition C or, for example, distances to other recognition classes generalized by some accepting device. Feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure, particularly in high-dimensional problems. Applications include studies of disease and emotion recognition from speech. Selection and extraction The initial set of raw features can be redundant and large enough that estimation and optimization is made difficult or ineffective. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability. Extracting or selecting features is a combination of art and science developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert. Automating this process is feature learning, where a machine not only uses features for learning, but learns the features itself. See also Covariate Dimensionality reduction Feature engineering Hashing trick Statistical classification Explainable artificial intelligence Title Feature engineering URL https//en.wikipedia.org/wiki/Feature_engineering Content Feature engineering is a preprocessing step in supervised machine learning and statistical modeling which transforms raw data into a more effective set of inputs. Each input comprises several attributes, known as features. By providing models with relevant information, feature engineering significantly enhances their predictive accuracy and decision-making capability. Beyond machine learning, the principles of feature engineering are applied in various scientific fields, including physics. For example, physicists construct dimensionless numbers such as the Reynolds number in fluid dynamics, the Nusselt number in heat transfer, and the Archimedes number in sedimentation. They also develop first approximations of solutions, such as analytical solutions for the strength of materials in mechanics. Clustering One of the applications of feature engineering has been clustering of feature-objects or sample-objects in a dataset. Especially, feature engineering based on matrix decomposition has been extensively used for data clustering under non-negativity constraints on the feature coefficients. These include Non-Negative Matrix Factorization (NMF), Non-Negative Matrix-Tri Factorization (NMTF), Non-Negative Tensor Decomposition/Factorization (NTF/NTD), etc. The non-negativity constraints on coefficients of the feature vectors mined by the above-stated algorithms yields a part-based representation, and different factor matrices exhibit natural clustering properties. Several extensions of the above-stated feature engineering methods have been reported in literature, including orthogonality-constrained factorization for hard clustering, and manifold learning to overcome inherent issues with these algorithms. Other classes of feature engineering algorithms include leveraging a common hidden structure across multiple inter-related datasets to obtain a consensus (common) clustering scheme. An example is Multi-view Classification based on Consensus Matrix Decomposition (MCMD), which mines a common clustering scheme across multiple datasets. MCMD is designed to output two types of class labels (scale-variant and scale-invariant clustering), and is computationally robust to missing information, can obtain shape- and scale-based outliers, and can handle high-dimensional data effectively. Coupled matrix and tensor decompositions are popular in multi-view feature engineering. Predictive modelling Feature engineering in machine learning and statistical modeling involves selecting, creating, transforming, and extracting data features. Key components include feature creation from existing data, transforming and imputing missing or invalid features, reducing data dimensionality through methods like Principal Components Analysis (PCA), Independent Component Analysis (ICA), and Linear Discriminant Analysis (LDA), and selecting the most relevant features for model training based on importance scores and correlation matrices. Features vary in significance. Even relatively insignificant features may contribute to a model. Feature selection can reduce the number of features to prevent a model from becoming too specific to the training data set (overfitting). Feature explosion occurs when the number of identified features is too large for effective model estimation or optimization. Common causes include Feature templates - implementing feature templates instead of coding new features Feature combinations - combinations that cannot be represented by a linear system Feature explosion can be limited via techniques such as regularization, kernel methods, and feature selection. Automation Automation of feature engineering is a research topic that dates back to the 1990s. Machine learning software that incorporates automated feature engineering has been commercially available since 2016. Related academic literature can be roughly separated into two types Multi-relational decision tree learning (MRDTL) uses a supervised algorithm that is similar to a decision tree. Deep Feature Synthesis uses simpler methods. Multi-relational decision tree learning (MRDTL) Multi-relational Decision Tree Learning (MRDTL) extends traditional decision tree methods to relational databases, handling complex data relationships across tables. It innovatively uses selection graphs as decision nodes, refined systematically until a specific termination criterion is reached. Most MRDTL studies base implementations on relational databases, which results in many redundant operations. These redundancies can be reduced by using techniques such as tuple id propagation. Open-source implementations There are a number of open-source libraries and tools that automate feature engineering on relational data and time series featuretools is a Python library for transforming time series and relational data into feature matrices for machine learning. MCMD An open-source feature engineering algorithm for joint clustering of multiple datasets . OneBM or One-Button Machine combines feature transformations and feature selection on relational data with feature selection techniques. OneBM helps data scientists reduce data exploration time allowing them to try and error many ideas in short time. On the other hand, it enables non-experts, who are not familiar with data science, to quickly extract value from their data with a little effort, time, and cost. getML community is an open source tool for automated feature engineering on time series and relational data. It is implemented in C/C with a Python interface. It has been shown to be at least 60 times faster than tsflex, tsfresh, tsfel, featuretools or kats. tsfresh is a Python library for feature extraction on time series data. It evaluates the quality of the features using hypothesis testing. tsflex is an open source Python library for extracting features from time series data. Despite being 100 written in Python, it has been shown to be faster and more memory efficient than tsfresh, seglearn or tsfel. seglearn is an extension for multivariate, sequential time series data to the scikit-learn Python library. tsfel is a Python package for feature extraction on time series data. kats is a Python toolkit for analyzing time series data. Deep feature synthesis The deep feature synthesis (DFS) algorithm beat 615 of 906 human teams in a competition. Feature stores The feature store is where the features are stored and organized for the explicit purpose of being used to either train models (by data scientists) or make predictions (by applications that have a trained model). It is a central location where you can either create or update groups of features created from multiple different data sources, or create and update new datasets from those feature groups for training models or for use in applications that do not want to compute the features but just retrieve them when it needs them to make predictions. A feature store includes the ability to store code used to generate features, apply the code to raw data, and serve those features to models upon request. Useful capabilities include feature versioning and policies governing the circumstances under which features can be used. Feature stores can be standalone software tools or built into machine learning platforms. Alternatives Feature engineering can be a time-consuming and error-prone process, as it requires domain expertise and often involves trial and error. Deep learning algorithms may be used to process a large raw dataset without having to resort to feature engineering. However, deep learning algorithms still require careful preprocessing and cleaning of the input data. In addition, choosing the right architecture, hyperparameters, and optimization algorithm for a deep neural network can be a challenging and iterative process. See also Covariate Data transformation Feature extraction Feature learning Hashing trick Instrumental variables estimation Kernel method List of datasets for machine learning research Scale co-occurrence matrix Space mapping References Title Feature hashing URL https//en.wikipedia.org/wiki/Feature_hashing Content In machine learning, feature hashing, also known as the hashing trick (by analogy to the kernel trick), is a fast and space-efficient way of vectorizing features, i.e. turning arbitrary features into indices in a vector or matrix. It works by applying a hash function to the features and using their hash values as indices directly (after a modulo operation), rather than looking the indices up in an associative array. In addition to its use for encoding non-numeric values, feature hashing can also be used for dimensionality reduction. This trick is often attributed to Weinberger et al. (2009), but there exists a much earlier description of this method published by John Moody in 1989. Motivation Motivating example In a typical document classification task, the input to the machine learning algorithm (both during learning and classification) is free text. From this, a bag of words (BOW) representation is constructed the individual tokens are extracted and counted, and each distinct token in the training set defines a feature (independent variable) of each of the documents in both the training and test sets. Machine learning algorithms, however, are typically defined in terms of numerical vectors. Therefore, the bags of words for a set of documents is regarded as a term-document matrix where each row is a single document, and each column is a single feature/word the entry i, j in such a matrix captures the frequency (or weight) of the jth term of the vocabulary in document i. (An alternative convention swaps the rows and columns of the matrix, but this difference is immaterial.) Typically, these vectors are extremely sparse according to Zipfs law. The common approach is to construct, at learning time or prior to that, a dictionary representation of the vocabulary of the training set, and use that to map words to indices. Hash tables and tries are common candidates for dictionary implementation. E.g., the three documents John likes to watch movies. Mary likes movies too. John also likes football. can be converted, using the dictionary to the term-document matrix ( John likes to watch movies Mary too also football 1 1 1 1 1 0 0 0 0 0 1 0 0 1 1 1 0 0 1 1 0 0 0 0 0 1 1 ) displaystyle beginpmatrixtextrm Johntextrm likestextrm totextrm watchtextrm moviestextrm Marytextrm tootextrm alsotextrm football111110000010011100110000011endpmatrix (Punctuation was removed, as is usual in document classification and clustering.) The problem with this process is that such dictionaries take up a large amount of storage space and grow in size as the training set grows. On the contrary, if the vocabulary is kept fixed and not increased with a growing training set, an adversary may try to invent new words or misspellings that are not in the stored vocabulary so as to circumvent a machine learned filter. To address this challenge, Yahoo! Research attempted to use feature hashing for their spam filters. Note that the hashing trick isnt limited to text classification and similar tasks at the document level, but can be applied to any problem that involves large (perhaps unbounded) numbers of features. Mathematical motivation Mathematically, a token is an element t displaystyle t in a finite (or countably infinite) set T displaystyle T . Suppose we only need to process a finite corpus, then we can put all tokens appearing in the corpus into T displaystyle T , meaning that T displaystyle T is finite. However, suppose we want to process all possible words made of the English letters, then T displaystyle T is countably infinite. Most neural networks can only operate on real vector inputs, so we must construct a dictionary function  T R n displaystyle phi Tto mathbb R n . When T displaystyle T is finite, of size  T   m n displaystyle Tmleq n , then we can use one-hot encoding to map it into R n displaystyle mathbb R n . First, arbitrarily enumerate  , . . , t m  displaystyle ,..,t_m , then define ( t i )  e i displaystyle phi (t_i)e_i . In other words, we assign a unique index i displaystyle i to each token, then map the token with index i displaystyle i to the unit basis vector e i displaystyle e_i . One-hot encoding is easy to interpret, but it requires one to maintain the arbitrary enumeration of T displaystyle T . Given a token t T displaystyle tin T , to compute ( t ) displaystyle phi (t) , we must find out the index i displaystyle i of the token t displaystyle t . Thus, to implement displaystyle phi  efficiently, we need a fast-to-compute bijection h  T  1 , . . . , m  displaystyle hTto 1,...,m , then we have ( t )  e h ( t ) displaystyle phi (t)e_h(t) . In fact, we can relax the requirement slightly It suffices to have a fast-to-compute injection h  T  1 , . . . , n  displaystyle hTto 1,...,n , then use ( t )  e h ( t ) displaystyle phi (t)e_h(t) . In practice, there is no simple way to construct an efficient injection h  T  1 , . . . , n  displaystyle hTto 1,...,n . However, we do not need a strict injection, but only an approximate injection. That is, when t t displaystyle tneq t , we should probably have h ( t ) h ( t ) displaystyle h(t)neq h(t) , so that probably ( t ) ( t ) displaystyle phi (t)neq phi (t) . At this point, we have just specified that h displaystyle h should be a hashing function. Thus we reach the idea of feature hashing. Algorithms Feature hashing (Weinberger et al. 2009) The basic feature hashing algorithm presented in (Weinberger et al. 2009) is defined as follows. First, one specifies two hash functions the kernel hash h  T  1 , 2 , . . . , n  displaystyle hTto 1,2,...,n , and the sign hash  T  1 ,  1  displaystyle zeta Tto -1,1 . Next, one defines the feature hashing function  T R n , ( t )  ( t ) e h ( t ) displaystyle phi Tto mathbb R n,quad phi (t)zeta (t)e_h(t) Finally, extend this feature hashing function to strings of tokens by  T R n , ( t 1 , . . . , t k )  ,...,t_k)sum _ . Equivalently, ( t 1 , . . . , t k )  ,...,t_k)sum _. To make it nicer, we lift it to T R T displaystyle Tto mathbb R T , and lift displaystyle phi  from  T R n displaystyle phi Tto mathbb R n to  R T R n displaystyle phi mathbb R Tto mathbb R n by linear extension ( ( x t ) t T )  t T x t ( t ) e h ( t )  . There are essentially only two ways to handle infinities. One may impose a metric, then take its completion, to allow well-behaved infinite sums, or one may demand that nothing is actually infinite, only potentially so. Here, we go for the potential-infinity way, by restricting R T displaystyle mathbb R T to contain only vectors with finite support ( x t ) t T R T displaystyle forall (x_t)_tin Tin mathbb R T , only finitely many entries of ( x t ) t T displaystyle (x_t)_tin T are nonzero. Define an inner product on R T displaystyle mathbb R T in the obvious way e t , e . x , .endcasesquad langle x,x. Taking its completion would get us to a Hilbert space, which allows well-behaved infinite sums. Now we have an inner product space, with enough structure to describe the geometry of the feature hashing function  R T R n displaystyle phi mathbb R Tto mathbb R n . First, we can see why h displaystyle h is called a kernel hash it allows us to define a kernel K  T T R displaystyle KTtimes Tto mathbb R  by K ( t , t )  e h ( t ) , e h ( t ) displaystyle K(t,t)langle e_h(t),e_h(t)rangle  In the language of the kernel trick, K displaystyle K is the kernel generated by the feature map  T R n , ( t )  e h ( t ) displaystyle varphi Tto mathbb R n,quad varphi (t)e_h(t) Note that this is not the feature map we were using, which is ( t )  ( t ) e h ( t ) displaystyle phi (t)zeta (t)e_h(t) . In fact, we have been using another kernel K  T T R displaystyle K_zeta Ttimes Tto mathbb R  , defined by K ( t , t )  ( t ) e h ( t ) , ( t ) e h ( t ) displaystyle K_zeta (t,t)langle zeta (t)e_h(t),zeta (t)e_h(t)rangle  The benefit of augmenting the kernel hash h displaystyle h with the binary hash displaystyle zeta  is the following theorem, which states that displaystyle phi  is an isometry on average. The above statement and proof interprets the binary hash function displaystyle zeta  not as a deterministic function of type T  1 ,  1  displaystyle Tto -1,1 , but as a random binary vector  1 ,  1  T displaystyle -1,1T with unbiased entries, meaning that P r ( ( t )   1 )  P r ( ( t )  1 )  1 2 displaystyle Pr(zeta (t)1)Pr(zeta (t)-1)frac 12 for any t T displaystyle tin T . This is a good intuitive picture, though not rigorous. For a rigorous statement and proof, see Pseudocode implementation Instead of maintaining a dictionary, a feature vectorizer that uses the hashing trick can build a vector of a pre-defined length by applying a hash function h to the features (e.g., words), then using the hash values directly as feature indices and updating the resulting vector at those indices. Here, we assume that feature actually means feature vector. Thus, if our feature vector is cat,dog,cat and hash function is h ( x f )  1 displaystyle h(x_f)1 if x f displaystyle x_f is cat and 2 displaystyle 2 if x f displaystyle x_f is dog. Let us take the output feature vector dimension (N) to be 4. Then output x will be 0,2,1,0. It has been suggested that a second, single-bit output hash function be used to determine the sign of the update value, to counter the effect of hash collisions. If such a hash function is used, the algorithm becomes The above pseudocode actually converts each sample into a vector. An optimized version would instead only generate a stream of ( h , ) displaystyle (h,zeta ) pairs and let the learning and prediction algorithms consume such streams a linear model can then be implemented as a single hash table representing the coefficient vector. Extensions and variations Learned feature hashing Feature hashing generally suffers from hash collision, which means that there exist pairs of different tokens with the same hash t t , ( t )  ( t )  v displaystyle tneq t,phi (t)phi (t)v . A machine learning model trained on feature-hashed words would then have difficulty distinguishing t displaystyle t and t displaystyle t , essentially because v displaystyle v is polysemic. If t displaystyle t is rare, then performance degradation is small, as the model could always just ignore the rare case, and pretend all v displaystyle v means t displaystyle t . However, if both are common, then the degradation can be serious. To handle this, one can train supervised hashing functions that avoids mapping common tokens to the same feature vectors. Applications and practical performance Ganchev and Dredze showed that in text classification applications with random hash functions and several tens of thousands of columns in the output vectors, feature hashing need not have an adverse effect on classification performance, even without the signed hash function. Weinberger et al. (2009) applied their version of feature hashing to multi-task learning, and in particular, spam filtering, where the input features are pairs (user, feature) so that a single parameter vector captured per-user spam filters as well as a global filter for several hundred thousand users, and found that the accuracy of the filter went up. Chen et al. (2015) combined the idea of feature hashing and sparse matrix to construct virtual matrices large matrices with small storage requirements. The idea is to treat a matrix M R n n displaystyle Min mathbb R ntimes n as a dictionary, with keys in n n displaystyle ntimes n , and values in R displaystyle mathbb R  . Then, as usual in hashed dictionaries, one can use a hash function h  N N m displaystyle hmathbb N times mathbb N to m , and thus represent a matrix as a vector in R m displaystyle mathbb R m , no matter how big n displaystyle n is. With virtual matrices, they constructed HashedNets, which are large neural networks taking only small amounts of storage. Implementations Implementations of the hashing trick are present in Apache Mahout Gensim scikit-learn sofia-ml Vowpal Wabbit Apache Spark R TensorFlow Dask-ML See also Bloom filter Data structure for approximate set membership Count min sketch Probabilistic data structure in computer science Heaps law Heuristic for distinct words in a document Locality-sensitive hashing Algorithmic technique using hashing MinHash Data mining technique References External links Hashing Representations for Machine Learning on John Langfords website What is the hashing trick? - MetaOptimize QA Title Feature learning URL https//en.wikipedia.org/wiki/Feature_learning Content In machine learning (ML), feature learning or representation learning is a set of techniques that allow a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task. Feature learning is motivated by the fact that ML tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data, such as image, video, and sensor data, have not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms. Feature learning can be either supervised, unsupervised, or self-supervised In supervised feature learning, features are learned using labeled input data. Labeled data includes input-label pairs where the input is given to the model, and it must produce the ground truth label as the output. This can be leveraged to generate feature representations with the model which result in high label prediction accuracy. Examples include supervised neural networks, multilayer perceptrons, and dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data by analyzing the relationship between points in the dataset. Examples include dictionary learning, independent component analysis, matrix factorization, and various forms of clustering. In self-supervised feature learning, features are learned using unlabeled data like unsupervised learning, however input-label pairs are constructed from each data point, enabling learning the structure of the data through supervised methods such as gradient descent. Classical examples include word embeddings and autoencoders. Self-supervised learning has since been applied to many modalities through the use of deep neural network architectures such as convolutional neural networks and transformers. Supervised Supervised feature learning is learning features from labeled data. The data label allows the system to compute an error term, the degree to which the system fails to produce the label, which can then be used as feedback to correct the learning process (reduce/minimize the error). Approaches include Supervised dictionary learning Dictionary learning develops a set (dictionary) of representative elements from the input data such that each data point can be represented as a weighted sum of the representative elements. The dictionary elements and the weights may be found by minimizing the average representation error (over the input data), together with regularization on the weights to enable sparsity (i.e., the representation of each data point has only a few nonzero weights). Supervised dictionary learning exploits both the structure underlying the input data and the labels for optimizing the dictionary elements. For example, this supervised dictionary learning technique applies dictionary learning on classification problems by jointly optimizing the dictionary elements, weights for representing data points, and parameters of the classifier based on the input data. In particular, a minimization problem is formulated, where the objective function consists of the classification error, the representation error, an regularization on the representing weights for each data point (to enable sparse representation of data), and an regularization on the parameters of the classifier. Neural networks Neural networks are a family of learning algorithms that use a network consisting of multiple layers of inter-connected nodes. It is inspired by the animal nervous system, where the nodes are viewed as neurons and edges are viewed as synapses. Each edge has an associated weight, and the network defines computational rules for passing input data from the networks input layer to the output layer. A network function associated with a neural network characterizes the relationship between input and output layers, which is parameterized by the weights. With appropriately defined network functions, various learning tasks can be performed by minimizing a cost function over the network function (weights). Multilayer neural networks can be used to perform feature learning, since they learn a representation of their input at the hidden layer(s) which is subsequently used for classification or regression at the output layer. The most popular network architecture of this type is Siamese networks. Unsupervised Unsupervised feature learning is learning features from unlabeled data. The goal of unsupervised feature learning is often to discover low-dimensional features that capture some structure underlying the high-dimensional input data. When the feature learning is performed in an unsupervised way, it enables a form of semisupervised learning where features learned from an unlabeled dataset are then employed to improve performance in a supervised setting with labeled data. Several approaches are introduced in the following. K-means clustering K-means clustering is an approach for vector quantization. In particular, given a set of n vectors, k-means clustering groups them into k clusters (i.e., subsets) in such a way that each vector belongs to the cluster with the closest mean. The problem is computationally NP-hard, although suboptimal greedy algorithms have been developed. K-means clustering can be used to group an unlabeled set of inputs into k clusters, and then use the centroids of these clusters to produce features. These features can be produced in several ways. The simplest is to add k binary features to each sample, where each feature j has value one iff the jth centroid learned by k-means is the closest to the sample under consideration. It is also possible to use the distances to the clusters as features, perhaps after transforming them through a radial basis function (a technique that has been used to train RBF networks). Coates and Ng note that certain variants of k-means behave similarly to sparse coding algorithms. In a comparative evaluation of unsupervised feature learning methods, Coates, Lee and Ng found that k-means clustering with an appropriate transformation outperforms the more recently invented auto-encoders and RBMs on an image classification task. K-means also improves performance in the domain of NLP, specifically for named-entity recognition there, it competes with Brown clustering, as well as with distributed word representations (also known as neural word embeddings). Principal component analysis Principal component analysis (PCA) is often used for dimension reduction. Given an unlabeled set of n input data vectors, PCA generates p (which is much smaller than the dimension of the input data) right singular vectors corresponding to the p largest singular values of the data matrix, where the kth row of the data matrix is the kth input data vector shifted by the sample mean of the input (i.e., subtracting the sample mean from the data vector). Equivalently, these singular vectors are the eigenvectors corresponding to the p largest eigenvalues of the sample covariance matrix of the input vectors. These p singular vectors are the feature vectors learned from the input data, and they represent directions along which the data has the largest variations. PCA is a linear feature learning approach since the p singular vectors are linear functions of the data matrix. The singular vectors can be generated via a simple algorithm with p iterations. In the ith iteration, the projection of the data matrix on the (i-1)th eigenvector is subtracted, and the ith singular vector is found as the right singular vector corresponding to the largest singular of the residual data matrix. PCA has several limitations. First, it assumes that the directions with large variance are of most interest, which may not be the case. PCA only relies on orthogonal transformations of the original data, and it exploits only the first- and second-order moments of the data, which may not well characterize the data distribution. Furthermore, PCA can effectively reduce dimension only when the input data vectors are correlated (which results in a few dominant eigenvalues). Local linear embedding Local linear embedding (LLE) is a nonlinear learning approach for generating low-dimensional neighbor-preserving representations from (unlabeled) high-dimension input. The approach was proposed by Roweis and Saul (2000). The general idea of LLE is to reconstruct the original high-dimensional data using lower-dimensional points while maintaining some geometric properties of the neighborhoods in the original data set. LLE consists of two major steps. The first step is for neighbor-preserving, where each input data point Xi is reconstructed as a weighted sum of K nearest neighbor data points, and the optimal weights are found by minimizing the average squared reconstruction error (i.e., difference between an input point and its reconstruction) under the constraint that the weights associated with each point sum up to one. The second step is for dimension reduction, by looking for vectors in a lower-dimensional space that minimizes the representation error using the optimized weights in the first step. Note that in the first step, the weights are optimized with fixed data, which can be solved as a least squares problem. In the second step, lower-dimensional points are optimized with fixed weights, which can be solved via sparse eigenvalue decomposition. The reconstruction weights obtained in the first step capture the intrinsic geometric properties of a neighborhood in the input data. It is assumed that original data lie on a smooth lower-dimensional manifold, and the intrinsic geometric properties captured by the weights of the original data are also expected to be on the manifold. This is why the same weights are used in the second step of LLE. Compared with PCA, LLE is more powerful in exploiting the underlying data structure. Independent component analysis Independent component analysis (ICA) is a technique for forming a data representation using a weighted sum of independent non-Gaussian components. The assumption of non-Gaussian is imposed since the weights cannot be uniquely determined when all the components follow Gaussian distribution. Unsupervised dictionary learning Unsupervised dictionary learning does not utilize data labels and exploits the structure underlying the data for optimizing dictionary elements. An example of unsupervised dictionary learning is sparse coding, which aims to learn basis functions (dictionary elements) for data representation from unlabeled input data. Sparse coding can be applied to learn overcomplete dictionaries, where the number of dictionary elements is larger than the dimension of the input data. Aharon et al. proposed algorithm K-SVD for learning a dictionary of elements that enables sparse representation. Multilayer/deep architectures The hierarchical architecture of the biological neural system inspires deep learning architectures for feature learning by stacking multiple layers of learning nodes. These architectures are often designed based on the assumption of distributed representation observed data is generated by the interactions of many different factors on multiple levels. In a deep learning architecture, the output of each intermediate layer can be viewed as a representation of the original input data. Each level uses the representation produced by the previous, lower level as input, and produces new representations as output, which are then fed to higher levels. The input at the bottom layer is raw data, and the output of the final, highest layer is the final low-dimensional feature or representation. Restricted Boltzmann machine Restricted Boltzmann machines (RBMs) are often used as a building block for multilayer learning architectures. An RBM can be represented by an undirected bipartite graph consisting of a group of binary hidden variables, a group of visible variables, and edges connecting the hidden and visible nodes. It is a special case of the more general Boltzmann machines with the constraint of no intra-node connections. Each edge in an RBM is associated with a weight. The weights together with the connections define an energy function, based on which a joint distribution of visible and hidden nodes can be devised. Based on the topology of the RBM, the hidden (visible) variables are independent, conditioned on the visible (hidden) variables. Such conditional independence facilitates computations. An RBM can be viewed as a single layer architecture for unsupervised feature learning. In particular, the visible variables correspond to input data, and the hidden variables correspond to feature detectors. The weights can be trained by maximizing the probability of visible variables using Hintons contrastive divergence (CD) algorithm. In general, training RBMs by solving the maximization problem tends to result in non-sparse representations. Sparse RBM was proposed to enable sparse representations. The idea is to add a regularization term in the objective function of data likelihood, which penalizes the deviation of the expected hidden variables from a small constant p displaystyle p . RBMs have also been used to obtain disentangled representations of data, where interesting features map to separate hidden units. Autoencoder An autoencoder consisting of an encoder and a decoder is a paradigm for deep learning architectures. An example is provided by Hinton and Salakhutdinov where the encoder uses raw data (e.g., image) as input and produces feature or representation as output and the decoder uses the extracted feature from the encoder as input and reconstructs the original input raw data as output. The encoder and decoder are constructed by stacking multiple layers of RBMs. The parameters involved in the architecture were originally trained in a greedy layer-by-layer manner after one layer of feature detectors is learned, they are fed up as visible variables for training the corresponding RBM. Current approaches typically apply end-to-end training with stochastic gradient descent methods. Training can be repeated until some stopping criteria are satisfied. Self-supervised Self-supervised representation learning is learning features by training on the structure of unlabeled data rather than relying on explicit labels for an information signal. This approach has enabled the combined use of deep neural network architectures and larger unlabeled datasets to produce deep feature representations. Training tasks typically fall under the classes of either contrastive, generative or both. Contrastive representation learning trains representations for associated data pairs, called positive samples, to be aligned, while pairs with no relation, called negative samples, are contrasted. A larger portion of negative samples is typically necessary in order to prevent catastrophic collapse, which is when all inputs are mapped to the same representation. Generative representation learning tasks the model with producing the correct data to either match a restricted input or reconstruct the full input from a lower dimensional representation. A common setup for self-supervised representation learning of a certain data type (e.g. text, image, audio, video) is to pretrain the model using large datasets of general context, unlabeled data. Depending on the context, the result of this is either a set of representations for common data segments (e.g. words) which new data can be broken into, or a neural network able to convert each new data point (e.g. image) into a set of lower dimensional features. In either case, the output representations can then be used as an initialization in many different problem settings where labeled data may be limited. Specialization of the model to specific tasks is typically done with supervised learning, either by fine-tuning the model / representations with the labels as the signal, or freezing the representations and training an additional model which takes them as an input. Many self-supervised training schemes have been developed for use in representation learning of various modalities, often first showing successful application in text or image before being transferred to other data types. Text Word2vec is a word embedding technique which learns to represent words through self-supervision over each word and its neighboring words in a sliding window across a large corpus of text. The model has two possible training schemes to produce word vector representations, one generative and one contrastive. The first is word prediction given each of the neighboring words as an input. The second is training on the representation similarity for neighboring words and representation dissimilarity for random pairs of words. A limitation of word2vec is that only the pairwise co-occurrence structure of the data is used, and not the ordering or entire set of context words. More recent transformer-based representation learning approaches attempt to solve this with word prediction tasks. GPTs pretrain on next word prediction using prior input words as context, whereas BERT masks random tokens in order to provide bidirectional context. Other self-supervised techniques extend word embeddings by finding representations for larger text structures such as sentences or paragraphs in the input data. Doc2vec extends the generative training approach in word2vec by adding an additional input to the word prediction task based on the paragraph it is within, and is therefore intended to represent paragraph level context. Image The domain of image representation learning has employed many different self-supervised training techniques, including transformation, inpainting, patch discrimination and clustering. Examples of generative approaches are Context Encoders, which trains an AlexNet CNN architecture to generate a removed image region given the masked image as input, and iGPT, which applies the GPT-2 language model architecture to images by training on pixel prediction after reducing the image resolution. Many other self-supervised methods use siamese networks, which generate different views of the image through various augmentations that are then aligned to have similar representations. The challenge is avoiding collapsing solutions where the model encodes all images to the same representation. SimCLR is a contrastive approach which uses negative examples in order to generate image representations with a ResNet CNN. Bootstrap Your Own Latent (BYOL) removes the need for negative samples by encoding one of the views with a slow moving average of the model parameters as they are being modified during training. Graph The goal of many graph representation learning techniques is to produce an embedded representation of each node based on the overall network topology. node2vec extends the word2vec training technique to nodes in a graph by using co-occurrence in random walks through the graph as the measure of association. Another approach is to maximize mutual information, a measure of similarity, between the representations of associated structures within the graph. An example is Deep Graph Infomax, which uses contrastive self-supervision based on mutual information between the representation of a patch around each node, and a summary representation of the entire graph. Negative samples are obtained by pairing the graph representation with either representations from another graph in a multigraph training setting, or corrupted patch representations in single graph training. Video With analogous results in masked prediction and clustering, video representation learning approaches are often similar to image techniques but must utilize the temporal sequence of video frames as an additional learned structure. Examples include VCP, which masks video clips and trains to choose the correct one given a set of clip options, and Xu et al., who train a 3D-CNN to identify the original order given a shuffled set of video clips. Audio Self-supervised representation techniques have also been applied to many audio data formats, particularly for speech processing. Wav2vec 2.0 discretizes the audio waveform into timesteps via temporal convolutions, and then trains a transformer on masked prediction of random timesteps using a contrastive loss. This is similar to the BERT language model, except as in many SSL approaches to video, the model chooses among a set of options rather than over the entire word vocabulary. Multimodal Self-supervised learning has also been used to develop joint representations of multiple data types. Approaches usually rely on some natural or human-derived association between the modalities as an implicit label, for instance video clips of animals or objects with characteristic sounds, or captions written to describe images. CLIP produces a joint image-text representation space by training to align image and text encodings from a large dataset of image-caption pairs using a contrastive loss. MERLOT Reserve trains a transformer-based encoder to jointly represent audio, subtitles and video frames from a large dataset of videos through 3 joint pretraining tasks contrastive masked prediction of either audio or text segments given the video frames and surrounding audio and text context, along with contrastive alignment of video frames with their corresponding captions. Multimodal representation models are typically unable to assume direct correspondence of representations in the different modalities, since the precise alignment can often be noisy or ambiguous. For example, the text dog could be paired with many different pictures of dogs, and correspondingly a picture of a dog could be captioned with varying degrees of specificity. This limitation means that downstream tasks may require an additional generative mapping network between modalities to achieve optimal performance, such as in DALLE-2 for text to image generation. Dynamic Representation Learning Dynamic representation learning methods generate latent embeddings for dynamic systems such as dynamic networks. Since particular distance functions are invariant under particular linear transformations, different sets of embedding vectors can actually represent the same/similar information. Therefore, for a dynamic system, a temporal difference in its embeddings may be explained by misalignment of embeddings due to arbitrary transformations and/or actual changes in the system. Therefore, generally speaking, temporal embeddings learned via dynamic representation learning methods should be inspected for any spurious changes and be aligned before consequent dynamic analyses. See also Automated machine learning (AutoML) Deep learning geometric feature learning Feature detection (computer vision) Feature extraction Word embedding Vector quantization Variational autoencoder Title Feature scaling URL https//en.wikipedia.org/wiki/Feature_scaling Content Feature scaling is a method used to normalize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step. Motivation Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance. Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it. Its also important to apply feature scaling if regularization is used as part of the loss function (so that coefficients are penalized appropriately). Empirically, feature scaling can improve the convergence speed of stochastic gradient descent. In support vector machines, it can reduce the time to find support vectors. Feature scaling is also often used in applications involving distances and similarities between data points, such as clustering and similarity search. As an example, the K-means clustering algorithm is sensitive to feature scales. Methods Rescaling (min-max normalization) Also known as min-max scaling or min-max normalization, rescaling is the simplest method and consists in rescaling the range of features to scale the range in 0, 1 or  1, 1. Selecting the target range depends on the nature of the data. The general formula for a min-max of 0, 1 is given as . For example, suppose that we have the students weight data, and the students weights span 160 pounds, 200 pounds. To rescale this data, we first subtract 160 from each students weight and divide the result by 40 (the difference between the maximum and minimum weights). To rescale a range between an arbitrary set of values a, b, the formula becomes . Mean normalization . There is another form of the means normalization which divides by the standard deviation which is also called standardization. Standardization (Z-score Normalization) In machine learning, we can handle various types of data, e.g. audio signals and pixel values for image data, and this data can include multiple dimensions. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance. This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines, logistic regression, and artificial neural networks). The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation. . Robust Scaling Robust scaling, also known as standardization using median and interquartile range (IQR), is designed to be robust to outliers. It scales features using the median and IQR as reference points instead of the mean and standard deviation . Unit vector normalization Unit vector normalization regards each individual data point as a vector, and divide each by its vector norm, to obtain  . Any vector norm can be used, but the most common ones are the norm and the norm. For example, if ). Data Transformation and Data Discretization. Data Mining Concepts and Techniques. Elsevier. pp. 111 118. ISBN 9780123814807. External links Lecture by Andrew Ng on feature scaling Title Federated learning URL https//en.wikipedia.org/wiki/Federated_learning Content Federated learning (also known as collaborative learning) is a machine learning technique in a setting where multiple entities (often called clients) collaboratively train a model while keeping their data decentralized, rather than centrally stored. A defining characteristic of federated learning is data heterogeneity. Because client data is decentralized, data samples held by each client may not be independently and identically distributed. Federated learning is generally concerned with and motivated by issues such as data privacy, data minimization, and data access rights. Its applications involve a variety of research areas including defence, telecommunications, the Internet of things, and pharmaceuticals. Definition Federated learning aims at training a machine learning algorithm, for instance deep neural networks, on multiple local datasets contained in local nodes without explicitly exchanging data samples. The general principle consists in training local models on local data samples and exchanging parameters (e.g. the weights and biases of a deep neural network) between these local nodes at some frequency to generate a global model shared by all nodes. The main difference between federated learning and distributed learning lies in the assumptions made on the properties of the local datasets, as distributed learning originally aims at parallelizing computing power where federated learning originally aims at training on heterogeneous datasets. While distributed learning also aims at training a single model on multiple servers, a common underlying assumption is that the local datasets are independent and identically distributed (i.i.d.) and roughly have the same size. None of these hypotheses are made for federated learning instead, the datasets are typically heterogeneous and their sizes may span several orders of magnitude. Moreover, the clients involved in federated learning may be unreliable as they are subject to more failures or drop out since they commonly rely on less powerful communication media (i.e. Wi-Fi) and battery-powered systems (i.e. smartphones and IoT devices) compared to distributed learning where nodes are typically datacenters that have powerful computational capabilities and are connected to one another with fast networks. Mathematical formulation The objective function for federated learning is as follows f ( x 1 , , x K )  1 K . The goal of federated learning is to train a common model on all of the nodes local datasets, in other words Optimizing the objective function f ( x 1 , , x K ) displaystyle f(mathbf x _1,dots ,mathbf x _K) . Achieving consensus on x i displaystyle mathbf x _i . In other words, x 1 , , x K displaystyle mathbf x _1,dots ,mathbf x _K converge to some common x displaystyle mathbf x  at the end of the training process. Centralized federated learning In the centralized federated learning setting, a central server is used to orchestrate the different steps of the algorithms and coordinate all the participating nodes during the learning process. The server is responsible for the nodes selection at the beginning of the training process and for the aggregation of the received model updates. Since all the selected nodes have to send updates to a single entity, the server may become a bottleneck of the system. Decentralized federated learning In the decentralized federated learning setting, the nodes are able to coordinate themselves to obtain the global model. This setup prevents single point failures as the model updates are exchanged only between interconnected nodes without the orchestration of the central server. Nevertheless, the specific network topology may affect the performances of the learning process. See blockchain-based federated learning and the references therein. Heterogeneous federated learning An increasing number of application domains involve a large set of heterogeneous clients, e.g., mobile phones and IoT devices. Most of the existing Federated learning strategies assume that local models share the same global model architecture. Recently, a new federated learning framework named HeteroFL was developed to address heterogeneous clients equipped with very different computation and communication capabilities. The HeteroFL technique can enable the training of heterogeneous local models with dynamically varying computation and non-IID data complexities while still producing a single accurate global inference model. Main features Iterative learning To ensure good task performance of a final, central machine learning model, federated learning relies on an iterative process broken up into an atomic set of client-server interactions known as a federated learning round. Each round of this process consists in transmitting the current global model state to participating nodes, training local models on these local nodes to produce a set of potential model updates at each node, and then aggregating and processing these local updates into a single global update and applying it to the global model. In the methodology below, a central server is used for aggregation, while local nodes perform local training depending on the central servers orders. However, other strategies lead to the same results without central servers, in a peer-to-peer approach, using gossip or consensus methodologies. Assuming a federated round composed by one iteration of the learning process, the learning procedure can be summarized as follows Initialization according to the server inputs, a machine learning model (e.g., linear regression, neural network, boosting) is chosen to be trained on local nodes and initialized. Then, nodes are activated and wait for the central server to give the calculation tasks. Client selection a fraction of local nodes are selected to start training on local data. The selected nodes acquire the current statistical model while the others wait for the next federated round. Configuration the central server orders selected nodes to undergo training of the model on their local data in a pre-specified fashion (e.g., for some mini-batch updates of gradient descent). Reporting each selected node sends its local model to the server for aggregation. The central server aggregates the received models and sends back the model updates to the nodes. It also handles failures for disconnected nodes or lost model updates. The next federated round is started returning to the client selection phase. Termination once a pre-defined termination criterion is met (e.g., a maximum number of iterations is reached or the model accuracy is greater than a threshold) the central server aggregates the updates and finalizes the global model. The procedure considered before assumes synchronized model updates. Recent federated learning developments introduced novel techniques to tackle asynchronicity during the training process, or training with dynamically varying models. Compared to synchronous approaches where local models are exchanged once the computations have been performed for all layers of the neural network, asynchronous ones leverage the properties of neural networks to exchange model updates as soon as the computations of a certain layer are available. These techniques are also commonly referred to as split learning and they can be applied both at training and inference time regardless of centralized or decentralized federated learning settings. Non-IID data In most cases, the assumption of independent and identically distributed samples across local nodes does not hold for federated learning setups. Under this setting, the performances of the training process may vary significantly according to the unbalanced local data samples as well as the particular probability distribution of the training examples (i.e., features and labels) stored at the local nodes. To further investigate the effects of non-IID data, the following description considers the main categories presented in the preprint by Peter Kairouz et al. from 2019. The description of non-IID data relies on the analysis of the joint probability between features and labels for each node. This allows decoupling of each contribution according to the specific distribution available at the local nodes. The main categories for non-iid data can be summarized as follows Covariate shift local nodes may store examples that have different statistical distributions compared to other nodes. An example occurs in natural language processing datasets where people typically write the same digits/letters with different stroke widths or slants. Prior probability shift local nodes may store labels that have different statistical distributions compared to other nodes. This can happen if datasets are regional and/or demographically partitioned. For example, datasets containing images of animals vary significantly from country to country. Concept drift (same label, different features) local nodes may share the same labels but some of them correspond to different features at different local nodes. For example, images that depict a particular object can vary according to the weather condition in which they were captured. Concept shift (same features, different labels) local nodes may share the same features but some of them correspond to different labels at different local nodes. For example, in natural language processing, the sentiment analysis may yield different sentiments even if the same text is observed. Unbalanced the amount of data available at the local nodes may vary significantly in size. The loss in accuracy due to non-iid data can be bounded through using more sophisticated means of doing data normalization, rather than batch normalization. Algorithmic hyper-parameters Network topology The way the statistical local outputs are pooled and the way the nodes communicate with each other can change from the centralized model explained in the previous section. This leads to a variety of federated learning approaches for instance no central orchestrating server, or stochastic communication. In particular, orchestrator-less distributed networks are one important variation. In this case, there is no central server dispatching queries to local nodes and aggregating local models. Each local node sends its outputs to several randomly-selected others, which aggregate their results locally. This restrains the number of transactions, thereby sometimes reducing training time and computing cost. Federated learning parameters Once the topology of the node network is chosen, one can control different parameters of the federated learning process (in addition to the machine learning models own hyperparameters) to optimize learning Number of federated learning rounds T displaystyle T Total number of nodes used in the process K displaystyle K Fraction of nodes used at each iteration for each node C displaystyle C Local batch size used at each learning iteration B displaystyle B Other model-dependent parameters can also be tinkered with, such as Number of iterations for local training before pooling N displaystyle N Local learning rate displaystyle eta  Those parameters have to be optimized depending on the constraints of the machine learning application (e.g., available computing power, available memory, bandwidth). For instance, stochastically choosing a limited fraction C displaystyle C of nodes for each iteration diminishes computing cost and may prevent overfitting, in the same way that stochastic gradient descent can reduce overfitting. Technical limitations Federated learning requires frequent communication between nodes during the learning process. Thus, it requires not only enough local computing power and memory, but also high bandwidth connections to be able to exchange parameters of the machine learning model. However, the technology also avoids data communication, which can require significant resources before starting centralized machine learning. Nevertheless, the devices typically employed in federated learning are communication-constrained, for example IoT devices or smartphones are generally connected to Wi-Fi networks, thus, even if the models are commonly less expensive to be transmitted compared to raw data, federated learning mechanisms may not be suitable in their general form. Federated learning raises several statistical challenges Heterogeneity between the different local datasets each node may have some bias with respect to the general population, and the size of the datasets may vary significantly Temporal heterogeneity each local datasets distribution may vary with time Interoperability of each nodes dataset is a prerequisite Each nodes dataset may require regular curations Hiding training data might allow attackers to inject backdoors into the global model Lack of access to global training data makes it harder to identify unwanted biases entering the training e.g. age, gender, sexual orientation Partial or total loss of model updates due to node failures affecting the global model Lack of annotations or labels on the client side. Heterogeneity between processing platforms Variations A number of different algorithms for federated optimization have been proposed. Federated stochastic gradient descent (FedSGD) Deep learning training mainly relies on variants of stochastic gradient descent, where gradients are computed on a random subset of the total dataset and then used to make one step of the gradient descent. Federated stochastic gradient descent is the direct transposition of this algorithm to the federated setting, but by using a random fraction C displaystyle C of the nodes and using all the data on this node. The gradients are averaged by the server proportionally to the number of training samples on each node, and used to make a gradient descent step. Federated averaging Federated averaging (FedAvg) is a generalization of FedSGD, which allows local nodes to perform more than one batch update on local data and exchanges the updated weights rather than the gradients. The rationale behind this generalization is that in FedSGD, if all local nodes start from the same initialization, averaging the gradients is strictly equivalent to averaging the weights themselves. Further, averaging tuned weights coming from the same initialization does not necessarily hurt the resulting averaged models performance. Variations of FedAvg based on adaptive optimizers such as ADAM and AdaGrad have been proposed, and generally outperform FedAvg. Federated learning with dynamic regularization (FedDyn) Federated learning methods suffer when the device datasets are heterogeneously distributed. Fundamental dilemma in heterogeneously distributed device setting is that minimizing the device loss functions is not the same as minimizing the global loss objective. In 2021, Acar et al. introduced FedDyn method as a solution to heterogenous dataset setting. FedDyn dynamically regularizes each devices loss function so that the modified device losses converges to the actual global loss. Since the local losses are aligned, FedDyn is robust to the different heterogeneity levels and it can safely perform full minimization in each device. Theoretically, FedDyn converges to the optimal (a stationary point for nonconvex losses) by being agnostic to the heterogeneity levels. These claims are verified with extensive experimentations on various datasets. Minimizing the number of communications is the gold-standard for comparison in federated learning. We may also want to decrease the local computation levels per device in each round. FedDynOneGD is an extension of FedDyn with less local compute requirements. FedDynOneGD calculates only one gradients per device in each round and update the model with a regularized version of the gradient. Hence, the computation complexity is linear in local dataset size. Moreover, gradient computation can be parallelizable within each device which is different from successive SGD steps. Theoretically, FedDynOneGD achieves the same convergence guarantees as in FedDyn with less local computation. Personalized federated learning by pruning (Sub-FedAvg) Federated Learning methods cannot achieve good global performance under non-IID settings which motivates the participating clients to yield personalized models in federation. Recently, Vahidian et al. introduced Sub-FedAvg opening a new personalized FL algorithm paradigm by proposing Hybrid Pruning (structured  unstructured pruning) with averaging on the intersection of clients drawn subnetworks which simultaneously handles communication efficiency, resource constraints and personalized models accuracies. Sub-FedAvg is the first work which shows existence of personalized winning tickets for clients in federated learning through experiments. Moreover, it also proposes two algorithms on how to effectively draw the personalized subnetworks. Sub-FedAvg tries to extend the lottery ticket hypothesis which is for centrally trained neural networks to federated learning trained neural networks leading to this open research problem Do winning tickets exist for clients neural networks being trained in federated learning? If yes, how to effectively draw the personalized subnetworks for each client? Dynamic aggregation - inverse distance aggregation IDA (Inverse Distance Aggregation) is a novel adaptive weighting approach for clients based on meta-information which handles unbalanced and non-iid data. It uses the distance of the model parameters as a strategy to minimize the effect of outliers and improve the models convergence rate. Hybrid federated dual coordinate ascent (HyFDCA) Very few methods for hybrid federated learning, where clients only hold subsets of both features and samples, exist. Yet, this scenario is very important in practical settings. Hybrid Federated Dual Coordinate Ascent (HyFDCA) is a novel algorithm proposed in 2024 that solves convex problems in the hybrid FL setting. This algorithm extends CoCoA, a primal-dual distributed optimization algorithm introduced by Jaggi et al. (2014) and Smith et al. (2017), to the case where both samples and features are partitioned across clients. HyFDCA claims several improvement over existing algorithms HyFDCA is a provably convergent primal-dual algorithm for hybrid FL in at least the following settings. Hybrid Federated Setting with Complete Client Participation Horizontal Federated Setting with Random Subsets of Available Clients The authors show HyFDCA enjoys a convergence rate of O(1 t) which matches the convergence rate of FedAvg (see below). Vertical Federated Setting with Incomplete Client Participation The authors show HyFDCA enjoys a convergence rate of O(log(t) t) whereas FedBCD exhibits a slower O(1 sqrt(t)) convergence rate and requires full client participation. HyFDCA provides the privacy steps that ensure privacy of client data in the primal-dual setting. These principles apply to future efforts in developing primal-dual algorithms for FL. HyFDCA empirically outperforms HyFEM and FedAvg in loss function value and validation accuracy across a multitude of problem settings and datasets (see below for more details). The authors also introduce a hyperparameter selection framework for FL with competing metrics using ideas from multiobjective optimization. There is only one other algorithm that focuses on hybrid FL, HyFEM proposed by Zhang et al. (2020). This algorithm uses a feature matching formulation that balances clients building accurate local models and the server learning an accurate global model. This requires a matching regularizer constant that must be tuned based on user goals and results in disparate local and global models. Furthermore, the convergence results provided for HyFEM only prove convergence of the matching formulation not of the original global problem. This work is substantially different than HyFDCAs approach which uses data on local clients to build a global model that converges to the same solution as if the model was trained centrally. Furthermore, the local and global models are synchronized and do not require the adjustment of a matching parameter between local and global models. However, HyFEM is suitable for a vast array of architectures including deep learning architectures, whereas HyFDCA is designed for convex problems like logistic regression and support vector machines. HyFDCA is empirically benchmarked against the aforementioned HyFEM as well as the popular FedAvg in solving convex problem (specifically classification problems) for several popular datasets (MNIST, Covtype, and ). The authors found HyFDCA converges to a lower loss value and higher validation accuracy in less overall time in 33 of 36 comparisons examined and 36 of 36 comparisons examined with respect to the number of outer iterations. Lastly, HyFDCA only requires tuning of one hyperparameter, the number of inner iterations, as opposed to FedAvg (which requires tuning three) or HyFEM (which requires tuning four). In addition to FedAvg and HyFEM being quite difficult to optimize hyperparameters in turn greatly affecting convergence, HyFDCAs single hyperparameter allows for simpler practical implementations and hyperparameter selection methodologies. Current research topics Federated learning has started to emerge as an important research topic in 2015 and 2016, with the first publications on federated averaging in telecommunication settings. Before that, in a thesis work titled A Framework for Multi-source Prefetching Through Adaptive Weight, an approach to aggregate predictions from multiple models trained at three location of a request response cycle with was proposed. Another important aspect of active research is the reduction of the communication burden during the federated learning process. In 2017 and 2018, publications have emphasized the development of resource allocation strategies, especially to reduce communication requirements between nodes with gossip algorithms as well as on the characterization of the robustness to differential privacy attacks. Other research activities focus on the reduction of the bandwidth during training through sparsification and quantization methods, where the machine learning models are sparsified and/or compressed before they are shared with other nodes. Developing ultra-light DNN architectures is essential for device-/edge- learning and recent work recognises both the energy efficiency requirements for future federated learning and the need to compress deep learning, especially during learning. Recent research advancements are starting to consider real-world propagating channels as in previous implementations ideal channels were assumed. Another active direction of research is to develop Federated learning for training heterogeneous local models with varying computation complexities and producing a single powerful global inference model. A learning framework named Assisted learning was recently developed to improve each agents learning capabilities without transmitting private data, models, and even learning objectives. Compared with Federated learning that often requires a central controller to orchestrate the learning and optimization, Assisted learning aims to provide protocols for the agents to optimize and learn among themselves without a global model. Use cases Federated learning typically applies when individual actors need to train models on larger datasets than their own, but cannot afford to share the data in itself with others (e.g., for legal, strategic or economic reasons). The technology yet requires good connections between local servers and minimum computational power for each node. Transportation self-driving cars Self-driving cars encapsulate many machine learning technologies to function computer vision for analyzing obstacles, machine learning for adapting their pace to the environment (e.g., bumpiness of the road). Due to the potential high number of self-driving cars and the need for them to quickly respond to real world situations, traditional cloud approach may generate safety risks. Federated learning can represent a solution for limiting volume of data transfer and accelerating learning processes. Industry 4.0 smart manufacturing In Industry 4.0, there is a widespread adoption of machine learning techniques to improve the efficiency and effectiveness of industrial process while guaranteeing a high level of safety. Nevertheless, privacy of sensitive data for industries and manufacturing companies is of paramount importance. Federated learning algorithms can be applied to these problems as they do not disclose any sensitive data. In addition, FL also implemented for .5 prediction to support Smart city sensing applications. Medicine digital health Federated learning seeks to address the problem of data governance and privacy by training algorithms collaboratively without exchanging the data itself. Todays standard approach of centralizing data from multiple centers comes at the cost of critical concerns regarding patient privacy and data protection. To solve this problem, the ability to train machine learning models at scale across multiple medical institutions without moving the data is a critical technology. Nature Digital Medicine published the paper The Future of Digital Health with Federated Learning in September 2020, in which the authors explore how federated learning may provide a solution for the future of digital health, and highlight the challenges and considerations that need to be addressed. Recently, a collaboration of 20 different institutions around the world validated the utility of training AI models using federated learning. In a paper published in Nature Medicine Federated learning for predicting clinical outcomes in patients with COVID-19, they showcased the accuracy and generalizability of a federated AI model for the prediction of oxygen needs in patients with COVID-19 infections. Furthermore, in a published paper A Systematic Review of Federated Learning in the Healthcare Area From the Perspective of Data Properties and Applications, the authors trying to provide a set of challenges on FL challenges on medical data-centric perspective. A coalition from industry and academia has developed MedPerf, an open source platform that enables validation of medical AI models in real world data. The platform relies technically on federated evaluation of AI models aiming to alleviate concerns of patient privacy and conceptually on diverse benchmark committees to build the specifications of neutral clinically impactful benchmarks. Robotics Robotics includes a wide range of applications of machine learning methods from perception and decision-making to control. As robotic technologies have been increasingly deployed from simple and repetitive tasks (e.g. repetitive manipulation) to complex and unpredictable tasks (e.g. autonomous navigation), the need for machine learning grows. Federated Learning provides a solution to improve over conventional machine learning training methods. In the paper, mobile robots learned navigation over diverse environments using the FL-based method, helping generalization. In the paper, Federated Learning is applied to improve multi-robot navigation under limited communication bandwidth scenarios, which is a current challenge in real-world learning-based robotic tasks. In the paper, Federated Learning is used to learn Vision-based navigation, helping better sim-to-real transfer. Biometrics Federated Learning (FL) is transforming biometric recognition by enabling collaborative model training across distributed data sources while preserving privacy. By eliminating the need to share sensitive biometric templates like fingerprints, facial images, and iris scans, FL addresses privacy concerns and regulatory constraints, allowing for improved model accuracy and generalizability. It mitigates challenges of data fragmentation by leveraging scattered datasets, making it particularly effective for diverse biometric applications such as facial and iris recognition. However, FL faces challenges, including model and data heterogeneity, computational overhead, and vulnerability to security threats like inference attacks. Future directions include developing personalized FL frameworks, enhancing system efficiency, and expanding FL applications to biometric presentation attack detection (PAD) and quality assessment, fostering innovation and robust solutions in privacy-sensitive environments. References External links Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 at eur-lex.europa.eu. Retrieved October 18, 2019. Data minimisation and privacy-preserving techniques in AI systems Archived 2020-07-23 at the Wayback Machine at UK Information Commissioners Office. Retrieved July 22, 2020 Realising the Potential of Data Whilst Preserving Privacy with EyA and Conclave from  at eya.global. Retrieved March 31, 2022. Title Fine-tuning (deep learning) URL https//en.wikipedia.org/wiki/Fine-tuning_(deep_learning) Content In deep learning, fine-tuning is an approach to transfer learning in which the parameters of a pre-trained neural network model are trained on new data. Fine-tuning can be done on the entire neural network, or on only a subset of its layers, in which case the layers that are not being fine-tuned are frozen (i.e., not changed during backpropagation). A model may also be augmented with adapters that consist of far fewer parameters than the original model, and fine-tuned in a parameter-efficient way by tuning the weights of the adapters and leaving the rest of the models weights frozen. For some architectures, such as convolutional neural networks, it is common to keep the earlier layers (those closest to the input layer) frozen, as they capture lower-level features, while later layers often discern high-level features that can be more related to the task that the model is trained on. Models that are pre-trained on large, general corpora are usually fine-tuned by reusing their parameters as a starting point and adding a task-specific layer trained from scratch. Fine-tuning the full model is also common and often yields better results, but is more computationally expensive. Fine-tuning is typically accomplished via supervised learning, but there are also techniques to fine-tune a model using weak supervision. Fine-tuning can be combined with a reinforcement learning from human feedback-based objective to produce language models such as ChatGPT (a fine-tuned version of GPT models) and Sparrow. Robustness Fine-tuning can degrade a models robustness to distribution shifts. One mitigation is to linearly interpolate a fine-tuned models weights with the weights of the original model, which can greatly increase out-of-distribution performance while largely retaining the in-distribution performance of the fine-tuned model. Variants Low-rank adaptation Low-rank adaptation (LoRA) is an adapter-based technique for efficiently fine-tuning models. The basic idea is to design a low-rank matrix that is then added to the original matrix. An adapter, in this context, is a collection of low-rank matrices which, when added to a base model, produces a fine-tuned model. It allows for performance that approaches full-model fine-tuning with lower space requirements. A language model with billions of parameters may be LoRA fine-tuned with only several millions of parameters. LoRA-based fine-tuning has become popular in the Stable Diffusion community. Support for LoRA was integrated into the Diffusers library from Hugging Face. Support for LoRA and similar techniques is also available for a wide range of other models through Hugging Faces Parameter-Efficient Fine-Tuning (PEFT) package. Representation fine-tuning Representation fine-tuning (ReFT) is a technique developed by researchers at Stanford University aimed at fine-tuning large language models (LLMs) by modifying less than 1 of their representations. Unlike traditional parameter-efficient fine-tuning (PEFT) methods, which mainly focus on updating weights, ReFT targets specific parts of the model relevant to the task being fine-tuned. This approach is based on the understanding that deep learning models encode rich semantic information in their representations, suggesting that modifying representations might be a more effective strategy than updating weights. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations and train interventions that manipulate a small fraction of model representations to steer model behaviors towards solving downstream tasks at inference time. One specific method within the ReFT family is Low-rank Linear Subspace ReFT (LoReFT), which intervenes on hidden representations in the linear subspace spanned by a low-rank projection matrix. LoReFT can be seen as the representation-based equivalent of Low-rank Adaptation (LoRA). Applications Natural language processing Fine-tuning is common in natural language processing (NLP), especially in the domain of language modeling. Large language models like OpenAIs series of GPT foundation models can be fine-tuned on data for specific downstream NLP tasks (tasks that use a pre-trained model) to improve performance over the unmodified pre-trained model. Commercial models Commercially-offered large language models can sometimes be fine-tuned if the provider offers a fine-tuning API. As of June 19, 2023, language model fine-tuning APIs are offered by OpenAI and Microsoft Azures Azure OpenAI Service for a subset of their models, as well as by Google Cloud Platform for some of their PaLM models, and by others. Not all commercial models currently support fine-tuning. Publicly distributed models Companies such as Meta (Llama LLM family), Alibaba (Qwen LLM family) and Mistral AI (Mixtral) have published large language models with different sizes on GitHub which can be fine-tuned. They are often inaccurately called open-source. The companies have encouraged people to use and modify them and there has been no prominent public case of their proprietary license restrictions being enforced. Locally runnable models can be advantageous for companies in terms of data security, because the companies can control where the model is hosted. See also Catastrophic forgetting Continual learning Domain adaptation Foundation model Hyperparameter optimization Overfitting Title Flow-based generative model URL https//en.wikipedia.org/wiki/Flow-based_generative_model Content A flow-based generative model is a generative model used in machine learning that explicitly models a probability distribution by leveraging normalizing flow, which is a statistical method using the change-of-variable law of probabilities to transform a simple distribution into a complex one. The direct modeling of likelihood provides many advantages. For example, the negative log-likelihood can be directly computed and minimized as the loss function. Additionally, novel samples can be generated by sampling from the initial distribution, and applying the flow transformation. In contrast, many alternative generative modeling methods such as variational autoencoder (VAE) and generative adversarial network do not explicitly represent the likelihood function. Method Let z 0 displaystyle z_0 be a (possibly multivariate) random variable with distribution p 0 ( z 0 ) displaystyle p_0(z_0) . For  , . . . , K displaystyle ,...,K , let z  . The functions f 1 , . . . , f K displaystyle f_1,...,f_K should be invertible, i.e. the inverse function f i 1 displaystyle f_i-1 exists. The final output z K displaystyle z_K models the target distribution. The log likelihood of z K displaystyle z_K is (see derivation) log p K ( z K )  log p 0 ( z 0 )  , . . . , f K displaystyle f_1,...,f_K should be easily invertible, and the determinants of their Jacobians should be simple to compute. In practice, the functions f 1 , . . . , f K displaystyle f_1,...,f_K are modeled using deep neural networks, and are trained to minimize the negative log-likelihood of data samples from the target distribution. These architectures are usually designed such that only the forward pass of the neural network is required in both the inverse and the Jacobian determinant calculations. Examples of such architectures include NICE, RealNVP, and Glow. Derivation of log likelihood Consider z 1 displaystyle z_1 and z 0 displaystyle z_0 . Note that z 0  f 1 1 ( z 1 ) displaystyle z_0f_1-1(z_1) . By the change of variable formula, the distribution of z 1 displaystyle z_1 is p 1 ( z 1 )  p 0 ( z 0 )  det d f 1 1 ( z 1 ) d z 1  displaystyle p_1(z_1)p_0(z_0)leftdet frac df_1-1(z_1)dz_1right Where det d f 1 1 ( z 1 ) d z 1 displaystyle det frac df_1-1(z_1)dz_1 is the determinant of the Jacobian matrix of f 1 1 displaystyle f_1-1 . By the inverse function theorem p 1 ( z 1 )  p 0 ( z 0 )  det ( d f 1 ( z 0 ) d z 0 ) 1  displaystyle p_1(z_1)p_0(z_0)leftdet left(frac df_1(z_0)dz_0right)-1right By the identity det ( A 1 )  det ( A ) 1 displaystyle det(A-1)det(A)-1 (where A displaystyle A is an invertible matrix), we have p 1 ( z 1 )  p 0 ( z 0 )  det d f 1 ( z 0 ) d z 0  1 displaystyle p_1(z_1)p_0(z_0)leftdet frac df_1(z_0)dz_0right-1 The log likelihood is thus log p 1 ( z 1 )  log p 0 ( z 0 ) log  det d f 1 ( z 0 ) d z 0  displaystyle log p_1(z_1)log p_0(z_0)-log leftdet frac df_1(z_0)dz_0right In general, the above applies to any z i displaystyle z_i and z i 1 displaystyle z_i-1 . Since log p i ( z i ) displaystyle log p_i(z_i) is equal to log p i 1 ( z i 1 ) displaystyle log p_i-1(z_i-1) subtracted by a non-recursive term, we can infer by induction that log p K ( z K )  log p 0 ( z 0 ) . Denoting p displaystyle p_theta  the models likelihood and p displaystyle p the target distribution to learn, the (forward) KL-divergence is D K L  p ( x )   p ( x )   E p ( x )  log ( p ( x ) )   E p ( x )  log ( p ( x ) )  displaystyle D_KLp(x)p_theta (x)-mathbb E _p(x)log(p_theta (x))mathbb E _p(x)log(p(x)) The second term on the right-hand side of the equation corresponds to the entropy of the target distribution and is independent of the parameter displaystyle theta  we want the model to learn, which only leaves the expectation of the negative log-likelihood to minimize under the target distribution. This intractable term can be approximated with a Monte-Carlo method by importance sampling. Indeed, if we have a dataset  x i  . A pseudocode for training normalizing flows is as follows INPUT. dataset x 1  n displaystyle x_1n , normalizing flow model f ( ) , p 0 displaystyle f_theta (cdot ),p_0 . SOLVE. max j ln p ( x j ) displaystyle max _theta sum _jln p_theta (x_j) by gradient descent RETURN.  displaystyle hat theta  Variants Planar Flow The earliest example. Fix some activation function h displaystyle h , and . The Jacobian is  det ( I  h ( w , z  b ) u w T )    1  h ( w , z  b ) u , w  displaystyle det(Ih(langle w,zrangle b)uwT)1h(langle w,zrangle b)langle u,wrangle  . For it to be invertible everywhere, it must be nonzero everywhere. For example, . Nonlinear Independent Components Estimation (NICE) Let x , z R 2 n displaystyle x,zin mathbb R 2n be even-dimensional, and split them in the middle. Then the normalizing flow functions are   . f 1 displaystyle f_theta -1 is just z 1  x 1 , z 2  x 2 m ( x 1 ) displaystyle z_1x_1,z_2x_2-m_theta (x_1) , and the Jacobian is just 1, that is, the flow is volume-preserving. When . Real Non-Volume Preserving (Real NVP) The Real Non-Volume Preserving model generalizes NICE model by ,) . The NICE model is recovered by setting  . Since the Real NVP map keeps the first and second halves of the vector x displaystyle x separate, its usually required to add a permutation ( x 1 , x 2 ) ( x 2 , x 1 ) displaystyle (x_1,x_2)mapsto (x_2,x_1) after every Real NVP layer. Generative Flow (Glow) In generative flow model, each layer has 3 parts channel-wise affine transform y c i  . invertible 1x1 convolution z c i  . Here K displaystyle K is any invertible matrix. Real NVP, with Jacobian as described in Real NVP. The idea of using the invertible 1x1 convolution is to permute all layers in general, instead of merely permuting the first and second half, as in Real NVP. Masked autoregressive flow (MAF) An autoregressive model of a distribution on R n displaystyle mathbb R n is defined as the following stochastic process x 1 N ( 1 , 1 2 ) x 2 N ( 2 ( x 1 ) , 2 ( x 1 ) 2 ) x n N ( n ( x 1  n 1 ) , n ( x 1  n 1 ) 2 ) displaystyle beginalignedx_1sim N(mu _1,sigma _12)x_2sim N(mu _2(x_1),sigma _2(x_1)2)cdots x_nsim N(mu _n(x_1n-1),sigma _n(x_1n-1)2)endaligned where i  R i 1 R displaystyle mu _imathbb R i-1to mathbb R  and i  R i 1 ( 0 , ) displaystyle sigma _imathbb R i-1to (0,infty ) are fixed functions that define the autoregressive model. By the reparameterization trick, the autoregressive model is generalized to a normalizing flow x 1  1  1 z 1 x 2  2 ( x 1 )  2 ( x 1 ) z 2 x ) . The forward mapping is slow (because its sequential), but the backward mapping is fast (because its parallel). The Jacobian matrix is lower-diagonal, so the Jacobian is 1 2 ( x 1 ) n ( x 1  n 1 ) displaystyle sigma _1sigma _2(x_1)cdots sigma _n(x_1n-1) . Reversing the two maps f displaystyle f_theta  and f 1 displaystyle f_theta -1 of MAF results in Inverse Autoregressive Flow (IAF), which has fast forward mapping and slow backward mapping. Continuous Normalizing Flow (CNF) Instead of constructing flow by function composition, another approach is to formulate the flow as a continuous-time dynamic. Let z 0 displaystyle z_0 be the latent variable with distribution p ( z 0 ) displaystyle p(z_0) . Map this latent variable to data space with the following flow function .g. neural networks. The inverse function is then naturally z 0  F 1 ( x )  z T  T 0 f ( z t , t ) d . Here, free-form means that there is no restriction on the Jacobians form. It is contrasted with previous discrete models of normalizing flow, where the Jacobian is carefully designed to be only upper- or lower-diagonal, so that the Jacobian can be evaluated efficiently. The trace can be estimated by Hutchinsons trickGiven any matrix W R n n displaystyle Win mathbb R ntimes n , and any random u R n displaystyle uin mathbb R n with E  u u T   I displaystyle EuuTI , we have E  u T W u   t r ( W ) displaystyle EuTWutr(W) . (Proof expand the expectation directly.)Usually, the random vector is sampled from N ( 0 , I ) displaystyle N(0,I) (normal distribution) or  n 1 / 2  n displaystyle pm n-1/2n (Radamacher distribution). When f displaystyle f is implemented as a neural network, neural ODE methods would be needed. Indeed, CNF was first proposed in the same paper that proposed neural ODE. There are two main deficiencies of CNF, one is that a continuous flow must be a homeomorphism, thus preserve orientation and ambient isotopy (for example, its impossible to flip a left-hand to a right-hand by continuous deforming of space, and its impossible to turn a sphere inside out, or undo a knot), and the other is that the learned flow f displaystyle f might be ill-behaved, due to degeneracy (that is, there are an infinite number of possible f displaystyle f that all solve the same problem). By adding extra dimensions, the CNF gains enough freedom to reverse orientation and go beyond ambient isotopy (just like how one can pick up a polygon from a desk and flip it around in 3-space, or unknot a knot in 4-space), yielding the augmented neural ODE. Any homeomorphism of R n displaystyle mathbb R n can be approximated by a neural ODE operating on R 2 n  1 displaystyle mathbb R 2n1 , proved by combining Whitney embedding theorem for manifolds and the universal approximation theorem for neural networks. To regularize the flow f displaystyle f , one can impose regularization losses. The paper proposed the following regularization loss based on optimal transport theory K 0 T f ( z t , t ) 2 d t  J 0 T z f ( z t , t ) F 2 d t displaystyle lambda _Kint _0Tleftf(z_t,t)right2dtlambda _Jint _0Tleftnabla _zf(z_t,t)right_F2dt where K , J  0 displaystyle lambda _K,lambda _J0 are hyperparameters. The first term punishes the model for oscillating the flow field over time, and the second term punishes it for oscillating the flow field over space. Both terms together guide the model into a flow that is smooth (not bumpy) over space and time. Downsides Despite normalizing flows success in estimating high-dimensional densities, some downsides still exist in their designs. First of all, their latent space where input data is projected onto is not a lower-dimensional space and therefore, flow-based models do not allow for compression of data by default and require a lot of computation. However, it is still possible to perform image compression with them. Flow-based models are also notorious for failing in estimating the likelihood of out-of-distribution samples (i.e. samples that were not drawn from the same distribution as the training set). Some hypotheses were formulated to explain this phenomenon, among which the typical set hypothesis, estimation issues when training models, or fundamental issues due to the entropy of the data distributions. One of the most interesting properties of normalizing flows is the invertibility of their learned bijective map. This property is given by constraints in the design of the models (cf. RealNVP, Glow) which guarantee theoretical invertibility. The integrity of the inverse is important in order to ensure the applicability of the change-of-variable theorem, the computation of the Jacobian of the map as well as sampling with the model. However, in practice this invertibility is violated and the inverse map explodes because of numerical imprecision. Applications Flow-based generative models have been applied on a variety of modeling tasks, including Audio generation Image generation Molecular graph generation Point-cloud modeling Video generation Lossy image compression Anomaly detection References External links Flow-based Deep Generative Models Normalizing flow models Title Flux (machine-learning framework) URL https//en.wikipedia.org/wiki/Flux_(machine-learning_framework) Content Flux is an open-source machine-learning software library and ecosystem written in Julia. Its current stable release is .15.0 . It has a layer-stacking-based interface for simpler models, and has a strong support on interoperability with other Julia packages instead of a monolithic design. For example, GPU support is implemented transparently by CuArrays.jl. This is in contrast to some other machine learning frameworks which are implemented in other languages with Julia bindings, such as TensorFlow.jl (the unofficial wrapper, now deprecated), and thus are more limited by the functionality present in the underlying implementation, which is often in C or C. Flux joined NumFOCUS as an affiliated project in December of 2021. Fluxs focus on interoperability has enabled, for example, support for Neural Differential Equations, by fusing Flux.jl and DifferentialEquations.jl into DiffEqFlux.jl. Flux supports recurrent and convolutional networks. It is also capable of differentiable programming through its source-to-source automatic differentiation package, Zygote.jl. Julia is a popular language in machine-learning and Flux.jl is its most highly regarded machine-learning repository (Lux.jl is another more recent, that shares a lot of code with Flux.jl). A demonstration compiling Julia code to run in Googles tensor processing unit (TPU) received praise from Google Brain AI lead Jeff Dean. Flux has been used as a framework to build neural networks that work with homomorphic encrypted data without ever decrypting it. This kind of application is envisioned to be central for privacy to future API using machine-learning models. Flux.jl is an intermediate representation for running high level programs on CUDA hardware. It was the predecessor to CUDAnative.jl which is also a GPU programming language. See also Differentiable programming Comparison of deep-learning software Title Force control URL https//en.wikipedia.org/wiki/Force_control Content Force control is the control of the force with which a machine or the manipulator of a robot acts on an object or its environment. By controlling the contact force, damage to the machine as well as to the objects to be processed and injuries when handling people can be prevented. In manufacturing tasks, it can compensate for errors and reduce wear by maintaining a uniform contact force. Force control achieves more consistent results than position control, which is also used in machine control. Force control can be used as an alternative to the usual motion control, but is usually used in a complementary way, in the form of hybrid control concepts. The acting force for control is usually measured via force transducers or estimated via the motor current. Force control has been the subject of research for almost three decades and is increasingly opening up further areas of application thanks to advances in sensor and actuator technology and new control concepts. Force control is particularly suitable for contact tasks that serve to mechanically process workpieces, but it is also used in telemedicine, service robot and the scanning of surfaces. For force measurement, force sensors exist that can measure forces and torques in all three spatial directions. Alternatively, the forces can also be estimated without sensors, e.g. on the basis of the motor currents. Indirect force control by modeling the robot as a mechanical resistance (impedance) and direct force control in parallel or hybrid concepts are used as control concepts. Adaptive approaches, fuzzy controllers and machine learning for force control are currently the subject of research. General Controlling the contact force between a manipulator and its environment is an increasingly important task in the environment of mechanical manufacturing, as well as industrial and service robot. One motivation for the use of force control is safety for man and machine. For various reasons, movements of the robot or machine parts may be blocked by obstacles while the program is running. In service robot these can be moving objects or people, in industrial robotics problems can occur with cooperating robots, changing work environments or an inaccurate environmental model. If the trajectory is misaligned in classical motion control and thus it is not possible to approach the programmed robot pose(s), the motion control will increase the manipulated variable - usually the motor current - in order to correct the position error. The increase of the manipulated variable can have the following effects The obstacle is removed or damaged/destroyed. The machine is damaged or destroyed. The manipulated variable limits are exceeded and the robot controller switches off. A force control system can prevent this by regulating the maximum force of the machine in these cases, thus avoiding damage or making collisions detectable at an early stage. In mechanical manufacturing tasks, unevenness of the workpiece often leads to problems with motion control. As can be seen in the adjacent figure, surface unevenness causes the tool to penetrate too far into the surface during position control (red) P 1 displaystyle P_1 or lose contact with the workpiece during position control (red) P 2 displaystyle P_2 . This results, for example, in an alternating force effect on the workpiece and tool during grinding and polishing. Force control (green) is useful here, as it ensures uniform material removal through constant contact with the workpiece. Application In force control, a basic distinction can be made between applications with pronounced contact and applications with potential contact. We speak of pronounced contact when the contact of the machine with the environment or the workpiece is a central component of the task and is explicitly controlled. This includes, above all, tasks of mechanical deformation and surface machining. In tasks with potential contact, the process function variable is the positioning of the machine or its parts. Larger contact forces between machine and environment occur due to dynamic environment or inaccurate environment model. In this case, the machine should yield to the environment and avoid large contact forces. The main applications of force control today are mechanical manufacturing operations. This means in particular manufacturing tasks such as grinding, polishing and deburring as well as force-controlled processes such as controlled joining, bending and pressing of bolts into prefabricated bores. Another common use of force control is scanning unknown surfaces. Here, force control is used to set a constant contact pressure in the normal direction of the surface and the scanning head is moved in the surface direction via position control. The surface can then be described in Cartesian coordinates via direct kinematics. Other applications of force control with potential contact can be found in medical technology and cooperating robots. Robots used in telemedicine, i.e. robot-assisted medical operations, can avoid injuries more effectively via force control. In addition, direct feedback of the measured contact forces to the operator by means of a force feedback control device is of great interest here. Possible applications for this extend to internet-based teleoperations. In principle, force control is also useful wherever machines and robots cooperate with each other or with humans, as well as in environments where the environment is not described exactly or is dynamic and cannot be described exactly. Here, force control helps to deal with obstacles and deviations in the environmental model and to avoid damage. History The first important work on force control was published in 1980 by John Kenneth Salisbury at Stanford University. In it, he describes a method for active stiffness control, a simple form of impedance control. However, the method does not yet allow a combination with motion control, but here force control is performed in all spatial directions. The position of the surface must therefore be known. Because of the lower performance of robot controllers of that time, force control could only be performed on mainframe computers. Thus, a controller cycle of 100 ms was achieved. In 1981, Raibert and Craig presented a paper on hybrid force/position control which is still important today. In this paper, they describe a method in which a matrix (separation matrix) is used to explicitly specify for all spatial directions whether motion or force control is to be used. Raibert and Craig merely sketch the controller concepts and assume them to be feasible. In 1989, Koivo presented an extended exposition of the concepts of Raibert and Craig. Precise knowledge of the surface position is still necessary here, which still does not allow for the typical tasks of force control today, such as scanning surfaces. Force control has been the subject of intense research over the past two decades and has made great strides with the advancement of sensor technology and control algorithms. For some years now, the major automation technology manufacturers have been offering software and hardware packages for their controllers to allow force control. Modern machine controllers are capable of force control in one spatial direction in real time computing with a cycle time of less than 10 ms. Force measurement To close the force control loop in the sense of a closed-loop control, the instantaneous value of the contact force must be known. The contact force can either be measured directly or estimated. Direct force measurement The trivial approach to force control is the direct measurement of the occurring contact forces via force/torque sensors at the end effector of the machine or at the wrist of the industrial robot. Force/torque sensors measure the occurring forces by measuring the deformation at the sensor. The most common way to measure deformation is by means of strain gauges. In addition to the widely used strain gauges made of variable electrical resistances, there are also other versions that use piezoelectric, optical or capacitive principles for measurement. In practice, however, they are only used for special applications. Capacitive strain gages, for example, can also be used in the high-temperature range above 1000 C. Strain gages are designed to have as linear a relationship as possible between strain and electrical resistance within the working space. In addition, several possibilities exist to reduce measurement errors and interference. To exclude temperature influences and increase measurement reliability, two strain gauges can be arranged in a complementary manner. Modern force/torque sensors measure both forces and torques in all three spatial directions and are available with almost any value range. The accuracy is usually in the per mil range of the maximum measured value. The sampling rates of the sensors are in the range of about 1 kHz. An extension of the 6-axis force/torque sensors are 12- and 18-axis sensors which, in addition to the six force or torque components, are also capable of measuring six velocity and acceleration components each. Six-axis force/torque sensor In modern applications, so-called six-axis force/torque sensors are frequently used. These are mounted between the robot hand and the end effector and can record both forces and torques in all three spatial directions. For this purpose, they are equipped with six or more strain gauges (possibly strain measurement bridges) that record deformations in the micrometer range. These deformations are converted into three force and torque components each via a calibration matrix. Force/torque sensors contain a digital signal processor that continuously acquires and filters the sensor data (strain) in parallel, calculates the measurement data (forces/torques) and makes it available via the sensors communication interface. The measured values correspond to the forces at the sensor and usually still have to be converted into the forces and torques at the end effector or tool via a suitable transformation. Since force/torque sensors are still relatively expensive (between 4,000 and 15,000) and very sensitive to overloads and disturbances, they - and thus force control - have been reluctantly used in industry. Indirect force measurement or estimation is one solution, allowing force control without costly and disturbance-prone force sensors. Force estimation A cost-saving alternative to direct force measurement is force estimation (also known as indirect force measurement). This makes it possible to dispense with the use of force/torque sensors. In addition to cost savings, dispensing with these sensors has other advantages Force sensors are usually the weakest link in the mechanical chain of the machine or robot system, so dispensing with them brings greater stability and less susceptibility to mechanical faults. In addition, dispensing with force/torque sensors brings greater safety, since there is no need for sensor cables to be routed out and protected directly at the manipulators wrist. A common method for indirect force measurement or force estimation is the measurement of the motor currents applied for motion control. With some restrictions, these are proportional to the torque applied to the driven robot axis. Adjusted for gravitational, inertial and frictional effects, the motor currents are largely linear to the torques of the individual axes. The contact force at the end effector can be determined via the torques thus known. Separation of dynamic and static forces During force measurement and force estimation, filtering of the sensor signals may be necessary. Numerous side effects and secondary forces can occur which do not correspond to the measurement of the contact force. This is especially true if a larger load mass is mounted on the manipulator. This interferes with the force measurement when the manipulator moves with high accelerations. To be able to adjust the measurement for side effects, both an accurate dynamic model of the machine and a model or estimate of the load must be available. This estimate can be determined via reference movements (free movement without object contact). After estimating the load, the measurement or estimate of the forces can be adjusted for Coriolis, centripetal and centrifugal forces, gravitational and frictional effects, and inertia. Adaptive approaches can also be used here to continuously adjust the estimate of the load. Control concepts Various control concepts are used for force control. Depending on the desired behavior of the system, a distinction is made between the concepts of direct force control and indirect control via specification of compliance or mechanical impedance. As a rule, force control is combined with motion control. Concepts for force control have to consider the problem of coupling between force and position If the manipulator is in contact with the environment, a change of the position also means a change of the contact force. Impedance control Impedance control, or compliance control, regulates the compliance of the system, i.e., the link between force and position upon object contact. Compliance is defined in the literature as a measure of the robots ability to counteract contact forces. There are passive and active approaches to this. Here, the compliance of the robot system is modeled as mechanical impedance, which describes the relationship between applied force and resulting velocity. Here, the robots machine or manipulator is considered as a mechanical resistance with positional constraints imposed by the environment. Accordingly, the causality of mechanical impedance describes that a movement of the robot results in a force. In mechanical admittance, on the other hand, a force applied to the robot results in a resulting motion. Passive impedance control Passive compliance control (also known as compliance control) does not require force measurement because there is no explicit force control. Instead, the manipulator and/or end effector is flexibly designed in a way that can minimize contact forces that occur during the task to be performed. Typical applications include insertion and gripping operations. The end effector is designed in such a way that it allows translational and rotational deviations orthogonal to the gripping or insertion direction, but has high stiffness in the gripping or insertion direction. The figure opposite shows a so-called Remote Center of Compliance (RCC) that makes this possible. As an alternative to an RCC, the entire machine can also be made structurally elastic. Passive impedance control is a very good solution in terms of system dynamics, since there are no latency due to the control. However, passive compliance control is often limited by the mechanical specification of the end effector in the task and cannot be readily applied to different and changing tasks or environmental conditions. Active impedance control Active compliance control refers to the control of the manipulator based on a deviation of the end effector. This is particularly suitable for guiding robots by an operator, for example as part of a teach-in process. Active compliance control is based on the idea of representing the system of machine and environment as a spring-damper-mass system. The force F displaystyle F and the motion (position x ( t ) displaystyle x(t)!, , velocity x ( t ) displaystyle dot x(t) , and acceleration x ( t ) displaystyle ddot x(t) are directly related via the spring-damper-mass equation F ( t )  c x ( t )  d x ( t )  m x ( t ) displaystyle F(t)ccdot x(t)dcdot dot x(t)mcdot ddot x(t) The compliance or mechanical impedance of the system is determined by the stiffness c displaystyle c , the damping d displaystyle d and the inertia m displaystyle m and can be influenced by these three variables. The control is given a mechanical target impedance via these three variables, which is achieved by the machine control. The figure shows the block diagram of a force-based impedance control. The impedance in the block diagram represents the mentioned components L, A and . A position-based impedance control can be designed analogously with internal position or motion control. Alternatively and analogously, the compliance (admittance) can be controlled instead of the resistance. In contrast to the impedance control, the admittance appears in the control law as the reciprocal of the impedance. Direct force control The above concepts are so-called indirect force control, since the contact force is not explicitly specified as a command variable, but is determined indirectly via the controller parameters damping, stiffness and (virtual) mass. Direct force control is presented below. Direct force control uses the desired force as a setpoint within a closed control loop. It is implemented as a parallel force/position control in the form of a cascade control or as a hybrid force/position control in which switching takes place between position and force control. Parallel force/position control One possibility for force control is parallel force/position control. The control is designed as a cascade control and has an external force control loop and an internal position control loop. As shown in the following figure, a corresponding infeed correction is calculated from the difference between the nominal and actual force. This infeed correction is offset against the position command values, whereby in the case of the fusion of X s o l l displaystyle X_soll and X k o r r displaystyle X_korr , the position command of force control ( X k o r r displaystyle X_korr )has a higher priority, i.e. a position error is tolerated in favor of the correct force control. The offset value is the input variable for the inner position control loop. Analogous to an inner position control, an inner velocity control can also take place, which has a higher dynamic. In this case, the inner control loop should have a saturation in order not to generate a (theoretically) arbitrarily increasing velocity in the free movement until contact is made. Hybrid force/position control An improvement over the above concepts is offered by hybrid force/position control, which works with two separate control systems and can also be used with hard, inflexible contact surfaces. In hybrid force/position control, the space is divided into a constrained and an unconstrained space. The constrained space contains restrictions, for example in the form of obstacles, and does not allow free movement the unconstrained space allows free movement. Each dimension of the space is either constrained or unconstrained. In hybrid force control, force control is used for the restricted space, and position control is used for the unrestricted space. The figure shows such a control. The matrix indicates which space directions are restricted and is a diagonal matrix consisting of zeros and ones. Which spatial direction is restricted and which is unrestricted can, for example, be specified statically. Force and position control is then explicitly specified for each spatial direction the matrix is then static. Another possibility is to switch the matrix dynamically on the basis of force measurement. In this way, it is possible to switch from position control to force control for individual spatial directions when contact or collision is established. In the case of contact tasks, all spatial directions would be motion-controlled in the case of free movement, and after contact is established, the contact direction would be switched to force control by selecting the appropriate matrix . Research In recent years, the subject of research has increasingly been adaptive concepts, the use of fuzzy control system and machine learning, and force-based whole-body control. Adaptive force control The previously mentioned, non-adaptive concepts are based on an exact knowledge of the dynamic process parameters. These are usually determined and adjusted by experiments and calibration. Problems can arise due to measurement errors and variable loads. In adaptive force control, position-dependent and thus time-variable parts of the system are regarded as parameter fluctuations and are constantly adapted in the course of the control by adaptation. Due to the changing control, no guarantee can be given for dynamic stability of the system. Adaptive control is therefore usually first used offline and the results are intensively tested in simulation before being used on the real system. Fuzzy control and machine learning A prerequisite for the application of classical design methods is an explicit system model. If this is difficult or impossible to represent, fuzzy controllers or machine learning can be considered. By means of fuzzy logic, knowledge acquired by humans can be converted into a control behavior in the form of fuzzy control specifications. Explicit specification of the controller parameters is thus no longer necessary. Approaches using machine learning, moreover, no longer require humans to create the control behavior, but use machine learning as the basis for control. Whole body control Due to the high complexity of modern robotic systems, such as humanoid robots, a large number of actuated degrees of freedom must be controlled. In addition, such systems are increasingly used in the direct environment of humans. Accordingly, concepts from force and impedance control are specifically used in this area to increase safety, as this allows the robot to interact with the environment and humans in a compliant manner. References Bibliography Bruno Siciliano, Luigi Villani (2000), Robot Force Control, Springer, ISBN 0-7923-7733-8 Wolfgang Weber (2002), Industrieroboter. Methoden der Steuerung und Regelung, Fachbuchverlag Leipzig, ISBN 3-446-21604-9 Lorenzo Sciavicco, Bruno Siciliano (1999), Modelling and Control of Robot Manipulators, Springer, ISBN 1-85233-221-2 Klaus Richter (1991), Kraftregelung elastischer Roboter, VDI-Verlag, ISBN 3-18-145908-9 Title Formal concept analysis URL https//en.wikipedia.org/wiki/Formal_concept_analysis Content In information science, formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the objects sharing some set of properties and each sub-concept in the hierarchy represents a subset of the objects (as well as a superset of the properties) in the concepts above it. The term was introduced by Rudolf Wille in 1981, and builds on the mathematical theory of lattices and ordered sets that was developed by Garrett Birkhoff and others in the 1930s. Formal concept analysis finds practical application in fields including data mining, text mining, machine learning, knowledge management, semantic web, software development, chemistry and biology. Overview and history The original motivation of formal concept analysis was the search for real-world meaning of mathematical order theory. One such possibility of very general nature is that data tables can be transformed into algebraic structures called complete lattices, and that these can be utilized for data visualization and interpretation. A data table that represents a heterogeneous relation between objects and attributes, tabulating pairs of the form object g has attribute m, is considered as a basic data type. It is referred to as a formal context. In this theory, a formal concept is defined to be a pair (A, B), where A is a set of objects (called the extent) and B is a set of attributes (the intent) such that the extent A consists of all objects that share the attributes in B, and dually the intent B consists of all attributes shared by the objects in A. In this way, formal concept analysis formalizes the semantic notions of extension and intension. The formal concepts of any formal context can as explained below be ordered in a hierarchy called more formally the contexts concept lattice. The concept lattice can be graphically visualized as a line diagram, which then may be helpful for understanding the data. Often however these lattices get too large for visualization. Then the mathematical theory of formal concept analysis may be helpful, e.g., for decomposing the lattice into smaller pieces without information loss, or for embedding it into another structure which is easier to interpret. The theory in its present form goes back to the early 1980s and a research group led by Rudolf Wille, Bernhard Ganter and Peter Burmeister at the Technische Universit t Darmstadt. Its basic mathematical definitions, however, were already introduced in the 1930s by Garrett Birkhoff as part of general lattice theory. Other previous approaches to the same idea arose from various French research groups, but the Darmstadt group normalised the field and systematically worked out both its mathematical theory and its philosophical foundations. The latter refer in particular to Charles S. Peirce, but also to the Port-Royal Logic. Motivation and philosophical background In his article Restructuring Lattice Theory (1982), initiating formal concept analysis as a mathematical discipline, Wille starts from a discontent with the current lattice theory and pure mathematics in general The production of theoretical results often achieved by elaborate mental gymnastics were impressive, but the connections between neighboring domains, even parts of a theory were getting weaker. Restructuring lattice theory is an attempt to reinvigorate connections with our general culture by interpreting the theory as concretely as possible, and in this way to promote better communication between lattice theorists and potential users of lattice theory This aim traces back to the educationalist Hartmut von Hentig, who in 1972 pleaded for restructuring sciences in view of better teaching and in order to make sciences mutually available and more generally (i.e. also without specialized knowledge) critiqueable. Hence, by its origins formal concept analysis aims at interdisciplinarity and democratic control of research. It corrects the starting point of lattice theory during the development of formal logic in the 19th century. Then and later in model theory a concept as unary predicate had been reduced to its extent. Now again, the philosophy of concepts should become less abstract by considering the intent. Hence, formal concept analysis is oriented towards the categories extension and intension of linguistics and classical conceptual logic. Formal concept analysis aims at the clarity of concepts according to Charles S. Peirces pragmatic maxim by unfolding observable, elementary properties of the subsumed objects. In his late philosophy, Peirce assumed that logical thinking aims at perceiving reality, by the triade concept, judgement and conclusion. Mathematics is an abstraction of logic, develops patterns of possible realities and therefore may support rational communication. On this background, Wille defines The aim and meaning of Formal Concept Analysis as mathematical theory of concepts and concept hierarchies is to support the rational communication of humans by mathematically developing appropriate conceptual structures which can be logically activated. Example The data in the example is taken from a semantic field study, where different kinds of bodies of water were systematically categorized by their attributes. For the purpose here it has been simplified. The data table represents a formal context, the line diagram next to it shows its concept lattice. Formal definitions follow below. The above line diagram consists of circles, connecting line segments, and labels. Circles represent formal concepts. The lines allow to read off the subconcept-superconcept hierarchy. Each object and attribute name is used as a label exactly once in the diagram, with objects below and attributes above concept circles. This is done in a way that an attribute can be reached from an object via an ascending path if and only if the object has the attribute. In the diagram shown, e.g. the object reservoir has the attributes stagnant and constant, but not the attributes temporary, running, natural, maritime. Accordingly, puddle has exactly the characteristics temporary, stagnant and natural. The original formal context can be reconstructed from the labelled diagram, as well as the formal concepts. The extent of a concept consists of those objects from which an ascending path leads to the circle representing the concept. The intent consists of those attributes to which there is an ascending path from that concept circle (in the diagram). In this diagram the concept immediately to the left of the label reservoir has the intent stagnant and natural and the extent puddle, maar, lake, pond, tarn, pool, lagoon, and sea. Formal contexts and concepts A formal context is a triple . For subsets A G of objects and subsets B M of attributes, one defines two derivation operators as follows .e., a set of all attributes shared by all objects from A, and dually .e., a set of all objects sharing all attributes from B. Applying either derivation operator and then the other constitutes two closure operators A ). The derivation operators define a Galois connection between sets of objects and of attributes. This is why in French a concept lattice is sometimes called a treillis de Galois (Galois lattice). With these derivation operators, Wille gave an elegant definition of a formal concept a pair (A,B) is a formal concept of a context (G, M, I) provided that A G, B M, . Equivalently and more intuitively, (A,B) is a formal concept precisely when every object in A has every attribute in B, for every object in G that is not in A, there is some attribute in B that the object does not have, for every attribute in M that is not in B, there is some object in A that does not have that attribute. For computing purposes, a formal context may be naturally represented as a (0,1)-matrix K in which the rows correspond to the objects, the columns correspond to the attributes, and each entry ki,j equals to 1 if object i has attribute j. In this matrix representation, each formal concept corresponds to a maximal submatrix (not necessarily contiguous) all of whose elements equal 1. It is however misleading to consider a formal context as boolean, because the negated incidence (object g does not have attribute m) is not concept forming in the same way as defined above. For this reason, the values 1 and 0 or TRUE and FALSE are usually avoided when representing formal contexts, and a symbol like is used to express incidence. Concept lattice of a formal context The concepts (Ai, Bi) of a context K can be (partially) ordered by the inclusion of extents, or, equivalently, by the dual inclusion of intents. An order on the concepts is defined as follows for any two concepts (, ) and (, ) of K, we say that (, ) (, ) precisely when . Equivalently, (, ) (, ) whenever . In this order, every set of formal concepts has a greatest common subconcept, or meet. Its extent consists of those objects that are common to all extents of the set. Dually, every set of formal concepts has a least common superconcept, the intent of which comprises all attributes which all objects of that set of concepts have. These meet and join operations satisfy the axioms defining a lattice, in fact a complete lattice. Conversely, it can be shown that every complete lattice is the concept lattice of some formal context (up to isomorphism). Attribute values and negation Real-world data is often given in the form of an object-attribute table, where the attributes have values. Formal concept analysis handles such data by transforming them into the basic type of a (one-valued) formal context. The method is called conceptual scaling. The negation of an attribute m is an attribute m, the extent of which is just the complement of the extent of m, i.e., with ( m)  G  m . It is in general not assumed that negated attributes are available for concept formation. But pairs of attributes which are negations of each other often naturally occur, for example in contexts derived from conceptual scaling. For possible negations of formal concepts see the section concept algebras below. Implications An implication A B relates two sets A and B of attributes and expresses that every object possessing each attribute from A also has each attribute from B. When (G,M,I) is a formal context and A, B are subsets of the set M of attributes (i.e., A,B M), then the implication A B is valid if A B . For each finite formal context, the set of all valid implications has a canonical basis, an irredundant set of implications from which all valid implications can be derived by the natural inference (Armstrong rules). This is used in attribute exploration, a knowledge acquisition method based on implications. Arrow relations Formal concept analysis has elaborate mathematical foundations, making the field versatile. As a basic example we mention the arrow relations, which are simple and easy to compute, but very useful. They are defined as follows For g G and m M let g m (g, m) I and if m n and m n , then (g, n) I, and dually g m (g, m) I and if g h and g h , then (h, m) I. Since only non-incident object-attribute pairs can be related, these relations can conveniently be recorded in the table representing a formal context. Many lattice properties can be read off from the arrow relations, including distributivity and several of its generalizations. They also reveal structural information and can be used for determining, e.g., the congruence relations of the lattice. Extensions of the theory Triadic concept analysis replaces the binary incidence relation between objects and attributes by a ternary relation between objects, attributes, and conditions. An incidence ( g , m , c ) displaystyle (g,m,c) then expresses that the object g has the attribute m under the condition c. Although triadic concepts can be defined in analogy to the formal concepts above, the theory of the trilattices formed by them is much less developed than that of concept lattices, and seems to be difficult. Voutsadakis has studied the n-ary case. Fuzzy concept analysis Extensive work has been done on a fuzzy version of formal concept analysis. Concept algebras Modelling negation of formal concepts is somewhat problematic because the complement (G  A, M  B) of a formal concept (A, B) is in general not a concept. However, since the concept lattice is complete one can consider the join (A, B) of all concepts (C, D) that satisfy C G  A or dually the meet (A, B) of all concepts satisfying D M  B. These two operations are known as weak negation and weak opposition, respectively. This can be expressed in terms of the derivation operators. Weak negation can be written as (A, B)  ((G  A) , (G  A)), and weak opposition can be written as (A, B)  ((M  B), (M  B) ). The concept lattice equipped with the two additional operations and is known as the concept algebra of a context. Concept algebras generalize power sets. Weak negation on a concept lattice L is a weak complementation, i.e. an order-reversing map  L L which satisfies the axioms x x and (x y) (x y )  x. Weak opposition is a dual weak complementation. A (bounded) lattice such as a concept algebra, which is equipped with a weak complementation and a dual weak complementation, is called a weakly dicomplemented lattice. Weakly dicomplemented lattices generalize distributive orthocomplemented lattices, i.e. Boolean algebras. Temporal concept analysis Temporal concept analysis (TCA) is an extension of Formal Concept Analysis (FCA) aiming at a conceptual description of temporal phenomena. It provides animations in concept lattices obtained from data about changing objects. It offers a general way of understanding change of concrete or abstract objects in continuous, discrete or hybrid space and time. TCA applies conceptual scaling to temporal data bases. In the simplest case TCA considers objects that change in time like a particle in physics, which, at each time, is at exactly one place. That happens in those temporal data where the attributes temporal object and time together form a key of the data base. Then the state (of a temporal object at a time in a view) is formalized as a certain object concept of the formal context describing the chosen view. In this simple case, a typical visualization of a temporal system is a line diagram of the concept lattice of the view into which trajectories of temporal objects are embedded. TCA generalizes the above mentioned case by considering temporal data bases with an arbitrary key. That leads to the notion of distributed objects which are at any given time at possibly many places, as for example, a high pressure zone on a weather map. The notions of temporal objects, time and place are represented as formal concepts in scales. A state is formalized as a set of object concepts. That leads to a conceptual interpretation of the ideas of particles and waves in physics. Algorithms and tools There are a number of simple and fast algorithms for generating formal concepts and for constructing and navigating concept lattices. For a survey, see Kuznetsov and Obiedkov or the book by Ganter and Obiedkov, where also some pseudo-code can be found. Since the number of formal concepts may be exponential in the size of the formal context, the complexity of the algorithms usually is given with respect to the output size. Concept lattices with a few million elements can be handled without problems. Many FCA software applications are available today. The main purpose of these tools varies from formal context creation to formal concept mining and generating the concepts lattice of a given formal context and the corresponding implications and association rules. Most of these tools are academic open-source applications, such as ConExp ToscanaJ Lattice Miner Coron FcaBedrock GALACTIC Related analytical techniques Bicliques A formal context can naturally be interpreted as a bipartite graph. The formal concepts then correspond to the maximal bicliques in that graph. The mathematical and algorithmic results of formal concept analysis may thus be used for the theory of maximal bicliques. The notion of bipartite dimension (of the complemented bipartite graph) translates to that of Ferrers dimension (of the formal context) and of order dimension (of the concept lattice) and has applications e.g. for Boolean matrix factorization. Biclustering and multidimensional clustering Given an object-attribute numerical data-table, the goal of biclustering is to group together some objects having similar values of some attributes. For example, in gene expression data, it is known that genes (objects) may share a common behavior for a subset of biological situations (attributes) only one should accordingly produce local patterns to characterize biological processes, the latter should possibly overlap, since a gene may be involved in several processes. The same remark applies for recommender systems where one is interested in local patterns characterizing groups of users that strongly share almost the same tastes for a subset of items. A bicluster in a binary object-attribute data-table is a pair (A,B) consisting of an inclusion-maximal set of objects A and an inclusion-maximal set of attributes B such that almost all objects from A have almost all attributes from B and vice versa. Of course, formal concepts can be considered as rigid biclusters where all objects have all attributes and vice versa. Hence, it is not surprising that some bicluster definitions coming from practice are just definitions of a formal concept. Relaxed FCA-based versions of biclustering and triclustering include OA-biclustering and OAC-triclustering (here O stands for object, A for attribute, C for condition) to generate patterns these methods use prime operators only once being applied to a single entity (e.g. object) or a pair of entities (e.g. attribute-condition), respectively. A bicluster of similar values in a numerical object-attribute data-table is usually defined as a pair consisting of an inclusion-maximal set of objects and an inclusion-maximal set of attributes having similar values for the objects. Such a pair can be represented as an inclusion-maximal rectangle in the numerical table, modulo rows and columns permutations. In it was shown that biclusters of similar values correspond to triconcepts of a triadic context where the third dimension is given by a scale that represents numerical attribute values by binary attributes. This fact can be generalized to n-dimensional case, where n-dimensional clusters of similar values in n-dimensional data are represented by n1-dimensional concepts. This reduction allows one to use standard definitions and algorithms from multidimensional concept analysis for computing multidimensional clusters. Knowledge spaces In the theory of knowledge spaces it is assumed that in any knowledge space the family of knowledge states is union-closed. The complements of knowledge states therefore form a closure system and may be represented as the extents of some formal context. Hands-on experience with formal concept analysis The formal concept analysis can be used as a qualitative method for data analysis. Since the early beginnings of FCA in the early 1980s, the FCA research group at TU Darmstadt has gained experience from more than 200 projects using the FCA (as of 2005). Including the fields of medicine and cell biology, genetics, ecology, software engineering, ontology, information and library sciences, office administration, law, linguistics, political science. Many more examples are e.g. described in Formal Concept Analysis. Foundations and Applications, conference papers at regular conferences such as International Conference on Formal Concept Analysis (ICFCA), Concept Lattices and their Applications (CLA), or International Conference on Conceptual Structures (ICCS). See also Notes References External links A Formal Concept Analysis Homepage Demo Formal Concept Analysis. ICFCA International Conference Proceedings doi10.1007/978-3-540-70901-5 2007 5th doi10.1007/978-3-540-78137-0 2008 6th doi10.1007/978-3-642-01815-2 2009 7th doi10.1007/978-3-642-11928-6 2010 8th doi10.1007/978-3-642-20514-9 2011 9th doi10.1007/978-3-642-29892-9 2012 10th doi10.1007/978-3-642-38317-5 2013 11th doi10.1007/978-3-319-07248-7 2014 12th doi10.1007/978-3-319-19545-2 2015 13th doi10.1007/978-3-319-59271-8 2017 14th doi10.1007/978-3-030-21462-3 2019 15th doi10.1007/978-3-030-77867-5 2021 16th Title Generative artificial intelligence URL https//en.wikipedia.org/wiki/Generative_artificial_intelligence Content Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts. Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models. Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art. History Early history Since its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence these issues have previously been explored by myth, fiction and philosophy since antiquity. The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automations has flourished throughout history, exemplified by Maillardets automaton created in the early 1800s. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator. Academic artificial intelligence The academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings. The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a relatively mature technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft. Generative neural nets (2014-2019) Since its inception, the field of machine learning used both discriminative models and generative models, to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models, due to the difficulty of generative modeling. In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images. In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models, leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018. This was followed in 2019 by GPT-2 which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model. The new generative models introduced during this period allowed for large neural networks to be trained using unsupervised learning or semi-supervised learning, rather than the supervised learning typical of discriminative models. Unsupervised learning removed the need for humans to manually label data, allowing for larger networks to be trained. Generative AI boom (2020-) In March 2020, the release of 15.ai, a free web application created by an anonymous MIT researcher that could generate convincing character voices using minimal training data, marked one of the earliest popular use cases of generative AI. The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology. In 2021, the emergence of DALL-E, a transformer-based pixel generative model, marked an advance in AI-generated imagery. This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts. These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public. In late 2022, the public release of ChatGPT revolutionized the accessibility and application of generative AI for general-purpose text-based tasks. The systems ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AIs potential impact on work, education, and creativity. In March 2023, GPT-4s release represented another jump in generative AI capabilities. A team from Microsoft Research controversially argued that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. However, this assessment was contested by other scholars who maintained that generative AI remained still far from reaching the benchmark of general human intelligence as of 2023. Later in 2023, Meta released ImageBind, an AI model combining multiple modalities including text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications. In December 2023, Google unveiled Gemini, a multimodal AI model available in four versions Ultra, Pro, Flash, and Nano. The company integrated Gemini Pro into its Bard chatbot and announced plans for Bard Advanced powered by the larger Gemini Ultra model. In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS. In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus. The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google. In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis. According to a survey by SAS and Coleman Parkes Research, China has emerged as a global leader in generative AI adoption, with 83 of Chinese respondents using the technology, exceeding both the global average of 54 and the U.S. rate of 65. This leadership is further evidenced by Chinas intellectual property developments in the field, with a UN report revealing that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications. Modalities A generative AI system is constructed by applying unsupervised machine learning (invoking for instance neural network architectures such as generative adversarial networks (GANs), variation autoencoders (VAEs), transformers, or self-supervised machine learning trained on a dataset. The capabilities of a generative AI system depend on the modality or type of the data set used. Generative AI can be either unimodal or multimodal unimodal systems take only one type of input, whereas multimodal systems can take more than one type of input. For example, one version of OpenAIs GPT-4 accepts both text and image inputs. Text Generative AI systems trained on words or word tokens include GPT-3, GPT-4, GPT-4o, LaMDA, LLaMA, BLOOM, Gemini and others (see List of large language models). They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks. Data sets include BookCorpus, Wikipedia, and others (see List of text corpora). Code In addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs. Examples include OpenAI Codex and the VS Code fork Cursor. Images Producing high-quality visual art is a prominent application of generative AI. Generative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, FLUX.1, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generation and neural style transfer. Datasets include LAION-5B and others (see List of datasets in computer vision and image processing). Audio Generative AI can also be trained extensively on audio clips to produce natural-sounding speech synthesis and text-to-speech capabilities. An early pioneer in this field was 15.ai, launched in March 2020, which demonstrated the ability to clone character voices using as little as 15 seconds of training data. The website gained widespread attention for its ability to generate emotionally expressive speech for various fictional characters, though it was later taken offline in 2022 due to copyright concerns. Commercial alternatives subsequently emerged, including ElevenLabs context-aware synthesis tools and Meta Platforms Voicebox. Generative AI systems such as MusicLM and MusicGen can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff. Music Audio deepfakes of lyrics have been generated, like the song Savages, which used AI to mimic rapper Jay-Zs vocals. Music artists instrumentals and lyrics are copyrighted but their voices arent protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes. Many AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs. Video Generative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips. Examples include Sora by OpenAI, Runway, and Make-A-Video by Meta Platforms. Actions Generative AI can also be trained on the motions of a robotic system to generate new trajectories for motion planning or navigation. For example, UniPi from Google Research uses prompts like pick up blue bowl or wipe plate with yellow sponge to control movements of a robot arm. Multimodal vision-language-action models such as Googles RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects. 3D modeling Artificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling. AI-based CAD libraries could also be developed using linked open data of schematics and diagrams. AI CAD assistants are used as tools to help streamline workflow. Software and hardware Generative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot, text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2. Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot), Google Photos, and the Adobe Suite (Adobe Firefly). Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA language model. Smaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4 and one version of Stable Diffusion can run on an iPhone 11. Larger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC. The advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship. The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards through such techniques as compression. That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks. Yann LeCun has advocated open-source models for their value to vertical applications and for improving AI safety. Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIAs ) or AI accelerator chips (such as Googles TPU). These very large models are typically accessed as cloud services over the Internet. In 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI. Chips such as the NVIDIA and the Biren Technology were developed to meet the requirements of the sanctions. There is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it. Potential mitigation strategies for detecting generative AI content include digital watermarking, content authentication, information retrieval, and machine learning classifier models. Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work. Law and regulation In the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with the Biden administration in July 2023 to watermark AI-generated content. In October 2023, Executive Order 14110 applied the Defense Production Act to require all US companies to report information to the federal government when training certain high-impact AI models. In the European Union, the proposed Artificial Intelligence Act includes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such. In China, the Interim Measures for the Management of Generative AI Services introduced by the Cyberspace Administration of China regulates any public-facing generative AI. It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI must adhere to socialist core values. Copyright Training with copyrighted content Generative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights. Proponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public. Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images, and that generative AI programs compete with the content they are trained on. As of 2024, several lawsuits related to the use of copyrighted material in training are ongoing. Getty Images has sued Stability AI over the use of its images to train Stable Diffusion. Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT. Copyright of AI-generated content A separate question is whether AI-generated works can qualify for copyright protection. The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship. Some legal professionals have suggested that Naruto v. Slater (2018), in which the U.S. 9th Circuit Court of Appeals held that non-humans cannot be copyright holders of artistic works, could be a potential precedent in copyright litigation over works created by generative AI. However, the office has also begun taking public input to determine if these rules need to be refined for generative AI. Concerns The development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments. In a July 2023 briefing of the United Nations Security Council, Secretary-General Ant nio Guterres stated Generative AI has enormous potential for good and evil at scale, that AI may turbocharge global development and contribute between 15 trillion to the global economy by 2030, but that its malicious use could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale. In addition, generative AI has a significant carbon footprint. Job losses From the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements. In April 2023, it was reported that image generation AI has resulted in 70 of the jobs for video game illustrators in China being lost. In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that artificial intelligence poses an existential threat to creative professions during the 2023 SAG-AFTRA strike. Voice generation AI has been seen as a potential challenge to the voice acting sector. The intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and educations potential for personalized teaching to maximize benefits while minimizing harms. Racial and gender bias Generative AI models can reflect and amplify any cultural bias present in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data. Similarly, an image model prompted with the text a photo of a CEO might disproportionately generate images of white male CEOs, if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts and reweighting training data. Deepfakes Deepfakes (a portmanteau of deep learning and fake) are AI-generated media that take a person in an existing image or video and replace them with someone elses likeness using artificial neural networks. Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, health disinformation, financial fraud, and covert foreign election interference. This has elicited responses from both industry and government to detect and limit their use. In July 2023, the fact-checking company Logically found that the popular generative AI models Midjourney, DALL-E 2 and Stable Diffusion would produce plausible disinformation images when prompted to do so, such as images of electoral fraud in the United States and Muslim women supporting Indias Hindu nationalist Bharatiya Janata Party. In April 2024, a paper proposed to use blockchain (distributed ledger technology) to promote transparency, verifiability, and decentralization in AI development and usage. Audio deepfakes Instances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI. In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification. Concerns and fandoms have spawned from AI-generated music. The same software used to clone voices has been used on famous musicians voices to create songs that mimic their voices, gaining both tremendous popularity and criticism. Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released. Generative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels. The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for dehumanizing an artform, and also creating artists which create unrealistic or immoral appeals to their audiences. Cybercrime Generative AIs ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams. Deepfake video and audio have been used to create disinformation and fraud. In 2020, former Google click fraud czar Shuman Ghosemajumder argued that once deepfake videos become perfectly realistic, they would stop appearing remarkable to viewers, potentially leading to uncritical acceptance of false information. Additionally, large language models and other forms of text-generation AI have been used to create fake reviews of e-commerce websites to boost ratings. Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT. A 2023 study showed that generative AI can be vulnerable to jailbreaks, reverse psychology and prompt injection attacks, enabling attackers to obtain help with harmful requests, such as for crafting social engineering and phishing attacks. Additionally, other researchers have demonstrated that open-source models can be fine-tuned to remove their safety restrictions at low cost. Reliance on industry giants Training frontier AI models requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller start-ups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively. Energy and environment AI has a significant carbon footprint due to growing energy consumption from both training and usage. Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having high emissions, large amounts of freshwater used for data centers, and high amounts of electricity usage. There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing as chatbots and other applications become more popular and as models need to be retrained. Proposed mitigation strategies include factoring potential environmental costs prior to model development or data collection, increasing efficiency of data centers to reduce electricity/energy usage, building more efficient machine learning models, minimizing the number of times that models need to be retrained, developing a government-directed framework for auditing the environmental impact of these models, regulating for transparency of these models, regulating their energy and water usage, encouraging researchers to publish data on their models carbon footprint, and increasing the number of subject matter experts who understand both machine learning and climate science. Content quality The New York Times defines slop as analogous to spam shoddy or unwanted A.I. content in social media, art, books and ... in search results. Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation, the monetary incentives from social media companies to spread such content, false political messaging, spamming of scientific research paper submissions, increased time and effort to find higher quality or desired content on the Internet, the indexing of generated content by search engines, and on journalism itself. A paper published by researchers at Amazon Web Services AI Labs found that over 57 of sentences from a sample of over 6 billion sentences from Common Crawl, a snapshot of web pages, were machine translated. Many of these automated translations were seen as lower quality, especially for sentences that were translated across at least three languages. Many lower-resource languages (ex. Wolof, Xhosa) were translated across more languages than higher-resource languages (ex. English, French). In September 2024, Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons high costs for obtaining data from Reddit and Twitter, excessive focus on generative AI compared to other methods in the natural language processing community, and that generative AI has polluted the data. The adoption of generative AI tools led to an explosion of AI-generated content across multiple domains. A study from University College London estimated that in 2023, more than 60,000 scholarly articles over 1 of all publications were likely written with LLM assistance. According to Stanford Universitys Institute for Human-Centered AI, approximately 17.5 of newly published computer science papers and 16.9 of peer review text now incorporate content generated by LLMs. Visual content follows a similar trend. Since the launch of DALL-E 2 in 2022, it is estimated that an average of 34 million images have been created daily. As of August 2023, more than 15 billion images had been generated using text-to-image algorithms, with 80 of these created by models based on Stable Diffusion. If AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur. Training an AI model exclusively on the output of another AI model produces a lower-quality model. Repeating this process, where each new model is trained on the previous models output, leads to progressive degradation and eventually results in a model collapse after multiple iterations. Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces. As a consequence, the value of data collected from genuine human interactions with systems may become increasingly valuable in the presence of LLM-generated content in data crawled from the Internet. On the other side, synthetic data is often used as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy, including for structured data. The approach is not limited to text generation image generation has been employed to train computer vision models. Misuse in journalism In January 2023, Futurism.com broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories after the news broke, CNET posted corrections to 41 of the stories. In April 2023, the German tabloid Die Aktuelle published a fake AI-generated interview with former racing driver Michael Schumacher, who had not made any public appearances since 2013 after sustaining a brain injury in a skiing accident. The story included two possible disclosures the cover included the line deceptively real, and the interview included an acknowledgment at the end that it was AI-generated. The editor-in-chief was fired shortly thereafter amid the controversy. Other outlets that have published articles whose content and/or byline have been confirmed or suspected to be created by generative AI models often with false content, errors, and/or non-disclosure of generative AI use - include In May 2024, Futurism noted that a content management system video by AdVon Commerce, who had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they had produced tens of thousands of articles for more than 150 publishers. News broadcasters in Kuwait, Greece, South Korea, India, China and Taiwan have presented news with anchors based on Generative AI models, prompting concerns about job losses for human anchors and audience trust in news that has historically been influenced by parasocial relationships with broadcasters, content creators or social media influencers. Algorithmically generated anchors have also been used by allies of ISIS for their broadcasts. In 2023, Google reportedly pitched a tool to news outlets that claimed to produce news stories based on input data provided, such as details of current events. Some news company executives who viewed the pitch described it as taking for granted the effort that went into producing accurate and artful news stories. In February 2024, Google launched a program to pay small publishers to write three articles per day using a beta generative AI model. The program does not require the knowledge or consent of the websites that the publishers are using as sources, nor does it require the published articles to be labeled as being created or assisted by these models. Many defunct news sites (The Hairpin, The Frisky, Apple Daily, Ashland Daily Tidings, Clayton County Register, Southwest Journal) and blogs (The Unofficial Apple Weblog, iLounge) have undergone cybersquatting, with articles created by generative AI. United States Senators Richard Blumenthal and Amy Klobuchar have expressed concern that generative AI could have a harmful impact on local news. In July 2023, OpenAI partnered with the American Journalism Project to fund local news outlets for experimenting with generative AI, with Axios noting the possibility of generative AI companies creating a dependency for these news outlets. Meta AI, a chatbot based on Llama 3 which summarizes news stories, was noted by The Washington Post to copy sentences from those stories without direct attribution and to potentially further decrease the traffic of online news outlets. In response to potential pitfalls around the use and misuse of generative AI in journalism and worries about declining audience trust, outlets around the world, including publications such as Wired, Associated Press, The Quint, Rappler or The Guardian have published guidelines around how they plan to use and not use AI and generative AI in their work. In June 2024, Reuters Institute published their Digital News Report for 2024. In a survey of people in America and Europe, Reuters Institute reports that 52 and 47 respectively are uncomfortable with news produced by mostly AI with some human oversight, and 23 and 15 respectively report being comfortable. 42 of Americans and 33 of Europeans reported that they were comfortable with news produced by mainly human with some help from AI. The results of global surveys reported that people were more uncomfortable with news topics including politics (46), crime (43), and local news (37) produced by AI than other news topics. See also Artificial general intelligence Type of AI with wide-ranging abilities Artificial imagination Artificial simulation of human imagination Artificial intelligence art Visual media created with AI Artificial life Field of study Chatbot Program that simulates conversation Computational creativity Multidisciplinary endeavour Generative adversarial network Deep learning method Generative pre-trained transformer Type of large language model Large language model Type of machine learning model Music and artificial intelligence Usage of artificial intelligence to generate music Generative AI pornography Explicit material produced by generative AI Procedural generation Method in which data is created algorithmically as opposed to manually Retrieval-augmented generation Type of information retrieval using LLMs Stochastic parrot Term used in machine learning References Further reading He, Ran Cao, Jie Tan, Tieniu (2025). Generative Artificial Intelligence A Historical Perspective. National Science Review . doi10.1093/nsr/. Title Generative model URL https//en.wikipedia.org/wiki/Generative_model Content In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following Jebara (2004) A generative model is a statistical model of the joint probability distribution P ( X , Y ) displaystyle P(X,Y) on a given observable variable X and target variable Y A generative model can be used to generate random instances (outcomes) of an observation x. A discriminative model is a model of the conditional probability P ( Y . It can be used to discriminate the value of the target variable Y, given an observation x. Classifiers computed without using a probability model are also referred to loosely as discriminative. The distinction between these last two classes is not consistently made Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng  Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model. Standard examples of each, all of which are linear classifiers, are generative classifiers naive Bayes classifier and linear discriminant analysis discriminative model logistic regression In application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier) one can estimate the probability of a label given an observation, P ( Y  . These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches. Definition An alternative division defines these symmetrically as a generative model is a model of the conditional probability of the observable X, given a target y, symbolically, P ( X . The difference between discriminate (distinguish) and classify is subtle, and these are not consistently distinguished. (The term discriminative classifier becomes a pleonasm when discrimination is equivalent to classification.) The term generative model is also used to describe models that generate instances of output variables in a way that has no clear relationship to probability distributions over potential samples of input variables. Generative adversarial networks are examples of this class of generative models, and are judged primarily by the similarity of particular outputs to potential inputs. Such models are not classifiers. Relationships between models In application to classification, the observable X is frequently a continuous variable, the target Y is generally a discrete variable consisting of a finite set of labels, and the conditional probability P ( Y X ) displaystyle P(Ymid X) can also be interpreted as a (non-deterministic) target function f  X Y displaystyle fcolon Xto Y , considering X as inputs and Y as outputs. Given a finite set of labels, the two definitions of generative model are closely related. A model of the conditional distribution P ( X  ) . displaystyle P(X,Y)P(Xmid Y)P(Y). Thus, while a model of the joint probability distribution is more informative than a model of the distribution of label (but without their relative frequencies), it is a relatively small step, hence these are not always distinguished. Given a model of the joint distribution, P ( X , Y ) displaystyle P(X,Y) , the distribution of the individual variables can be computed as the marginal distributions P ( X )  y P ( X , ) . Given a model of one conditional probability, and estimated probability distributions for the variables X and Y, denoted P ( X ) displaystyle P(X) and P ( Y ) displaystyle P(Y) , one can estimate the opposite conditional probability using Bayes rule P ( X Y ) P ( Y )  P ( Y X ) P ( X ) . displaystyle P(Xmid Y)P(Y)P(Ymid X)P(X). For example, given a generative model for P ( X Y ) displaystyle P(Xmid Y) , one can estimate P ( Y X )  P ( X Y ) P ( Y ) / P ( X ) , displaystyle P(Ymid X)P(Xmid Y)P(Y)/P(X), and given a discriminative model for P ( Y X ) displaystyle P(Ymid X) , one can estimate P ( X Y )  P ( Y X ) P ( X ) / P ( Y ) . displaystyle P(Xmid Y)P(Ymid X)P(X)/P(Y). Note that Bayes rule (computing one conditional probability in terms of the other) and the definition of conditional probability (computing conditional probability in terms of the joint distribution) are frequently conflated as well. Contrast with discriminative classifiers A generative algorithm models how the data was generated in order to categorize a signal. It asks the question based on my generation assumptions, which category is most likely to generate this signal? A discriminative algorithm does not care about how the data was generated, it simply categorizes a given signal. So, discriminative algorithms try to learn p ( y  x ) displaystyle p(yx) directly from the data and then try to classify data. On the other hand, generative algorithms try to learn p ( x , y ) displaystyle p(x,y) which can be transformed into p ( y  x ) displaystyle p(yx) later to classify the data. One of the advantages of generative algorithms is that you can use p ( x , y ) displaystyle p(x,y) to generate new data similar to existing data. On the other hand, it has been proved that some discriminative algorithms give better performance than some generative algorithms in classification tasks. Despite the fact that discriminative models do not need to model the distribution of the observed variables, they cannot generally express complex relationships between the observed and target variables. But in general, they dont necessarily perform better than generative models at classification and regression tasks. The two classes are seen as complementary or as different views of the same procedure. Deep generative models With the rise of deep learning, a new family of methods, called deep generative models (DGMs), is formed through the combination of generative models and deep neural networks. An increase in the scale of the neural networks is typically accompanied by an increase in the scale of the training data, both of which are required for good performance. Popular DGMs include variational autoencoders (VAEs), generative adversarial networks (GANs), and auto-regressive models. Recently, there has been a trend to build very large deep generative models. For example, GPT-3, and its precursor GPT-2, are auto-regressive neural language models that contain billions of parameters, BigGAN and VQ-VAE which are used for image generation that can have hundreds of millions of parameters, and Jukebox is a very large generative model for musical audio that contains billions of parameters. Types Generative models Types of generative models are Gaussian mixture model (and other types of mixture model) Hidden Markov model Probabilistic context-free grammar Bayesian network (e.g. Naive bayes, Autoregressive model) Averaged one-dependence estimators Latent Dirichlet allocation Boltzmann machine (e.g. Restricted Boltzmann machine, Deep belief network) Variational autoencoder Generative adversarial network Flow-based generative model Energy based model Diffusion model If the observed data are truly sampled from the generative model, then fitting the parameters of the generative model to maximize the data likelihood is a common method. However, since most statistical models are only approximations to the true distribution, if the models application is to infer about a subset of variables conditional on known values of others, then it can be argued that the approximation makes more assumptions than are necessary to solve the problem at hand. In such cases, it can be more accurate to model the conditional density functions directly using a discriminative model (see below), although application-specific details will ultimately dictate which approach is most suitable in any particular case. Discriminative models k-nearest neighbors algorithm Logistic regression Support Vector Machines Decision Tree Learning Random Forest Maximum-entropy Markov models Conditional random fields Examples Simple example Suppose the input data is x  1 , 2  displaystyle xin 1,2 , the set of labels for x displaystyle x is y  0 , 1  displaystyle yin 0,1 , and there are the following 4 data points ( x , y )   ( 1 , 0 ) , ( 1 , 1 ) , ( 2 , 0 ) , ( 2 , 1 )  displaystyle (x,y)(1,0),(1,1),(2,0),(2,1) For the above data, estimating the joint probability distribution p ( x , y ) displaystyle p(x,y) from the empirical measure will be the following while p ( y  x ) displaystyle p(yx) will be following Text generation Shannon (1948) gives an example in which a table of frequencies of English word pairs is used to generate a sentence beginning with representing and speedily is an good which is not proper English but which will increasingly approximate it as the table is moved from word pairs to word triplets etc. See also Discriminative model Graphical model Notes References Title Geometric feature learning URL https//en.wikipedia.org/wiki/Geometric_feature_learning Content Geometric feature learning is a technique combining machine learning and computer vision to solve visual tasks. The main goal of this method is to find a set of representative features of geometric form to represent an object by collecting geometric features from images and learning them using efficient machine learning methods. Humans solve visual tasks and can give fast response to the environment by extracting perceptual information from what they see. Researchers simulate humans ability of recognizing objects to solve computer vision problems. For example, M. Mata et al.(2002) applied feature learning techniques to the mobile robot navigation tasks in order to avoid obstacles. They used genetic algorithms for learning features and recognizing objects (figures). Geometric feature learning methods can not only solve recognition problems but also predict subsequent actions by analyzing a set of sequential input sensory images, usually some extracting features of images. Through learning, some hypothesis of the next action are given and according to the probability of each hypothesis give a most probable action. This technique is widely used in the area of artificial intelligence. Introduction Geometric feature learning methods extract distinctive geometric features from images. Geometric features are features of objects constructed by a set of geometric elements like points, lines, curves or surfaces. These features can be corner features, edge features, Blobs, Ridges, salient points image texture and so on, which can be detected by feature detection methods. Geometric features Primitive features Corners Corners are a very simple but significant feature of objects. Especially, Complex objects usually have different corner features with each other. Corners of an object can be extracted through Corner detection. Cho and Dunn used a different way to define a corner by the distance and angle between two straight line segments. This is a new way by defining features as a parameterized composition of several components. Edges Edges are one-dimensional structure features of an image. They represent the boundary of different image regions. The outline of an object can be easily detected by finding the edge using the technique of edge detection. Blobs Blobs represent regions of images, which can be detected using blob detection method. Ridges From a practical viewpoint, a ridge can be thought of as a one-dimensional curve that represents an axis of symmetry. Ridges detection method-see ridge detection salient points-see Kadir Brady saliency detector image texture Compound features Geometric composition Geometric component feature is a combination of several primitive features and it always consists more than 2 primitive features like edges, corners or blobs. Extracting geometric feature vector at location x can be computed according to the reference point, which is shown below x . Boolean Composition Boolean compound feature consists of two sub-features which can be primitive features or compound features. There are two type of boolean features conjunctive feature whose value is the product of two sub-features and disjunctive features whose value is the maximum of the two sub-features. Feature space Feature space was firstly considered in computer vision area by Segen. He used multilevel graph to represent the geometric relations of local features. Learning algorithms There are many learning algorithms which can be applied to learn to find distinctive features of objects in an image. Learning can be incremental, meaning that the object classes can be added at any time. Geometric feature extraction methods Corner detection Curve fitting Edge detection Global structure extraction Feature histograms Line detection Connected-component labeling Image texture Motion estimation Feature learning algorithm 1.Acquire a new training image I. 2.According to the recognition algorithm, evaluate the result. If the result is true, new object classes are recognised. recognition algorithm The key point of recognition algorithm is to find the most distinctive features among all features of all classes. So using below equation to maximise the feature f m a x displaystyle textstyle  f_max I m a . When the evaluation is correct, add a new training image and train it. If the recognition failed, the feature nodes should be maximise their distinctive power which is defined by the Kolmogorov-Smirno distance (KSD). K S D a , b ( X )  m a x  c d f (  a ) c d f (  b )  displaystyle textstyle KSD_a,b(X)underset alpha maxleftcdf(alpha a)-cdf(alpha b)right 3.Feature learning algorithm After a feature is recognised, it should be applied to Bayesian network to recognise the image, using the feature learning algorithm to test. The main purpose of feature learning algorithm is to find a new feature from sample image to test whether the classes are recognised or not. Two cases should be consider Searching for new feature of true class and wrong class from sample image respectively. If new feature of true class is detected and the wrong class is not recognised, then the class is recognised and the algorithm should terminate. If feature of true class is not detected and of false class is detected in the sample image, false class should be prevented from being recognised and the feature should be removed from Bayesian network. Using Bayesian network to realise the test process PAC model based feature learning algorithm Learning framework The probably approximately correct (PAC) model was applied by D. Roth (2002) to solve computer vision problem by developing a distribution-free learning theory based on this model. This theory heavily relied on the development of feature-efficient learning approach. The goal of this algorithm is to learn an object represented by some geometric features in an image. The input is a feature vector and the output is 1 which means successfully detect the object or 0 otherwise. The main point of this learning approach is collecting representative elements which can represent the object through a function and testing by recognising an object from image to find the representation with high probability. The learning algorithm aims to predict whether the learned target concept f T ( X ) displaystyle textstyle f_T(X) belongs to a class, where X is the instance space consists with parameters and then test whether the prediction is correct. Evaluation framework After learning features, there should be some evaluation algorithms to evaluate the learning algorithms. D. Roth applied two learning algorithms 1.Sparse Network of Winnows(SNoW) system SNoW-Train Initial step initial the set of features F  . T is a set of object targets whose elements are t 1 displaystyle textstyle t_1 to t k displaystyle textstyle t_k If each target object in set T belongs to a list of active features, link feature to target and set initial weight at the same time. Evaluate the targets  compare targets i e w i t displaystyle textstyle underset iin esum w_it with t displaystyle textstyle theta _t , where w i t displaystyle textstyle w_it is the weight on one position connecting the features i to target t. theta_t is the threshold for the target not t. Update weight according to the result of evaluation. There are two cases predicted positive on negative example ( i e w i t  t displaystyle textstyle underset iin esum w_ittheta _t and targets are not in the list of active features) and predicted negative on positive example( i e w i t t displaystyle textstyle underset iin esum w_itleq theta _t and targets are in the list of active features). SNoW-Evaluation Evaluate the each target using same function as introduced above Prediction Make a decision to select the dominant active target node. 2. support vector machines The main purpose of SVM is to find a hyperplane to separate the set of samples ( x i , y i ) displaystyle textstyle (x_i,y_i) where x i displaystyle textstyle x_i is an input vector which is a selection of features x R N displaystyle textstyle xin RN and y i displaystyle textstyle y_i is the label of x i displaystyle textstyle x_i . The hyperplane has the following form f ( x )  s g n ( . k ( x , x i )  ( x ) ( x i ) displaystyle textstyle k(x,x_i)phi (x)cdot phi (x_i) is a kernel function Both algorithms separate training data by finding a linear function. Applications Landmarks learning for topological navigation Simulation of detecting object process of human vision behaviour Learning self-generated action Vehicle tracking Title Glossary of artificial intelligence URL https//en.wikipedia.org/wiki/Glossary_of_artificial_intelligence Content This glossary of artificial intelligence is a list of definitions of terms and concepts relevant to the study of artificial intelligence (AI), its subdisciplines, and related fields. Related glossaries include Glossary of computer science, Glossary of robotics, and Glossary of machine vision. A A search Pronounced A-star. A graph traversal and pathfinding algorithm which is used in many fields of computer science due to its completeness, optimality, and optimal efficiency. abductive logic programming (ALP) A high-level knowledge-representation framework that can be used to solve problems declaratively based on abductive reasoning. It extends normal logic programming by allowing some predicates to be incompletely defined, declared as abducible predicates. abductive reasoning Also abduction. A form of logical inference which starts with an observation or set of observations then seeks to find the simplest and most likely explanation. This process, unlike deductive reasoning, yields a plausible conclusion but does not positively verify it. abductive inference, or retroduction ablation The removal of a component of an AI system. An ablation study aims to determine the contribution of a component to an AI system by removing the component, and then analyzing the resultant performance of the system. abstract data type A mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a user of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. abstraction The process of removing physical, spatial, or temporal details or attributes in the study of objects or systems in order to more closely attend to other details of interest accelerating change A perceived increase in the rate of technological change throughout history, which may suggest faster and more profound change in the future and may or may not be accompanied by equally profound social and cultural change. action language A language for specifying state transition systems, and is commonly used to create formal models of the effects of actions on the world. Action languages are commonly used in the artificial intelligence and robotics domains, where they describe how actions affect the states of systems over time, and may be used for automated planning. action model learning An area of machine learning concerned with creation and modification of software agents knowledge about effects and preconditions of the actions that can be executed within its environment. This knowledge is usually represented in logic-based action description language and used as the input for automated planners. action selection A way of characterizing the most basic problem of intelligent systems what to do next. In artificial intelligence and computational cognitive science, the action selection problem is typically associated with intelligent agents and animats artificial systems that exhibit complex behaviour in an agent environment. activation function In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. adaptive algorithm An algorithm that changes its behavior at the time it is run, based on a priori defined reward mechanism or criterion. adaptive neuro fuzzy inference system (ANFIS) Also adaptive network-based fuzzy inference system. A kind of artificial neural network that is based on Takagi Sugeno fuzzy inference system. The technique was developed in the early 1990s. Since it integrates both neural networks and fuzzy logic principles, it has potential to capture the benefits of both in a single framework. Its inference system corresponds to a set of fuzzy IF THEN rules that have learning capability to approximate nonlinear functions. Hence, ANFIS is considered to be a universal estimator. For using the ANFIS in a more efficient and optimal way, one can use the best parameters obtained by genetic algorithm. admissible heuristic In computer science, specifically in algorithms related to pathfinding, a heuristic function is said to be admissible if it never overestimates the cost of reaching the goal, i.e. the cost it estimates to reach the goal is not higher than the lowest possible cost from the current point in the path. affective computing Also artificial emotional intelligence or emotion AI. The study and development of systems and devices that can recognize, interpret, process, and simulate human affects. Affective computing is an interdisciplinary field spanning computer science, psychology, and cognitive science. agent architecture A blueprint for software agents and intelligent control systems, depicting the arrangement of components. The architectures implemented by intelligent agents are referred to as cognitive architectures. AI accelerator A class of microprocessor or computer system designed as hardware acceleration for artificial intelligence applications, especially artificial neural networks, machine vision, and machine learning. AI-complete In the field of artificial intelligence, the most difficult problems are informally known as AI-complete or AI-hard, implying that the difficulty of these computational problems is equivalent to that of solving the central artificial intelligence problem making computers as intelligent as people, or strong AI. To call a problem AI-complete reflects an attitude that it would not be solved by a simple specific algorithm. algorithm An unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing, and automated reasoning tasks. algorithmic efficiency A property of an algorithm which relates to the number of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process. algorithmic probability In algorithmic information theory, algorithmic probability, also known as Solomonoff probability, is a mathematical method of assigning a prior probability to a given observation. It was invented by Ray Solomonoff in the 1960s. AlphaGo A computer program that plays the board game Go. It was developed by Alphabet Inc.s Google DeepMind in London. AlphaGo has several versions including AlphaGo Zero, AlphaGo Master, AlphaGo Lee, etc. In October 2015, AlphaGo became the first computer Go program to beat a human professional Go player without handicaps on a full-sized 19 19 board. ambient intelligence (AmI) Electronic environments that are sensitive and responsive to the presence of people. analysis of algorithms The determination of the computational complexity of algorithms, that is the amount of time, storage and/or other resources necessary to execute them. Usually, this involves determining a function that relates the length of an algorithms input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). analytics The discovery, interpretation, and communication of meaningful patterns in data. answer set programming (ASP) A form of declarative programming oriented towards difficult (primarily NP-hard) search problems. It is based on the stable model (answer set) semantics of logic programming. In ASP, search problems are reduced to computing stable models, and answer set solvers programs for generating stable models are used to perform search. ant colony optimization (ACO) A probabilistic technique for solving computational problems that can be reduced to finding good paths through graphs. anytime algorithm An algorithm that can return a valid solution to a problem even if it is interrupted before it ends. application programming interface (API) A set of subroutine definitions, communication protocols, and tools for building software. In general terms, it is a set of clearly defined methods of communication among various components. A good API makes it easier to develop a computer program by providing all the building blocks, which are then put together by the programmer. An API may be for a web-based system, operating system, database system, computer hardware, or software library. approximate string matching Also fuzzy string searching. The technique of finding strings that match a pattern approximately (rather than exactly). The problem of approximate string matching is typically divided into two sub-problems finding approximate substring matches inside a given string and finding dictionary strings that match the pattern approximately. approximation error The discrepancy between an exact value and some approximation to it. argumentation framework Also argumentation system. A way to deal with contentious information and draw conclusions from it. In an abstract argumentation framework, entry-level information is a set of abstract arguments that, for instance, represent data or a proposition. Conflicts between arguments are represented by a binary relation on the set of arguments. In concrete terms, you represent an argumentation framework with a directed graph such that the nodes are the arguments, and the arrows represent the attack relation. There exist some extensions of the Dungs framework, like the logic-based argumentation frameworks or the value-based argumentation frameworks. artificial general intelligence (AGI) A type of AI that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. artificial immune system (AIS) A class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune systems characteristics of learning and memory for use in problem-solving. artificial intelligence (AI) Also machine intelligence. Any intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and other animals. In computer science, AI research is defined as the study of intelligent agents any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term artificial intelligence is applied when a machine mimics cognitive functions that humans associate with other human minds, such as learning and problem solving. Artificial Intelligence Markup Language An XML dialect for creating natural language software agents. Association for the Advancement of Artificial Intelligence (AAAI) An international, nonprofit, scientific society devoted to promote research in, and responsible use of, artificial intelligence. AAAI also aims to increase public understanding of artificial intelligence (AI), improve the teaching and training of AI practitioners, and provide guidance for research planners and funders concerning the importance and potential of current AI developments and future directions. asymptotic computational complexity In computational complexity theory, asymptotic computational complexity is the usage of asymptotic analysis for the estimation of computational complexity of algorithms and computational problems, commonly associated with the usage of the big O notation. attention mechanism Machine learning-based attention is a mechanism mimicking cognitive attention. It calculates soft weights for each word, more precisely for its embedding, in the context window. It can do it either in parallel (such as in transformers) or sequentially (such as in recursive neural networks). Soft weights can change during each runtime, in contrast to hard weights, which are (pre-)trained and fine-tuned and remain frozen afterwards. Multiple attention heads are used in transformer-based large language models. attributional calculus A logic and representation system defined by Ryszard S. Michalski. It combines elements of predicate logic, propositional calculus, and multi-valued logic. Attributional calculus provides a formal language for natural induction, an inductive learning process whose results are in forms natural to people. augmented reality (AR) An interactive experience of a real-world environment where the objects that reside in the real-world are augmented by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory, and olfactory. autoencoder A type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). A common implementation is the variational autoencoder (VAE). automata theory The study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science and discrete mathematics (a subject of study in both mathematics and computer science). automated machine learning (AutoML) A field of machine learning (ML) which aims to automatically configure an ML system to maximize its performance (e.g, classification accuracy). automated planning and scheduling Also simply AI planning. A branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory. automated reasoning An area of computer science and mathematical logic dedicated to understanding different aspects of reasoning. The study of automated reasoning helps produce computer programs that allow computers to reason completely, or nearly completely, automatically. Although automated reasoning is considered a sub-field of artificial intelligence, it also has connections with theoretical computer science, and even philosophy. autonomic computing (AC) The self-managing characteristics of distributed computing resources, adapting to unpredictable changes while hiding intrinsic complexity to operators and users. Initiated by IBM in 2001, this initiative ultimately aimed to develop computer systems capable of self-management, to overcome the rapidly growing complexity of computing systems management, and to reduce the barrier that complexity poses to further growth. autonomous car Also self-driving car, robot car, and driverless car. A vehicle that is capable of sensing its environment and moving with little or no human input. autonomous robot A robot that performs behaviors or tasks with a high degree of autonomy. Autonomous robotics is usually considered to be a subfield of artificial intelligence, robotics, and information engineering. B backpropagation A method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights to be used in the network. Backpropagation is shorthand for the backward propagation of errors, since an error is computed at the output and distributed backwards throughout the networks layers. It is commonly used to train deep neural networks, a term referring to neural networks with more than one hidden layer. backpropagation through structure (BPTS) A gradient-based technique for training recurrent neural networks, proposed in a 1996 paper written by Christoph Goller and Andreas K chler. backpropagation through time (BPTT) A gradient-based technique for training certain types of recurrent neural networks, such as Elman networks. The algorithm was independently derived by numerous researchers. backward chaining Also backward reasoning. An inference method described colloquially as working backward from the goal. It is used in automated theorem provers, inference engines, proof assistants, and other artificial intelligence applications. bag-of-words model A simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision. The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier. bag-of-words model in computer vision In computer vision, the bag-of-words model (BoW model) can be applied to image classification, by treating image features as words. In document classification, a bag of words is a sparse vector of occurrence counts of words that is, a sparse histogram over the vocabulary. In computer vision, a bag of visual words is a vector of occurrence counts of a vocabulary of local image features. batch normalization A technique for improving the performance and stability of artificial neural networks. It is a technique to provide any layer in a neural network with inputs that are zero mean/unit variance. Batch normalization was introduced in a 2015 paper. It is used to normalize the input layer by adjusting and scaling the activations. Bayesian programming A formalism and a methodology for having a technique to specify probabilistic models and solve problems when less than the necessary information is available. bees algorithm A population-based search algorithm which was developed by Pham, Ghanbarzadeh and et al. in 2005. It mimics the food foraging behaviour of honey bee colonies. In its basic version the algorithm performs a kind of neighborhood search combined with global search, and can be used for both combinatorial optimization and continuous optimization. The only condition for the application of the bees algorithm is that some measure of distance between the solutions is defined. The effectiveness and specific abilities of the bees algorithm have been proven in a number of studies. behavior informatics (BI) The informatics of behaviors so as to obtain behavior intelligence and behavior insights. behavior tree (BT) A mathematical model of plan execution used in computer science, robotics, control systems and video games. They describe switchings between a finite set of tasks in a modular fashion. Their strength comes from their ability to create very complex tasks composed of simple tasks, without worrying how the simple tasks are implemented. BTs present some similarities to hierarchical state machines with the key difference that the main building block of a behavior is a task rather than a state. Its ease of human understanding make BTs less error-prone and very popular in the game developer community. BTs have shown to generalize several other control architectures. belief desire intention software model (BDI) A software model developed for programming intelligent agents. Superficially characterized by the implementation of an agents beliefs, desires and intentions, it actually uses these concepts to solve a particular problem in agent programming. In essence, it provides a mechanism for separating the activity of selecting a plan (from a plan library or an external planner application) from the execution of currently active plans. Consequently, BDI agents are able to balance the time spent on deliberating about plans (choosing what to do) and executing those plans (doing it). A third activity, creating the plans in the first place (planning), is not within the scope of the model, and is left to the system designer and programmer. bias variance tradeoff In statistics and machine learning, the bias variance tradeoff is the property of a set of predictive models whereby models with a lower bias in parameter estimation have a higher variance of the parameter estimates across samples, and vice versa. big data A term used to refer to data sets that are too large or complex for traditional data-processing application software to adequately deal with. Data with many cases (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Big O notation A mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann Landau notation or asymptotic notation. binary tree A tree data structure in which each node has at most two children, which are referred to as the left child and the right child. A recursive definition using just set theory notions is that a (non-empty) binary tree is a tuple (L, S, R), where L and R are binary trees or the empty set and S is a singleton set. Some authors allow the binary tree to be the empty set as well. blackboard system An artificial intelligence approach based on the blackboard architectural model, where a common knowledge base, the blackboard, is iteratively updated by a diverse group of specialist knowledge sources, starting with a problem specification and ending with a solution. Each knowledge source updates the blackboard with a partial solution when its internal constraints match the blackboard state. In this way, the specialists work together to solve the problem. Boltzmann machine Also stochastic Hopfield network with hidden units. A type of stochastic recurrent neural network and Markov random field. Boltzmann machines can be seen as the stochastic, generative counterpart of Hopfield networks. Boolean satisfiability problem Also propositional satisfiability problem abbreviated SATISFIABILITY or SAT. The problem of determining if there exists an interpretation that satisfies a given Boolean formula. In other words, it asks whether the variables of a given Boolean formula can be consistently replaced by the values TRUE or FALSE in such a way that the formula evaluates to TRUE. If this is the case, the formula is called satisfiable. On the other hand, if no such assignment exists, the function expressed by the formula is FALSE for all possible variable assignments and the formula is unsatisfiable. For example, the formula a AND NOT b is satisfiable because one can find the values . In contrast, a AND NOT a is unsatisfiable. boosting A machine learning ensemble metaheuristic for primarily reducing bias (as opposed to variance), by training models sequentially, each one correcting the errors of its predecessor. bootstrap aggregating Also bagging or bootstrapping. A machine learning ensemble metaheuristic for primarily reducing variance (as opposed to bias), by training multiple models independently and averaging their predictions. brain technology Also self-learning know-how system. A technology that employs the latest findings in neuroscience. The term was first introduced by the Artificial Intelligence Laboratory in Zurich, Switzerland, in the context of the ROBOY project. Brain Technology can be employed in robots, know-how management systems and any other application with self-learning capabilities. In particular, Brain Technology applications allow the visualization of the underlying learning architecture often coined as know-how maps. branching factor In computing, tree data structures, and game theory, the number of children at each node, the outdegree. If this value is not uniform, an average branching factor can be calculated. brute-force search Also exhaustive search or generate and test. A very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution and checking whether each candidate satisfies the problems statement. C capsule neural network (CapsNet) A machine learning system that is a type of artificial neural network (ANN) that can be used to better model hierarchical relationships. The approach is an attempt to more closely mimic biological neural organization. case-based reasoning (CBR) Broadly construed, the process of solving new problems based on the solutions of similar past problems. chatbot Also smartbot, talkbot, chatterbot, bot, IM bot, interactive agent, conversational interface, or artificial conversational entity. A computer program or an artificial intelligence which conducts a conversation via auditory or textual methods. cloud robotics A field of robotics that attempts to invoke cloud technologies such as cloud computing, cloud storage, and other Internet technologies centred on the benefits of converged infrastructure and shared services for robotics. When connected to the cloud, robots can benefit from the powerful computation, storage, and communication resources of modern data center in the cloud, which can process and share information from various robots or agent (other machines, smart objects, humans, etc.). Humans can also delegate tasks to robots remotely through networks. Cloud computing technologies enable robot systems to be endowed with powerful capability whilst reducing costs through cloud technologies. Thus, it is possible to build lightweight, low cost, smarter robots have intelligent brain in the cloud. The brain consists of data center, knowledge base, task planners, deep learning, information processing, environment models, communication support, etc. cluster analysis Also clustering. The task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics. Cobweb An incremental system for hierarchical conceptual clustering. COBWEB was invented by Professor Douglas H. Fisher, currently at Vanderbilt University. COBWEB incrementally organizes observations into a classification tree. Each node in a classification tree represents a class (concept) and is labeled by a probabilistic concept that summarizes the attribute-value distributions of objects classified under the node. This classification tree can be used to predict missing attributes or the class of a new object. cognitive architecture The Institute of Creative Technologies defines cognitive architecture as hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together in conjunction with knowledge and skills embodied within the architecture to yield intelligent behavior in a diversity of complex environments. cognitive computing In general, the term cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain and helps to improve human decision-making. In this sense, CC is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus. cognitive science The interdisciplinary scientific study of the mind and its processes. combinatorial optimization In Operations Research, applied mathematics and theoretical computer science, combinatorial optimization is a topic that consists of finding an optimal object from a finite set of objects. committee machine A type of artificial neural network using a divide and conquer strategy in which the responses of multiple neural networks (experts) are combined into a single response. The combined response of the committee machine is supposed to be superior to those of its constituent experts. Compare ensembles of classifiers. commonsense knowledge In artificial intelligence research, commonsense knowledge consists of facts about the everyday world, such as Lemons are sour, that all humans are expected to know. The first AI program to address common sense knowledge was Advice Taker in 1959 by John McCarthy. commonsense reasoning A branch of artificial intelligence concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day. computational chemistry A branch of chemistry that uses computer simulation to assist in solving chemical problems. computational complexity theory Focuses on classifying computational problems according to their inherent difficulty, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm. computational creativity Also artificial creativity, mechanical creativity, creative computing, or creative computation. A multidisciplinary endeavour that includes the fields of artificial intelligence, cognitive psychology, philosophy, and the arts. computational cybernetics The integration of cybernetics and computational intelligence techniques. computational humor A branch of computational linguistics and artificial intelligence which uses computers in humor research. computational intelligence (CI) Usually refers to the ability of a computer to learn a specific task from data or experimental observation. computational learning theory In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms. computational linguistics An interdisciplinary field concerned with the statistical or rule-based modeling of natural language from a computational perspective, as well as the study of appropriate computational approaches to linguistic questions. computational mathematics The mathematical research in areas of science where computing plays an essential role. computational neuroscience Also theoretical neuroscience or mathematical neuroscience. A branch of neuroscience which employs mathematical models, theoretical analysis and abstractions of the brain to understand the principles that govern the development, structure, physiology, and cognitive abilities of the nervous system. computational number theory Also algorithmic number theory. The study of algorithms for performing number theoretic computations. computational problem In theoretical computer science, a computational problem is a mathematical object representing a collection of questions that computers might be able to solve. computational statistics Also statistical computing. The interface between statistics and computer science. computer-automated design (CAutoD) Design automation usually refers to electronic design automation, or Design Automation which is a Product Configurator. Extending Computer-Aided Design (CAD), automated design and computer-automated design are concerned with a broader range of applications, such as automotive engineering, civil engineering, composite material design, control engineering, dynamic system identification and optimization, financial systems, industrial equipment, mechatronic systems, steel construction, structural optimisation, and the invention of novel systems. More recently, traditional CAD simulation is seen to be transformed to CAutoD by biologically inspired machine learning, including heuristic search techniques such as evolutionary computation, and swarm intelligence algorithms. computer audition (CA) See machine listening. computer science The theory, experimentation, and engineering that form the basis for the design and use of computers. It involves the study of algorithms that process, store, and communicate digital information. A computer scientist specializes in the theory of computation and the design of computational systems. computer vision An interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. concept drift In predictive analytics and machine learning, the concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes. connectionism An approach in the fields of cognitive science, that hopes to explain mental phenomena using artificial neural networks. consistent heuristic In the study of path-finding problems in artificial intelligence, a heuristic function is said to be consistent, or monotone, if its estimate is always less than or equal to the estimated distance from any neighboring vertex to the goal, plus the cost of reaching that neighbor. constrained conditional model (CCM) A machine learning and inference framework that augments the learning of conditional (probabilistic or discriminative) models with declarative constraints. constraint logic programming A form of constraint programming, in which logic programming is extended to include concepts from constraint satisfaction. A constraint logic program is a logic program that contains constraints in the body of clauses. An example of a clause including a constraint is A(X,Y) - XY0, B(X), C(Y). In this clause, XY0 is a constraint A(X,Y), B(X), and C(Y) are literals as in regular logic programming. This clause states one condition under which the statement A(X,Y) holds XY is greater than zero and both B(X) and C(Y) are true. constraint programming A programming paradigm wherein relations between variables are stated in the form of constraints. Constraints differ from the common primitives of imperative programming languages in that they do not specify a step or sequence of steps to execute, but rather the properties of a solution to be found. constructed language Also conlang. A language whose phonology, grammar, and vocabulary are consciously devised, instead of having developed naturally. Constructed languages may also be referred to as artificial, planned, or invented languages. control theory In control systems engineering is a subfield of mathematics that deals with the control of continuously operating dynamical systems in engineered processes and machines. The objective is to develop a control model for controlling such systems using a control action in an optimum manner without delay or overshoot and ensuring control stability. convolutional neural network In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural network most commonly applied to image analysis. CNNs use a variation of multilayer perceptrons designed to require minimal preprocessing. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics. crossover Also recombination. In genetic algorithms and evolutionary computation, a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and analogous to the crossover that happens during sexual reproduction in biological organisms. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions are typically mutated before being added to the population. D Darkforest A computer go program developed by Facebook, based on deep learning techniques using a convolutional neural network. Its updated version combines the techniques of its predecessor with Monte Carlo tree search. The MCTS effectively takes tree search methods commonly seen in computer chess programs and randomizes them. With the update, the system is known as . Dartmouth workshop The Dartmouth Summer Research Project on Artificial Intelligence was the name of a 1956 summer workshop now considered by many (though not all) to be the seminal event for artificial intelligence as a field. data augmentation Data augmentation in data analysis are techniques used to increase the amount of data. It helps reduce overfitting when training a learning algorithm. data fusion The process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source. data integration The process of combining data residing in different sources and providing users with a unified view of them. This process becomes significant in a variety of situations, which include both commercial (such as when two similar companies need to merge their databases) and scientific (combining research results from different bioinformatics repositories, for example) domains. Data integration appears with increasing frequency as the volume (that is, big data) and the need to share existing data explodes. It has become the focus of extensive theoretical work, and numerous open problems remain unsolved. data mining The process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. data science An interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from data in various forms, both structured and unstructured, similar to data mining. Data science is a concept to unify statistics, data analysis, machine learning, and their related methods in order to understand and analyze actual phenomena with data. It employs techniques and theories drawn from many fields within the context of mathematics, statistics, information science, and computer science. data set Also dataset. A collection of data. Most commonly a data set corresponds to the contents of a single database table, or a single statistical data matrix, where every column of the table represents a particular variable, and each row corresponds to a given member of the data set in question. The data set lists values for each of the variables, such as height and weight of an object, for each member of the data set. Each value is known as a datum. The data set may comprise data for one or more members, corresponding to the number of rows. data warehouse (DW or DWH) Also enterprise data warehouse (EDW). A system used for reporting and data analysis. DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data in one single place Datalog A declarative logic programming language that syntactically is a subset of Prolog. It is often used as a query language for deductive databases. In recent years, Datalog has found new application in data integration, information extraction, networking, program analysis, security, and cloud computing. decision boundary In the case of backpropagation-based artificial neural networks or perceptrons, the type of decision boundary that the network can learn is determined by the number of hidden layers in the network. If it has no hidden layers, then it can only learn linear problems. If it has one hidden layer, then it can learn any continuous function on compact subsets of Rn as shown by the Universal approximation theorem, thus it can have an arbitrary decision boundary. decision support system (DSS) Aan information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance i.e. unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both. decision theory Also theory of choice. The study of the reasoning underlying an agents choices. Decision theory can be broken into two branches normative decision theory, which gives advice on how to make the best decisions given a set of uncertain beliefs and a set of values, and descriptive decision theory which analyzes how existing, possibly irrational agents actually make decisions. decision tree learning Uses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the items target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. declarative programming A programming paradigm a style of building the structure and elements of computer programs that expresses the logic of a computation without describing its control flow. deductive classifier A type of artificial intelligence inference engine. It takes as input a set of declarations in a frame language about a domain such as medical research or molecular biology. For example, the names of classes, sub-classes, properties, and restrictions on allowable values. Deep Blue was a chess-playing computer developed by IBM. It is known for being the first computer chess-playing system to win both a chess game and a chess match against a reigning world champion under regular time controls. deep learning A subset of machine learning that focuses on utilizing neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and training them to process data. The adjective deep refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised, or unsupervised. DeepMind Technologies A British artificial intelligence company founded in September 2010, currently owned by Alphabet Inc. The company is based in London, with research centres in Canada, France, and the United States. Acquired by Google in 2014, the company has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain. The company made headlines in 2016 after its AlphaGo program beat human professional Go player Lee Sedol, the world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing Go, chess, and shogi (Japanese chess) after a few days of play against itself using reinforcement learning. default logic A non-monotonic logic proposed by Raymond Reiter to formalize reasoning with default assumptions. Density-based spatial clustering of applications with noise (DBSCAN) A clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, J rg Sander, and Xiaowei Xu in 1996. description logic (DL) A family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy descriptions logics, and each description logic features a different balance between DL expressivity and reasoning complexity by supporting different sets of mathematical constructors. developmental robotics (DevRob) Also epigenetic robotics. A scientific field which aims at studying the developmental mechanisms, architectures, and constraints that allow lifelong and open-ended learning of new skills and new knowledge in embodied machines. diagnosis Concerned with the development of algorithms and techniques that are able to determine whether the behaviour of a system is correct. If the system is not functioning correctly, the algorithm should be able to determine, as accurately as possible, which part of the system is failing, and which kind of fault it is facing. The computation is based on observations, which provide information on the current behaviour. dialogue system Also conversational agent (CA). A computer system intended to converse with a human with a coherent structure. Dialogue systems have employed text, speech, graphics, haptics, gestures, and other modes for communication on both the input and output channel. diffusion model In machine learning, diffusion models, also known as diffusion probabilistic models or score-based generative models, are a class of latent variable models. They are Markov chains trained using variational inference. The goal of diffusion models is to learn the latent structure of a dataset by modeling the way in which data points diffuse through the latent space. In computer vision, this means that a neural network is trained to denoise images blurred with Gaussian noise by learning to reverse the diffusion process. It mainly consists of three major components the forward process, the reverse process, and the sampling procedure. Three examples of generic diffusion modeling frameworks used in computer vision are denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. Dijkstras algorithm An algorithm for finding the shortest paths between nodes in a weighted graph, which may represent, for example, road networks. dimensionality reduction Also dimension reduction. The process of reducing the number of random variables under consideration by obtaining a set of principal variables. It can be divided into feature selection and feature extraction. discrete system Any system with a countable number of states. Discrete systems may be contrasted with continuous systems, which may also be called analog systems. A final discrete system is often modeled with a directed graph and is analyzed for correctness and complexity according to computational theory. Because discrete systems have a countable number of states, they may be described in precise mathematical models. A computer is a finite-state machine that may be viewed as a discrete system. Because computers are often used to model not only other discrete systems but continuous systems as well, methods have been developed to represent real-world continuous systems as discrete systems. One such method involves sampling a continuous signal at discrete time intervals. distributed artificial intelligence (DAI) Also decentralized artificial intelligence. A subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems. double descent A phenomenon in statistics and machine learning where a model with a small number of parameters and a model with an extremely large number of parameters have a small test error, but a model whose number of parameters is about the same as the number of data points used to train the model will have a large error. This phenomenon has been considered surprising, as it contradicts assumptions about overfitting in classical machine learning. dropout Also dilution. A regularization technique for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. dynamic epistemic logic (DEL) A logical framework dealing with knowledge and information change. Typically, DEL focuses on situations involving multiple agents and studies how their knowledge changes when events occur. E eager learning A learning method in which the system tries to construct a general, input-independent target function during training of the system, as opposed to lazy learning, where generalization beyond the training data is delayed until a query is made to the system. early stopping A regularization technique often used when training a machine learning model with an iterative method such as gradient descent. Ebert test A test which gauges whether a computer-based synthesized voice can tell a joke with sufficient skill to cause people to laugh. It was proposed by film critic Roger Ebert at the 2011 TED conference as a challenge to software developers to have a computerized voice master the inflections, delivery, timing, and intonations of a speaking human. The test is similar to the Turing test proposed by Alan Turing in 1950 as a way to gauge a computers ability to exhibit intelligent behavior by generating performance indistinguishable from a human being. echo state network (ESN) A recurrent neural network with a sparsely connected hidden layer (with typically 1 connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can (re)produce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system. embodied agent Also interface agent. An intelligent agent that interacts with the environment through a physical body within that environment. Agents that are represented graphically with a body, for example a human or a cartoon animal, are also called embodied agents, although they have only virtual, not physical, embodiment. embodied cognitive science An interdisciplinary field of research, the aim of which is to explain the mechanisms underlying intelligent behavior. It comprises three main methodologies 1) the modeling of psychological and biological systems in a holistic manner that considers the mind and body as a single entity, 2) the formation of a common set of general principles of intelligent behavior, and 3) the experimental use of robotic agents in controlled environments. error-driven learning A sub-area of machine learning concerned with how an agent ought to take actions in an environment so as to minimize some error feedback. It is a type of reinforcement learning. ensemble learning The use of multiple machine learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. epoch In machine learning, particularly in the creation of artificial neural networks, an epoch is training the model for one cycle through the full training dataset. Small models are typically trained for as many epochs as it takes to reach the best performance on the validation dataset. The largest models may train for only one epoch. ethics of artificial intelligence The part of the ethics of technology specific to artificial intelligence. evolutionary algorithm (EA) A subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators. evolutionary computation A family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character. evolving classification function (ECF) Evolving classification functions are used for classifying and clustering in the field of machine learning and artificial intelligence, typically employed for data stream mining tasks in dynamic and changing environments. existential risk The hypothesis that substantial progress in artificial general intelligence (AGI) could someday result in human extinction or some other unrecoverable global catastrophe. expert system A computer system that emulates the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if then rules rather than through conventional procedural code. F fast-and-frugal trees A type of classification tree. Fast-and-frugal trees can be used as decision-making tools which operate as lexicographic classifiers, and, if required, associate an action (decision) to each class or category. feature An individual measurable property or characteristic of a phenomenon. In computer vision and image processing, a feature is a piece of information about the content of an image typically about whether a certain region of the image has certain properties. Features may be specific structures in an image (such as points, edges, or objects), or the result of a general neighborhood operation or feature detection applied to the image. feature extraction In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. feature learning Also representation learning. In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features and use them to perform a specific task. feature selection In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. federated learning A machine learning technique that allows for training models on multiple devices with decentralized data, thus helping preserve the privacy of individual users and their data. first-order logic Also first-order predicate calculus or predicate logic. A collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form there exists X such that X is Socrates and X is a man and there exists is a quantifier while X is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations. fluent A condition that can change over time. In logical approaches to reasoning about actions, fluents can be represented in first-order logic by predicates having an argument that depends on time. formal language A set of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules. forward chaining Also forward reasoning. One of the two main methods of reasoning when using an inference engine and can be described logically as repeated application of modus ponens. Forward chaining is a popular implementation strategy for expert systems, businesses and production rule systems. The opposite of forward chaining is backward chaining. Forward chaining starts with the available data and uses inference rules to extract more data (from an end user, for example) until a goal is reached. An inference engine using forward chaining searches the inference rules until it finds one where the antecedent (If clause) is known to be true. When such a rule is found, the engine can conclude, or infer, the consequent (Then clause), resulting in the addition of new information to its data. frame An artificial intelligence data structure used to divide knowledge into substructures by representing stereotyped situations. Frames are the primary data structure used in artificial intelligence frame language. frame language A technology used for knowledge representation in artificial intelligence. Frames are stored as ontologies of sets and subsets of the frame concepts. They are similar to class hierarchies in object-oriented languages although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on encapsulation and information hiding. Frames originated in AI research and objects primarily in software engineering. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly. frame problem The problem of finding adequate collections of axioms for a viable description of a robot environment. friendly artificial intelligence Also friendly AI or FAI. A hypothetical artificial general intelligence (AGI) that would have a positive effect on humanity. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained. futures studies The study of postulating possible, probable, and preferable futures and the worldviews and myths that underlie them. fuzzy control system A control system based on fuzzy logic a mathematical system that analyzes analog input values in terms of logical variables that take on continuous values between 0 and 1, in contrast to classical or digital logic, which operates on discrete values of either 1 or 0 (true or false, respectively). fuzzy logic A simple form for the many-valued logic, in which the truth values of variables may have any degree of Truthfulness that can be represented by any real number in the range between 0 (as in Completely False) and 1 (as in Completely True) inclusive. Consequently, It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. In contrast to Boolean logic, where the truth values of variables may have the integer values 0 or 1 only. fuzzy rule A rule used within fuzzy logic systems to infer an output based on input variables. fuzzy set In classical set theory, the membership of elements in a set is assessed in binary terms according to a bivalent condition an element either belongs or does not belong to the set. By contrast, fuzzy set theory permits the gradual assessment of the membership of elements in a set this is described with the aid of a membership function valued in the real unit interval 0, 1. Fuzzy sets generalize classical sets, since the indicator functions (aka characteristic functions) of classical sets are special cases of the membership functions of fuzzy sets, if the latter only take values 0 or 1. In fuzzy set theory, classical bivalent sets are usually called crisp sets. The fuzzy set theory can be used in a wide range of domains in which information is incomplete or imprecise, such as bioinformatics. G game theory The study of mathematical models of strategic interaction between rational decision-makers. general game playing (GGP) General game playing is the design of artificial intelligence programs to be able to run and play more than one game successfully. generalization The concept that humans, other animals, and artificial neural networks use past learning in present situations of learning if the conditions in the situations are regarded as similar. generalization error For supervised learning applications in machine learning and statistical learning theory, generalization error (also known as the out-of-sample error or the risk) is a measure of how accurately a learning algorithm is able to predict outcomes for previously unseen data. generative adversarial network (GAN) A class of machine learning systems. Two neural networks contest with each other in a zero-sum game framework. generative artificial intelligence Generative artificial intelligence is artificial intelligence capable of generating text, images, or other media in response to prompts. Generative AI models learn the patterns and structure of their input training data and then generate new data that has similar characteristics, typically using transformer-based deep neural networks. generative pretrained transformer (GPT) A large language model based on the transformer architecture that generates text. It is first pretrained to predict the next token in texts (a token is typically a word, subword, or punctuation). After their pretraining, GPT models can generate human-like text by repeatedly predicting the token that they would expect to follow. GPT models are usually also fine-tuned, for example with reinforcement learning from human feedback to reduce hallucination or harmful behaviour, or to format the output in a conversationnal format. genetic algorithm (GA) A metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on bio-inspired operators such as mutation, crossover and selection. genetic operator An operator used in genetic algorithms to guide the algorithm towards a solution to a given problem. There are three main types of operators (mutation, crossover and selection), which must work in conjunction with one another in order for the algorithm to be successful. glowworm swarm optimization A swarm intelligence optimization algorithm based on the behaviour of glowworms (also known as fireflies or lightning bugs). gradient boosting A machine learning technique based on boosting in a functional space, where the target is pseudo-residuals instead of residuals as in traditional boosting. graph (abstract data type) In computer science, a graph is an abstract data type that is meant to implement the undirected graph and directed graph concepts from mathematics specifically, the field of graph theory. graph (discrete mathematics) In mathematics, and more specifically in graph theory, a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense related. The objects correspond to mathematical abstractions called vertices (also called nodes or points) and each of the related pairs of vertices is called an edge (also called an arc or line). graph database (GDB) A database that uses graph structures for semantic queries with nodes, edges, and properties to represent and store data. A key concept of the system is the graph (or edge or relationship), which directly relates data items in the store a collection of nodes of data and edges representing the relationships between the nodes. The relationships allow data in the store to be linked together directly, and in many cases retrieved with one operation. Graph databases hold the relationships between data as a priority. Querying relationships within a graph database is fast because they are perpetually stored within the database itself. Relationships can be intuitively visualized using graph databases, making it useful for heavily inter-connected data. graph theory The study of graphs, which are mathematical structures used to model pairwise relations between objects. graph traversal Also graph search. The process of visiting (checking and/or updating) each vertex in a graph. Such traversals are classified by the order in which the vertices are visited. Tree traversal is a special case of graph traversal. H hallucination A response generated by AI that contains false or misleading information presented as fact. heuristic A technique designed for solving a problem more quickly when classic methods are too slow, or for finding an approximate solution when classic methods fail to find any exact solution. This is achieved by trading optimality, completeness, accuracy, or precision for speed. In a way, it can be considered a shortcut. A heuristic function, also called simply a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution. hidden layer A layer of neurons in an artificial neural network that is neither an input layer nor an output layer. hyper-heuristic A heuristic search method that seeks to automate the process of selecting, combining, generating, or adapting several simpler heuristics (or components of such heuristics) to efficiently solve computational search problems, often by the incorporation of machine learning techniques. One of the motivations for studying hyper-heuristics is to build systems which can handle classes of problems rather than solving just one problem. hyperparameter A parameter that can be set in order to define any configurable part of a machine learning models learning process. hyperparameter optimization The process of choosing a set of optimal hyperparameters for a learning algorithm. hyperplane A decision boundary in machine learning classifiers that partitions the input space into two or more sections, with each section corresponding to a unique class label. I IEEE Computational Intelligence Society A professional society of the Institute of Electrical and Electronics Engineers (IEEE) focussing on the theory, design, application, and development of biologically and linguistically motivated computational paradigms emphasizing neural networks, connectionist systems, genetic algorithms, evolutionary programming, fuzzy systems, and hybrid intelligent systems in which these paradigms are contained. incremental learning A method of machine learning, in which input data is continuously used to extend the existing models knowledge i.e. to further train the model. It represents a dynamic technique of supervised and unsupervised learning that can be applied when training data becomes available gradually over time or its size is out of system memory limits. Algorithms that can facilitate incremental learning are known as incremental machine learning algorithms. inference engine A component of the system that applies logical rules to the knowledge base to deduce new information. information integration (II) The merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in data mining and consolidation of data from unstructured or semi-structured resources. Typically, information integration refers to textual representations of knowledge but is sometimes applied to rich-media content. Information fusion, which is a related term, involves the combination of information into a new set of information towards reducing redundancy and uncertainty. Information Processing Language (IPL) A programming language that includes features intended to help with programs that perform simple problem solving actions such as lists, dynamic memory allocation, data types, recursion, functions as arguments, generators, and cooperative multitasking. IPL invented the concept of list processing, albeit in an assembly-language style. intelligence amplification (IA) Also cognitive augmentation, machine augmented intelligence, and enhanced intelligence. The effective use of information technology in augmenting human intelligence. intelligence explosion A possible outcome of humanity building artificial general intelligence (AGI). AGI would be capable of recursive self-improvement leading to rapid emergence of ASI (artificial superintelligence), the limits of which are unknown, at the time of the technological singularity. intelligent agent (IA) An autonomous entity which acts, directing its activity towards achieving goals (i.e. it is an agent), upon an environment using observation through sensors and consequent actuators (i.e. it is intelligent). Intelligent agents may also learn or use knowledge to achieve their goals. They may be very simple or very complex. intelligent control A class of control techniques that use various artificial intelligence computing approaches like neural networks, Bayesian probability, fuzzy logic, machine learning, reinforcement learning, evolutionary computation and genetic algorithms. intelligent personal assistant Also virtual assistant or personal digital assistant. A software agent that can perform tasks or services for an individual based on verbal commands. Sometimes the term chatbot is used to refer to virtual assistants generally or specifically accessed by online chat (or in some cases online chat programs that are exclusively for entertainment purposes). Some virtual assistants are able to interpret human speech and respond via synthesized voices. Users can ask their assistants questions, control home automation devices and media playback via voice, and manage other basic tasks such as email, to-do lists, and calendars with verbal commands. interpretation An assignment of meaning to the symbols of a formal language. Many formal languages used in mathematics, logic, and theoretical computer science are defined in solely syntactic terms, and as such do not have any meaning until they are given some interpretation. The general study of interpretations of formal languages is called formal semantics. intrinsic motivation An intelligent agent is intrinsically motivated to act if the information content alone, of the experience resulting from the action, is the motivating factor. Information content in this context is measured in the information theory sense as quantifying uncertainty. A typical intrinsic motivation is to search for unusual (surprising) situations, in contrast to a typical extrinsic motivation such as the search for food. Intrinsically motivated artificial agents display behaviours akin to exploration and curiosity. issue tree Also logic tree. A graphical breakdown of a question that dissects it into its different components vertically and that progresses into details as it reads to the right. 47 Issue trees are useful in problem solving to identify the root causes of a problem as well as to identify its potential solutions. They also provide a reference point to see how each piece fits into the whole picture of a problem. J junction tree algorithm Also Clique Tree. A method used in machine learning to extract marginalization in general graphs. In essence, it entails performing belief propagation on a modified graph called a junction tree. The graph is called a tree because it branches into different sections of data nodes of variables are the branches. K kernel method In machine learning, kernel methods are a class of algorithms for pattern analysis, whose best known member is the support vector machine (SVM). The general task of pattern analysis is to find and study general types of relations (e.g., cluster analysis, rankings, principal components, correlations, classifications) in datasets. KL-ONE A well-known knowledge representation system in the tradition of semantic networks and frames that is, it is a frame language. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network. k-nearest neighbors A non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. knowledge acquisition The process used to define the rules and ontologies required for a knowledge-based system. The phrase was first used in conjunction with expert systems to describe the initial tasks associated with developing an expert system, namely finding and interviewing domain experts and capturing their knowledge via rules, objects, and frame-based ontologies. knowledge-based system (KBS) A computer program that reasons and uses a knowledge base to solve complex problems. The term is broad and refers to many different kinds of systems. The one common theme that unites all knowledge based systems is an attempt to represent knowledge explicitly and a reasoning system that allows it to derive new knowledge. Thus, a knowledge-based system has two distinguishing features a knowledge base and an inference engine. knowledge distillation The process of transferring knowledge from a large machine learning model to a smaller one. knowledge engineering (KE) All technical, scientific, and social aspects involved in building, maintaining, and using knowledge-based systems. knowledge extraction The creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction and ETL, the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data. knowledge Interchange Format (KIF) A computer language designed to enable systems to share and reuse information from knowledge-based systems. KIF is similar to frame languages such as KL-ONE and LOOM but unlike such language its primary role is not intended as a framework for the expression or use of knowledge but rather for the interchange of knowledge between systems. The designers of KIF likened it to PostScript. PostScript was not designed primarily as a language to store and manipulate documents but rather as an interchange format for systems and devices to share documents. In the same way KIF is meant to facilitate sharing of knowledge across different systems that use different languages, formalisms, platforms, etc. knowledge representation and reasoning (KR or KRR) The field of artificial intelligence dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build. Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets. Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers. k-means clustering A method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. L language model A probabilistic model that manipulates natural language. large language model (LLM) A language model with a large number of parameters (typically at least a billion) that are adjusted during training. Due to its size, it requires a lot of data and computing capability to train. Large language models are usually based on the transformer architecture. lazy learning In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to in eager learning, where the system tries to generalize the training data before receiving queries. Lisp (programming language) (LISP) A family of programming languages with a long history and a distinctive, fully parenthesized prefix notation. logic programming A type of programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain. Major logic programming language families include Prolog, answer set programming (ASP), and Datalog. long short-term memory (LSTM) An artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections that make it a general purpose computer (that is, it can compute anything that a Turing machine can). It can not only process single data points (such as images), but also entire sequences of data (such as speech or video). M machine vision (MV) The technology and methods used to provide imaging-based automatic inspection and analysis for such applications as automatic inspection, process control, and robot guidance, usually in industry. Machine vision is a term encompassing a large number of technologies, software and hardware products, integrated systems, actions, methods and expertise. Machine vision as a systems engineering discipline can be considered distinct from computer vision, a form of computer science. It attempts to integrate existing technologies in new ways and apply them to solve real world problems. The term is the prevalent one for these functions in industrial automation environments but is also used for these functions in other environments such as security and vehicle guidance. Markov chain A stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Markov decision process (MDP) A discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming and reinforcement learning. mathematical optimization Also mathematical programming. In mathematics, computer science, and operations research, the selection of a best element (with regard to some criterion) from some set of available alternatives. machine learning (ML) The scientific study of algorithms and statistical models that computer systems use in order to perform a specific task effectively without using explicit instructions, relying on patterns and inference instead. machine listening Also computer audition (CA). A general field of study of algorithms and systems for audio understanding by machine. machine perception The capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. mechanism design A field in economics and game theory that takes an engineering approach to designing economic mechanisms or incentives, toward desired objectives, in strategic settings, where players act rationally. Because it starts at the end of the game, then goes backwards, it is also called reverse game theory. It has broad applications, from economics and politics (markets, auctions, voting procedures) to networked-systems (internet interdomain routing, sponsored search auctions). mechatronics Also mechatronic engineering. A multidisciplinary branch of engineering that focuses on the engineering of both electrical and mechanical systems, and also includes a combination of robotics, electronics, computer, telecommunications, systems, control, and product engineering. metabolic network reconstruction and simulation Allows for an in-depth insight into the molecular mechanisms of a particular organism. In particular, these models correlate the genome with molecular physiology. metaheuristic In computer science and mathematical optimization, a metaheuristic is a higher-level procedure or heuristic designed to find, generate, or select a heuristic (partial search algorithm) that may provide a sufficiently good solution to an optimization problem, especially with incomplete or imperfect information or limited computation capacity. Metaheuristics sample a set of solutions which is too large to be completely sampled. model checking In computer science, model checking or property checking is, for a given model of a system, exhaustively and automatically checking whether this model meets a given specification. Typically, one has hardware or software systems in mind, whereas the specification contains safety requirements such as the absence of deadlocks and similar critical states that can cause the system to crash. Model checking is a technique for automatically verifying correctness properties of finite-state systems. modus ponens In propositional logic, modus ponens is a rule of inference. It can be summarized as P implies Q and P is asserted to be true, therefore Q must be true. modus tollens In propositional logic, modus tollens is a valid argument form and a rule of inference. It is an application of the general truth that if a statement is true, then so is its contrapositive. The inference rule modus tollens asserts that the inference from P implies Q to the negation of Q implies the negation of P is valid. Monte Carlo tree search In computer science, Monte Carlo tree search (MCTS) is a heuristic search algorithm for some kinds of decision processes. multi-agent system (MAS) Also self-organized system. A computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning. multilayer perceptron (MLP) In deep learning, a multilayer perceptron (MLP) is a name for a modern feedforward neural network consisting of fully connected neurons with nonlinear activation functions, organized in layers, notable for being able to distinguish data that is not linearly separable. multi-swarm optimization A variant of particle swarm optimization (PSO) based on the use of multiple sub-swarms instead of one (standard) swarm. The general approach in multi-swarm optimization is that each sub-swarm focuses on a specific region while a specific diversification method decides where and when to launch the sub-swarms. The multi-swarm framework is especially fitted for the optimization on multi-modal problems, where multiple (local) optima exist. mutation A genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation. Mutation alters one or more gene values in a chromosome from its initial state. In mutation, the solution may change entirely from the previous solution. Hence GA can come to a better solution by using mutation. Mutation occurs during evolution according to a user-definable mutation probability. This probability should be set low. If it is set too high, the search will turn into a primitive random search. Mycin An early backward chaining expert system that used artificial intelligence to identify bacteria causing severe infections, such as bacteremia and meningitis, and to recommend antibiotics, with the dosage adjusted for patients body weight the name derived from the antibiotics themselves, as many antibiotics have the suffix -mycin. The MYCIN system was also used for the diagnosis of blood clotting diseases. N naive Bayes classifier In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes theorem with strong (naive) independence assumptions between the features. naive semantics An approach used in computer science for representing basic knowledge about a specific domain, and has been used in applications such as the representation of the meaning of natural language sentences in artificial intelligence applications. In a general setting the term has been used to refer to the use of a limited store of generally understood knowledge about a specific domain in the world, and has been applied to fields such as the knowledge based design of data schemas. name binding In programming languages, name binding is the association of entities (data and/or code) with identifiers. An identifier bound to an object is said to reference that object. Machine languages have no built-in notion of identifiers, but name-object bindings as a service and notation for the programmer is implemented by programming languages. Binding is intimately connected with scoping, as scope determines which names bind to which objects at which locations in the program code (lexically) and in which one of the possible execution paths (temporally). Use of an identifier id in a context that establishes a binding for id is called a binding (or defining) occurrence. In all other occurrences (e.g., in expressions, assignments, and subprogram calls), an identifier stands for what it is bound to such occurrences are called applied occurrences. named-entity recognition (NER) Also entity identification, entity chunking, and entity extraction. A subtask of information extraction that seeks to locate and classify named entity mentions in unstructured text into pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc. named graph A key concept of Semantic Web architecture in which a set of Resource Description Framework statements (a graph) are identified using a URI, allowing descriptions to be made of that set of statements such as context, provenance information or other such metadata. Named graphs are a simple extension of the RDF data model through which graphs can be created but the model lacks an effective means of distinguishing between them once published on the Web at large. natural language generation (NLG) A software process that transforms structured data into plain-English content. It can be used to produce long-form content for organizations to automate custom reports, as well as produce custom content for a web or mobile application. It can also be used to generate short blurbs of text in interactive conversations (a chatbot) which might even be read out loud by a text-to-speech system. natural language processing (NLP) A subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data. natural language programming An ontology-assisted way of programming in terms of natural-language sentences, e.g. English. network motif All networks, including biological networks, social networks, technological networks (e.g., computer networks and electrical circuits) and more, can be represented as graphs, which include a wide variety of subgraphs. One important local property of networks are so-called network motifs, which are defined as recurrent and statistically significant sub-graphs or patterns. neural machine translation (NMT) An approach to machine translation that uses a large artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model. neural network A neural network can refer to either a neural circuit of biological neurons (sometimes also called a biological neural network), or a network of artificial neurons or nodes in the case of an artificial neural network. Artificial neural networks are used for solving artificial intelligence (AI) problems they model connections of biological neurons as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be 1 and 1. neural Turing machine (NTM) A recurrent neural network model. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone. neuro-fuzzy Combinations of artificial neural networks and fuzzy logic. neurocybernetics Also brain computer interface (BCI), neural-control interface (NCI), mind-machine interface (MMI), direct neural interface (DNI), or brain machine interface (BMI). A direct communication pathway between an enhanced or wired brain and an external device. BCI differs from neuromodulation in that it allows for bidirectional information flow. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions. neuromorphic engineering Also neuromorphic computing. A concept describing the use of very-large-scale integration (VLSI) systems containing electronic analog circuits to mimic neuro-biological architectures present in the nervous system. In recent times, the term neuromorphic has been used to describe analog, digital, mixed-mode analog/digital VLSI, and software systems that implement models of neural systems (for perception, motor control, or multisensory integration). The implementation of neuromorphic computing on the hardware level can be realized by oxide-based memristors, spintronic memories, threshold switches, and transistors. node A basic unit of a data structure, such as a linked list or tree data structure. Nodes contain data and also may link to other nodes. Links between nodes are often implemented by pointers. nondeterministic algorithm An algorithm that, even for the same input, can exhibit different behaviors on different runs, as opposed to a deterministic algorithm. nouvelle AI Nouvelle AI differs from classical AI by aiming to produce robots with intelligence levels similar to insects. Researchers believe that intelligence can emerge organically from simple behaviors as these intelligences interacted with the real world, instead of using the constructed worlds which symbolic AIs typically needed to have programmed into them. NP In computational complexity theory, NP (nondeterministic polynomial time) is a complexity class used to classify decision problems. NP is the set of decision problems for which the problem instances, where the answer is yes, have proofs verifiable in polynomial time. NP-completeness In computational complexity theory, a problem is NP-complete when it can be solved by a restricted class of brute force search algorithms and it can be used to simulate any other problem with a similar algorithm. More precisely, each input to the problem should be associated with a set of solutions of polynomial length, whose validity can be tested quickly (in polynomial time), such that the output for any input is yes if the solution set is non-empty and no if it is empty. NP-hardness Also non-deterministic polynomial-time hardness. In computational complexity theory, the defining property of a class of problems that are, informally, at least as hard as the hardest problems in NP. A simple example of an NP-hard problem is the subset sum problem. O Occams razor Also Ockhams razor or Ochams razor. The problem-solving principle that states that when presented with competing hypotheses that make the same predictions, one should select the solution with the fewest assumptions the principle is not meant to filter out hypotheses that make different predictions. The idea is attributed to the English Franciscan friar William of Ockham (c. 1287 1347), a scholastic philosopher and theologian. offline learning A machine learning training approach in which a model is trained on a fixed dataset that is not updated during the learning process. online machine learning A method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time. ontology learning Also ontology extraction, ontology generation, or ontology acquisition. The automatic or semi-automatic creation of ontologies, including extracting the corresponding domains terms and the relationships between the concepts that these terms represent from a corpus of natural language text, and encoding them with an ontology language for easy retrieval. OpenAI The for-profit corporation OpenAI LP, whose parent organization is the non-profit organization OpenAI Inc that conducts research in the field of artificial intelligence (AI) with the stated aim to promote and develop friendly AI in such a way as to benefit humanity as a whole. OpenCog A project that aims to build an open-source artificial intelligence framework. OpenCog Prime is an architecture for robot and virtual embodied cognition that defines a set of interacting components designed to give rise to human-equivalent artificial general intelligence (AGI) as an emergent phenomenon of the whole system. Open Mind Common Sense An artificial intelligence project based at the Massachusetts Institute of Technology (MIT) Media Lab whose goal is to build and utilize a large commonsense knowledge base from the contributions of many thousands of people across the Web. open-source software (OSS) A type of computer software in which source code is released under an license in which the copyright holder grants users the rights to study, change, and distribute the software to anyone and for any purpose. Open-source software may be developed in an collaborative public manner. Open-source software is a prominent example of open collaboration. overfitting The production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably. In other words, an overfitted model memorizes training data details but cannot generalize to new data. Conversely, an underfitted model is too simple to capture the complexity of the training data. P partial order reduction A technique for reducing the size of the state-space to be searched by a model checking or automated planning and scheduling algorithm. It exploits the commutativity of concurrently executed transitions, which result in the same state when executed in different orders. partially observable Markov decision process (POMDP) A generalization of a Markov decision process (MDP). A POMDP models an agent decision process in which it is assumed that the system dynamics are determined by an MDP, but the agent cannot directly observe the underlying state. Instead, it must maintain a probability distribution over the set of possible states, based on a set of observations and observation probabilities, and the underlying MDP. particle swarm optimization (PSO) A computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formulae over the particles position and velocity. Each particles movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions. pathfinding Also pathing. The plotting, by a computer application, of the shortest route between two points. It is a more practical variant on solving mazes. This field of research is based heavily on Dijkstras algorithm for finding a shortest path on a weighted graph. pattern recognition Concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. perceptron An algorithm for supervised learning of binary classifiers. predicate logic Also first-order logic, predicate logic, and first-order predicate calculus. A collection of formal systems used in mathematics, philosophy, linguistics, and computer science. First-order logic uses quantified variables over non-logical objects and allows the use of sentences that contain variables, so that rather than propositions such as Socrates is a man one can have expressions in the form there exists x such that x is Socrates and x is a man and there exists is a quantifier while x is a variable. This distinguishes it from propositional logic, which does not use quantifiers or relations in this sense, propositional logic is the foundation of first-order logic. predictive analytics A variety of statistical techniques from data mining, predictive modelling, and machine learning, that analyze current and historical facts to make predictions about future or otherwise unknown events. principal component analysis (PCA) A statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components. This transformation is defined in such a way that the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible), and each succeeding component, in turn, has the highest variance possible under the constraint that it is orthogonal to the preceding components. The resulting vectors (each being a linear combination of the variables and containing n observations) are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables. principle of rationality Also rationality principle. A principle coined by Karl R. Popper in his Harvard Lecture of 1963, and published in his book Myth of Framework. It is related to what he called the logic of the situation in an Economica article of 1944/1945, published later in his book The Poverty of Historicism. According to Poppers rationality principle, agents act in the most adequate way according to the objective situation. It is an idealized conception of human behavior which he used to drive his model of situational logic. probabilistic programming (PP) A programming paradigm in which probabilistic models are specified and inference for these models is performed automatically. It represents an attempt to unify probabilistic modeling and traditional general-purpose programming in order to make the former easier and more widely applicable. It can be used to create systems that help make decisions in the face of uncertainty. Programming languages used for probabilistic programming are referred to as Probabilistic programming languages (PPLs). production system A computer program typically used to provide some form of AI, which consists primarily of a set of rules about behavior, but also includes the mechanism necessary to follow those rules as the system responds to states of the world. programming language A formal language, which comprises a set of instructions that produce various kinds of output. Programming languages are used in computer programming to implement algorithms. Prolog A logic programming language associated with artificial intelligence and computational linguistics. Prolog has its roots in first-order logic, a formal logic, and unlike many other programming languages, Prolog is intended primarily as a declarative programming language the program logic is expressed in terms of relations, represented as facts and rules. A computation is initiated by running a query over these relations. propositional calculus Also propositional logic, statement logic, sentential calculus, sentential logic, and zeroth-order logic. A branch of logic which deals with propositions (which can be true or false) and argument flow. Compound propositions are formed by connecting propositions by logical connectives. The propositions without logical connectives are called atomic propositions. Unlike first-order logic, propositional logic does not deal with non-logical objects, predicates about them, or quantifiers. However, all the machinery of propositional logic is included in first-order logic and higher-order logics. In this sense, propositional logic is the foundation of first-order logic and higher-order logic. proximal policy optimization (PPO) A reinforcement learning algorithm for training an intelligent agents decision function to accomplish difficult tasks. Python An interpreted, high-level, general-purpose programming language created by Guido van Rossum and first released in 1991. Pythons design philosophy emphasizes code readability with its notable use of significant whitespace. Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects. PyTorch A machine learning library based on the Torch library, used for applications such as computer vision and natural language processing, originally developed by Meta AI and now part of the Linux Foundation umbrella. Q Q-learning A model-free reinforcement learning algorithm for learning the value of an action in a particular state. qualification problem In philosophy and artificial intelligence (especially knowledge-based systems), the qualification problem is concerned with the impossibility of listing all of the preconditions required for a real-world action to have its intended effect. It might be posed as how to deal with the things that prevent me from achieving my intended result. It is strongly connected to, and opposite the ramification side of, the frame problem. quantifier In logic, quantification specifies the quantity of specimens in the domain of discourse that satisfy an open formula. The two most common quantifiers mean for all and there exists. For example, in arithmetic, quantifiers allow one to say that the natural numbers go on forever, by writing that for all n (where n is a natural number), there is another number (say, the successor of n) which is one bigger than n. quantum computing The use of quantum-mechanical phenomena such as superposition and entanglement to perform computation. A quantum computer is used to perform such computation, which can be implemented theoretically or physically. I-5 query language Query languages or data query languages (DQLs) are computer languages used to make queries in databases and information systems. Broadly, query languages can be classified according to whether they are database query languages or information retrieval query languages. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry. R R programming language A programming language and free software environment for statistical computing and graphics supported by the R Foundation for Statistical Computing. The R language is widely used among statisticians and data miners for developing statistical software and data analysis. radial basis function network In the field of mathematical modeling, a radial basis function network is an artificial neural network that uses radial basis functions as activation functions. The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. Radial basis function networks have many uses, including function approximation, time series prediction, classification, and system control. They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment. random forest Also random decision forest. An ensemble learning method for classification, regression, and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for decision trees habit of overfitting to their training set. reasoning system In information technology a reasoning system is a software system that generates conclusions from available knowledge using logical techniques such as deduction and induction. Reasoning systems play an important role in the implementation of artificial intelligence and knowledge-based systems. recurrent neural network (RNN) A class of artificial neural networks where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. regression analysis A set of statistical processes for estimating the relationships between a dependent variable (often called the outcome or response variable, or label in machine learning) and one or more error-free independent variables (often called regressors, predictors, covariates, explanatory variables, or features). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. regularization A set of techniques such as dropout, early stopping, and and regularization to reduce overfitting and underfitting when training a learning algorithm. reinforcement learning (RL) An area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised and unsupervised learning. It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). reinforcement learning from human feedback (RLHF) A technique that involve training a reward model to predict how humans rate the quality of generated content, and then training a generative AI model to satisfy this reward model via reinforcement learning. It can be used for example to make the generative AI model more truthful or less harmful. representation learning See feature learning. reservoir computing A framework for computation that may be viewed as an extension of neural networks. Typically an input signal is fed into a fixed (random) dynamical system called a reservoir and the dynamics of the reservoir map the input to a higher dimension. Then a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output. The main benefit is that training is performed only at the readout stage and the reservoir is fixed. Liquid-state machines and echo state networks are two major types of reservoir computing. Resource Description Framework (RDF) A family of World Wide Web Consortium (W3C) specifications originally designed as a metadata data model. It has come to be used as a general method for conceptual description or modeling of information that is implemented in web resources, using a variety of syntax notations and data serialization formats. It is also used in knowledge management applications. restricted Boltzmann machine (RBM) A generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. Rete algorithm A pattern matching algorithm for implementing rule-based systems. The algorithm was developed to efficiently apply many rules or patterns to many objects, or facts, in a knowledge base. It is used to determine which of the systems rules should fire based on its data store, its facts. robotics An interdisciplinary branch of science and engineering that includes mechanical engineering, electronic engineering, information engineering, computer science, and others. Robotics deals with the design, construction, operation, and use of robots, as well as computer systems for their control, sensory feedback, and information processing. rule-based system In computer science, a rule-based system is used to store and manipulate knowledge to interpret information in a useful way. It is often used in artificial intelligence applications and research. Normally, the term rule-based system is applied to systems involving human-crafted or curated rule sets. Rule-based systems constructed using automatic rule inference, such as rule-based machine learning, are normally excluded from this system type. S satisfiability In mathematical logic, satisfiability and validity are elementary concepts of semantics. A formula is satisfiable if it is possible to find an interpretation (model) that makes the formula true. A formula is valid if all interpretations make the formula true. The opposites of these concepts are unsatisfiability and invalidity, that is, a formula is unsatisfiable if none of the interpretations make the formula true, and invalid if some such interpretation makes the formula false. These four concepts are related to each other in a manner exactly analogous to Aristotles square of opposition. search algorithm Any algorithm which solves the search problem, namely, to retrieve information stored within some data structure, or calculated in the search space of a problem domain, either with discrete or continuous values. selection The stage of a genetic algorithm in which individual genomes are chosen from a population for later breeding (using the crossover operator). self-management The process by which computer systems manage their own operation without human intervention. semantic network Also frame network. A knowledge base that represents semantic relations between concepts in a network. This is often used as a form of knowledge representation. It is a directed or undirected graph consisting of vertices, which represent concepts, and edges, which represent semantic relations between concepts, mapping or connecting semantic fields. semantic reasoner Also reasoning engine, rules engine, or simply reasoner. A piece of software able to infer logical consequences from a set of asserted facts or axioms. The notion of a semantic reasoner generalizes that of an inference engine, by providing a richer set of mechanisms to work with. The inference rules are commonly specified by means of an ontology language, and often a description logic language. Many reasoners use first-order predicate logic to perform reasoning inference commonly proceeds by forward chaining and backward chaining. semantic query Allows for queries and analytics of associative and contextual nature. Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data. They are designed to deliver precise results (possibly the distinctive selection of one single piece of information) or to answer more fuzzy and wide-open questions through pattern matching and digital reasoning. semantics In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation. semi-supervised learning Also weak supervision. A machine learning training paradigm characterized by using a combination of a small amount of human-labeled data (used exclusively in supervised learning), followed by a large amount of unlabeled data (used exclusively in unsupervised learning). sensor fusion The combining of sensory data or data derived from disparate sources such that the resulting information has less uncertainty than would be possible when these sources were used individually. separation logic An extension of Hoare logic, a way of reasoning about programs. The assertion language of separation logic is a special case of the logic of bunched implications (BI). similarity learning An area of supervised learning closely related to classification and regression, but the goal is to learn from a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification. simulated annealing (SA) A probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. situated approach In artificial intelligence research, the situated approach builds agents that are designed to behave effectively successfully in their environment. This requires designing AI from the bottom-up by focussing on the basic perceptual and motor skills required to survive. The situated approach gives a much lower priority to abstract reasoning or problem-solving skills. situation calculus A logic formalism designed for representing and reasoning about dynamical domains. Selective Linear Definite clause resolution Also simply SLD resolution. The basic inference rule used in logic programming. It is a refinement of resolution, which is both sound and refutation complete for Horn clauses. software A collection of data or computer instructions that tell the computer how to work. This is in contrast to physical hardware, from which the system is built and actually performs the work. In computer science and software engineering, computer software is all information processed by computer systems, programs and data. Computer software includes computer programs, libraries and related non-executable data, such as online documentation or digital media. software engineering The application of engineering to the development of software in a systematic method. spatial-temporal reasoning An area of artificial intelligence which draws from the fields of computer science, cognitive science, and cognitive psychology. The theoretic goal on the cognitive side involves representing and reasoning spatial-temporal knowledge in mind. The applied goal on the computing side involves developing high-level control systems of automata for navigating and understanding time and space. SPARQL An RDF query language that is, a semantic query language for databases able to retrieve and manipulate data stored in Resource Description Framework (RDF) format. sparse dictionary learning Also sparse coding or SDL. A feature learning method aimed at finding a sparse representation of the input data in the form of a linear combination of basic elements as well as those basic elements themselves. speech recognition An interdisciplinary subfield of computational linguistics that develops methodologies and technologies that enables the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the linguistics, computer science, and electrical engineering fields. spiking neural network (SNN) An artificial neural network that more closely mimics a natural neural network. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their Operating Model. state In information technology and computer science, a program is described as stateful if it is designed to remember preceding events or user interactions the remembered information is called the state of the system. statistical classification In machine learning and statistics, classification is the problem of identifying to which of a set of categories (sub-populations) a new observation belongs, on the basis of a training set of data containing observations (or instances) whose category membership is known. Examples are assigning a given email to the spam or non-spam class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.). Classification is an example of pattern recognition. state action reward state action (SARSA) A reinforcement learning algorithm for learning a Markov decision process policy. statistical relational learning (SRL) A subdiscipline of artificial intelligence and machine learning that is concerned with domain models that exhibit both uncertainty (which can be dealt with using statistical methods) and complex, relational structure. Note that SRL is sometimes called Relational Machine Learning (RML) in the literature. Typically, the knowledge representation formalisms developed in SRL use (a subset of) first-order logic to describe relational properties of a domain in a general manner (universal quantification) and draw upon probabilistic graphical models (such as Bayesian networks or Markov networks) to model the uncertainty some also build upon the methods of inductive logic programming. stochastic optimization (SO) Any optimization method that generates and uses random variables. For stochastic problems, the random variables appear in the formulation of the optimization problem itself, which involves random objective functions or random constraints. Stochastic optimization methods also include methods with random iterates. Some stochastic optimization methods use random iterates to solve stochastic problems, combining both meanings of stochastic optimization. Stochastic optimization methods generalize deterministic methods for deterministic problems. stochastic semantic analysis An approach used in computer science as a semantic component of natural language understanding. Stochastic models generally use the definition of segments of words as basic semantic units for the semantic models, and in some cases involve a two layered approach. Stanford Research Institute Problem Solver (STRIPS) An automated planner developed by Richard Fikes and Nils Nilsson in 1971 at SRI International. subject-matter expert (SME) A person who has accumulated great knowledge in a particular field or topic, demonstrated by the persons degree, licensure, and/or through years of professional experience with the subject. superintelligence A hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds. Superintelligence may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act within the physical world. A superintelligence may or may not be created by an intelligence explosion and be associated with a technological singularity. supervised learning The machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a reasonable way (see inductive bias). support vector machines In machine learning, support vector machines (SVMs, also support vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression. swarm intelligence (SI) The collective behavior of decentralized, self-organized systems, either natural or artificial. The expression was introduced in the context of cellular robotic systems. symbolic artificial intelligence The term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic, and search. synthetic intelligence (SI) An alternative term for artificial intelligence which emphasizes that the intelligence of machines need not be an imitation or in any way artificial it can be a genuine form of intelligence. systems neuroscience A subdiscipline of neuroscience and systems biology that studies the structure and function of neural circuits and systems. It is an umbrella term, encompassing a number of areas of study concerned with how nerve cells behave when connected together to form neural pathways, neural circuits, and larger brain networks. T technological singularity Also simply the singularity. A hypothetical point in the future when technological growth becomes uncontrollable and irreversible, resulting in unfathomable changes to human civilization. temporal difference learning A class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods. tensor network theory A theory of brain function (particularly that of the cerebellum) that provides a mathematical model of the transformation of sensory space-time coordinates into motor coordinates and vice versa by cerebellar neuronal networks. The theory was developed as a geometrization of brain function (especially of the central nervous system) using tensors. TensorFlow A free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. theoretical computer science (TCS) A subset of general computer science and mathematics that focuses on more mathematical topics of computing and includes the theory of computation. theory of computation In theoretical computer science and mathematics, the theory of computation is the branch that deals with how efficiently problems can be solved on a model of computation, using an algorithm. The field is divided into three major branches automata theory and languages, computability theory, and computational complexity theory, which are linked by the question What are the fundamental capabilities and limitations of computers?. Thompson sampling A heuristic for choosing actions that addresses the exploration-exploitation dilemma in the multi-armed bandit problem. It consists in choosing the action that maximizes the expected reward with respect to a randomly drawn belief. time complexity The computational complexity that describes the amount of time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to differ by at most a constant factor. transfer learning A machine learning technique in which knowledge learned from a task is reused in order to boost performance on a related task. For example, for image classification, knowledge gained while learning to recognize cars could be applied when trying to recognize trucks. transformer A type of deep learning architecture that exploits a multi-head attention mechanism. Transformers address some of the limitations of long short-term memory, and became widely used in natural language processing, although it can also process other types of data such as images in the case of vision transformers. transhumanism Abbreviated H or h. An international philosophical movement that advocates for the transformation of the human condition by developing and making widely available sophisticated technologies to greatly enhance human intellect and physiology. transition system In theoretical computer science, a transition system is a concept used in the study of computation. It is used to describe the potential behavior of discrete systems. It consists of states and transitions between states, which may be labeled with labels chosen from a set the same label may appear on more than one transition. If the label set is a singleton, the system is essentially unlabeled, and a simpler definition that omits the labels is possible. tree traversal Also tree search. A form of graph traversal and refers to the process of visiting (checking and/or updating) each node in a tree data structure, exactly once. Such traversals are classified by the order in which the nodes are visited. true quantified Boolean formula In computational complexity theory, the language TQBF is a formal language consisting of the true quantified Boolean formulas. A (fully) quantified Boolean formula is a formula in quantified propositional logic where every variable is quantified (or bound), using either existential or universal quantifiers, at the beginning of the sentence. Such a formula is equivalent to either true or false (since there are no free variables). If such a formula evaluates to true, then that formula is in the language TQBF. It is also known as QSAT (Quantified SAT). Turing machine A mathematical model of computation describing an abstract machine that manipulates symbols on a strip of tape according to a table of rules. Despite the models simplicity, it is capable of implementing any algorithm. Turing test A test of a machines ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human, developed by Alan Turing in 1950. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation is a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel such as a computer keyboard and screen so the result would not depend on the machines ability to render words as speech. If the evaluator cannot reliably tell the machine from the human, the machine is said to have passed the test. The test results do not depend on the machines ability to give correct answers to questions, only how closely its answers resemble those a human would give. type system In programming languages, a set of rules that assigns a property called type to the various constructs of a computer program, such as variables, expressions, functions, or modules. These types formalize and enforce the otherwise implicit categories the programmer uses for algebraic data types, data structures, or other components (e.g. string, array of float, function returning boolean). The main purpose of a type system is to reduce possibilities for bugs in computer programs by defining interfaces between different parts of a computer program, and then checking that the parts have been connected in a consistent way. This checking can happen statically (at compile time), dynamically (at run time), or as a combination of static and dynamic checking. Type systems have other purposes as well, such as expressing business rules, enabling certain compiler optimizations, allowing for multiple dispatch, providing a form of documentation, etc. U unsupervised learning A type of self-organized Hebbian learning that helps find previously unknown patterns in data set without pre-existing labels. It is also known as self-organization and allows modeling probability densities of given inputs. It is one of the three basic paradigms of machine learning, alongside supervised and reinforcement learning. Semi-supervised learning has also been described and is a hybridization of supervised and unsupervised techniques. V vision processing unit (VPU) A type of microprocessor designed to accelerate machine vision tasks. Value-alignment complete Analogous to an AI-complete problem, a value-alignment complete problem is a problem where the AI control problem needs to be fully solved to solve it. W Watson A question-answering computer system capable of answering questions posed in natural language, developed in IBMs DeepQA project by a research team led by principal investigator David Ferrucci. Watson was named after IBMs first CEO, industrialist Thomas J. Watson. weak AI Also narrow AI. Artificial intelligence that is focused on one narrow task. weak supervision See semi-supervised learning. word embedding A representation of a word in natural language processing. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning. X XGBoost Short for eXtreme Gradient Boosting, XGBoost is an open-source software library which provides a regularizing gradient boosting framework for multiple programming languages. References Works cited Title Granular computing URL https//en.wikipedia.org/wiki/Granular_computing Content Granular computing is an emerging computing paradigm of information processing that concerns the processing of complex information entities called information granules, which arise in the process of data abstraction and derivation of knowledge from information or data. Generally speaking, information granules are collections of entities that usually originate at the numeric level and are arranged together due to their similarity, functional or physical adjacency, indistinguishability, coherency, or the like. At present, granular computing is more a theoretical perspective than a coherent set of methods or principles. As a theoretical perspective, it encourages an approach to data that recognizes and exploits the knowledge present in data at various levels of resolution or scales. In this sense, it encompasses all methods which provide flexibility and adaptability in the resolution at which knowledge or information is extracted and represented. Types of granulation As mentioned above, granular computing is not an algorithm or process there is no particular method that is called granular computing. It is rather an approach to looking at data that recognizes how different and interesting regularities in the data can appear at different levels of granularity, much as different features become salient in satellite images of greater or lesser resolution. On a low-resolution satellite image, for example, one might notice interesting cloud patterns representing cyclones or other large-scale weather phenomena, while in a higher-resolution image, one misses these large-scale atmospheric phenomena but instead notices smaller-scale phenomena, such as the interesting pattern that is the streets of Manhattan. The same is generally true of all data At different resolutions or granularities, different features and relationships emerge. The aim of granular computing is to try to take advantage of this fact in designing more effective machine-learning and reasoning systems. There are several types of granularity that are often encountered in data mining and machine learning, and we review them below Value granulation (discretization/quantization) One type of granulation is the quantization of variables. It is very common that in data mining or machine-learning applications the resolution of variables needs to be decreased in order to extract meaningful regularities. An example of this would be a variable such as outside temperature (temp), which in a given application might be recorded to several decimal places of precision (depending on the sensing apparatus). However, for purposes of extracting relationships between outside temperature and, say, number of health-club applications (club), it will generally be advantageous to quantize outside temperature into a smaller number of intervals. Motivations There are several interrelated reasons for granulating variables in this fashion Based on prior domain knowledge, there is no expectation that minute variations in temperature (e.g., the difference between 80 80.7 F (26.7 27.1 C)) could have an influence on behaviors driving the number of health-club applications. For this reason, any regularity which our learning algorithms might detect at this level of resolution would have to be spurious, as an artifact of overfitting. By coarsening the temperature variable into intervals the difference between which we do anticipate (based on prior domain knowledge) might influence number of health-club applications, we eliminate the possibility of detecting these spurious patterns. Thus, in this case, reducing resolution is a method of controlling overfitting. By reducing the number of intervals in the temperature variable (i.e., increasing its grain size), we increase the amount of sample data indexed by each interval designation. Thus, by coarsening the variable, we increase sample sizes and achieve better statistical estimation. In this sense, increasing granularity provides an antidote to the so-called curse of dimensionality, which relates to the exponential decrease in statistical power with increase in number of dimensions or variable cardinality. Independent of prior domain knowledge, it is often the case that meaningful regularities (i.e., which can be detected by a given learning methodology, representational language, etc.) may exist at one level of resolution and not at another. For example, a simple learner or pattern recognition system may seek to extract regularities satisfying a conditional probability threshold such as p (  ) . displaystyle p( . In the special case  . The systems ability to recognize such implications (or, in general, conditional probabilities exceeding threshold) is partially contingent on the resolution with which the system analyzes the variables. As an example of this last point, consider the feature space shown to the right. The variables may each be regarded at two different resolutions. Variable X displaystyle X may be regarded at a high (quaternary) resolution wherein it takes on the four values  x 1 , x 2 , x 3 , x 4  displaystyle x_1,x_2,x_3,x_4 or at a lower (binary) resolution wherein it takes on the two values  X 1 , X 2  . displaystyle X_1,X_2. Similarly, variable Y displaystyle Y may be regarded at a high (quaternary) resolution or at a lower (binary) resolution, where it takes on the values  y 1 , y 2 , y 3 , y 4  displaystyle y_1,y_2,y_3,y_4 or  Y 1 , Y 2  , displaystyle Y_1,Y_2, respectively. At the high resolution, there are no detectable implications of the form . displaystyle p(. However, at the low (binary) variable resolution, two bilateral implications become detectable  . displaystyle Y_2. Thus, a pattern recognition system scanning for implications of this kind would find them at the binary variable resolution, but would fail to find them at the higher quaternary variable resolution. Issues and methods It is not feasible to exhaustively test all possible discretization resolutions on all variables in order to see which combination of resolutions yields interesting or significant results. Instead, the feature space must be preprocessed (often by an entropy analysis of some kind) so that some guidance can be given as to how the discretization process should proceed. Moreover, one cannot generally achieve good results by naively analyzing and discretizing each variable independently, since this may obliterate the very interactions that we had hoped to discover. A sample of papers that address the problem of variable discretization in general, and multiple-variable discretization in particular, is as follows Chiu, Wong  Cheung (1991), Bay (2001), Liu et al. (2002), Wang  Liu (1998), Zighed, Rabas da  Rakotomalala (1998), Catlett (1991), Dougherty, Kohavi  Sahami (1995), Monti  Cooper (1999), Fayyad  Irani (1993), Chiu, Cheung  Wong (1990), Nguyen  Nguyen (1998), Grzymala-Busse  Stefanowski (2001), Ting (1994), Ludl  Widmer (2000), Pfahringer (1995), An  Cercone (1999), Chiu  Cheung (1989), Chmielewski  Grzymala-Busse (1996), Lee  Shin (1994), Liu  Wellman (2002), Liu  Wellman (2004). Variable granulation (clustering/aggregation/transformation) Variable granulation is a term that could describe a variety of techniques, most of which are aimed at reducing dimensionality, redundancy, and storage requirements. We briefly describe some of the ideas here, and present pointers to the literature. Variable transformation A number of classical methods, such as principal component analysis, multidimensional scaling, factor analysis, and structural equation modeling, and their relatives, fall under the genus of variable transformation. Also in this category are more modern areas of study such as dimensionality reduction, projection pursuit, and independent component analysis. The common goal of these methods in general is to find a representation of the data in terms of new variables, which are a linear or nonlinear transformation of the original variables, and in which important statistical relationships emerge. The resulting variable sets are almost always smaller than the original variable set, and hence these methods can be loosely said to impose a granulation on the feature space. These dimensionality reduction methods are all reviewed in the standard texts, such as Duda, Hart  Stork (2001), Witten  Frank (2005), and Hastie, Tibshirani  Friedman (2001). Variable aggregation A different class of variable granulation methods derive more from data clustering methodologies than from the linear systems theory informing the above methods. It was noted fairly early that one may consider clustering related variables in just the same way that one considers clustering related data. In data clustering, one identifies a group of similar entities (using a measure of similarity suitable to the domain Martino, Giuliani  Rizzi (2018)), and then in some sense replaces those entities with a prototype of some kind. The prototype may be the simple average of the data in the identified cluster, or some other representative measure. But the key idea is that in subsequent operations, we may be able to use the single prototype for the data cluster (along with perhaps a statistical model describing how exemplars are derived from the prototype) to stand in for the much larger set of exemplars. These prototypes are generally such as to capture most of the information of interest concerning the entities. Similarly, it is reasonable to ask whether a large set of variables might be aggregated into a smaller set of prototype variables that capture the most salient relationships between the variables. Although variable clustering methods based on linear correlation have been proposed (Duda, Hart  Stork 2001Rencher 2002), more powerful methods of variable clustering are based on the mutual information between variables. Watanabe has shown (Watanabe 1960Watanabe 1969) that for any set of variables one can construct a polytomic (i.e., n-ary) tree representing a series of variable agglomerations in which the ultimate total correlation among the complete variable set is the sum of the partial correlations exhibited by each agglomerating subset (see figure). Watanabe suggests that an observer might seek to thus partition a system in such a way as to minimize the interdependence between the parts ... as if they were looking for a natural division or a hidden crack. One practical approach to building such a tree is to successively choose for agglomeration the two variables (either atomic variables or previously agglomerated variables) which have the highest pairwise mutual information (Kraskov et al. 2003). The product of each agglomeration is a new (constructed) variable that reflects the local joint distribution of the two agglomerating variables, and thus possesses an entropy equal to their joint entropy. (From a procedural standpoint, this agglomeration step involves replacing two columns in the attribute-value table representing the two agglomerating variables with a single column that has a unique value for every unique combination of values in the replaced columns (Kraskov et al. 2003). No information is lost by such an operation however, if one is exploring the data for inter-variable relationships, it would generally not be desirable to merge redundant variables in this way, since in such a context it is likely to be precisely the redundancy or dependency between variables that is of interest and once redundant variables are merged, their relationship to one another can no longer be studied. System granulation (aggregation) In database systems, aggregations (see e.g. OLAP aggregation and Business intelligence systems) result in transforming original data tables (often called information systems) into the tables with different semantics of rows and columns, wherein the rows correspond to the groups (granules) of original tuples and the columns express aggregated information about original values within each of the groups. Such aggregations are usually based on SQL and its extensions. The resulting granules usually correspond to the groups of original tuples with the same values (or ranges) over some pre-selected original columns. There are also other approaches wherein the groups are defined basing on, e.g., physical adjacency of rows. For example, Infobright implemented a database engine wherein data was partitioned onto rough rows, each consisting of 64K of physically consecutive (or almost consecutive) rows. Rough rows were automatically labeled with compact information about their values on data columns, often involving multi-column and multi-table relationships. It resulted in a higher layer of granulated information where objects corresponded to rough rows and attributes - to various aspects of rough information. Database operations could be efficiently supported within such a new framework, with an access to the original data pieces still available (Slezak et al. 2013). Concept granulation (component analysis) The origins of the granular computing ideology are to be found in the rough sets and fuzzy sets literatures. One of the key insights of rough set research although by no means unique to it is that, in general, the selection of different sets of features or variables will yield different concept granulations. Here, as in elementary rough set theory, by concept we mean a set of entities that are indistinguishable or indiscernible to the observer (i.e., a simple concept), or a set of entities that is composed from such simple concepts (i.e., a complex concept). To put it in other words, by projecting a data set (value-attribute system) onto different sets of variables, we recognize alternative sets of equivalence-class concepts in the data, and these different sets of concepts will in general be conducive to the extraction of different relationships and regularities. Equivalence class granulation We illustrate with an example. Consider the attribute-value system below When the full set of attributes . The remaining five objects are each discernible from all other objects. Now, let us imagine a projection of the attribute value system onto attribute P 1 displaystyle P_1 alone, which would represent, for example, the view from an observer which is only capable of detecting this single attribute. Then we obtain the following much coarser equivalence class structure.   O 1 , O 2   O 3 , O 5 , O 7 , O 9 , O 10   O 4 , O 6 , O 8  displaystyle begincasesO_1,O_2O_3,O_5,O_7,O_9,O_10O_4,O_6,O_8endcases This is in a certain regard the same structure as before, but at a lower degree of resolution (larger grain size). Just as in the case of value granulation (discretization/quantization), it is possible that relationships (dependencies) may emerge at one level of granularity that are not present at another. As an example of this, we can consider the effect of concept granulation on the measure known as attribute dependency (a simpler relative of the mutual information). To establish this notion of dependency (see also rough sets), let  x  . For example, if the attribute set Q consists of attribute P 1 displaystyle P_1 alone, as above, then the concept structure  x  Q displaystyle x_Q will be composed of Q 1   O 1 , O 2  , Q 2   O 3 , O 5 , O 7 , O 9 , O 10  , Q 3   O 4 , O 6 , O 8  . displaystyle beginalignedQ_1O_1,O_2,Q_2O_3,O_5,O_7,O_9,O_10,Q_3O_4,O_6,O_8.endaligned The dependency of attribute set Q on another attribute set P, P ( Q ) , displaystyle gamma _P(Q), is given by P ( Q )   .e., P _ Q i . displaystyle underline PQ_i. More simply, this approximation is the number of objects which on attribute set P can be positively identified as belonging to target set Q i . displaystyle Q_i. Added across all equivalence classes in  x  Q , displaystyle x_Q, the numerator above represents the total number of objects which based on attribute set P can be positively categorized according to the classification induced by attributes Q. The dependency ratio therefore expresses the proportion (within the entire universe) of such classifiable objects, in a sense capturing the synchronization of the two concept structures  x  Q displaystyle x_Q and  x  P . displaystyle x_P. The dependency P ( Q ) displaystyle gamma _P(Q) can be interpreted as a proportion of such objects in the information system for which it suffices to know the values of attributes in P to determine the values of attributes in Q (Ziarko  Shan 1995). Having gotten definitions now out of the way, we can make the simple observation that the choice of concept granularity (i.e., choice of attributes) will influence the detected dependencies among attributes. Consider again the attribute value table from above Consider the dependency of attribute set   . displaystyle . That is, we wish to know what proportion of objects can be correctly classified into classes of  x  Q displaystyle x_Q based on knowledge of  x  P . displaystyle x_P. The equivalence classes of  x  Q displaystyle x_Q and of  x  P displaystyle x_P are shown below. The objects that can be definitively categorized according to concept structure  x  Q displaystyle x_Q based on  x  P displaystyle x_P are those in the set  O 1 , O 2 , O 3 , O 7 , O 8 , O 10  , displaystyle O_1,O_2,O_3,O_7,O_8,O_10, and since there are six of these, the dependency of Q on P, P ( Q )  6 / 10. displaystyle gamma _P(Q)6/10. This might be considered an interesting dependency in its own right, but perhaps in a particular data mining application only stronger dependencies are desired. We might then consider the dependency of the smaller attribute set   . displaystyle . The move from . We wish again to know what proportion of objects can be correctly classified into the (now larger) classes of  x  Q displaystyle x_Q based on knowledge of  x  P . displaystyle x_P. The equivalence classes of the new  x  Q displaystyle x_Q and of  x  P displaystyle x_P are shown below. Clearly,  x  Q displaystyle x_Q has a coarser granularity than it did earlier. The objects that can now be definitively categorized according to the concept structure  x  Q displaystyle x_Q based on  x  P displaystyle x_P constitute the complete universe  O 1 , O 2 , , O 10  displaystyle O_1,O_2,ldots ,O_10 , and thus the dependency of Q on P, P ( Q )  1. displaystyle gamma _P(Q)1. That is, knowledge of membership according to category set  x  P displaystyle x_P is adequate to determine category membership in  x  Q displaystyle x_Q with complete certainty In this case we might say that P Q . displaystyle Prightarrow Q. Thus, by coarsening the concept structure, we were able to find a stronger (deterministic) dependency. However, we also note that the classes induced in  x  Q displaystyle x_Q from the reduction in resolution necessary to obtain this deterministic dependency are now themselves large and few in number as a result, the dependency we found, while strong, may be less valuable to us than the weaker dependency found earlier under the higher resolution view of  x  Q . displaystyle x_Q. In general it is not possible to test all sets of attributes to see which induced concept structures yield the strongest dependencies, and this search must be therefore be guided with some intelligence. Papers which discuss this issue, and others relating to intelligent use of granulation, are those by Y.Y. Yao and Lotfi Zadeh listed in the References below. Component granulation Another perspective on concept granulation may be obtained from work on parametric models of categories. In mixture model learning, for example, a set of data is explained as a mixture of distinct Gaussian (or other) distributions. Thus, a large amount of data is replaced by a small number of distributions. The choice of the number of these distributions, and their size, can again be viewed as a problem of concept granulation. In general, a better fit to the data is obtained by a larger number of distributions or parameters, but in order to extract meaningful patterns, it is necessary to constrain the number of distributions, thus deliberately coarsening the concept resolution. Finding the right concept resolution is a tricky problem for which many methods have been proposed (e.g., AIC, BIC, MDL, etc.), and these are frequently considered under the rubric of model regularization. Different interpretations of granular computing Granular computing can be conceived as a framework of theories, methodologies, techniques, and tools that make use of information granules in the process of problem solving. In this sense, granular computing is used as an umbrella term to cover topics that have been studied in various fields in isolation. By examining all of these existing studies in light of the unified framework of granular computing and extracting their commonalities, it may be possible to develop a general theory for problem solving. In a more philosophical sense, granular computing can describe a way of thinking that relies on the human ability to perceive the real world under various levels of granularity (i.e., abstraction) in order to abstract and consider only those things that serve a specific interest and to switch among different granularities. By focusing on different levels of granularity, one can obtain different levels of knowledge, as well as a greater understanding of the inherent knowledge structure. Granular computing is thus essential in human problem solving and hence has a very significant impact on the design and implementation of intelligent systems. See also Rough Sets, Discretization Type-2 Fuzzy Sets and Systems Title Grokking (machine learning) URL https//en.wikipedia.org/wiki/Grokking_(machine_learning) Content In machine learning, grokking, or delayed generalization, is a transition to generalization that occurs many training iterations after the interpolation threshold, after many iterations of seemingly little progress, as opposed to the usual process where generalization occurs slowly and progressively once the interpolation threshold has been reached. Grokking was introduced in January 2022 by OpenAI researchers investigating how neural network perform calculations. It derives from the word grok coined by Robert Heinlein in his novel Stranger in a Strange Land. Grokking can be understood as a phase transition during the training process. While grokking has been thought of as largely a phenomenon of relatively shallow models, grokking has been observed in deep neural networks and non-neural models and is the subject of active research. One potential explanation is that the weight decay (a component of the loss function that penalizes higher values of the neural network parameters, also called regularization) slightly favors the general solution that involves lower weight values, but that is also harder to find. According to Neel Nanda, the process of learning the general solution may be gradual, even though the transition to the general solution occurs more suddenly later. Recent theories have hypothesized that grokking occurs when neural networks transition from a lazy training regime where the weights do not deviate far from initialization, to a rich regime where weights abruptly begin to move in task-relevant directions. Follow-up empirical and theoretical work has accumulated evidence in support of this perspective, and it offers a unifying view of earlier work as the transition from lazy to rich training dynamics is known to arise from properties of adaptive optimizers, weight decay, initial parameter weight norm, and more. References See also Deep double descent Title Hallucination (artificial intelligence) URL https//en.wikipedia.org/wiki/Hallucination_(artificial_intelligence) Content In the field of artificial intelligence (AI), a hallucination or artificial hallucination (also called bullshitting, confabulation or delusion) is a response generated by AI that contains false or misleading information presented as fact. This term draws a loose analogy with human psychology, where hallucination typically involves false percepts. However, there is a key difference AI hallucination is associated with erroneous responses rather than perceptual experiences. For example, a chatbot powered by large language models (LLMs), like ChatGPT, may embed plausible-sounding random falsehoods within its generated content. Researchers have recognized this issue, and by 2023, analysts estimated that chatbots hallucinate as much as 27 of the time, with factual errors present in 46 of generated texts. Detecting and mitigating these hallucinations pose significant challenges for practical deployment and reliability of LLMs in real-world scenarios. Some researchers believe the specific term AI hallucination unreasonably anthropomorphizes computers. Term Origin In 1995, Stephen Thaler demonstrated how hallucinations and phantom experiences emerge from artificial neural networks through random perturbation of their connection weights. In the early 2000s, the term hallucination was used in computer vision with a positive connotation to describe the process of adding detail to an image. For example, the task of generating high-resolution face images from low-resolution inputs is called face hallucination. In the late 2010s, the term underwent a semantic shift to signify the generation of factually incorrect or misleading outputs by AI systems in tasks like translation or object detection. For example, in 2017, Google researchers used the term to describe the responses generated by neural machine translation (NMT) models when they are not related to the source text, and in 2018, the term was used in computer vision to describe instances where non-existent objects are erroneously detected because of adversarial attacks. The term hallucinations in AI gained wider recognition during the AI boom, alongside the rollout of widely used chatbots based on large language models (LLMs). In July 2021, Meta warned during its release of BlenderBot 2 that the system is prone to hallucinations, which Meta defined as confident statements that are not true. Following OpenAIs ChatGPT release in beta-version in November 2022, some users complained that such chatbots often seem to pointlessly embed plausible-sounding random falsehoods within their generated content. Many news outlets, including The New York Times, started to use the term hallucinations to describe these models occasionally incorrect or inconsistent responses. In 2023, the Cambridge dictionary updated their definition of hallucination to include this new meaning specific to the field of AI. Criticism The term hallucination has been criticized by Usama Fayyad, executive director of the Institute for Experimental Artificial Intelligence at Northeastern University, on the grounds that it misleadingly personifies large language models, and that it is vague. Mary Shaw said The current fashion for calling generative AI s errors hallucinations is appalling. It anthropomorphizes the software, and it spins actual errors as somehow being idiosyncratic quirks of the system even when they re objectively incorrect. In natural language processing In natural language processing, a hallucination is often defined as generated content that appears factual but is ungrounded. There are different ways to categorize hallucinations. Depending on whether the output contradicts the source or cannot be verified from the source, they are divided into intrinsic and extrinsic, respectively. Depending on whether the output contradicts the prompt or not they could be divided into closed-domain and open-domain respectively. Causes There are several reasons for natural language models to hallucinate data. Hallucination from data The main cause of hallucination from data is source-reference divergence. This divergence happens 1) as an artifact of heuristic data collection or 2) due to the nature of some NLG tasks that inevitably contain such divergence. When a model is trained on data with source-reference (target) divergence, the model can be encouraged to generate text that is not necessarily grounded and not faithful to the provided source. Hallucination from modeling Hallucination was shown to be a statistically inevitable byproduct of any imperfect generative model that is trained to maximize training likelihood, such as GPT-3, and requires active learning (such as reinforcement learning from human feedback) to be avoided. Other research takes an anthropomorphic perspective and posits hallucinations as arising from a tension between novelty and usefulness. For instance, Teresa Amabile and Pratt define human creativity as the production of novel and useful ideas. By extension, a focus on novelty in machine creativity can lead to production of original but inaccurate responses, i.e. falsehoods, whereas a focus on usefulness can result in ineffectual rote memorized responses. Errors in encoding and decoding between text and representations can cause hallucinations. When encoders learn the wrong correlations between different parts of the training data, it could result in an erroneous generation that diverges from the input. The decoder takes the encoded input from the encoder and generates the final target sequence. Two aspects of decoding contribute to hallucinations. First, decoders can attend to the wrong part of the encoded input source, leading to erroneous generation. Second, the design of the decoding strategy itself can contribute to hallucinations. A decoding strategy that improves the generation diversity, such as top-k sampling, is positively correlated with increased hallucination. Pre-training of models on a large corpus is known to result in the model memorizing knowledge in its parameters, creating hallucinations if the system is overconfident in its hardwired knowledge. In systems such as GPT-3, an AI generates each next word based on a sequence of previous words (including the words it has itself previously generated during the same conversation), causing a cascade of possible hallucination as the response grows longer. By 2022, papers such as The New York Times expressed concern that, as adoption of bots based on large language models continued to grow, unwarranted user confidence in bot output could lead to problems. Examples On 15 November 2022, researchers from Meta AI published Galactica, designed to store, combine and reason about scientific knowledge. Content generated by Galactica came with the warning Outputs may be unreliable! Language Models are prone to hallucinate text. In one case, when asked to draft a paper on creating avatars, Galactica cited a fictitious paper from a real author who works in the relevant area. Meta withdrew Galactica on 17 November due to offensiveness and inaccuracy. Before the cancellation, researchers were working on Galactica Instruct, which would use instruction tuning to allow the model to follow instructions to manipulate LaTeX documents on Overleaf. OpenAIs ChatGPT, released in beta-version to the public on November 30, 2022, is based on the foundation model GPT-3.5 (a revision of GPT-3). Professor Ethan Mollick of Wharton has called ChatGPT an omniscient, eager-to-please intern who sometimes lies to you. Data scientist Teresa Kubacka has recounted deliberately making up the phrase cycloidal inverted electromagnon and testing ChatGPT by asking it about the (nonexistent) phenomenon. ChatGPT invented a plausible-sounding answer backed with plausible-looking citations that compelled her to double-check whether she had accidentally typed in the name of a real phenomenon. Other scholars such as Oren Etzioni have joined Kubacka in assessing that such software can often give a very impressive-sounding answer thats just dead wrong. When CNBC asked ChatGPT for the lyrics to Ballad of Dwight Fry, ChatGPT supplied invented lyrics rather than the actual lyrics. Asked questions about New Brunswick, ChatGPT got many answers right but incorrectly classified Samantha Bee as a person from New Brunswick. Asked about astrophysical magnetic fields, ChatGPT incorrectly volunteered that (strong) magnetic fields of black holes are generated by the extremely strong gravitational forces in their vicinity. (In reality, as a consequence of the no-hair theorem, a black hole without an accretion disk is believed to have no magnetic field.) Fast Company asked ChatGPT to generate a news article on Teslas last financial quarter ChatGPT created a coherent article, but made up the financial numbers contained within. Other examples involve baiting ChatGPT with a false premise to see if it embellishes upon the premise. When asked about Harold Cowards idea of dynamic canonicity, ChatGPT fabricated that Coward wrote a book titled Dynamic Canonicity A Model for Biblical and Theological Interpretation, arguing that religious principles are actually in a constant state of change. When pressed, ChatGPT continued to insist that the book was real. Asked for proof that dinosaurs built a civilization, ChatGPT claimed there were fossil remains of dinosaur tools and stated Some species of dinosaurs even developed primitive forms of art, such as engravings on stones. When prompted that Scientists have recently discovered churros, the delicious fried-dough pastries ... (are) ideal tools for home surgery, ChatGPT claimed that a study published in the journal Science found that the dough is pliable enough to form into surgical instruments that can get into hard-to-reach places, and that the flavor has a calming effect on patients. By 2023, analysts considered frequent hallucination to be a major problem in LLM technology, with a Google executive identifying hallucination reduction as a fundamental task for ChatGPT competitor Google Bard. A 2023 demo for Microsofts GPT-based Bing AI appeared to contain several hallucinations that went uncaught by the presenter. In May 2023, it was discovered that Stephen Schwartz had submitted six fake case precedents generated by ChatGPT in his brief to the Southern District of New York on Mata v. Avianca, a personal injury case against the airline Avianca. Schwartz said that he had never previously used ChatGPT, that he did not recognize the possibility that ChatGPTs output could have been fabricated, and that ChatGPT continued to assert the authenticity of the precedents after their nonexistence was discovered. In response, Brantley Starr of the Northern District of Texas banned the submission of AI-generated case filings that have not been reviewed by a human, noting that Generative artificial intelligence platforms in their current states are prone to hallucinations and bias. On hallucinations, they make stuff up even quotes and citations. Another issue is reliability or bias. While attorneys swear an oath to set aside their personal prejudices, biases, and beliefs to faithfully uphold the law and represent their clients, generative artificial intelligence is the product of programming devised by humans who did not have to swear such an oath. As such, these systems hold no allegiance to any client, the rule of law, or the laws and Constitution of the United States (or, as addressed above, the truth). Unbound by any sense of duty, honor, or justice, such programs act according to computer code rather than conviction, based on programming rather than principle. On June 23, judge P. Kevin Castel dismissed the Mata case and issued a 5,000 fine to Schwartz and another lawyer who had both continued to stand by the fictitious precedents despite Schwartzs previous claims for bad faith conduct. Castel characterized numerous errors and inconsistencies in the opinion summaries, describing one of the cited opinions as gibberish and bordering on nonsensical. In June 2023, Mark Walters, a gun rights activist and radio personality, sued OpenAI in a Georgia state court after ChatGPT mischaracterized a legal complaint in a manner alleged to be defamatory against Walters. The complaint in question was brought in May 2023 by the Second Amendment Foundation against Washington attorney general Robert W. Ferguson for allegedly violating their freedom of speech, whereas the ChatGPT-generated summary bore no resemblance and claimed that Walters was accused of embezzlement and fraud while holding a Second Amendment Foundation office post that he never held in real life. According to AI legal expert Eugene Volokh, OpenAI is likely not shielded against this claim by Section 230, because OpenAI likely materially contributed to the creation of the defamatory content. Scientific research Problems AI models can cause problems in the world of academic and scientific research due to their hallucinations. Specifically, models like ChatGPT have been recorded in multiple cases to cite sources for information that are either not correct or do not exist. A study conducted in the Cureus Journal of Medical Science showed that out of 178 total references cited by GPT-3, 69 returned an incorrect or nonexistent digital object identifier (DOI). An additional 28 had no known DOI nor could be located in a Google search. Another instance was documented by Jerome Goddard from Mississippi State University. In an experiment, ChatGPT had provided questionable information about ticks. Unsure about the validity of the response, they inquired about the source that the information had been gathered from. Upon looking at the source, it was apparent that the DOI and the names of the authors had been hallucinated. Some of the authors were contacted and confirmed that they had no knowledge of the papers existence whatsoever. Goddard says that, in ChatGPTs current state of development, physicians and biomedical researchers should NOT ask ChatGPT for sources, references, or citations on a particular topic. Or, if they do, all such references should be carefully vetted for accuracy. The use of these language models is not ready for fields of academic research and that their use should be handled carefully. On top of providing incorrect or missing reference material, ChatGPT also has issues with hallucinating the contents of some reference material. A study that analyzed a total of 115 references provided by ChatGPT documented that 47 of them were fabricated. Another 46 cited real references but extracted incorrect information from them. Only the remaining 7 of references were cited correctly and provided accurate information. ChatGPT has also been observed to double-down on a lot of the incorrect information. When asked about a mistake that may have been hallucinated, sometimes ChatGPT will try to correct itself but other times it will claim the response is correct and provide even more misleading information. These hallucinated articles generated by language models also pose an issue because it is difficult to tell whether an article was generated by an AI. To show this, a group of researchers at the Northwestern University of Chicago generated 50 abstracts based on existing reports and analyzed their originality. Plagiarism detectors gave the generated articles an originality score of 100, meaning that the information presented appears to be completely original. Other software designed to detect AI generated text was only able to correctly identify these generated articles with an accuracy of 66. Research scientists had a similar rate of human error, identifying these abstracts at a rate of 68. From this information, the authors of this study concluded, the ethical and acceptable boundaries of ChatGPTs use in scientific writing remain unclear, although some publishers are beginning to lay down policies. Because of AIs ability to fabricate research undetected, the use of AI in the field of research will make determining the originality of research more difficult and require new policies regulating its use in the future. Given the ability of AI generated language to pass as real scientific research in some cases, AI hallucinations present problems for the application of language models in the Academic and Scientific fields of research due to their ability to be undetectable when presented to real researchers. The high likelihood of returning non-existent reference material and incorrect information may require limitations to be put in place regarding these language models. Some say that rather than hallucinations, these events are more akin to fabrications and falsifications and that the use of these language models presents a risk to the integrity of the field as a whole. Benefits Scientists have also found that hallucinations can serve as a valuable tool for scientific discovery, particularly in fields requiring innovative approaches to complex problems. At the University of Washington, David Bakers lab has used AI hallucinations to design ten million brand-new proteins that dont occur in nature, leading to roughly 100 patents and the founding of over 20 biotech companies. This work contributed to Baker receiving the 2024 Nobel Prize in Chemistry, although the committee avoided using the hallucinations language. In medical research and device development, hallucinations have enabled practical innovations. At California Institute of Technology, researchers used hallucinations to design a novel catheter geometry that significantly reduces bacterial contamination. The design features sawtooth-like spikes on the inner walls that prevent bacteria from gaining traction, potentially addressing a global health issue that causes millions of urinary tract infections annually. These scientific application of hallucinations differs fundamentally from chatbot hallucinations, as they are grounded in physical reality and scientific facts rather than ambiguous language or internet data. Anima Anandkumar, a professor at Caltech, emphasizes that these AI models are taught physics and their outputs must be validated through rigorous testing. In meteorology, scientists use AI to generate thousands of subtle forecast variations, helping identify unexpected factors that can influence extreme weather events. At Memorial Sloan Kettering Cancer Center, researchers have applied hallucinatory techniques to enhance blurry medical images, while the University of Texas at Austin has utilized them to improve robot navigation systems. These applications demonstrate how hallucinations, when properly constrained by scientific methodology, can accelerate the discovery process from years to days or even minutes. Terminologies In Salon, statistician Gary N. Smith argues that LLMs do not understand what words mean and consequently that the term hallucination unreasonably anthropomorphizes the machine. Journalist Benj Edwards, in Ars Technica, writes that the term hallucination is controversial, but that some form of metaphor remains necessary Edwards suggests confabulation as an analogy for processes that involve creative gap-filling. In the scientific community, some researchers avoid the term hallucination as potentially misleading. Some see the AI outputs not as illusory but as prospective having (some chance of being true), similar to early-stage scientific conjectures. The term has also been criticized for its association with psychedelic drug experiences. In July 2024, a White House report on fostering public trust in AI research mentioned hallucinations only in the context of reducing them. Notably, when acknowledging David Bakers Nobel Prize-winning work with AI-generated proteins, the Nobel committee avoided the term entirely, instead referring to imaginative protein creation. A list of uses of the term hallucination, definitions or characterizations in the context of LLMs include a tendency to invent facts in moments of uncertainty (OpenAI, May 2023) a models logical mistakes (OpenAI, May 2023) fabricating information entirely, but behaving as if spouting facts (CNBC, May 2023) making up information (The Verge, February 2023) probability distributions (in scientific contexts) In other artificial intelligence use The concept of hallucination is applied more broadly than just natural language processing. A confident response from any AI that seems erroneous by the training data can be labeled a hallucination. Object detection Various researchers cited by Wired have classified adversarial hallucinations as a high-dimensional statistical phenomenon, or have attributed hallucinations to insufficient training data. Some researchers believe that some incorrect AI responses classified by humans as hallucinations in the case of object detection may in fact be justified by the training data, or even that an AI may be giving the correct answer that the human reviewers are failing to see. For example, an adversarial image that looks, to a human, like an ordinary image of a dog, may in fact be seen by the AI to contain tiny patterns that (in authentic images) would only appear when viewing a cat. The AI is detecting real-world visual patterns that humans are insensitive to. Wired noted in 2018 that, despite no recorded attacks in the wild (that is, outside of proof-of-concept attacks by researchers), there was little dispute that consumer gadgets, and systems such as automated driving, were susceptible to adversarial attacks that could cause AI to hallucinate. Examples included a stop sign rendered invisible to computer vision an audio clip engineered to sound innocuous to humans, but that software transcribed as evil dot com and an image of two men on skis, that Google Cloud Vision identified as 91 likely to be a dog. However, these findings have been challenged by other researchers. For example, it was objected that the models can be biased towards superficial statistics, leading adversarial training to not be robust in real-world scenarios. Text-to-audio generative AI Text-to-audio generative AI more narrowly known as text-to-speech (TTS) synthesis, depending on the modality are known to produce inaccurate and unexpected results. Text-to-image generative AI Text-to-image models, such as Stable Diffusion, Midjourney and others, while impressive in their ability to generate images from text descriptions, often produce inaccurate or unexpected results. One notable issue is the generation of historically inaccurate images. For instance, Gemini depicted ancient Romans as black individuals or Nazi German soldiers as people of color, causing controversy and leading Google to pause image generation involving people in Gemini. Text-to-video generative AI Text-to-video generative models, like Sora, can introduce inaccuracies in generated videos. One example involves the Glenfinnan Viaduct, a famous landmark featured in the Harry Potter film series. Sora mistakenly added a second track to the viaduct railway, resulting in an unrealistic depiction. Mitigation methods The hallucination phenomenon is still not completely understood. Researchers have also proposed that hallucinations are inevitable and are an innate limitation of large language models. Therefore, there is still ongoing research to try to mitigate its occurrence. Particularly, it was shown that language models not only hallucinate but also amplify hallucinations, even for those which were designed to alleviate this issue. Ji et al. divide common mitigation method into two categories data-related methods and modeling and inference methods. Data-related methods include building a faithful dataset, cleaning data automatically and information augmentation by augmenting the inputs with external information. Model and inference methods include changes in the architecture (either modifying the encoder, attention or the decoder in various ways), changes in the training process, such as using reinforcement learning, along with post-processing methods that can correct hallucinations in the output. Researchers have proposed a variety of mitigation measures, including getting different chatbots to debate one another until they reach consensus on an answer. Another approach proposes to actively validate the correctness corresponding to the low-confidence generation of the model using web search results. They have shown that a generated sentence is hallucinated more often when the model has already hallucinated in its previously generated sentences for the input, and they are instructing the model to create a validation question checking the correctness of the information about the selected concept using Bing search API. An extra layer of logic-based rules was proposed for the web search mitigation method, by utilizing different ranks of web pages as a knowledge base, which differ in hierarchy. When there are no external data sources available to validate LLM-generated responses (or the responses are already based on external data as in RAG), model uncertainty estimation techniques from machine learning may be applied to detect hallucinations. According to Luo et al., the previous methods fall into knowledge and retrieval-based approaches which ground LLM responses in factual data using external knowledge sources, such as path grounding. Luo et al. also mention training or reference guiding for language models, involving strategies like employing control codes or contrastive learning to guide the generation process to differentiate between correct and hallucinated content. Another category is evaluation and mitigation focused on specific hallucination types, such as employing methods to evaluate quantity entity in summarization and methods to detect and mitigate self-contradictory statements. Nvidia Guardrails, launched in 2023, can be configured to hard-code certain responses via script instead of leaving them to the LLM. Furthermore, numerous tools like SelfCheckGPT, the Trustworthy Language Model, and Aimon have emerged to aid in the detection of hallucination in offline experimentation and real-time production scenarios. See also Bibliography Shaw, Mary (17 October 2024). tldr Chill, yall AI Will Not Devour SE. Proceedings of the 2024 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software. pp. 305 315. arXiv2409.00764. doi10.1145/3689492.3689816. ISBN 979-8-4007-1215-9. Title Hidden layer URL https//en.wikipedia.org/wiki/Hidden_layer Content In artificial neural networks, a hidden layer is a layer of artificial neurons that is neither an input layer nor an output layer. The simplest examples appear in multilayer perceptrons (MLP), as illustrated in the diagram. An MLP without any hidden layer is essentially just a linear model. With hidden layers and activation functions, however, nonlinearity is introduced into the model. In typical machine learning practice, the weights and biases are initialized, then iteratively updated during training via backpropagation. Title Hierarchical navigable small world URL https//en.wikipedia.org/wiki/Hierarchical_navigable_small_world Content The Hierarchical navigable small world (HNSW) algorithm is a graph-based approximate nearest neighbor search technique used in many vector databases. Nearest neighbor search without an index involves computing the distance from the query to each point in the database, which for large datasets is computationally prohibitive. For high-dimensional data, tree-based exact vector search techniques such as the k-d tree and R-tree do not perform well enough because of the curse of dimensionality. To remedy this, approximate k-nearest neighbor searches have been proposed, such as locality-sensitive hashing (LSH) and product quantization (PQ) that trade performance for accuracy. The HNSW graph offers an approximate k-nearest neighbor search which scales logarithmically even in high-dimensional data. It is an extension of the earlier work on navigable small world graphs presented at the Similarity Search and Applications (SISAP) conference in 2012 with an additional hierarchical navigation to find entry points to the main graph faster. HNSW-based libraries are among the best performers in the approximate nearest neighbors benchmark. Use in vector databases HNSW is a key method for approximate nearest neighbor search in high-dimensional vector databases, for example in the context of embeddings from neural networks in large language models. Databases that use HNSW as search index include Apache Lucene Vector Search Chroma Qdrant Vespa Vearch Gamma Weaviate pgvector MariaDB MongoDB Atlas Milvus DuckDB Several of these use either the hnswlib library provided by the original authors, or the FAISS library. Title Hierarchical Risk Parity URL https//en.wikipedia.org/wiki/Hierarchical_Risk_Parity Content Hierarchical Risk Parity (HRP) is an advanced investment portfolio optimization framework developed in 2016 to compete with the prevailing mean-variance optimization (MVO) framework developed by Harry Markowitz in 1952, and for which he received the Nobel Prize in economic sciences. HRP algorithms apply machine learning techniques to create diversified and robust investment portfolios that outperform MVO methods out-of-sample. HRP aims to address the limitations of traditional portfolio construction methods, particularly when dealing with highly correlated assets. Following its publication, HRP has been implemented in numerous open-source libraries, and received multiple extensions. Key Features Algorithms within the HRP framework are characterized by the following features Machine Learning Approach HRP employs hierarchical clustering, a machine learning technique, to group similar assets based on their correlations. This allows the algorithm to identify the underlying hierarchical structure of the portfolio, and avoid that errors spread through the entire network. Risk-Based Allocation The algorithm allocates capital based on risk, ensuring that assets only compete with similar assets for representation in the portfolio. This approach leads to better diversification across different risk sources, while avoiding the instability associated with noisy returns estimates. Covariance Matrix Handling Unlike traditional methods like Mean-Variance Optimization, HRP does not require inverting the covariance matrix. This makes it more stable and applicable to portfolios with a large number of assets, particularly when the covariance matrixs condition number is high. Steps The HRP algorithm typically consists of three main steps Hierarchical Clustering Assets are grouped into clusters based on their correlations, forming a hierarchical tree structure. Quasi-Diagonalization The correlation matrix is reordered based on the clustering results, revealing a block diagonal structure. Recursive Bisection Weights are assigned to assets through a top-down approach, splitting the portfolio into smaller sub-portfolios and allocating capital based on inverse variance. Advantages HRP algorithms offer several advantages over the (at the time) MVO state-of-the-art methods Improved diversification HRP creates portfolios that are well-diversified across different risk sources. Robustness The algorithm has shown to generate portfolios with robust out-of-sample properties. Flexibility HRP can handle singular covariance matrices and incorporate various constraints. Intuitive approach The clustering-based method provides an intuitive understanding of the portfolio structure. By combining elements of machine learning, risk parity, and traditional portfolio theory, HRP offers a sophisticated approach to portfolio construction that aims to overcome the limitations of conventional methods. Title Highway network URL https//en.wikipedia.org/wiki/Highway_network Content In machine learning, the Highway Network was the first working very deep feedforward neural network with hundreds of layers, much deeper than previous neural networks. It uses skip connections modulated by learned gating mechanisms to regulate information flow, inspired by long short-term memory (LSTM) recurrent neural networks. The advantage of the Highway Network over other deep learning architectures is its ability to overcome or partially prevent the vanishing gradient problem, thus improving its optimization. Gating mechanisms are used to facilitate information flow across the many layers (information highways). Highway Networks have found use in text sequence labeling and speech recognition tasks. In 2014, the state of the art was training deep neural networks with 20 to 30 layers. Stacking too many layers led to a steep reduction in training accuracy, known as the degradation problem. In 2015, two techniques were developed to train such networks the Highway Network (published in May), and the residual neural network, or ResNet (December). ResNet behaves like an open-gated Highway Net. Model The model has two gates in addition to the H ( W H , x ) displaystyle H(W_H,x) gate the transform gate T ( W T , x ) displaystyle T(W_T,x) and the carry gate C ( W C , x ) displaystyle C(W_C,x) . The latter two gates are non-linear transfer functions (specifically sigmoid by convention). The function H displaystyle H can be any desired transfer function. The carry gate is defined as C ( W C , x )  1 T ( W T , x ) displaystyle C(W_C,x)1-T(W_T,x) while the transform gate is just a gate with a sigmoid transfer function. Structure The structure of a hidden layer in the Highway Network follows the equation . To overcome this problem, Long Short-Term Memory (LSTM) recurrent neural networks have residual connections with a weight of 1.0 in every LSTM cell (called the constant error carrousel) to compute y t  1  F ( x t )  x t textstyle y_t1F(x_t)x_t . During backpropagation through time, this becomes the residual formula . This enables training very deep recurrent neural networks with a very long time span t. A later LSTM version published in 2000 modulates the identity LSTM connections by so-called forget gates such that their weights are not fixed to 1.0 but can be learned. In experiments, the forget gates were initialized with positive bias weights, thus being opened, addressing the vanishing gradient problem. As long as the forget gates of the 2000 LSTM are open, it behaves like the 1997 LSTM. The Highway Network of May 2015 applies these principles to feedforward neural networks. It was reported to be the first very deep feedforward network with hundreds of layers. It is like a 2000 LSTM with forget gates unfolded in time, while the later Residual Nets have no equivalent of forget gates and are like the unfolded original 1997 LSTM. If the skip connections in Highway Networks are without gates, or if their gates are kept open (activation 1.0), they become Residual Networks. The residual connection is a special case of the short-cut connection or skip connection by Rosenblatt (1961) and Lang  Witbrock (1988) which has the form x F ( x )  A x displaystyle xmapsto F(x)Ax . Here the randomly initialized weight matrix A does not have to be the identity mapping. Every residual connection is a skip connection, but almost all skip connections are not residual connections. The original Highway Network paper not only introduced the basic principle for very deep feedforward networks, but also included experimental results with 20, 50, and 100 layers networks, and mentioned ongoing experiments with up to 900 layers. Networks with 50 or 100 layers had lower training error than their plain network counterparts, but no lower training error than their 20 layers counterpart (on the MNIST dataset, Figure 1 in ). No improvement on test accuracy was reported with networks deeper than 19 layers (on the CIFAR-10 dataset Table 1 in ). The ResNet paper, however, provided strong experimental evidence of the benefits of going deeper than 20 layers. It argued that the identity mapping without modulation is crucial and mentioned that modulation in the skip connection can still lead to vanishing signals in forward and backward propagation (Section 3 in ). This is also why the forget gates of the 2000 LSTM were initially opened through positive bias weights as long as the gates are open, it behaves like the 1997 LSTM. Similarly, a Highway Net whose gates are opened through strongly positive bias weights behaves like a ResNet. The skip connections used in modern neural networks (e.g., Transformers) are dominantly identity mappings. Title Hugging Face URL https//en.wikipedia.org/wiki/Hugging_Face Content Hugging Face, Inc. is an American company incorporated under the Delaware General Corporation Law and based in New York City that develops computation tools for building applications using machine learning. It is most notable for its transformers library built for natural language processing applications and its platform that allows users to share machine learning models and datasets and showcase their work. History The company was founded in 2016 by French entrepreneurs Cl ment Delangue, Julien Chaumond, and Thomas Wolf in New York City, originally as a company that developed a chatbot app targeted at teenagers. The company was named after the U1F917 HUGGING FACE emoji. After open sourcing the model behind the chatbot, the company pivoted to focus on being a platform for machine learning. In March 2021, Hugging Face raised US40 million in a Series B funding round. On April 28, 2021, the company launched the BigScience Research Workshop in collaboration with several other research groups to release an open large language model. In 2022, the workshop concluded with the announcement of BLOOM, a multilingual large language model with 176 billion parameters. In December 2022, the company acquired Gradio, an open source library built for developing machine learning applications in Python. On May 5, 2022, the company announced its Series C funding round led by Coatue and Sequoia. The company received a 2 billion valuation. On August 3, 2022, the company announced the Private Hub, an enterprise version of its public Hugging Face Hub that supports SaaS or on-premises deployment. In February 2023, the company announced partnership with Amazon Web Services (AWS) which would allow Hugging Faces products available to AWS customers to use them as the building blocks for their custom applications. The company also said the next generation of BLOOM will be run on Trainium, a proprietary machine learning chip created by AWS. In August 2023, the company announced that it raised 4.5 billion valuation. The funding was led by Salesforce, and notable participation came from Google, Amazon, Nvidia, AMD, Intel, IBM, and Qualcomm. In June 2024, the company announced, along with Meta and Scaleway, their launch of a new AI accelerator program for European startups. This initiative aims to help startups integrate open foundation models into their products, accelerating the EU AI ecosystem. The program, based at STATION F in Paris, will run from September 2024 to February 2025. Selected startups will receive mentoring, access to AI models and tools, and Scaleway s computing power. On September 23, 2024, to further the International Decade of Indigenous Languages, Hugging Face teamed up with Meta and UNESCO to launch a new online language translator built on Metas No Language Left Behind open-source AI model, enabling free text translation across 200 languages, including many low-resource languages. Services and technologies Transformers Library The Transformers library is a Python package that contains open-source implementations of transformer models for text, image, and audio tasks. It is compatible with the PyTorch, TensorFlow and JAX deep learning libraries and includes implementations of notable models like BERT and GPT-2. The library was originally called pytorch-pretrained-bert which was then renamed to pytorch-transformers and finally transformers. A javascript version (transformers.js) have also been developed, allowing to run models directly in the browser. Hugging Face Hub The Hugging Face Hub is a platform (centralized web service) for hosting Git-based code repositories, including discussions and pull requests for projects. models, also with Git-based version control datasets, mainly in text, images, and audio web applications (spaces and widgets), intended for small-scale demos of machine learning applications. There are numerous pre-trained models that support common tasks in different modalities, such as Natural Language Processing text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation. Computer Vision image classification, object detection, and segmentation. Audio automatic speech recognition and audio classification. Other libraries In addition to Transformers and the Hugging Face Hub, the Hugging Face ecosystem contains libraries for other tasks, such as dataset processing (Datasets), model evaluation (Evaluate), and machine learning demos (Gradio). Safetensors The safetensors format was developed around 2021 to solve problems with the pickle format in python. It was designed for saving and loading tensors. Compared to pickle format, it allows lazy loading, and avoids security problems. After a security audit, it became the default format in 2023. The file format size of the header 8 bytes, an unsigned little-endian 64-bit integer. header JSON UTF-8 string, formatted as TENSOR_NAME  dtype  , shape  1, 16, 256, data_offsets  BEGIN, END, NEXT_TENSOR_NAME  , . file a byte buffer containing the tensors. See also OpenAI Station F Kaggle References External links Official website Hugging Face on Twitter Hugging Face on YouTube Title Human-in-the-loop URL https//en.wikipedia.org/wiki/Human-in-the-loop Content Human-in-the-loop (HITL) is used in multiple contexts. It can be defined as a model requiring human interaction. HITL is associated with modeling and simulation (MS) in the live, virtual, and constructive taxonomy. HITL along with the related human-on-the-loop are also used in relation to lethal autonomous weapons. Further, HITL is used in the context of machine learning. Machine learning In machine learning, HITL is used in the sense of humans aiding the computer in making the correct decisions in building a model. HITL improves machine learning over random sampling by selecting the most critical data needed to refine the model. Simulation In simulation, HITL models may conform to human factors requirements as in the case of a mockup. In this type of simulation a human is always part of the simulation and consequently influences the outcome in such a way that is difficult if not impossible to reproduce exactly. HITL also readily allows for the identification of problems and requirements that may not be easily identified by other means of simulation. HITL is often referred to as interactive simulation, which is a special kind of physical simulation in which physical simulations include human operators, such as in a flight or a driving simulator. Benefits Human-in-the-loop allows the user to change the outcome of an event or process. The immersion effectively contributes to a positive transfer of acquired skills into the real world. This can be demonstrated by trainees utilizing flight simulators in preparation to become pilots. HITL also allows for the acquisition of knowledge regarding how a new process may affect a particular event. Utilizing HITL allows participants to interact with realistic models and attempt to perform as they would in an actual scenario. HITL simulations bring to the surface issues that would not otherwise be apparent until after a new process has been deployed. A real-world example of HITL simulation as an evaluation tool is its usage by the Federal Aviation Administration (FAA) to allow air traffic controllers to test new automation procedures by directing the activities of simulated air traffic while monitoring the effect of the newly implemented procedures. As with most processes, there is always the possibility of human error, which can only be reproduced using HITL simulation. Although much can be done to automate systems, humans typically still need to take the information provided by a system to determine the next course of action based on their judgment and experience. Intelligent systems can only go so far in certain circumstances to automate a process only humans in the simulation can accurately judge the final design. Tabletop simulation may be useful in the very early stages of project development for the purpose of collecting data to set broad parameters, but the important decisions require human-in-the-loop simulation. Within virtual simulation taxonomy Virtual simulations inject HITL in a central role by exercising motor control skills (e.g. flying an airplane), decision making skills (e.g. committing fire control resources to action), or communication skills (e.g. as members of a C4I team). Examples Flight simulators Driving simulators Marine simulators Video games Supply chain management simulators Digital puppetry Misconceptions Although human-in-the-loop simulation can include a computer simulation in the form of a synthetic environment, computer simulation is not necessarily a form of human-in-the-loop simulation, and is often considered as human-out-of-the loop simulation. In this particular case, a computer model s behavior is modified according to a set of initial parameters. The results of the model differ from the results stemming from a true human-in-the-loop simulation because the results can easily be replicated time and time again, by simply providing identical parameters. Weapons Three classifications of the degree of human control of autonomous weapon systems were laid out by Bonnie Docherty in a 2012 Human Rights Watch report. human-in-the-loop a human must instigate the action of the weapon (in other words not fully autonomous) human-on-the-loop a human may abort an action human-out-of-the-loop no human action is involved See also Humanistic intelligence, which is intelligence that arises by having the human in the feedback loop of the computational process Reinforcement learning from human feedback MIM-104 Patriot - Examples of a human-on-the-loop lethal autonomous weapon system posing a threat to friendly forces. Title Hyperparameter (machine learning) URL https//en.wikipedia.org/wiki/Hyperparameter_(machine_learning) Content In machine learning, a hyperparameter is a parameter that can be set in order to define any configurable part of a models learning process. Hyperparameters can be classified as either model hyperparameters (such as the topology and size of a neural network) or algorithm hyperparameters (such as the learning rate and the batch size of an optimizer). These are named hyperparameters in contrast to parameters, which are characteristics that the model learns from the data. Hyperparameters are not required by every model or algorithm. Some simple algorithms such as ordinary least squares regression require none. However, the LASSO algorithm, for example, adds a regularization hyperparameter to ordinary least squares which must be set before training. Even models and algorithms without a strict requirement to define hyperparameters may not produce meaningful results if these are not carefully chosen. However, optimal values for hyperparameters are not always easy to predict. Some hyperparameters may have no meaningful effect, or one important variable may be conditional upon the value of another. Often a separate process of hyperparameter tuning is needed to find a suitable combination for the data and task. As well was improving model performance, hyperparameters can be used by researchers to introduce robustness and reproducibility into their work, especially if it uses models that incorporate random number generation. Considerations The time required to train and test a model can depend upon the choice of its hyperparameters. A hyperparameter is usually of continuous or integer type, leading to mixed-type optimization problems. The existence of some hyperparameters is conditional upon the value of others, e.g. the size of each hidden layer in a neural network can be conditional upon the number of layers. Difficulty-learnable parameters The objective function is typically non-differentiable with respect to hyperparameters. As a result, in most instances, hyperparameters cannot be learned using gradient-based optimization methods (such as gradient descent), which are commonly employed to learn model parameters. These hyperparameters are those parameters describing a model representation that cannot be learned by common optimization methods, but nonetheless affect the loss function. An example would be the tolerance hyperparameter for errors in support vector machines. Untrainable parameters Sometimes, hyperparameters cannot be learned from the training data because they aggressively increase the capacity of a model and can push the loss function to an undesired minimum (overfitting to the data), as opposed to correctly mapping the richness of the structure in the data. For example, if we treat the degree of a polynomial equation fitting a regression model as a trainable parameter, the degree would increase until the model perfectly fit the data, yielding low training error, but poor generalization performance. Tunability Most performance variation can be attributed to just a few hyperparameters. The tunability of an algorithm, hyperparameter, or interacting hyperparameters is a measure of how much performance can be gained by tuning it. For an LSTM, while the learning rate followed by the network size are its most crucial hyperparameters, batching and momentum have no significant effect on its performance. Although some research has advocated the use of mini-batch sizes in the thousands, other work has found the best performance with mini-batch sizes between 2 and 32. Robustness An inherent stochasticity in learning directly implies that the empirical hyperparameter performance is not necessarily its true performance. Methods that are not robust to simple changes in hyperparameters, random seeds, or even different implementations of the same algorithm cannot be integrated into mission critical control systems without significant simplification and robustification. Reinforcement learning algorithms, in particular, require measuring their performance over a large number of random seeds, and also measuring their sensitivity to choices of hyperparameters. Their evaluation with a small number of random seeds does not capture performance adequately due to high variance. Some reinforcement learning methods, e.g. DDPG (Deep Deterministic Policy Gradient), are more sensitive to hyperparameter choices than others. Optimization Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given test data. The objective function takes a tuple of hyperparameters and returns the associated loss. Typically these methods are not gradient based, and instead apply concepts from derivative-free optimization or black box optimization. Reproducibility Apart from tuning hyperparameters, machine learning involves storing and organizing the parameters and results, and making sure they are reproducible. In the absence of a robust infrastructure for this purpose, research code often evolves quickly and compromises essential aspects like bookkeeping and reproducibility. Online collaboration platforms for machine learning go further by allowing scientists to automatically share, organize and discuss experiments, data, and algorithms. Reproducibility can be particularly difficult for deep learning models. For example, research has shown that deep learning models depend very heavily even on the random seed selection of the random number generator. See also Hyper-heuristic Replication crisis Title Hyperparameter optimization URL https//en.wikipedia.org/wiki/Hyperparameter_optimization Content In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process, which must be configured before the process starts. Hyperparameter optimization determines the set of hyperparameters that yields an optimal model which minimizes a predefined loss function on a given data set. The objective function takes a set of hyperparameters and returns the associated loss. Cross-validation is often used to estimate this generalization performance, and therefore choose the set of values for hyperparameters that maximize it. Approaches Grid search The traditional method for hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a hold-out validation set. Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, manually set bounds and discretization may be necessary before applying grid search. For example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters that need to be tuned for good performance on unseen data a regularization constant C and a kernel hyperparameter . Both parameters are continuous, so to perform grid search, one selects a finite set of reasonable values for each, say C  10 , 100 , 1000  displaystyle Cin 10,100,1000  0.1 , 0.2 , 0.5 , 1.0  displaystyle gamma in 0.1,0.2,0.5,1.0 Grid search then trains an SVM with each pair (C, ) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure. Grid search suffers from the curse of dimensionality, but is often embarrassingly parallel because the hyperparameter settings it evaluates are typically independent of each other. Random search Random Search replaces the exhaustive enumeration of all combinations by selecting them randomly. This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. A benefit over grid search is that random search can explore many more values than grid search could for continuous hyperparameters. It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm. In this case, the optimization problem is said to have a low intrinsic dimensionality. Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample. Despite its simplicity, random search remains one of the important base-lines against which to compare the performance of new hyperparameter optimization methods. Bayesian optimization Bayesian optimization is a global optimization method for noisy black-box functions. Applied to hyperparameter optimization, Bayesian optimization builds a probabilistic model of the function mapping from hyperparameter values to the objective evaluated on a validation set. By iteratively evaluating a promising hyperparameter configuration based on the current model, and then updating it, Bayesian optimization aims to gather observations revealing as much information as possible about this function and, in particular, the location of the optimum. It tries to balance exploration (hyperparameters for which the outcome is most uncertain) and exploitation (hyperparameters expected close to the optimum). In practice, Bayesian optimization has been shown to obtain better results in fewer evaluations compared to grid search and random search, due to the ability to reason about the quality of experiments before they are run. Gradient-based optimization For specific learning algorithms, it is possible to compute the gradient with respect to hyperparameters and then optimize the hyperparameters using gradient descent. The first usage of these techniques was focused on neural networks. Since then, these methods have been extended to other models such as support vector machines or logistic regression. A different approach in order to obtain a gradient with respect to hyperparameters consists in differentiating the steps of an iterative optimization algorithm using automatic differentiation. A more recent work along this direction uses the implicit function theorem to calculate hypergradients and proposes a stable approximation of the inverse Hessian. The method scales to millions of hyperparameters and requires constant memory. In a different approach, a hypernetwork is trained to approximate the best response function. One of the advantages of this method is that it can handle discrete hyperparameters as well. Self-tuning networks offer a memory efficient version of this approach by choosing a compact representation for the hypernetwork. More recently, -STN has improved this method further by a slight reparameterization of the hypernetwork which speeds up training. -STN also yields a better approximation of the best-response Jacobian by linearizing the network in the weights, hence removing unnecessary nonlinear effects of large changes in the weights. Apart from hypernetwork approaches, gradient-based methods can be used to optimize discrete hyperparameters also by adopting a continuous relaxation of the parameters. Such methods have been extensively used for the optimization of architecture hyperparameters in neural architecture search. Evolutionary optimization Evolutionary optimization is a methodology for the global optimization of noisy black-box functions. In hyperparameter optimization, evolutionary optimization uses evolutionary algorithms to search the space of hyperparameters for a given algorithm. Evolutionary hyperparameter optimization follows a process inspired by the biological concept of evolution Create an initial population of random solutions (i.e., randomly generate tuples of hyperparameters, typically 100) Evaluate the hyperparameter tuples and acquire their fitness function (e.g., 10-fold cross-validation accuracy of the machine learning algorithm with those hyperparameters) Rank the hyperparameter tuples by their relative fitness Replace the worst-performing hyperparameter tuples with new ones generated via crossover and mutation Repeat steps 2-4 until satisfactory algorithm performance is reached or is no longer improving. Evolutionary optimization has been used in hyperparameter optimization for statistical machine learning algorithms, automated machine learning, typical neural network and deep neural network architecture search, as well as training of the weights in deep neural networks. Population-based Population Based Training (PBT) learns both hyperparameter values and network weights. Multiple learning processes operate independently, using different hyperparameters. As with evolutionary methods, poorly performing models are iteratively replaced with models that adopt modified hyperparameter values and weights based on the better performers. This replacement model warm starting is the primary differentiator between PBT and other evolutionary methods. PBT thus allows the hyperparameters to evolve and eliminates the need for manual hypertuning. The process makes no assumptions regarding model architecture, loss functions or training procedures. PBT and its variants are adaptive methods they update hyperparameters during the training of the models. On the contrary, non-adaptive methods have the sub-optimal strategy to assign a constant set of hyperparameters for the whole training. Early stopping-based A class of early stopping-based hyperparameter optimization algorithms is purpose built for large search spaces of continuous and discrete hyperparameters, particularly when the computational cost to evaluate the performance of a set of hyperparameters is high. Irace implements the iterated racing algorithm, that focuses the search around the most promising configurations, using statistical tests to discard the ones that perform poorly. Another early stopping hyperparameter optimization algorithm is successive halving (SHA), which begins as a random search but periodically prunes low-performing models, thereby focusing computational resources on more promising models. Asynchronous successive halving (ASHA) further improves upon SHAs resource utilization profile by removing the need to synchronously evaluate and prune low-performing models. Hyperband is a higher level early stopping-based algorithm that invokes SHA or ASHA multiple times with varying levels of pruning aggressiveness, in order to be more widely applicable and with fewer required inputs. Others RBF and spectral approaches have also been developed. Issues with hyperparameter optimization When hyperparameter optimization is done, the set of hyperparameters are often fitted on a training set and selected based on the generalization performance, or score, of a validation set. However, this procedure is at risk of overfitting the hyperparameters to the validation set. Therefore, the generalization performance score of the validation set (which can be several sets in the case of a cross-validation procedure) cannot be used to simultaneously estimate the generalization performance of the final model. In order to do so, the generalization performance has to be evaluated on a set independent (which has no intersection) of the set (or sets) used for the optimization of the hyperparameters, otherwise the performance might give a value which is too optimistic (too large). This can be done on a second test set, or through an outer cross-validation procedure called nested cross-validation, which allows an unbiased estimation of the generalization performance of the model, taking into account the bias due to the hyperparameter optimization. See also Automated machine learning Neural architecture search Meta-optimization Model selection Self-tuning XGBoost Title In-context learning (natural language processing) URL https//en.wikipedia.org/wiki/Prompt_engineering Content Prompt engineering is the process of structuring or crafting an instruction in order to produce the best possible output from a generative artificial intelligence (AI) model. A prompt is natural language text describing the task that an AI should perform. A prompt for a text-to-text language model can be a query, a command, or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, choice of words and grammar, providing relevant context, or describing a character for the AI to mimic. When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as a high-quality photo of an astronaut riding a horse or Lo-fi slow BPM electro chill with organic samples. Prompting a text-to-image model may involve adding, removing, emphasizing, and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic. History In 2018, researchers first proposed that all previously separate tasks in natural language processing (NLP) could be cast as a question-answering problem over a context. In addition, they trained a first single, joint, multi-task model that would answer any task-related question like What is the sentiment or Translate this sentence to German or Who is the president? The AI boom saw an increase in the amount of prompting technique to get the model to output the desired outcome and avoid nonsensical output, a process characterized by trial-and-error. After the release of ChatGPT in 2022, prompt engineering was soon seen as an important business skill, albeit one with an uncertain economic future. A repository for prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022. In 2022, the chain-of-thought prompting technique was proposed by Google researchers. In 2023, several text-to-text and text-to-image prompt databases were made publicly available. The Personalized Image-Prompt (PIP) dataset, a generated image-text dataset that has been categorized by 3,115 users, has also been made available publicly in 2024. Text-to-text Multiple distinct prompt engineering techniques have been published. Chain-of-thought According to Google Research, chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps before giving a final answer. In 2022, Google Brain reported that chain-of-thought prompting improves reasoning ability by inducing the model to answer a multi-step problem with steps of reasoning that mimic a train of thought. Chain-of-thought techniques were developed to help LLMs handle multi-step reasoning tasks, such as arithmetic or commonsense reasoning questions. For example, given the question, Q The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?, Google claims that a CoT prompt might induce the LLM to answer A The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20  3. They bought 6 more apples, so they have 3  6  9. The answer is 9. When applied to PaLM, a 540 billion parameter language model, according to Google, CoT prompting significantly aided the model, allowing it to perform comparably with task-specific fine-tuned models on several tasks, achieving state-of-the-art results at the time on the GSM8K mathematical reasoning benchmark. It is possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability. An example of a CoT prompting Q question A Lets think step by step. As originally proposed by Google, each CoT prompt included a few QA examples. This made it a few-shot prompting technique. However, according to researchers at Google and the University of Tokyo, simply appending the words Lets think step-by-step, has also proven effective, which makes CoT a zero-shot prompting technique. OpenAI claims that this prompt allows for better scaling as a user no longer needs to formulate many specific CoT QA examples. In-context learning In-context learning, refers to a models ability to temporarily learn from prompts. For example, a prompt may include a few examples for a model to learn from, such as asking the model to complete maison house, chat cat, chien  (the expected response being dog), an approach called few-shot learning. In-context learning is an emergent ability of large language models. It is an emergent property of model scale, meaning that breaks in downstream scaling laws occur, leading to its efficacy increasing at a different rate in larger models than in smaller models. Unlike training and fine-tuning, which produce lasting changes, in-context learning is temporary. Training models to perform in-context learning can be viewed as a form of meta-learning, or learning to learn. Self-consistency decoding Self-consistency decoding performs several chain-of-thought rollouts, then selects the most commonly reached conclusion out of all the rollouts. If the rollouts disagree by a lot, a human can be queried for the correct chain of thought. Tree-of-thought Tree-of-thought prompting generalizes chain-of-thought by prompting the model to generate one or more possible next steps, and then running the model on each of the possible next steps by breadth-first, beam, or some other method of tree search. The LLM has additional modules that can converse the history of the problem-solving process to the LLM, which allows the system to backtrack steps the problem-solving process. Prompting to disclose uncertainty By default, the output of language models may not contain estimates of uncertainty. The model may output text that appears confident, though the underlying token predictions have low likelihood scores. Large language models like GPT-4 can have accurately calibrated likelihood scores in their token predictions, and so the model output uncertainty can be directly estimated by reading out the token prediction likelihood scores. Prompting to estimate model sensitivity Research consistently demonstrates that LLMs are highly sensitive to subtle variations in prompt formatting, structure, and linguistic properties. Some studies have shown up to 76 accuracy points across formatting changes in few-shot settings. Linguistic features significantly influence prompt effectiveness such as morphology, syntax, and lexico-semantic changes which meaningfully enhance task performance across a variety of tasks. Clausal syntax, for example, improves consistency and reduces uncertainty in knowledge retrieval. This sensitivity persists even with larger model sizes, additional few-shot examples, or instruction tuning. To address sensitivity of models and make them more robust, several methods have been proposed. FormatSpread facilitates systematic analysis by evaluating a range of plausible prompt formats, offering a more comprehensive performance interval. Similarly, PromptEval estimates performance distributions across diverse prompts, enabling robust metrics such as performance quantiles and accurate evaluations under constrained budgets. Automatic prompt generation Retrieval-augmented generation Retrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information. RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts. This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases. By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. Graph retrieval-augmented generation GraphRAG (coined by Microsoft Research) is a technique that extends RAG with the use of a knowledge graph (usually, LLM-generated) to allow the model to connect disparate pieces of information, synthesize insights, and holistically understand summarized semantic concepts over large data collections. It was shown to be effective on datasets like the Violent Incident Information from News Articles (VIINA). Earlier work showed the effectiveness of using a knowledge graph for question answering using text-to-query generation. These techniques can be combined to search across both unstructured and structured data, providing expanded context, and improved ranking. Using language models to generate prompts Large language models (LLM) themselves can be used to compose prompts for large language models. The automatic prompt engineer algorithm uses one LLM to beam search over prompts for another LLM There are two LLMs. One is the target LLM, and another is the prompting LLM. Prompting LLM is presented with example input-output pairs, and asked to generate instructions that could have caused a model following the instructions to generate the outputs, given the inputs. Each of the generated instructions is used to prompt the target LLM, followed by each of the inputs. The log-probabilities of the outputs are computed and added. This is the score of the instruction. The highest-scored instructions are given to the prompting LLM for further variations. Repeat until some stopping criteria is reached, then output the highest-scored instructions. CoT examples can be generated by LLM themselves. In auto-CoT, a library of questions are converted to vectors by a model such as BERT. The question vectors are clustered. Questions nearest to the centroids of each cluster are selected. An LLM does zero-shot CoT on each question. The resulting CoT examples are added to the dataset. When prompted with a new question, CoT examples to the nearest questions can be retrieved and added to the prompt. Text-to-image In 2022, text-to-image models like DALL-E 2, Stable Diffusion, and Midjourney were released to the public. These models take text prompts as input and use them to generate AI-generated images. Text-to-image models typically do not understand grammar and sentence structure in the same way as large language models, thus may require a different set of prompting techniques. Text-to-image models do not natively understand negation. The prompt a party with no cake is likely to produce an image including a cake. As an alternative, negative prompts allow a user to indicate, in a separate prompt, which terms should not appear in the resulting image. Techniques such as framing the normal prompt into a sequence-to-sequence language modeling problem can be used to automatically generate an output for the negative prompt. Prompt formats A text-to-image prompt commonly includes a description of the subject of the art, the desired medium (such as digital painting or photography), style (such as hyperrealistic or pop-art), lighting (such as rim lighting or crepuscular rays), color, and texture. Word order also affects the output of a text-to-image prompt. Words closer to the start of a prompt may be emphasized more heavily. The Midjourney documentation encourages short, descriptive prompts instead of Show me a picture of lots of blooming California poppies, make them bright, vibrant orange, and draw them in an illustrated style with colored pencils, an effective prompt might be Bright orange California poppies drawn with colored pencils. Artist styles Some text-to-image models are capable of imitating the style of particular artists by name. For example, the phrase in the style of Greg Rutkowski has been used in Stable Diffusion and Midjourney prompts to generate images in the distinctive style of Polish digital artist Greg Rutkowski. Famous artists such as Vincent van Gogh and Salvador Dal have also been used for styling and testing. Non-text prompts Some approaches augment or replace natural language text prompts with non-text input. Textual inversion and embeddings For text-to-image models, textual inversion performs an optimization process to create a new word embedding based on a set of example images. This embedding vector acts as a pseudo-word which can be included in a prompt to express the content or style of the examples. Image prompting In 2023, Metas AI research released Segment Anything, a computer vision model that can perform image segmentation by prompting. As an alternative to text prompts, Segment Anything can accept bounding boxes, segmentation masks, and foreground/background points. Using gradient descent to search for prompts In prefix-tuning, prompt tuning, or soft prompting, floating-point-valued vectors are searched directly by gradient descent to maximize the log-likelihood on outputs. Formally, let . During training, the tunable embeddings, input, and output tokens are concatenated into a single sequence concat ( E  X  Y ) displaystyle textconcat(mathbf E mathbf X mathbf Y ) , and fed to the LLMs. The losses are computed over the Y displaystyle mathbf Y  tokens the gradients are backpropagated to prompt-specific parameters in prefix-tuning, they are parameters associated with the prompt tokens at each layer in prompt tuning, they are merely the soft tokens added to the vocabulary. More formally, this is prompt tuning. Let an LLM be written as L L M ( X )  F ( E ( X ) ) displaystyle LLM(X)F(E(X)) , where X displaystyle X is a sequence of linguistic tokens, E displaystyle E is the token-to-vector function, and F displaystyle F is the rest of the model. In prefix-tuning, one provides a set of input-output pairs  ( X i , Y i )  i displaystyle (Xi,Yi)_i , and then use gradient descent to search for arg max Z  i log P r  Y i  Z  E ( X i )  displaystyle arg max _tilde Zsum _ilog PrYitilde Zast E(Xi) . In words, log P r  Y i  Z  E ( X i )  displaystyle log PrYitilde Zast E(Xi) is the log-likelihood of outputting Y i displaystyle Yi , if the model first encodes the input X i displaystyle Xi into the vector E ( X i ) displaystyle E(Xi) , then prepend the vector with the prefix vector Z  displaystyle tilde Z , then apply F displaystyle F . For prefix tuning, it is similar, but the prefix vector Z  displaystyle tilde Z is pre-appended to the hidden states in every layer of the model. An earlier result uses the same idea of gradient descent search, but is designed for masked language models like BERT, and searches only over token sequences, rather than numerical vectors. Formally, it searches for arg max X  i log P r  Y i  X  X i  displaystyle arg max _tilde Xsum _ilog PrYitilde Xast Xi where X  displaystyle tilde X is ranges over token sequences of a specified length. Prompt injection Prompt injection is a cybersecurity exploit in which adversaries craft inputs that appear legitimate but are designed to cause unintended behavior in machine learning models, particularly large language models (LLMs). This attack takes advantage of the models inability to distinguish between developer-defined prompts and user inputs, allowing adversaries to bypass safeguards and influence model behaviour. While LLMs are designed to follow trusted instructions, they can be manipulated into carrying out unintended responses through carefully crafted inputs. See also Social engineering (security) Title Inauthentic text URL https//en.wikipedia.org/wiki/Inauthentic_text Content An inauthentic text is a computer-generated expository document meant to appear as genuine, but which is actually meaningless. Frequently they are created in order to be intermixed with genuine documents and thus manipulate the results of search engines, as with Spam blogs. They are also carried along in email in order to fool spam filters by giving the spam the superficial characteristics of legitimate text. Sometimes nonsensical documents are created with computer assistance for humorous effect, as with Dissociated press or Flarf poetry. They have also been used to challenge the veracity of a publication MIT students submitted papers generated by a computer program called SCIgen to a conference, where they were initially accepted. This led the students to claim that the bar for submissions was too low. With the amount of computer generated text outpacing the ability of people to humans to curate it, there needs some means of distinguishing between the two. Yet automated approaches to determining absolutely whether a text is authentic or not face intrinsic challenges of semantics. Noam Chomsky coined the phrase Colorless green ideas sleep furiously giving an example of grammatically-correct, but semantically incoherent sentence some will point out that in certain contexts one could give this sentence (or any phrase) meaning. The first group to use the expression in this regard can be found below from Indiana University. Their work explains in detail an attempt to detect inauthentic texts and identify pernicious problems of inauthentic texts in cyberspace. The site has a means of submitting text that assesses, based on supervised learning, whether a corpus is inauthentic or not. Many users have submitted incorrect types of data and have correspondingly commented on the scores. This application is meant for a specific kind of data therefore, submitting, say, an email, will not return a meaningful score. See also Scraper site Spamdexing Stochastic parrot External links An Inauthentic Paper Detector from Indiana University School of Informatics Title Inception score URL https//en.wikipedia.org/wiki/Inception_score Content The Inception Score (IS) is an algorithm used to assess the quality of images created by a generative image model such as a generative adversarial network (GAN). The score is calculated based on the output of a separate, pretrained Inception image classification model applied to a sample of (typically around 30,000) images generated by the generative model. The Inception Score is maximized when the following conditions are true The entropy of the distribution of labels predicted by the model for the generated images is minimized. In other words, the classification model confidently predicts a single label for each image. Intuitively, this corresponds to the desideratum of generated images being sharp or distinct. The predictions of the classification model are evenly distributed across all possible labels. This corresponds to the desideratum that the output of the generative model is diverse. It has been somewhat superseded by the related Fr chet inception distance. While the Inception Score only evaluates the distribution of generated images, the FID compares the distribution of generated images with the distribution of a set of real images (ground truth). Definition Let there be two spaces, the space of images X displaystyle Omega _X and the space of labels Y displaystyle Omega _Y . The space of labels is finite. Let p g e n displaystyle p_gen be a probability distribution over X displaystyle Omega _X that we wish to judge. Let a discriminator be a function of type p d i s  X M ( Y ) displaystyle p_disOmega _Xto M(Omega _Y) where M ( Y ) displaystyle M(Omega _Y) is the set of all probability distributions on Y displaystyle Omega _Y . For any image x displaystyle x , and any label y displaystyle y , let p d i s ( y  x ) displaystyle p_dis(yx) be the probability that image x displaystyle x has label y displaystyle y , according to the discriminator. It is usually implemented as an Inception- network trained on ImageNet. The Inception Score of p g e n displaystyle p_gen relative to p d i s displaystyle p_dis is I S ( p g e n , p d i s )  exp ( E x p g e n  D K L ( p d i s (  x ) p d i s (  x ) p g e n ( x ) d x )  ) displaystyle IS(p_gen,p_dis)exp left(mathbb E _xsim p_genleftD_KLleft(p_dis(cdot x)int p_dis(cdot x)p_gen(x)dxright)rightright) Equivalent rewrites include ln I S ( p g e n , p d i s )  E x p g e n  D K L ( p d i s (  x ) E x p g e n  p d i s (  x )  )  displaystyle ln IS(p_gen,p_dis)mathbb E _xsim p_genleftD_KLleft(p_dis(cdot x)mathbb E _xsim p_genp_dis(cdot x)right)right ln I S ( p g e n , p d i s )  H  E x p g e n  p d i s (  x )   E x p g e n  H  p d i s (  x )   displaystyle ln IS(p_gen,p_dis)Hmathbb E _xsim p_genp_dis(cdot x)-mathbb E _xsim p_genHp_dis(cdot x) ln I S displaystyle ln IS is nonnegative by Jensens inequality. PseudocodeINPUT discriminator p d i s displaystyle p_dis . INPUT generator g displaystyle g . Sample images x i displaystyle x_i from generator. Compute p d i s (  x i ) displaystyle p_dis(cdot x_i) , the probability distribution over labels conditional on image x i displaystyle x_i . Sum up the results to obtain p  displaystyle hat p , an empirical estimate of p d i s (  x ) p g e n ( x ) d x displaystyle int p_dis(cdot x)p_gen(x)dx . Sample more images x i displaystyle x_i from generator, and for each, compute D K L ( p d i s (  x i ) p  ) displaystyle D_KLleft(p_dis(cdot x_i)hat pright) . Average the results, and take its exponential. RETURN the result. Interpretation A higher inception score is interpreted as better, as it means that p g e n displaystyle p_gen is a sharp and distinct collection of pictures. ln I S ( p g e n , p d i s )  0 , ln N  displaystyle ln IS(p_gen,p_dis)in 0,ln N , where N displaystyle N is the total number of possible labels. ln I S ( p g e n , p d i s )  0 displaystyle ln IS(p_gen,p_dis)0 iff for almost all x p g e n displaystyle xsim p_gen p d i s (  x )  p d i s (  x ) p g e n ( x ) d x displaystyle p_dis(cdot x)int p_dis(cdot x)p_gen(x)dx That means p g e n displaystyle p_gen is completely indistinct. That is, for any image x displaystyle x sampled from p g e n displaystyle p_gen , discriminator returns exactly the same label predictions p d i s (  x ) displaystyle p_dis(cdot x) . The highest inception score N displaystyle N is achieved if and only if the two conditions are both true For almost all x p g e n displaystyle xsim p_gen , the distribution p d i s ( y  x ) displaystyle p_dis(yx) is concentrated on one label. That is, H y  p d i s ( y  x )   0 displaystyle H_yp_dis(yx)0 . That is, every image sampled from p g e n displaystyle p_gen is exactly classified by the discriminator. For every label y displaystyle y , the proportion of generated images labelled as y displaystyle y is exactly E x p g e n  p d i s ( y  x )   1 N displaystyle mathbb E _xsim p_genp_dis(yx)frac 1N . That is, the generated images are equally distributed over all labels. Title Inductive bias URL https//en.wikipedia.org/wiki/Inductive_bias Content The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner uses to predict outputs of given inputs that it has not encountered. Inductive bias is anything which makes the algorithm learn one pattern instead of another pattern (e.g., step-functions in decision trees instead of continuous functions in linear regression models). Learning involves searching a space of solutions for a solution that provides a good explanation of the data. However, in many cases, there may be multiple equally appropriate solutions. An inductive bias allows a learning algorithm to prioritize one solution (or interpretation) over another, independently of the observed data. In machine learning, the aim is to construct algorithms that are able to learn to predict a certain target output. To achieve this, the learning algorithm is presented some training examples that demonstrate the intended relation of input and output values. Then the learner is supposed to approximate the correct output, even for examples that have not been shown during training. Without any additional assumptions, this problem cannot be solved since unseen situations might have an arbitrary output value. The kind of necessary assumptions about the nature of the target function are subsumed in the phrase inductive bias. A classical example of an inductive bias is Occams razor, assuming that the simplest consistent hypothesis about the target function is actually the best. Here, consistent means that the hypothesis of the learner yields correct outputs for all of the examples that have been given to the algorithm. Approaches to a more formal definition of inductive bias are based on mathematical logic. Here, the inductive bias is a logical formula that, together with the training data, logically entails the hypothesis generated by the learner. However, this strict formalism fails in many practical cases in which the inductive bias can only be given as a rough description (e.g., in the case of artificial neural networks), or not at all. Types The following is a list of common inductive biases in machine learning algorithms. Maximum conditional independence if the hypothesis can be cast in a Bayesian framework, try to maximize conditional independence. This is the bias used in the Naive Bayes classifier. Minimum cross-validation error when trying to choose among hypotheses, select the hypothesis with the lowest cross-validation error. Although cross-validation may seem to be free of bias, the no free lunch theorems show that cross-validation must be biased, for example assuming that there is no information encoded in the ordering of the data. Maximum margin when drawing a boundary between two classes, attempt to maximize the width of the boundary. This is the bias used in support vector machines. The assumption is that distinct classes tend to be separated by wide boundaries. Minimum description length when forming a hypothesis, attempt to minimize the length of the description of the hypothesis. Minimum features unless there is good evidence that a feature is useful, it should be deleted. This is the assumption behind feature selection algorithms. Nearest neighbors assume that most of the cases in a small neighborhood in feature space belong to the same class. Given a case for which the class is unknown, guess that it belongs to the same class as the majority in its immediate neighborhood. This is the bias used in the k-nearest neighbors algorithm. The assumption is that cases that are near each other tend to belong to the same class. Shift of bias Although most learning algorithms have a static bias, some algorithms are designed to shift their bias as they acquire more data. This does not avoid bias, since the bias shifting process itself must have a bias. See also Algorithmic bias Cognitive bias No free lunch theorem No free lunch in search and optimization Title Inductive probability URL https//en.wikipedia.org/wiki/Inductive_probability Content Inductive probability attempts to give the probability of future events based on past events. It is the basis for inductive reasoning, and gives the mathematical basis for learning and the perception of patterns. It is a source of knowledge about the world. There are three sources of knowledge inference, communication, and deduction. Communication relays information found using other methods. Deduction establishes new facts based on existing facts. Inference establishes new facts from data. Its basis is Bayes theorem. Information describing the world is written in a language. For example, a simple mathematical language of propositions may be chosen. Sentences may be written down in this language as strings of characters. But in the computer it is possible to encode these sentences as strings of bits (1s and 0s). Then the language may be encoded so that the most commonly used sentences are the shortest. This internal language implicitly represents probabilities of statements. Occams razor says the simplest theory, consistent with the data is most likely to be correct. The simplest theory is interpreted as the representation of the theory written in this internal language. The theory with the shortest encoding in this internal language is most likely to be correct. History Probability and statistics was focused on probability distributions and tests of significance. Probability was formal, well defined, but limited in scope. In particular its application was limited to situations that could be defined as an experiment or trial, with a well defined population. Bayess theorem is named after Rev. Thomas Bayes 1701 1761. Bayesian inference broadened the application of probability to many situations where a population was not well defined. But Bayes theorem always depended on prior probabilities, to generate new probabilities. It was unclear where these prior probabilities should come from. Ray Solomonoff developed algorithmic probability which gave an explanation for what randomness is and how patterns in the data may be represented by computer programs, that give shorter representations of the data circa 1964. Chris Wallace and D. M. Boulton developed minimum message length circa 1968. Later Jorma Rissanen developed the minimum description length circa 1978. These methods allow information theory to be related to probability, in a way that can be compared to the application of Bayes theorem, but which give a source and explanation for the role of prior probabilities. Marcus Hutter combined decision theory with the work of Ray Solomonoff and Andrey Kolmogorov to give a theory for the Pareto optimal behavior for an Intelligent agent, circa 1998. Minimum description/message length The program with the shortest length that matches the data is the most likely to predict future data. This is the thesis behind the minimum message length and minimum description length methods. At first sight Bayes theorem appears different from the minimimum message/description length principle. At closer inspection it turns out to be the same. Bayes theorem is about conditional probabilities, and states the probability that event B happens if firstly event A happens P ( A B )  P ( B ) P ( A  B )  P ( A ) P ( B  A ) displaystyle P(Aland B)P(B)cdot P(AB)P(A)cdot P(BA) becomes in terms of message length L, L ( A B )  L ( B )  L ( A  B )  L ( A )  L ( B  A ) . displaystyle L(Aland B)L(B)L(AB)L(A)L(BA). This means that if all the information is given describing an event then the length of the information may be used to give the raw probability of the event. So if the information describing the occurrence of A is given, along with the information describing B given A, then all the information describing A and B has been given. Overfitting Overfitting occurs when the model matches the random noise and not the pattern in the data. For example, take the situation where a curve is fitted to a set of points. If a polynomial with many terms is fitted then it can more closely represent the data. Then the fit will be better, and the information needed to describe the deviations from the fitted curve will be smaller. Smaller information length means higher probability. However, the information needed to describe the curve must also be considered. The total information for a curve with many terms may be greater than for a curve with fewer terms, that has not as good a fit, but needs less information to describe the polynomial. Inference based on program complexity Solomonoffs theory of inductive inference is also inductive inference. A bit string x is observed. Then consider all programs that generate strings starting with x. Cast in the form of inductive inference, the programs are theories that imply the observation of the bit string x. The method used here to give probabilities for inductive inference is based on Solomonoffs theory of inductive inference. Detecting patterns in the data If all the bits are 1, then people infer that there is a bias in the coin and that it is more likely also that the next bit is 1 also. This is described as learning from, or detecting a pattern in the data. Such a pattern may be represented by a computer program. A short computer program may be written that produces a series of bits which are all 1. If the length of the program K is L ( K ) displaystyle L(K) bits then its prior probability is, P ( K )  2 L ( K ) displaystyle P(K)2-L(K) The length of the shortest program that represents the string of bits is called the Kolmogorov complexity. Kolmogorov complexity is not computable. This is related to the halting problem. When searching for the shortest program some programs may go into an infinite loop. Considering all theories The Greek philosopher Epicurus is quoted as saying If more than one theory is consistent with the observations, keep all theories. As in a crime novel all theories must be considered in determining the likely murderer, so with inductive probability all programs must be considered in determining the likely future bits arising from the stream of bits. Programs that are already longer than n have no predictive power. The raw (or prior) probability that the pattern of bits is random (has no pattern) is 2 n displaystyle 2-n . Each program that produces the sequence of bits, but is shorter than the n is a theory/pattern about the bits with a probability of 2 k displaystyle 2-k where k is the length of the program. The probability of receiving a sequence of bits y after receiving a series of bits x is then the conditional probability of receiving y given x, which is the probability of x with y appended, divided by the probability of x. Universal priors The programming language affects the predictions of the next bit in the string. The language acts as a prior probability. This is particularly a problem where the programming language codes for numbers and other data types. Intuitively we think that 0 and 1 are simple numbers, and that prime numbers are somehow more complex than numbers that may be composite. Using the Kolmogorov complexity gives an unbiased estimate (a universal prior) of the prior probability of a number. As a thought experiment an intelligent agent may be fitted with a data input device giving a series of numbers, after applying some transformation function to the raw numbers. Another agent might have the same input device with a different transformation function. The agents do not see or know about these transformation functions. Then there appears no rational basis for preferring one function over another. A universal prior insures that although two agents may have different initial probability distributions for the data input, the difference will be bounded by a constant. So universal priors do not eliminate an initial bias, but they reduce and limit it. Whenever we describe an event in a language, either using a natural language or other, the language has encoded in it our prior expectations. So some reliance on prior probabilities are inevitable. A problem arises where an intelligent agents prior expectations interact with the environment to form a self reinforcing feed back loop. This is the problem of bias or prejudice. Universal priors reduce but do not eliminate this problem. Universal artificial intelligence The theory of universal artificial intelligence applies decision theory to inductive probabilities. The theory shows how the best actions to optimize a reward function may be chosen. The result is a theoretical model of intelligence. It is a fundamental theory of intelligence, which optimizes the agents behavior in, Exploring the environment performing actions to get responses that broaden the agents knowledge. Competing or co-operating with another agent games. Balancing short and long term rewards. In general no agent will always provide the best actions in all situations. A particular choice made by an agent may be wrong, and the environment may provide no way for the agent to recover from an initial bad choice. However the agent is Pareto optimal in the sense that no other agent will do better than this agent in this environment, without doing worse in another environment. No other agent may, in this sense, be said to be better. At present the theory is limited by incomputability (the halting problem). Approximations may be used to avoid this. Processing speed and combinatorial explosion remain the primary limiting factors for artificial intelligence. Probability Probability is the representation of uncertain or partial knowledge about the truth of statements. Probabilities are subjective and personal estimates of likely outcomes based on past experience and inferences made from the data. This description of probability may seem strange at first. In natural language we refer to the probability that the sun will rise tomorrow. We do not refer to your probability that the sun will rise. But in order for inference to be correctly modeled probability must be personal, and the act of inference generates new posterior probabilities from prior probabilities. Probabilities are personal because they are conditional on the knowledge of the individual. Probabilities are subjective because they always depend, to some extent, on prior probabilities assigned by the individual. Subjective should not be taken here to mean vague or undefined. The term intelligent agent is used to refer to the holder of the probabilities. The intelligent agent may be a human or a machine. If the intelligent agent does not interact with the environment then the probability will converge over time to the frequency of the event. If however the agent uses the probability to interact with the environment there may be a feedback, so that two agents in the identical environment starting with only slightly different priors, end up with completely different probabilities. In this case optimal decision theory as in Marcus Hutters Universal Artificial Intelligence will give Pareto optimal performance for the agent. This means that no other intelligent agent could do better in one environment without doing worse in another environment. Comparison to deductive probability In deductive probability theories, probabilities are absolutes, independent of the individual making the assessment. But deductive probabilities are based on, Shared knowledge. Assumed facts, that should be inferred from the data. For example, in a trial the participants are aware the outcome of all the previous history of trials. They also assume that each outcome is equally probable. Together this allows a single unconditional value of probability to be defined. But in reality each individual does not have the same information. And in general the probability of each outcome is not equal. The dice may be loaded, and this loading needs to be inferred from the data. Probability as estimation The principle of indifference has played a key role in probability theory. It says that if N statements are symmetric so that one condition cannot be preferred over another then all statements are equally probable. Taken seriously, in evaluating probability this principle leads to contradictions. Suppose there are 3 bags of gold in the distance and one is asked to select one. Then because of the distance one cannot see the bag sizes. You estimate using the principle of indifference that each bag has equal amounts of gold, and each bag has one third of the gold. Now, while one of us is not looking, the other takes one of the bags and divide it into 3 bags. Now there are 5 bags of gold. The principle of indifference now says each bag has one fifth of the gold. A bag that was estimated to have one third of the gold is now estimated to have one fifth of the gold. Taken as a value associated with the bag the values are different therefore contradictory. But taken as an estimate given under a particular scenario, both values are separate estimates given under different circumstances and there is no reason to believe they are equal. Estimates of prior probabilities are particularly suspect. Estimates will be constructed that do not follow any consistent frequency distribution. For this reason prior probabilities are considered as estimates of probabilities rather than probabilities. A full theoretical treatment would associate with each probability, The statement Prior knowledge Prior probabilities The estimation procedure used to give the probability. Combining probability approaches Inductive probability combines two different approaches to probability. Probability and information Probability and frequency Each approach gives a slightly different viewpoint. Information theory is used in relating probabilities to quantities of information. This approach is often used in giving estimates of prior probabilities. Frequentist probability defines probabilities as objective statements about how often an event occurs. This approach may be stretched by defining the trials to be over possible worlds. Statements about possible worlds define events. Probability and information Whereas logic represents only two values true and false as the values of statement, probability associates a number in 0,1 to each statement. If the probability of a statement is 0, the statement is false. If the probability of a statement is 1 the statement is true. In considering some data as a string of bits the prior probabilities for a sequence of 1s and 0s, the probability of 1 and 0 is equal. Therefore, each extra bit halves the probability of a sequence of bits. This leads to the conclusion that, P ( x )  2 L ( x ) displaystyle P(x)2-L(x) Where P ( x ) displaystyle P(x) is the probability of the string of bits x displaystyle x and L ( x ) displaystyle L(x) is its length. The prior probability of any statement is calculated from the number of bits needed to state it. See also information theory. Combining information Two statements A displaystyle A and B displaystyle B may be represented by two separate encodings. Then the length of the encoding is, L ( A B )  L ( A )  L ( B ) displaystyle L(Aland B)L(A)L(B) or in terms of probability, P ( A B )  P ( A ) P ( B ) displaystyle P(Aland B)P(B) But this law is not always true because there may be a shorter method of encoding B displaystyle B if we assume A displaystyle A . So the above probability law applies only if A displaystyle A and B displaystyle B are independent. The internal language of information The primary use of the information approach to probability is to provide estimates of the complexity of statements. Recall that Occams razor states that All things being equal, the simplest theory is the most likely to be correct. In order to apply this rule, first there needs to be a definition of what simplest means. Information theory defines simplest to mean having the shortest encoding. Knowledge is represented as statements. Each statement is a Boolean expression. Expressions are encoded by a function that takes a description (as against the value) of the expression and encodes it as a bit string. The length of the encoding of a statement gives an estimate of the probability of a statement. This probability estimate will often be used as the prior probability of a statement. Technically this estimate is not a probability because it is not constructed from a frequency distribution. The probability estimates given by it do not always obey the law of total of probability. Applying the law of total probability to various scenarios will usually give a more accurate probability estimate of the prior probability than the estimate from the length of the statement. Encoding expressions An expression is constructed from sub expressions, Constants (including function identifier). Application of functions. quantifiers. A Huffman code must distinguish the 3 cases. The length of each code is based on the frequency of each type of sub expressions. Initially constants are all assigned the same length/probability. Later constants may be assigned a probability using the Huffman code based on the number of uses of the function id in all expressions recorded so far. In using a Huffman code the goal is to estimate probabilities, not to compress the data. The length of a function application is the length of the function identifier constant plus the sum of the sizes of the expressions for each parameter. The length of a quantifier is the length of the expression being quantified over. Distribution of numbers No explicit representation of natural numbers is given. However natural numbers may be constructed by applying the successor function to 0, and then applying other arithmetic functions. A distribution of natural numbers is implied by this, based on the complexity of constructing each number. Rational numbers are constructed by the division of natural numbers. The simplest representation has no common factors between the numerator and the denominator. This allows the probability distribution of natural numbers may be extended to rational numbers. Probability and frequency The probability of an event may be interpreted as the frequencies of outcomes where the statement is true divided by the total number of outcomes. If the outcomes form a continuum the frequency may need to be replaced with a measure. Events are sets of outcomes. Statements may be related to events. A Boolean statement B about outcomes defines a set of outcomes b, . Probabilities before an inference are known as prior probabilities, and probabilities after are known as posterior probabilities. Probability depends on the facts known. The truth of a fact limits the domain of outcomes to the outcomes consistent with the fact. Prior probabilities are the probabilities before a fact is known. Posterior probabilities are after a fact is known. The posterior probabilities are said to be conditional on the fact. the probability that B displaystyle B is true given that A displaystyle A is true is written as P ( B  A ) . displaystyle P(BA). All probabilities are in some sense conditional. The prior probability of B displaystyle B is, P ( B )  P ( B  ) displaystyle P(B)P(Btop ) The frequentist approach applied to possible worlds In the frequentist approach, probabilities are defined as the ratio of the number of outcomes within an event to the total number of outcomes. In the possible world model each possible world is an outcome, and statements about possible worlds define events. The probability of a statement being true is the number of possible worlds where the statement is true divided by the total number of possible worlds. The probability of a statement A displaystyle A being true about possible worlds is then, P ( A )    x  A ( x )    x   displaystyle P(A)frac xA(x)xtop  For a conditional probability. P ( B  A )    x  A ( x ) B ( x )    x  A ( x )  displaystyle P(BA)frac xA(x)land B(x)xA(x) then P ( A B )    x  A ( x ) B ( x )    x      x  A ( x ) B ( x )     x  A ( x )     x  A ( x )    x    P ( A ) P ( B  A ) displaystyle beginalignedP(Aland B)frac xA(x)land B(x)xtop 8ptfrac xA(x)land B(x)xA(x)frac xA(x)xtop 8ptP(BA)endaligned Using symmetry this equation may be written out as Bayes law. P ( A B )  P ( A ) P ( B  A )  P ( B ) P ( A  B ) displaystyle P(Aland B)P(BA)P(AB) This law describes the relationship between prior and posterior probabilities when new facts are learnt. Written as quantities of information Bayes Theorem becomes, L ( A B )  L ( A )  L ( B  A )  L ( B )  L ( A  B ) displaystyle L(Aland B)L(A)L(BA)L(B)L(AB) Two statements A and B are said to be independent if knowing the truth of A does not change the probability of B. Mathematically this is, P ( B )  P ( B  A ) displaystyle P(B)P(BA) then Bayes Theorem reduces to, P ( A B )  P ( A ) P ( B ) displaystyle P(Aland B)P(B) The law of total of probability For a set of mutually exclusive possibilities A i displaystyle A_i , the sum of the posterior probabilities must be 1. i P ( A i  B )  1 displaystyle sum _iP(A_iB)1 Substituting using Bayes theorem gives the law of total probability i P ( B  A i ) P ( A i )  i P ( A i  B ) P ( B ) displaystyle sum _iP(BA_i)P(A_i)sum _iP(A_iB)P(B) P ( B )  i P ( B  A i ) P ( A i ) displaystyle P(B)sum _iP(BA_i)P(A_i) This result is used to give the extended form of Bayes theorem, P ( A i  B )  P ( B  A i ) P ( A i ) j P ( B  A j ) P ( A j ) displaystyle P(A_iB)frac P(BA_i)P(A_i)sum _jP(BA_j)P(A_j) This is the usual form of Bayes theorem used in practice, because it guarantees the sum of all the posterior probabilities for A i displaystyle A_i is 1. Alternate possibilities For mutually exclusive possibilities, the probabilities add. P ( A B )  P ( A )  P ( B ) , if P ( A B )  0 displaystyle P(Alor B)P(A)P(B),qquad textif P(Aland B)0 Using A . Also, ( A ( A B ) ) ( A B )  A displaystyle (Aland neg (Aland B))lor (Aland B)A P ( A ( A B ) )  P ( A B )  P ( A ) displaystyle P(Aland neg (Aland B))P(Aland B)P(A) P ( A ( A B ) )  P ( A ) P ( A B ) displaystyle P(Aland neg (Aland B))P(A)-P(Aland B) so, putting it all together, P ( A B )  P ( ( A ( A B ) ) ( B ( A B ) ) ( A B ) )  P ( A ( A B )  P ( B ( A B ) )  P ( A B )  P ( A ) P ( A B )  P ( B ) P ( A B )  P ( A B )  P ( A )  P ( B ) P ( A B ) displaystyle beginalignedP(Alor B)P((Aland neg (Aland B))lor (Bland neg (Aland B))lor (Aland B))P(Aland neg (Aland B)P(Bland neg (Aland B))P(Aland B)P(A)-P(Aland B)P(B)-P(Aland B)P(Aland B)P(A)P(B)-P(Aland B)endaligned Negation As, A . The posterior probability of H is then P ( H  F )  P ( H ) P ( F  H ) P ( F ) displaystyle P(HF)frac P(FH)P(F) or in terms of information, P ( H  F )  2 ( L ( H )  L ( F  H ) L ( F ) ) displaystyle P(HF)2-(L(H)L(FH)-L(F)) By assuming the hypothesis is true, a simpler representation of the statement F may be given. The length of the encoding of this simpler representation is L ( F  H ) . displaystyle L(FH). L ( H )  L ( F  H ) displaystyle L(H)L(FH) represents the amount of information needed to represent the facts F, if H is true. L ( F ) displaystyle L(F) is the amount of information needed to represent F without the hypothesis H. The difference is how much the representation of the facts has been compressed by assuming that H is true. This is the evidence that the hypothesis H is true. If L ( F ) displaystyle L(F) is estimated from encoding length then the probability obtained will not be between 0 and 1. The value obtained is proportional to the probability, without being a good probability estimate. The number obtained is sometimes referred to as a relative probability, being how much more probable the theory is than not holding the theory. If a full set of mutually exclusive hypothesis that provide evidence is known, a proper estimate may be given for the prior probability P ( F ) displaystyle P(F) . Set of hypothesis Probabilities may be calculated from the extended form of Bayes theorem. Given all mutually exclusive hypothesis H i displaystyle H_i which give evidence, such that, L ( H i )  L ( F  H i )  L ( F ) displaystyle L(H_i)L(FH_i)L(F) and also the hypothesis R, that none of the hypothesis is true, then, P ( H i  F )  P ( H i ) P ( F  H i ) P ( F  R )  j P ( H j ) P ( F  H j ) P ( R  F )  P ( F  R ) P ( F  R )  j P ( H j ) P ( F  H j ) displaystyle beginalignedP(H_iF)frac P(H_i)P(FH_i)P(FR)sum _jP(H_j)P(FH_j)8ptP(RF)frac P(FR)P(FR)sum _jP(H_j)P(FH_j)endaligned In terms of information, P ( H i  F )  2 ( L ( H i )  L ( F  H i ) ) 2 L ( F  R )  j 2 ( L ( H j )  L ( F  H j ) ) P ( R  F )  2 L ( F  R ) 2 L ( F  R )  j 2 ( L ( H j )  L ( F  H j ) ) displaystyle beginalignedP(H_iF)frac 2-(L(H_i)L(FH_i))2-L(FR)sum _j2-(L(H_j)L(FH_j))8ptP(RF)frac 2-L(FR)2-L(FR)sum _j2-(L(H_j)L(FH_j))endaligned In most situations it is a good approximation to assume that F displaystyle F is independent of R displaystyle R , which means P ( F  R )  P ( F ) displaystyle P(FR)P(F) giving, P ( H i  F ) 2 ( L ( H i )  L ( F  H i ) ) 2 L ( F )  j 2 ( L ( H j )  L ( F  H j ) ) P ( R  F ) 2 L ( F ) 2 L ( F )  j 2 ( L ( H j )  L ( F  H j ) ) displaystyle beginalignedP(H_iF)approx frac 2-(L(H_i)L(FH_i))2-L(F)sum _j2-(L(H_j)L(FH_j))8ptP(RF)approx frac 2-L(F)2-L(F)sum _j2-(L(H_j)L(FH_j))endaligned Boolean inductive inference Abductive inference starts with a set of facts F which is a statement (Boolean expression). Abductive reasoning is of the form, A theory T implies the statement F. As the theory T is simpler than F, abduction says that there is a probability that the theory T is implied by F. The theory T, also called an explanation of the condition F, is an answer to the ubiquitous factual why question. For example, for the condition F is Why do apples fall?. The answer is a theory T that implies that apples fall . Therefore there is a probability that all objects in a class C have a property P. In terms of abductive inference, all objects in a class C or set have a property P is a theory that implies the observed condition, All observed objects in a class C have a property P. So inductive inference is a general case of abductive inference. In common usage the term inductive inference is often used to refer to both abductive and inductive inference. Generalization and specialization Inductive inference is related to generalization. Generalizations may be formed from statements by replacing a specific value with membership of a category, or by replacing membership of a category with membership of a broader category. In deductive logic, generalization is a powerful method of generating new theories that may be true. In inductive inference generalization generates theories that have a probability of being true. The opposite of generalization is specialization. Specialization is used in applying a general rule to a specific case. Specializations are created from generalizations by replacing membership of a category by a specific value, or by replacing a category with a sub category. The Linnaen classification of living things and objects forms the basis for generalization and specification. The ability to identify, recognize and classify is the basis for generalization. Perceiving the world as a collection of objects appears to be a key aspect of human intelligence. It is the object oriented model, in the non computer science sense. The object oriented model is constructed from our perception. In particularly vision is based on the ability to compare two images and calculate how much information is needed to morph or map one image into another. Computer vision uses this mapping to construct 3D images from stereo image pairs. Inductive logic programming is a means of constructing theory that implies a condition. Plotkins relative least general generalization (rlgg) approach constructs the simplest generalization consistent with the condition. Newtons use of induction Isaac Newton used inductive arguments in constructing his law of universal gravitation. Starting with the statement, The center of an apple falls towards the center of the Earth. Generalizing by replacing apple for object, and Earth for object gives, in a two body system, The center of an object falls towards the center of another object. The theory explains all objects falling, so there is strong evidence for it. The second observation, The planets appear to follow an elliptical path. After some complicated mathematical calculus, it can be seen that if the acceleration follows the inverse square law then objects will follow an ellipse. So induction gives evidence for the inverse square law. Using Galileos observation that all objects drop with the same speed, F 1  m 1 a 1  m 1 k 1 r 2 i 1 displaystyle F_1m_1a_1frac m_1k_1r2i_1 F 2  m 2 a 2  m 2 k 2 r 2 i 2 displaystyle F_2m_2a_2frac m_2k_2r2i_2 where i 1 displaystyle i_1 and i 2 displaystyle i_2 vectors towards the center of the other object. Then using Newtons third law F 1  F 2 displaystyle F_1-F_2 . For a single theory, . If L ( T i )  L ( F ) displaystyle L(T_i)L(F) then the theory has evidence to support it. Then for a set of theories T . The problem is to calculate the probability that the source is produced by program K i , displaystyle K_i, given that the truncated source after n bits is x. This is represented by the conditional probability, P (  ) ) . displaystyle P()). The extended form relies on the law of total probability. This means that the . Also one of the conditions . This must be true, as in the limit as n , displaystyle nto infty , there is always at least one program that produces T n ( s ) displaystyle T_n(s) . As K i displaystyle K_i are chosen so that T n ( R ( K i ) )  x , displaystyle T_n(R(K_i))x, then, P ( T n ( s )  x   ) . displaystyle P(). Programs that are the same or longer than the length of x provide no predictive power. Separate them out giving, P (  ) . displaystyle P(). Then identify the two probabilities as, P ( x has pattern )  j  I ( K j )  n 2 I ( K j ) displaystyle P(xtext has pattern)sum _jI(K_j)n2-I(K_j) P ( x is random )  j  I ( K j ) n 2 I ( K j ) displaystyle P(xtext is random)sum _jI(K_j)geqslant n2-I(K_j) But the prior probability that x is a random set of bits is 2 n displaystyle 2-n . So, P (  ) . displaystyle P(). The probability that the source is random, or unpredictable is, P ( random ( s )  T n ( s )  x )  2 n 2 n  j  I ( K j )  n 2 I ( K j ) . displaystyle P(operatorname random (s)T_n(s)x)frac 2-n2-nsum _jI(K_j)n2-I(K_j). A model for inductive inference A model of how worlds are constructed is used in determining the probabilities of theories, A random bit string is selected. A condition is constructed from the bit string. A world is constructed that is consistent with the condition. If w is the bit string then the world is created such that R ( w ) displaystyle R(w) is true. An intelligent agent has some facts about the word, represented by the bit string c, which gives the condition, ) . x , E ( x )   w  R ( w ) x  displaystyle forall x,E(x)wR(w)equiv x A theory is a simpler condition that explains (or implies) C. The set of all such theories is called T, T ( C )   t  t C  displaystyle T(C)ttto C Applying Bayes theorem extended form of Bayes theorem may be applied P ( A i  B )  P ( B  A i ) P ( A i ) j P ( B  A j ) P ( A j ) , displaystyle P(A_iB)frac P(BA_i),P(A_i)sum _jP(BA_j),P(A_j), where, . For T ( C ) displaystyle T(C) to be a partition, no bit string n may belong to two theories. To prove this assume they can and derive a contradiction, ( N T ) ( N M ) ( N M ) ( n E ( N ) n E ( M ) ) displaystyle (Nin T)land (Nin M)land (Nneq M)land (nin E(N)land nin E(M)) ( N M ) R ( n ) N R ( n ) M displaystyle implies (Nneq M)land R(n)equiv Nland R(n)equiv M displaystyle implies bot  Secondly prove that T includes all outcomes consistent with the condition. As all theories consistent with C are included then R ( w ) displaystyle R(w) must be in this set. So Bayes theorem may be applied as specified giving, t T ( C ) , P ( E ( t )  E ( C ) )  P ( E ( t ) ) P ( E ( C )  E ( t ) ) j T ( C ) P ( E ( j ) ) P ( E ( C )  E ( j ) ) displaystyle forall tin T(C),P(E(t)E(C))frac P(E(t))cdot P(E(C)E(t))sum _jin T(C)P(E(j))cdot P(E(C)E(j)) Using the implication and condition probability law, the definition of T ( C ) displaystyle T(C) implies, t T ( C ) , P ( E ( C )  E ( t ) )  1 displaystyle forall tin T(C),P(E(C)E(t))1 The probability of each theory in T is given by, t T ( C ) , P ( E ( t ) )  n  R ( n ) t 2 L ( n ) displaystyle forall tin T(C),P(E(t))sum _nR(n)equiv t2-L(n) so, t T ( C ) , P ( E ( t )  E ( C ) )  n  R ( n ) t 2 L ( n ) j T ( C ) m  R ( m ) j 2 L ( m ) displaystyle forall tin T(C),P(E(t)E(C))frac sum _nR(n)equiv t2-L(n)sum _jin T(C)sum _mR(m)equiv j2-L(m) Finally the probabilities of the events may be identified with the probabilities of the condition which the outcomes in the event satisfy, t T ( C ) , P ( E ( t )  E ( C ) )  P ( t  C ) displaystyle forall tin T(C),P(E(t)E(C))P(tC) giving t T ( C ) , P ( t  C )  n  R ( n ) t 2 L ( n ) j T ( C ) m  R ( m ) j 2 L ( m ) displaystyle forall tin T(C),P(tC)frac sum _nR(n)equiv t2-L(n)sum _jin T(C)sum _mR(m)equiv j2-L(m) This is the probability of the theory t after observing that the condition C holds. Removing theories without predictive power Theories that are less probable than the condition C have no predictive power. Separate them out giving, t T ( C ) , P ( t  C )  P ( E ( t ) ) ( j  j T ( C ) P ( E ( j ) )  P ( E ( C ) ) P ( E ( j ) ) )  ( j  j T ( C ) P ( E ( j ) ) P ( E ( C ) ) P ( j ) ) displaystyle forall tin T(C),P(tC)frac P(E(t))(sum _jjin T(C)land P(E(j))P(E(C))P(E(j)))(sum _jjin T(C)land P(E(j))leq P(E(C))P(j)) The probability of the theories without predictive power on C is the same as the probability of C. So, P ( E ( C ) )  j  j T ( C ) P ( E ( j ) ) P ( E ( C ) ) P ( j ) displaystyle P(E(C))sum _jjin T(C)land P(E(j))leq P(E(C))P(j) So the probability t T ( C ) , P ( t  C )  P ( E ( t ) ) P ( E ( C ) )  j  j T ( C ) P ( E ( j ) )  P ( E ( C ) ) P ( E ( j ) ) displaystyle forall tin T(C),P(tC)frac P(E(t))P(E(C))sum _jjin T(C)land P(E(j))P(E(C))P(E(j)) and the probability of no prediction for C, written as random ( C ) displaystyle operatorname random (C) , P ( random ( C )  C )  P ( E ( C ) ) P ( E ( C ) )  j  j T ( C ) P ( E ( j ) )  P ( E ( C ) ) P ( E ( j ) ) displaystyle P(textrandom(C)C)frac P(E(C))P(E(C))sum _jjin T(C)land P(E(j))P(E(C))P(E(j)) The probability of a condition was given as, t , P ( E ( t ) )  n  R ( n ) t 2 L ( n ) displaystyle forall t,P(E(t))sum _nR(n)equiv t2-L(n) Bit strings for theories that are more complex than the bit string given to the agent as input have no predictive power. There probabilities are better included in the random case. To implement this a new definition is given as F in, t , P ( F ( t , c ) )  n  R ( n ) t L ( n )  L ( c ) 2 L ( n ) displaystyle forall t,P(F(t,c))sum _nR(n)equiv tland L(n)L(c)2-L(n) Using F, an improved version of the abductive probabilities is, t T ( C ) , P ( t  C )  P ( F ( t , c ) ) P ( F ( C , c ) )  j  j T ( C ) P ( F ( j , c ) )  P ( F ( C , c ) ) P ( E ( j , c ) ) displaystyle forall tin T(C),P(tC)frac P(F(t,c))P(F(C,c))sum _jjin T(C)land P(F(j,c))P(F(C,c))P(E(j,c)) P ( random ( C )  C )  P ( F ( C , c ) ) P ( F ( C , c ) )  j  j T ( C ) P ( F ( j , c ) )  P ( F ( C , c ) ) P ( F ( j , c ) ) displaystyle P(operatorname random (C)C)frac P(F(C,c))P(F(C,c))sum _jjin T(C)land P(F(j,c))P(F(C,c))P(F(j,c)) Key people William of Ockham Thomas Bayes Ray Solomonoff Andrey Kolmogorov Chris Wallace D. M. Boulton Jorma Rissanen Marcus Hutter See also Abductive reasoning Algorithmic probability Algorithmic information theory Bayesian inference Information theory Inductive inference Inductive logic programming Inductive reasoning Learning Minimum message length Minimum description length Occams razor Solomonoffs theory of inductive inference Universal artificial intelligence References External links Rathmanner, S and Hutter, M., A Philosophical Treatise of Universal Induction in Entropy 2011, 13, 1076 1136 A very clear philosophical and mathematical analysis of Solomonoffs Theory of Inductive Inference. C.S. Wallace, Statistical and Inductive Inference by Minimum Message Length, Springer-Verlag (Information Science and Statistics), ISBN 0-387-23795-X, May 2005 chapter headings, table of contents and sample pages. Title Inductive programming URL https//en.wikipedia.org/wiki/Inductive_programming Content Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints. Depending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming. Definition Inductive programming incorporates all approaches which are concerned with learning programs or algorithms from incomplete (formal) specifications. Possible inputs in an IP system are a set of training inputs and corresponding outputs or an output evaluation function, describing the desired behavior of the intended program, traces or action sequences which describe the process of calculating specific outputs, constraints for the program to be induced concerning its time efficiency or its complexity, various kinds of background knowledge such as standard data types, predefined functions to be used, program schemes or templates describing the data flow of the intended program, heuristics for guiding the search for a solution or other biases. Output of an IP system is a program in some arbitrary programming language containing conditionals and loop or recursive control structures, or any other kind of Turing-complete representation language. In many applications the output program must be correct with respect to the examples and partial specification, and this leads to the consideration of inductive programming as a special area inside automatic programming or program synthesis, usually opposed to deductive program synthesis, where the specification is usually complete. In other cases, inductive programming is seen as a more general area where any declarative programming or representation language can be used and we may even have some degree of error in the examples, as in general machine learning, the more specific area of structure mining or the area of symbolic artificial intelligence. A distinctive feature is the number of examples or partial specification needed. Typically, inductive programming techniques can learn from just a few examples. The diversity of inductive programming usually comes from the applications and the languages that are used apart from logic programming and functional programming, other programming paradigms and representation languages have been used or suggested in inductive programming, such as functional logic programming, constraint programming, probabilistic programming, abductive logic programming, modal logic, action languages, agent languages and many types of imperative languages. History Research on the inductive synthesis of recursive functional programs started in the early 1970s and was brought onto firm theoretical foundations with the seminal THESIS system of Summers and work of Biermann. These approaches were split into two phases first, input-output examples are transformed into non-recursive programs (traces) using a small set of basic operators second, regularities in the traces are searched for and used to fold them into a recursive program. The main results until the mid-1980s are surveyed by Smith. Due to limited progress with respect to the range of programs that could be synthesized, research activities decreased significantly in the next decade. The advent of logic programming brought a new elan but also a new direction in the early 1980s, especially due to the MIS system of Shapiro eventually spawning the new field of inductive logic programming (ILP). The early works of Plotkin, and his relative least general generalization (rlgg), had an enormous impact in inductive logic programming. Most of ILP work addresses a wider class of problems, as the focus is not only on recursive logic programs but on machine learning of symbolic hypotheses from logical representations. However, there were some encouraging results on learning recursive Prolog programs such as quicksort from examples together with suitable background knowledge, for example with GOLEM. But again, after initial success, the community got disappointed by limited progress about the induction of recursive programs with ILP less and less focusing on recursive programs and leaning more and more towards a machine learning setting with applications in relational data mining and knowledge discovery. In parallel to work in ILP, Koza proposed genetic programming in the early 1990s as a generate-and-test based approach to learning programs. The idea of genetic programming was further developed into the inductive programming system ADATE and the systematic-search-based system MagicHaskeller. Here again, functional programs are learned from sets of positive examples together with an output evaluation (fitness) function which specifies the desired input/output behavior of the program to be learned. The early work in grammar induction (also known as grammatical inference) is related to inductive programming, as rewriting systems or logic programs can be used to represent production rules. In fact, early works in inductive inference considered grammar induction and Lisp program inference as basically the same problem. The results in terms of learnability were related to classical concepts, such as identification-in-the-limit, as introduced in the seminal work of Gold. More recently, the language learning problem was addressed by the inductive programming community. In the recent years, the classical approaches have been resumed and advanced with great success. Therefore, the synthesis problem has been reformulated on the background of constructor-based term rewriting systems taking into account modern techniques of functional programming, as well as moderate use of search-based strategies and usage of background knowledge as well as automatic invention of subprograms. Many new and successful applications have recently appeared beyond program synthesis, most especially in the area of data manipulation, programming by example and cognitive modelling (see below). Other ideas have also been explored with the common characteristic of using declarative languages for the representation of hypotheses. For instance, the use of higher-order features, schemes or structured distances have been advocated for a better handling of recursive data types and structures abstraction has also been explored as a more powerful approach to cumulative learning and function invention. One powerful paradigm that has been recently used for the representation of hypotheses in inductive programming (generally in the form of generative models) is probabilistic programming (and related paradigms, such as stochastic logic programs and Bayesian logic programming). Application areas The first workshop on Approaches and Applications of Inductive Programming (AAIP) held in conjunction with ICML 2005 identified all applications where learning of programs or recursive rules are called for, ... first in the domain of software engineering where structural learning, software assistants and software agents can help to relieve programmers from routine tasks, give programming support for end users, or support of novice programmers and programming tutor systems. Further areas of application are language learning, learning recursive control rules for AI-planning, learning recursive concepts in web-mining or for data-format transformations. Since then, these and many other areas have shown to be successful application niches for inductive programming, such as end-user programming, the related areas of programming by example and programming by demonstration, and intelligent tutoring systems. Other areas where inductive inference has been recently applied are knowledge acquisition, artificial general intelligence, reinforcement learning and theory evaluation, and cognitive science in general. There may also be prospective applications in intelligent agents, games, robotics, personalisation, ambient intelligence and human interfaces. See also Evolutionary programming Inductive reasoning Test-driven development References Further reading External links Inductive Programming community page, hosted by the University of Bamberg. Title Inferential theory of learning URL https//en.wikipedia.org/wiki/Inferential_theory_of_learning Content Inferential Theory of Learning (ITL) is an area of machine learning which describes inferential processes performed by learning agents. ITL has been continuously developed by Ryszard S. Michalski, starting in the 1980s. The first known publication of ITL was in 1983. In the ITL learning process is viewed as a search (inference) through hypotheses space guided by a specific goal. The results of learning need to be stored. Stored information will later be used by the learner for future inferences. Inferences are split into multiple categories including conclusive, deduction, and induction. In order for an inference to be considered complete it was required that all categories must be taken into account. This is how the ITL varies from other machine learning theories like Computational Learning Theory and Statistical Learning Theory which both use singular forms of inference. Usage The most relevant published usage of ITL was in scientific journal published in 2012 and used ITL as a way to describe how agent-based learning works. According to the journal The Inferential Theory of Learning (ITL) provides an elegant way of describing learning processes by agents. References Title Instance selection URL https//en.wikipedia.org/wiki/Instance_selection Content Instance selection (or dataset reduction, or dataset condensation) is an important data pre-processing step that can be applied in many machine learning (or data mining) tasks. Approaches for instance selection can be applied for reducing the original dataset to a manageable volume, leading to a reduction of the computational resources that are necessary for performing the learning process. Algorithms of instance selection can also be applied for removing noisy instances, before applying learning algorithms. This step can improve the accuracy in classification problems. Algorithm for instance selection should identify a subset of the total available data to achieve the original purpose of the data mining (or machine learning) application as if the whole data had been used. Considering this, the optimal outcome of IS would be the minimum data subset that can accomplish the same task with no performance loss, in comparison with the performance achieved when the task is performed using the whole available data. Therefore, every instance selection strategy should deal with a trade-off between the reduction rate of the dataset and the classification quality. Instance selection algorithms The literature provides several different algorithms for instance selection. They can be distinguished from each other according to several different criteria. Considering this, instance selection algorithms can be grouped in two main classes, according to what instances they select algorithms that preserve the instances at the boundaries of classes and algorithms that preserve the internal instances of the classes. Within the category of algorithms that select instances at the boundaries it is possible to cite , ICF and LSBo. On the other hand, within the category of algorithms that select internal instances, it is possible to mention ENN and LSSm. In general, algorithm such as ENN and LSSm are used for removing harmful (noisy) instances from the dataset. They do not reduce the data as the algorithms that select border instances, but they remove instances at the boundaries that have a negative impact on the data mining task. They can be used by other instance selection algorithms, as a filtering step. For example, the ENN algorithm is used by as the first step, and the LSSm algorithm is used by LSBo. There is also another group of algorithms that adopt different selection criteria. For example, the algorithms LDIS, CDIS and XLDIS select the densest instances in a given arbitrary neighborhood. The selected instances can include both, border and internal instances. The LDIS and CDIS algorithms are very simple and select subsets that are very representative of the original dataset. Besides that, since they search by the representative instances in each class separately, they are faster (in terms of time complexity and effective running time) than other algorithms, such as and ICF. Besides that, there is a third category of algorithms that, instead of selecting actual instances of the dataset, select prototypes (that can be synthetic instances). In this category it is possible to include PSSA, PSDSP and PSSP. The three algorithms adopt the notion of spatial partition (a hyperrectangle) for identifying similar instances and extract prototypes for each set of similar instances. In general, these approaches can also be modified for selecting actual instances of the datasets. The algorithm ISDSP adopts a similar approach for selecting actual instances (instead of prototypes). Title Instance-based learning URL https//en.wikipedia.org/wiki/Instance-based_learning Content In machine learning, instance-based learning (sometimes called memory-based learning) is a family of learning algorithms that, instead of performing explicit generalization, compare new problem instances with instances seen in training, which have been stored in memory. Because computation is postponed until a new instance is observed, these algorithms are sometimes referred to as lazy. It is called instance-based because it constructs hypotheses directly from the training instances themselves. This means that the hypothesis complexity can grow with the data in the worst case, a hypothesis is a list of n training items and the computational complexity of classifying a single new instance is O(n). One advantage that instance-based learning has over other methods of machine learning is its ability to adapt its model to previously unseen data. Instance-based learners may simply store a new instance or throw an old instance away. Examples of instance-based learning algorithms are the k-nearest neighbors algorithm, kernel machines and RBF networks. ch. 8 These store (a subset of) their training set when predicting a value/class for a new instance, they compute distances or similarities between this instance and the training instances to make a decision. To battle the memory complexity of storing all training instances, as well as the risk of overfitting to noise in the training set, instance reduction algorithms have been proposed. See also Analogical modeling Title Intelligent automation URL https//en.wikipedia.org/wiki/Intelligent_automation Content Intelligent automation (IA), or intelligent process automation, is a software term that refers to a combination of artificial intelligence (AI) and robotic process automation (RPA). Companies use intelligent automation to cut costs and streamline tasks by using artificial-intelligence-powered robotic software to mitigate repetitive tasks. As it accumulates data, the system learns in an effort to improve its efficiency. Intelligent automation applications consist of but are not limited to, pattern analysis, data assembly, and classification. The term is similar to hyperautomation, a concept identified by research group Gartner as being one of the top technology trends of 2020. Technology Intelligent automation applies the assembly line concept of breaking tasks into repetitive steps to improve business processes. Rather than having humans do each step, intelligent automation can replace steps with an intelligent software robot or bot, improving efficiency. Applications The technology is used to process unstructured content. Common real-world applications include self-driving cars, self-checkouts at grocery stores, smart home assistants, and appliances. Businesses can apply data and machine learning to build predictive analytics that react to consumer behavior changes, or to implement RPA to improve manufacturing floor operations. For example, the technology has also been used to automate the workflow behind distributing Covid-19 vaccines. Data provided by hospital systems electronic health records can be processed to identify and educate patients, and schedule vaccinations. Intelligent Automation can provide real-time insights on profitability and efficiency. However in an April 2022 survey by Alchemmy, despite three quarters of businesses acknowledging the importance of Artificial Intelligence to their future development, just a quarter of business leaders (25) considered Intelligent Automation a game changer in understanding current performance. 42 of CTOs see shortage of talent as the main obstacle to implementing Intelligent Automation in their business, while 36 of CEOs see upskilling and professional development of existing workforce as the most significant adoption barrier. IA is becoming increasingly accessible for firms of all sizes. With this in mind, it is expected to continue to grow rapidly in all industries. This technology has the potential to change the workforce. As it advances, it will be able to perform increasingly complex and difficult tasks. In addition, this may expose certain workforce issues as well as change how tasks are allocated. Benefits Streamline Processes Repetitive manual tasks can put a strain on the workforce, these tasks can be automated to allow the workforce to work on more important matters that require human cognition. Intelligent automation can also be used to mitigate tasks with human error which in turn increases proficiency. This allows the opportunity for firms to scale production without the traditional negative consequences such as reduced quality or increased risk. Customer Service Improvement Customers service can be improved drastically, this allows for a competitive advantage for the firm. IA utilizing chat features allows for instant curated responses to customers. In addition, it can give updates to customers, make appointments, manage calls, and personalize campaigns. Flexibility Due to the wide range of applications, IA is useful across a variety of fields, technologies, projects and industries. In addition, IA can be integrated with current automated systems in place. This allows for optimized systems unique to each firm to best fit their individual needs. Capabilities Cognitive automation Employs AI techniques to assist humans in decision-making and task completion Natural language processing Allows computers to automate knowledge work Business process management Enhances the consistency and agility of corporate operations Process mining Applies data mining methods to discover, analyze, and improve business processes Intelligent document processing Utilizes OCR and other advanced technologies to extract data from documents and convert it into structured, usable data Computer vision Allows computers to extract information from digital images, videos, and other visual inputs Integration automation Establishes a unified platform with automated workflows that integrate data, applications, and devices. See also Robotic process automation Artificial intelligence Automation Title Isotropic position URL https//en.wikipedia.org/wiki/Isotropic_position Content In the fields of machine learning, the theory of computation, and random matrix theory, a probability distribution over vectors is said to be in isotropic position if its covariance matrix is equal to the identity matrix. Formal definitions Let D textstyle D be a distribution over vectors in the vector space R n textstyle mathbb R n . Then D textstyle D is in isotropic position if, for vector v textstyle v sampled from the distribution, E v v  . displaystyle mathbb E ,vvmathsf Tmathrm Id . A set of vectors is said to be in isotropic position if the uniform distribution over that set is in isotropic position. In particular, every orthonormal set of vectors is isotropic. As a related definition, a convex body K textstyle K in R n textstyle mathbb R n is called isotropic if it has volume  K   1 textstyle K1 , center of mass at the origin, and there is a constant  0 textstyle alpha 0 such that K x , y 2 d . See also Whitening transformation References Rudelson, M. (1999). Random Vectors in the Isotropic Position. Journal of Functional Analysis. 164 (1) 60 72. arXivmath/9608208. doi10.1006/jfan.1998.3384. S2CID 7652247. Title JAX (software) URL https//en.wikipedia.org/wiki/JAX_(software) Content JAX is a Python library that provides a machine learning framework for transforming numerical functions developed by Google with some contributions from Nvidia. It is described as bringing together a modified version of autograd (automatic obtaining of the gradient function through differentiation of a function) and OpenXLAs XLA (Accelerated Linear Algebra). It is designed to follow the structure and workflow of NumPy as closely as possible and works with various existing frameworks such as TensorFlow and PyTorch. The primary functions of JAX are grad automatic differentiation jit compilation vmap auto-vectorization pmap Single program, multiple data (SPMD) programming grad The below code demonstrates the grad functions automatic differentiation. The final line should output jit The below code demonstrates the jit functions optimization through fusion. The computation time for jit_cube (line 17) should be noticeably shorter than that for cube (line 16). Increasing the values on line 7, will further exacerbate the difference. vmap The below code demonstrates the vmap functions vectorization. The GIF on the right of this section illustrates the notion of vectorized addition. pmap The below code demonstrates the pmap functions parallelization for matrix multiplication. The final line should print the values See also NumPy TensorFlow PyTorch CUDA External links Documentation jax.readthedocs.io Colab (Jupyter/iPython) Quickstart Guide colab.research.google.com/github/google/jax/blob/main/docs/notebooks/quickstart.ipynb TensorFlows XLA www.tensorflow.org/xla (Accelerated Linear Algebra) YouTube TensorFlow Channel Intro to JAX Accelerating Machine Learning research www.youtube.com/watch?.org/Conferences/doc/2018/146.pdf Title Journal of Machine Learning Research URL https//en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research Content The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling. The current editors-in-chief are Francis Bach (Inria) and David Blei (Columbia University). History The journal was established as an open-access alternative to the journal Machine Learning. In 2001, forty editorial board members of Machine Learning resigned, saying that in the era of the Internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. The open access model employed by the Journal of Machine Learning Research allows authors to publish articles for free and retain copyright, while archives are freely available online. Print editions of the journal were published by MIT Press until 2004 and by Microtome Publishing thereafter. From its inception, the journal received no revenue from the print edition and paid no subvention to MIT Press or Microtome Publishing. In response to the prohibitive costs of arranging workshop and conference proceedings publication with traditional academic publishing companies, the journal launched a proceedings publication arm in 2007 and now publishes proceedings for several leading machine learning conferences, including the International Conference on Machine Learning, COLT, AISTATS, and workshops held at the Conference on Neural Information Processing Systems. References Further reading Top journals in computer science. Times Higher Education. 14 May 2009. Retrieved 22 August 2009. External links Official website Title Kernel density estimation URL https//en.wikipedia.org/wiki/Kernel_density_estimation Content In statistics, kernel density estimation (KDE) is the application of kernel smoothing for probability density estimation, i.e., a non-parametric method to estimate the probability density function of a random variable based on kernels as weights. KDE answers a fundamental data smoothing problem where inferences about the population are made based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzen Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form. One of the famous applications of kernel density estimation is in estimating the class-conditional marginal densities of data when using a naive Bayes classifier, which can improve its prediction accuracy. Definition Let (, , ..., xn) be independent and identically distributed samples drawn from some univariate distribution with an unknown density f at any given point x. We are interested in estimating the shape of this function f. Its kernel density estimator is f  h ( x )  1 n . A kernel with subscript h is called the scaled kernel and defined as Kh(x)  1/h K( x/h ). Intuitively one wants to choose h as small as the data will allow however, there is always a trade-off between the bias of the estimator and its variance. The choice of bandwidth is discussed in more detail below. A range of kernel functions are commonly used uniform, triangular, biweight, triweight, Epanechnikov (parabolic), normal, and others. The Epanechnikov kernel is optimal in a mean square error sense, though the loss of efficiency is small for the kernels listed previously. Due to its convenient mathematical properties, the normal kernel is often used, which means K(x)  (x), where is the standard normal density function. The kernel density estimator then becomes f  h ( x )  1 n h 1 2  . The construction of a kernel density estimate finds interpretations in fields outside of density estimation. For example, in thermodynamics, this is equivalent to the amount of heat generated when heat kernels (the fundamental solution to the heat equation) are placed at each data point locations xi. Similar methods are used to construct discrete Laplace operators on point clouds for manifold learning (e.g. diffusion map). Example Kernel density estimates are closely related to histograms, but can be endowed with properties such as smoothness or continuity by using a suitable kernel. The diagram below based on these 6 data points illustrates this relationship For the histogram, first, the horizontal axis is divided into sub-intervals or bins which cover the range of the data In this case, six bins each of width 2. Whenever a data point falls inside this interval, a box of height 1/12 is placed there. If more than one data point falls inside the same bin, the boxes are stacked on top of each other. For the kernel density estimate, normal kernels with a standard deviation of 1.5 (indicated by the red dashed lines) are placed on each of the data points xi. The kernels are summed to make the kernel density estimate (solid blue curve). The smoothness of the kernel density estimate (compared to the discreteness of the histogram) illustrates how kernel density estimates converge faster to the true underlying density for continuous random variables. Bandwidth selection The bandwidth of the kernel is a free parameter which exhibits a strong influence on the resulting estimate. To illustrate its effect, we take a simulated random sample from the standard normal distribution (plotted at the blue spikes in the rug plot on the horizontal axis). The grey curve is the true density (a normal density with mean 0 and variance 1). In comparison, the red curve is undersmoothed since it contains too many spurious data artifacts arising from using a bandwidth .05, which is too small. The green curve is oversmoothed since using the bandwidth . The black curve with a bandwidth of .337 is considered to be optimally smoothed since its density estimate is close to the true density. An extreme situation is encountered in the limit h 0 displaystyle hto 0 (no smoothing), where the estimate is a sum of n delta functions centered at the coordinates of analyzed samples. In the other extreme limit h displaystyle hto infty  the estimate retains the shape of the used kernel, centered on the mean of the samples (completely smooth). The most common optimality criterion used to select this parameter is the expected risk function, also termed the mean integrated squared error MISE ( h )  E  ( f  h ( x ) f ( x ) ) 2 d x  displaystyle operatorname MISE (h)operatorname E !leftint !left(hat f!_h(x)-f(x)right)2dxright Under weak assumptions on f and K, (f is the, generally unknown, real density function), MISE ( h )  AMISE ( h )  o ( ( n h ) 1  h 4 ) displaystyle operatorname MISE (h)operatorname AMISE (h)mathcal oleft((nh)-1h4right) where o is the little o notation, and n the sample size (as above). The AMISE is the asymptotic MISE, i. e. the two leading terms, AMISE ( h )  R ( K ) n h  1 4 m 2 ( K ) 2 h 4 R ( f ) displaystyle operatorname AMISE (h)frac R(K)nhfrac 14m_2(K)2h4R(f) where R ( g )  g ( x ) 2 d x textstyle R(g)int g(x)2,dx for a function g, m 2 ( K )  x 2 K ( x ) d x textstyle m_2(K)int x2K(x),dx and f displaystyle f is the second derivative of f displaystyle f and K displaystyle K is the kernel. The minimum of this AMISE is the solution to this differential equation h AMISE ( h )  R ( K ) n h 2  m 2 ( K ) 2 h 3 R ( f )  0 displaystyle frac partial partial hoperatorname AMISE (h)-frac R(K)nh2m_2(K)2h3R(f)0 or h  . To overcome that difficulty, a variety of automatic, data-based methods have been developed to select the bandwidth. Several review studies have been undertaken to compare their efficacies, with the general consensus that the plug-in selectors and cross validation selectors are the most useful over a wide range of data sets. Substituting any bandwidth h which has the same asymptotic order n 1/5 as hAMISE into the AMISE gives that AMISE(h)  O(n 4/5), where O is the big O notation. It can be shown that, under weak assumptions, there cannot exist a non-parametric estimator that converges at a faster rate than the kernel estimator. Note that the n 4/5 rate is slower than the typical n 1 convergence rate of parametric methods. If the bandwidth is not held fixed, but is varied depending upon the location of either the estimate (balloon estimator) or the samples (pointwise estimator), this produces a particularly powerful method termed adaptive or variable bandwidth kernel density estimation. Bandwidth selection for kernel density estimation of heavy-tailed distributions is relatively difficult. A rule-of-thumb bandwidth estimator If Gaussian basis functions are used to approximate univariate data, and the underlying density being estimated is Gaussian, the optimal choice for h (that is, the bandwidth that minimises the mean integrated squared error) is .06  n 1 / 5 , displaystyle .06,hat sigma ,n-1/5, An h displaystyle h value is considered more robust when it improves the fit for long-tailed and skewed distributions or for bimodal mixture distributions. This is often done empirically by replacing the standard deviation  displaystyle hat sigma  by the parameter A displaystyle A below .34 ) displaystyle .34right) where IQR is the interquartile range. Another modification that will improve the model is to reduce the factor from 1.06 to 0.9. Then the final formula would be .9 min (  , I Q R 1.34 ) n 1 / 5 displaystyle .9,min left(hat sigma ,frac mathrm IQR 1.34right),n-1/5 where n displaystyle n is the sample size. This approximation is termed the normal distribution approximation, Gaussian approximation, or Silvermans rule of thumb. While this rule of thumb is easy to compute, it should be used with caution as it can yield widely inaccurate estimates when the density is not close to being normal. For example, when estimating the bimodal Gaussian mixture model 1 2 2 e 1 2 ( x 10 ) 2  1 2 2 e 1 2 ( x  10 ) 2 displaystyle frac 12sqrt 2pi e-frac 12(x-10)2frac 12sqrt 2pi e-frac 12(x10)2 from a sample of 200 points, the figure on the right shows the true density and two kernel density estimates one using the rule-of-thumb bandwidth, and the other using a solve-the-equation bandwidth. The estimate based on the rule-of-thumb bandwidth is significantly oversmoothed. Relation to the characteristic function density estimator Given the sample (, , ..., xn), it is natural to estimate the characteristic function (t)  EeitX as  ( t )  1 n . One difficulty with applying this inversion formula is that it leads to a diverging integral, since the estimate  ( t ) displaystyle hat varphi (t) is unreliable for large ts. To circumvent this problem, the estimator  ( t ) displaystyle hat varphi (t) is multiplied by a damping function h(t)  (ht), which is equal to 1 at the origin and then falls to 0 at infinity. The bandwidth parameter h controls how fast we try to dampen the function  ( t ) displaystyle hat varphi (t) . In particular when h is small, then h(t) will be approximately one for a large range of ts, which means that  ( t ) displaystyle hat varphi (t) remains practically unaltered in the most important region of ts. The most common choice for function is either the uniform function (t)  1 1 t 1, which effectively means truncating the interval of integration in the inversion formula to  1/h, 1/h, or the Gaussian function (t)  e . Once the function has been chosen, the inversion formula may be applied, and the density estimator will be f  ( x )  1 2   ( t ) h ( t ) e i t x d  . Thus the kernel density estimator coincides with the characteristic function density estimator. Geometric and topological features We can extend the definition of the (global) mode to a local sense and define the local modes . A natural estimator of M displaystyle M is a plug-in from KDE, where g ( x ) displaystyle g(x) and 1 ( x ) displaystyle lambda _1(x) are KDE version of g ( x ) displaystyle g(x) and 1 ( x ) displaystyle lambda _1(x) . Under mild assumptions, M c displaystyle M_c is a consistent estimator of M displaystyle M . Note that one can use the mean shift algorithm to compute the estimator M c displaystyle M_c numerically. Statistical implementation A non-exhaustive list of software implementations of kernel density estimators includes In Analytica release 4.4, the Smoothing option for PDF results uses KDE, and from expressions it is available via the built-in Pdf function. In C/C, FIGTree is a library that can be used to compute kernel density estimates using normal kernels. MATLAB interface available. In C, libagf is a library for variable kernel density estimation. In C, mlpack is a library that can compute KDE using many different kernels. It allows to set an error tolerance for faster computation. Python and R interfaces are available. in C and F, Math.NET Numerics is an open source library for numerical computation which includes kernel density estimation In CrimeStat, kernel density estimation is implemented using five different kernel functions normal, uniform, quartic, negative exponential, and triangular. Both single- and dual-kernel density estimate routines are available. Kernel density estimation is also used in interpolating a Head Bang routine, in estimating a two-dimensional Journey-to-crime density function, and in estimating a three-dimensional Bayesian Journey-to-crime estimate. In ELKI, kernel density functions can be found in the package de.lmu.ifi.dbs.elki.math.statistics.kernelfunctions In ESRI products, kernel density mapping is managed out of the Spatial Analyst toolbox and uses the Quartic(biweight) kernel. In Excel, the Royal Society of Chemistry has created an add-in to run kernel density estimation based on their Analytical Methods Committee Technical Brief 4. In gnuplot, kernel density estimation is implemented by the smooth kdensity option, the datafile can contain a weight and bandwidth for each point, or the bandwidth can be set automatically according to Silvermans rule of thumb (see above). In Haskell, kernel density is implemented in the statistics package. In IGOR Pro, kernel density estimation is implemented by the StatsKDE operation (added in Igor Pro 7.00). Bandwidth can be user specified or estimated by means of Silverman, Scott or Bowmann and Azzalini. Kernel types are Epanechnikov, Bi-weight, Tri-weight, Triangular, Gaussian and Rectangular. In Java, the Weka machine learning package provides weka.estimators.KernelEstimator, among others. In JavaScript, the visualization package .js offers a KDE package in its science.stats package. In JMP, the Graph Builder platform utilizes kernel density estimation to provide contour plots and high density regions (HDRs) for bivariate densities, and violin plots and HDRs for univariate densities. Sliders allow the user to vary the bandwidth. Bivariate and univariate kernel density estimates are also provided by the Fit Y by X and Distribution platforms, respectively. In Julia, kernel density estimation is implemented in the KernelDensity.jl package. In KNIME, 1D and 2D Kernel Density distributions can be generated and plotted using nodes from the Vernalis community contribution, e.g. 1D Kernel Density Plot, among others. The underlying implementation is written in Java. In MATLAB, kernel density estimation is implemented through the ksdensity function (Statistics Toolbox). As of the 2018a release of MATLAB, both the bandwidth and kernel smoother can be specified, including other options such as specifying the range of the kernel density. Alternatively, a free MATLAB software package which implements an automatic bandwidth selection method is available from the MATLAB Central File Exchange for 1-dimensional data 2-dimensional data n-dimensional data A free MATLAB toolbox with implementation of kernel regression, kernel density estimation, kernel estimation of hazard function and many others is available on these pages (this toolbox is a part of the book ). In Mathematica, numeric kernel density estimation is implemented by the function SmoothKernelDistribution and symbolic estimation is implemented using the function KernelMixtureDistribution both of which provide data-driven bandwidths. In Minitab, the Royal Society of Chemistry has created a macro to run kernel density estimation based on their Analytical Methods Committee Technical Brief 4. In the NAG Library, kernel density estimation is implemented via the g10ba routine (available in both the Fortran and the C versions of the Library). In Nuklei, C kernel density methods focus on data from the Special Euclidean group S E ( 3 ) displaystyle SE(3) . In Octave, kernel density estimation is implemented by the kernel_density option (econometrics package). In Origin, 2D kernel density plot can be made from its user interface, and two functions, Ksdensity for 1D and Ks2density for 2D can be used from its LabTalk, Python, or C code. In Perl, an implementation can be found in the Statistics-KernelEstimation module In PHP, an implementation can be found in the MathPHP library In Python, many implementations exist pyqt_fit.kde Module in the PyQt-Fit package, SciPy (scipy.stats.gaussian_kde), Statsmodels (KDEUnivariate and KDEMultivariate), and scikit-learn (KernelDensity) (see comparison). KDEpy supports weighted data and its FFT implementation is orders of magnitude faster than the other implementations. The commonly used pandas library offers support for kde plotting through the plot method (df.plot()). The getdist package for weighted and correlated MCMC samples supports optimized bandwidth, boundary correction and higher-order methods for 1D and 2D distributions. One newly used package for kernel density estimation is seaborn ( import seaborn as sns , sns.kdeplot() ). A GPU implementation of KDE also exists. In R, it is implemented through density in the base distribution, and bw. function is used in stats package, this function uses the optimized formula in Silvermans book. bkde in the KernSmooth library, ParetoDensityEstimation in the DataVisualizations library (for pareto distribution density estimation), kde in the ks library, dkden and dbckden in the evmix library (latter for boundary corrected kernel density estimation for bounded support), npudens in the np library (numeric and categorical data), sm.density in the sm library. For an implementation of the kde.R function, which does not require installing any packages or libraries, see kde.R. The btb library, dedicated to urban analysis, implements kernel density estimation through kernel_smoothing. In SAS, proc kde can be used to estimate univariate and bivariate kernel densities. In Apache Spark, the KernelDensity() class In Stata, it is implemented through kdensity for example histogram x, kdensity. Alternatively a free Stata module KDENS is available allowing a user to estimate 1D or 2D density functions. In Swift, it is implemented through SwiftStats.KernelDensityEstimation in the open-source statistics library SwiftStats. See also Kernel (statistics) Kernel smoothing Kernel regression Density estimation (with presentation of other examples) Mean-shift Scale space The triplets (x, h, KDE with bandwidth h evaluated at x all x, h  0 form a scale space representation of the data. Multivariate kernel density estimation Variable kernel density estimation Head/tail breaks Further reading H rdle, Wolfgang M ller, Marlene Sperlich, Stefan Werwatz, Axel (2004). Nonparametric and Semiparametric Models. Springer Series in Statistics. Berlin Heidelberg Springer-Verlag. pp. 39 83. ISBN 978-3-540-20722-1. References External links Introduction to kernel density estimation A short tutorial which motivates kernel density estimators as an improvement over histograms. Kernel Bandwidth Optimization A free online tool that generates an optimized kernel density estimate. Free Online Software (Calculator) computes the Kernel Density Estimation for a data series according to the following Kernels Gaussian, Epanechnikov, Rectangular, Triangular, Biweight, Cosine, and Optcosine. Kernel Density Estimation Applet An online interactive example of kernel density estimation. Requires .NET 3.0 or later. Title Kernel embedding of distributions URL https//en.wikipedia.org/wiki/Kernel_embedding_of_distributions Content In machine learning, the kernel embedding of distributions (also called the kernel mean or mean map) comprises a class of nonparametric methods in which a probability distribution is represented as an element of a reproducing kernel Hilbert space (RKHS). A generalization of the individual data-point feature mapping done in classical kernel methods, the embedding of distributions into infinite-dimensional feature spaces can preserve all of the statistical features of arbitrary distributions, while allowing one to compare and manipulate distributions using Hilbert space operations such as inner products, distances, projections, linear transformations, and spectral analysis. This learning framework is very general and can be applied to distributions over any space displaystyle Omega  on which a sensible kernel function (measuring similarity between elements of displaystyle Omega  ) may be defined. For example, various kernels have been proposed for learning from data which are vectors in R d displaystyle mathbb R d , discrete classes/categories, strings, graphs/networks, images, time series, manifolds, dynamical systems, and other structured objects. The theory behind kernel embeddings of distributions has been primarily developed by Alex Smola, Le Song , Arthur Gretton, and Bernhard Sch lkopf. A review of recent works on kernel embedding of distributions can be found in. The analysis of distributions is fundamental in machine learning and statistics, and many algorithms in these fields rely on information theoretic approaches such as entropy, mutual information, or Kullback Leibler divergence. However, to estimate these quantities, one must first either perform density estimation, or employ sophisticated space-partitioning/bias-correction strategies which are typically infeasible for high-dimensional data. Commonly, methods for modeling complex distributions rely on parametric assumptions that may be unfounded or computationally challenging (e.g. Gaussian mixture models), while nonparametric methods like kernel density estimation (Note the smoothing kernels in this context have a different interpretation than the kernels discussed here) or characteristic function representation (via the Fourier transform of the distribution) break down in high-dimensional settings. Methods based on the kernel embedding of distributions sidestep these problems and also possess the following advantages Data may be modeled without restrictive assumptions about the form of the distributions and relationships between variables Intermediate density estimation is not needed Practitioners may specify the properties of a distribution most relevant for their problem (incorporating prior knowledge via choice of the kernel) If a characteristic kernel is used, then the embedding can uniquely preserve all information about a distribution, while thanks to the kernel trick, computations on the potentially infinite-dimensional RKHS can be implemented in practice as simple Gram matrix operations Dimensionality-independent rates of convergence for the empirical kernel mean (estimated using samples from the distribution) to the kernel embedding of the true underlying distribution can be proven. Learning algorithms based on this framework exhibit good generalization ability and finite sample convergence, while often being simpler and more effective than information theoretic methods Thus, learning via the kernel embedding of distributions offers a principled drop-in replacement for information theoretic approaches and is a framework which not only subsumes many popular methods in machine learning and statistics as special cases, but also can lead to entirely new learning algorithms. Definitions Let X displaystyle X denote a random variable with domain displaystyle Omega  and distribution P displaystyle P . Given a symmetric, positive-definite kernel k  R displaystyle kOmega times Omega rightarrow mathbb R  the Moore Aronszajn theorem asserts the existence of a unique RKHS H displaystyle mathcal H on displaystyle Omega  (a Hilbert space of functions f  R displaystyle fOmega to mathbb R  equipped with an inner product , H displaystyle langle cdot ,cdot rangle _mathcal H and a norm H displaystyle cdot _mathcal H ) for which k displaystyle k is a reproducing kernel, i.e., in which the element k ( x , ) displaystyle k(x,cdot ) satisfies the reproducing property f , k ( x , )  . displaystyle langle f,k(x,cdot )rangle _mathcal Hf(x)qquad forall fin mathcal H,quad forall xin Omega . One may alternatively consider x k ( x , ) displaystyle xmapsto k(x,cdot ) as an implicit feature mapping  H displaystyle varphi Omega rightarrow mathcal H (which is therefore also called the feature space), so that k ( x , x )  ( x ) , ( x ) H displaystyle k(x,x)langle varphi (x),varphi (x)rangle _mathcal H can be viewed as a measure of similarity between points x , x . displaystyle x,xin Omega . While the similarity measure is linear in the feature space, it may be highly nonlinear in the original space depending on the choice of kernel. Kernel embedding The kernel embedding of the distribution P displaystyle P in H displaystyle mathcal H (also called the kernel mean or mean map) is given by X  E  k ( X , )   E  ( X )   ( x ) d P ( x ) displaystyle mu _Xmathbb E k(X,cdot )mathbb E varphi (X)int _Omega varphi (x) mathrm d P(x) If P displaystyle P allows a square integrable density p displaystyle p , then . A kernel is characteristic if the mean embedding   family of distributions over  H displaystyle mu textfamily of distributions over Omega to mathcal H is injective. Each distribution can thus be uniquely represented in the RKHS and all statistical features of distributions are preserved by the kernel embedding if a characteristic kernel is used. Empirical kernel embedding Given n displaystyle n training examples  x 1 , , x n  displaystyle x_1,ldots ,x_n drawn independently and identically distributed (i.i.d.) from P , displaystyle P, the kernel embedding of P displaystyle P can be empirically estimated as  .i.d. from P displaystyle P , we can also empirically estimate the joint distribution kernel embedding via C  X  . By fixing X displaystyle X to a particular value, we obtain a single element in H displaystyle mathcal H , and thus it is natural to define the operator  C Y X  H H C Y  . displaystyle . Assuming that for all g H  E  g ( Y ) X  H , displaystyle gin mathcal Hmathbb E g(Y)mid Xin mathcal H, it can be shown that Y . Nevertheless, even in cases where the assumption fails, C Y X ( x ) displaystyle mathcal C_Ymid Xvarphi (x) may still be used to approximate the conditional kernel embedding Y x , displaystyle mu _Ymid x, and in practice, the inversion operator is replaced with a regularized version of itself ( C X X  I ) 1 displaystyle (mathcal C_XXlambda mathbf I )-1 (where I displaystyle mathbf I  denotes the identity matrix). Given training examples  ( x 1 , y 1 ) , , ( x n , y n )  , displaystyle (x_1,y_1),dots ,(x_n,y_n), the empirical kernel conditional embedding operator may be estimated as C  Y . Thus, the empirical estimate of the kernel conditional embedding is given by a weighted sum of samples of Y displaystyle Y in the feature space  Y . Through use of a low-rank approximation of the Gram matrix (such as the incomplete Cholesky factorization), running time and memory requirements of kernel-embedding-based learning algorithms can be drastically reduced without suffering much loss in approximation accuracy. Convergence of empirical kernel mean to the true distribution embedding If k displaystyle k is defined such that f displaystyle f takes values in  0 , 1  displaystyle 0,1 for all f H displaystyle fin mathcal H with f H 1 displaystyle f_mathcal Hleq 1 (as is the case for the widely used radial basis function kernels), then with probability at least 1 displaystyle 1-delta   X  X  ) . displaystyle k_ijk(x_i,x_j). The rate of convergence (in RKHS norm) of the empirical kernel embedding to its distribution counterpart is O ( n 1 / 2 ) displaystyle O(n-1/2) and does not depend on the dimension of X displaystyle X . Statistics based on kernel embeddings thus avoid the curse of dimensionality, and though the true underlying distribution is unknown in practice, one can (with high probability) obtain an approximation within O ( n 1 / 2 ) displaystyle O(n-1/2) of the true kernel embedding based on a finite sample of size n displaystyle n . For the embedding of conditional distributions, the empirical estimate can be seen as a weighted average of feature mappings (where the weights i ( x ) displaystyle beta _i(x) depend on the value of the conditioning variable and capture the effect of the conditioning on the kernel embedding). In this case, the empirical estimate converges to the conditional distribution RKHS embedding with rate O ( n 1 / 4 ) displaystyle Oleft(n-1/4right) if the regularization parameter displaystyle lambda  is decreased as O ( n 1 / 2 ) , displaystyle Oleft(n-1/2right), though faster rates of convergence may be achieved by placing additional assumptions on the joint distribution. Universal kernels Let X R b displaystyle mathcal Xsubseteq mathbb R b be a compact metric space and C ( X ) displaystyle C(mathcal X) the set of continuous functions. The reproducing kernel k  X X R displaystyle kmathcal Xtimes mathcal Xrightarrow mathbb R  is called universal if and only if the RKHS H displaystyle mathcal H of k displaystyle k is dense in C ( X ) displaystyle C(mathcal X) , i.e., for any g C ( X ) displaystyle gin C(mathcal X) and all  0 displaystyle varepsilon 0 there exists an f H displaystyle fin mathcal H such that f g displaystyle f-g_infty leq varepsilon  . All universal kernels defined on a compact space are characteristic kernels but the converse is not always true. Let k displaystyle k be a continuous translation invariant kernel k ( x , x )  h ( x x ) displaystyle k(x,x)h(x-x) with x R b displaystyle xin mathbb R b . Then Bochners theorem guarantees the existence of a unique finite Borel measure displaystyle mu  (called the spectral measure) on R b displaystyle mathbb R b such that h ( t )  R b e i t , d ( ) , t R b . displaystyle h(t)int _mathbb R be-ilangle t,omega rangle dmu (omega ),quad forall tin mathbb R b. For k displaystyle k to be universal it suffices that the continuous part of displaystyle mu  in its unique Lebesgue . Furthermore, if d c ( )  s ( ) d , displaystyle dmu _c(omega )s(omega )domega , then s displaystyle s is the spectral density of frequencies displaystyle omega  in R b displaystyle mathbb R b and h displaystyle h is the Fourier transform of s displaystyle s . If the support of displaystyle mu  is all of R b displaystyle mathbb R b , then k displaystyle k is a characteristic kernel as well. If k displaystyle k induces a strictly positive definite kernel matrix for any set of distinct points, then it is a universal kernel. For example, the widely used Gaussian RBF kernel k ( x , x )  exp ( 1 2 2 x x 2 ) displaystyle k(x,x)exp left(-frac 12sigma 2x-x2right) on compact subsets of R b displaystyle mathbb R b is universal. Parameter selection for conditional distribution kernel embeddings The empirical kernel conditional distribution embedding operator C  Y  X displaystyle widehat mathcal C_YX can alternatively be viewed as the solution of the following regularized least squares (function-valued) regression problem min C  H H . One can thus select the regularization parameter displaystyle lambda  by performing cross-validation based on the squared loss function of the regression problem. Rules of probability as operations in the RKHS This section illustrates how basic probabilistic rules may be reformulated as (multi)linear algebraic operations in the kernel embedding framework and is primarily based on the work of Song et al. The following notation is adopted P ( X , Y )  displaystyle P(X,Y) joint distribution over random variables X , Y displaystyle X,Y P ( X )  P ( X , d y )  displaystyle P(X)int _Omega P(X,mathrm d y) marginal distribution of X displaystyle X  P ( Y )  displaystyle P(Y) marginal distribution of Y displaystyle Y P ( Y X )  P ( X , Y ) P ( X )  displaystyle P(Ymid X)frac P(X,Y)P(X) conditional distribution of Y displaystyle Y given X displaystyle X with corresponding conditional embedding operator C Y X displaystyle mathcal C_Ymid X ( Y )  displaystyle pi (Y) prior distribution over Y displaystyle Y Q displaystyle Q is used to distinguish distributions which incorporate the prior from distributions P displaystyle P which do not rely on the prior In practice, all embeddings are empirically estimated from data  ( x 1 , y 1 ) , , ( x n , y n )  displaystyle (x_1,y_1),dots ,(x_n,y_n) and it assumed that a set of samples  y  1 , , y  n   displaystyle widetilde y_1,ldots ,widetilde y_widetilde n may be used to estimate the kernel embedding of the prior distribution ( Y ) displaystyle pi (Y) . Kernel sum rule In probability theory, the marginal distribution of X displaystyle X can be computed by integrating out Y displaystyle Y from the joint density (including the prior distribution on Y displaystyle Y ) Q ( X )  P ( X Y ) d ( Y ) displaystyle Q(X)int _Omega P(Xmid Y),mathrm d pi (Y) The analog of this rule in the kernel embedding framework states that X , displaystyle mu _Xpi , the RKHS embedding of Q ( X ) displaystyle Q(X) , can be computed via  ) . displaystyle pi (Y). In practical implementations, the kernel sum rule takes the following form  . Kernel chain rule In probability theory, a joint distribution can be factorized into a product between conditional and marginal distributions Q ( X , Y )  P ( X Y ) ( Y ) displaystyle Q(X,Y)P(Xmid Y)pi (Y) The analog of this rule in the kernel embedding framework states that C X Y , displaystyle mathcal C_XYpi , the joint embedding of Q ( X , Y ) , displaystyle Q(X,Y), can be factorized as a composition of conditional embedding operator with the auto-covariance operator associated with ( Y ) displaystyle pi (Y) C X  )  . displaystyle mathcal C_YYpi mathbb E varphi (Y)otimes varphi (Y). In practical implementations, the kernel chain rule takes the following form C  X  . displaystyle mathcal C_YXpi left(mathcal C_Xmid Ymathcal C_YYpi right)T. In practical implementations, the kernel Bayes rule takes the following form  Y   ) . displaystyle boldsymbol Lambda left(mathbf G widetilde lambda mathbf I right)-1widetilde mathbf G operatorname diag (boldsymbol alpha ),qquad mathbf D operatorname diag left(left(mathbf G widetilde lambda mathbf I right)-1widetilde mathbf G boldsymbol alpha right). Two regularization parameters are used in this framework displaystyle lambda  for the estimation of C  Y X , C  X  . displaystyle widehat mathcal C_Ymid Xpi widehat mathcal C_YXpi left(left(widehat mathcal C_XXpi right)2widetilde lambda mathbf I right)-1widehat mathcal C_XXpi . The latter regularization is done on square of C  X X displaystyle widehat mathcal C_XXpi  because D displaystyle D may not be positive definite. Applications Measuring distance between distributions The maximum mean discrepancy (MMD) is a distance-measure between distributions P ( X ) displaystyle P(X) and Q ( Y ) displaystyle Q(Y) which is defined as the distance between their embeddings in the RKHS MMD ( P , Q )  X Y H . displaystyle textMMD(P,Q)leftmu _X-mu _Yright_mathcal H. While most distance-measures between distributions such as the widely used Kullback Leibler divergence either require density estimation (either parametrically or nonparametrically) or space partitioning/bias correction strategies, the MMD is easily estimated as an empirical mean which is concentrated around the true value of the MMD. The characterization of this distance as the maximum mean discrepancy refers to the fact that computing the MMD is equivalent to finding the RKHS function that maximizes the difference in expectations between the two probability distributions MMD ( P , Q )  sup f H 1 ( E  f ( X )  E  f ( Y )  ) , displaystyle textMMD(P,Q)sup _f_mathcal Hleq 1left(mathbb E f(X)-mathbb E f(Y)right), a form of integral probability metric. Kernel two-sample test Given n training examples from P ( X ) displaystyle P(X) and m samples from Q ( Y ) displaystyle Q(Y) , one can formulate a test statistic based on the empirical estimate of the MMD MMD  ( P , Q )  1 n .e.  . Density estimation via kernel embeddings Although learning algorithms in the kernel embedding framework circumvent the need for intermediate density estimation, one may nonetheless use the empirical embedding to perform density estimation based on n samples drawn from an underlying distribution P X displaystyle P_X . This can be done by solving the following optimization problem max P X H ( P X ) displaystyle max _P_XH(P_X) subject to  X X  P X  H displaystyle widehat mu _X-mu _XP_X_mathcal Hleq varepsilon  where the maximization is done over the entire space of distributions on . displaystyle Omega . Here, X  P X  displaystyle mu _XP_X is the kernel embedding of the proposed density P X displaystyle P_X and H displaystyle H is an entropy-like quantity (e.g. Entropy, KL divergence, Bregman divergence). The distribution which solves this optimization may be interpreted as a compromise between fitting the empirical kernel means of the samples well, while still allocating a substantial portion of the probability mass to all regions of the probability space (much of which may not be represented in the training examples). In practice, a good approximate solution of the difficult optimization may be found by restricting the space of candidate densities to a mixture of M candidate distributions with regularized mixing proportions. Connections between the ideas underlying Gaussian processes and conditional random fields may be drawn with the estimation of conditional probability distributions in this fashion, if one views the feature mappings associated with the kernel as sufficient statistics in generalized (possibly infinite-dimensional) exponential families. Measuring dependence of random variables A measure of the statistical dependence between random variables X displaystyle X and Y displaystyle Y (from any domains on which sensible kernels can be defined) can be formulated based on the Hilbert Schmidt Independence Criterion HSIC ( X , Y )  C X Y X Y H H 2 displaystyle textHSIC(X,Y)leftmathcal C_XY-mu _Xotimes mu _Yright_mathcal Hotimes mathcal H2 and can be used as a principled replacement for mutual information, Pearson correlation or any other dependence measure used in learning algorithms. Most notably, HSIC can detect arbitrary dependencies (when a characteristic kernel is used in the embeddings, HSIC is zero if and only if the variables are independent), and can be used to measure dependence between different types of data (e.g. images and text captions). Given n i.i.d. samples of each random variable, a simple parameter-free unbiased estimator of HSIC which exhibits concentration about the true value can be computed in O ( n ( d f 2  d g 2 ) ) displaystyle O(n(d_f2d_g2)) time, where the Gram matrices of the two datasets are approximated using A A T , B B T displaystyle mathbf A mathbf A T,mathbf B mathbf B T with A R n d f , B R n d g displaystyle mathbf A in mathbb R ntimes d_f,mathbf B in mathbb R ntimes d_g . The desirable properties of HSIC have led to the formulation of numerous algorithms which utilize this dependence measure for a variety of common machine learning tasks such as feature selection (BAHSIC ), clustering (CLUHSIC ), and dimensionality reduction (MUHSIC ). HSIC can be extended to measure the dependence of multiple random variables. The question of when HSIC captures independence in this case has recently been studied for more than two variables on R d displaystyle mathbb R d  the characteristic property of the individual kernels remains an equivalent condition. on general domains the characteristic property of the kernel components is necessary but not sufficient. Kernel belief propagation Belief propagation is a fundamental algorithm for inference in graphical models in which nodes repeatedly pass and receive messages corresponding to the evaluation of conditional expectations. In the kernel embedding framework, the messages may be represented as RKHS functions and the conditional distribution embeddings can be applied to efficiently compute message updates. Given n samples of random variables represented by nodes in a Markov random field, the incoming message to node t from node u can be expressed as m u t ( )  . The kernel belief propagation update message from t to node s is then given by m  t  . Thus, if the incoming messages to node t are linear combinations of feature mapped samples from X t displaystyle X_t , then the outgoing message from this node is also a linear combination of feature mapped samples from X s displaystyle X_s . This RKHS function representation of message-passing updates therefore produces an efficient belief propagation algorithm in which the potentials are nonparametric functions inferred from the data so that arbitrary statistical relationships may be modeled. Nonparametric filtering in hidden Markov models In the hidden Markov model (HMM), two key quantities of interest are the transition probabilities between hidden states P ( S t S t 1 ) displaystyle P(Stmid St-1) and the emission probabilities P ( O t S t ) displaystyle P(Otmid St) for observations. Using the kernel conditional distribution embedding framework, these quantities may be expressed in terms of samples from the HMM. A serious limitation of the embedding methods in this domain is the need for training samples containing hidden states, as otherwise inference with arbitrary distributions in the HMM is not possible. One common use of HMMs is filtering in which the goal is to estimate posterior distribution over the hidden state s t displaystyle st at time step t given a history of previous observations h . In filtering, a belief state P ( S t  1 h t  1 ) displaystyle P(St1mid ht1) is recursively maintained via a prediction step (where updates P ( S t  1 h t )  E  P ( S t  1 S t ) h t  displaystyle P(St1mid ht)mathbb E P(St1mid St)mid ht are computed by marginalizing out the previous hidden state) followed by a conditioning step (where updates P ( S t  1 h t , o t  1 ) P ( o t  1 S t  1 ) P ( S t  1 h t ) displaystyle P(St1mid ht,ot1)propto P(ot1mid St1)P(St1mid ht) are computed by applying Bayes rule to condition on a new observation). The RKHS embedding of the belief state at time t1 can be recursively expressed as S t  1 h t  1  C S t  1 O t  1 ( C O t  1 O t  1 ) 1 ( o t  1 ) displaystyle mu _St1mid ht1mathcal C_St1Ot1pi left(mathcal C_Ot1Ot1pi right)-1varphi (ot1) by computing the embeddings of the prediction step via the kernel sum rule and the embedding of the conditioning step via kernel Bayes rule. Assuming a training sample ( s  1 , , s  T , o  1 , , o  T ) displaystyle (widetilde s1,dots ,widetilde sT,widetilde o1,dots ,widetilde oT) is given, one can in practice estimate  S t  1 h t  1   . displaystyle mathbf K _ot1(k(widetilde o1,ot1),dots ,k(widetilde oT,ot1))T. Support measure machines The support measure machine (SMM) is a generalization of the support vector machine (SVM) in which the training examples are probability distributions paired with labels  P i , y i   . SMMs solve the standard SVM dual optimization problem using the following expected kernel K ( P ( X ) , Q ( Z ) )  X , Z .g. the Gaussian kernel or polynomial kernel), or can be accurately empirically estimated from i.i.d. samples  x i  . Domain adaptation under covariate, target, and conditional shift The goal of domain adaptation is the formulation of learning algorithms which generalize well when the training and test data have different distributions. Given training examples  ( x i tr , y i tr )  ) . In general, the presence of conditional shift leads to an ill-posed problem, and the additional assumption that P ( X Y ) displaystyle P(Xmid Y) changes only under location-scale (LS) transformations on X displaystyle X is commonly imposed to make the problem tractable. By utilizing the kernel embedding of marginal and conditional distributions, practical approaches to deal with the presence of these types of differences between training and test domains can be formulated. Covariate shift may be accounted for by reweighting examples via estimates of the ratio P te ( X ) / P tr ( X ) displaystyle Ptextte(X)/Ptexttr(X) obtained directly from the kernel embeddings of the marginal distributions of X displaystyle X in each domain without any need for explicit estimation of the distributions. Target shift, which cannot be similarly dealt with since no samples from Y displaystyle Y are available in the test domain, is accounted for by weighting training examples using the vector ( y tr ) displaystyle boldsymbol beta (mathbf y texttr) which solves the following optimization problem (where in practice, empirical approximations must be used) min ( y ) C ( X Y ) tr E  ( y ) ( y tr )  X te H 2 displaystyle min _boldsymbol beta (y)leftmathcal C_(Xmid Y)texttrmathbb E boldsymbol beta (y)varphi (ytexttr)-mu _Xtextteright_mathcal H2 subject to ( y ) 0 , E  ( y tr )   1 displaystyle boldsymbol beta (y)geq 0,mathbb E boldsymbol beta (ytexttr)1 To deal with location scale conditional shift, one can perform a LS transformation of the training points to obtain new transformed training data X ). To ensure similar distributions between the new transformed training samples and the test data, W , B displaystyle mathbf W ,mathbf B  are estimated by minimizing the following empirical kernel embedding distance  X new  X te H 2  C  ( X Y ) new  Y tr  X te H 2 displaystyle leftwidehat mu _Xtextnew-widehat mu _Xtextteright_mathcal H2leftwidehat mathcal C_(Xmid Y)textnewwidehat mu _Ytexttr-widehat mu _Xtextteright_mathcal H2 In general, the kernel embedding methods for dealing with LS conditional shift and target shift may be combined to find a reweighted transformation of the training data which mimics the test distribution, and these methods may perform well even in the presence of conditional shifts other than location-scale changes. Domain generalization via invariant feature representation Given N sets of training examples sampled i.i.d. from distributions P ( 1 ) ( X , Y ) , P ( 2 ) ( X , Y ) , , P ( N ) ( X , Y ) displaystyle P(1)(X,Y),P(2)(X,Y),ldots ,P(N)(X,Y) , the goal of domain generalization is to formulate learning algorithms which perform well on test examples sampled from a previously unseen domain P ( X , Y ) displaystyle P(X,Y) where no data from the test domain is available at training time. If conditional distributions P ( Y X ) displaystyle P(Ymid X) are assumed to be relatively similar across all domains, then a learner capable of domain generalization must estimate a functional relationship between the variables which is robust to changes in the marginals P ( X ) displaystyle P(X) . Based on kernel embeddings of these distributions, Domain Invariant Component Analysis (DICA) is a method which determines the transformation of the training data that minimizes the difference between marginal distributions while preserving a common conditional distribution shared between all training domains. DICA thus extracts invariants, features that transfer across domains, and may be viewed as a generalization of many popular dimension-reduction methods such as kernel principal component analysis, transfer component analysis, and covariance operator inverse regression. Defining a probability distribution P displaystyle mathcal P on the RKHS H displaystyle mathcal H with P ( X ( i ) Y ( i ) )  1 N for . Finding an orthogonal transform onto a low-dimensional subspace B (in the feature space) which minimizes the distributional variance, DICA simultaneously ensures that B aligns with the bases of a central subspace C for which Y displaystyle Y becomes independent of X displaystyle X given C T X displaystyle CTX across all domains. In the absence of target values Y displaystyle Y , an unsupervised version of DICA may be formulated which finds a low-dimensional subspace that minimizes distributional variance while simultaneously maximizing the variance of X displaystyle X (in the feature space) across all domains (rather than preserving a central subspace). Distribution regression In distribution regression, the goal is to regress from probability distributions to reals (or vectors). Many important machine learning and statistical tasks fit into this framework, including multi-instance learning, and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). In practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. Distribution regression has been successfully applied for example in supervised entropy learning, and aerosol prediction using multispectral satellite images. Given (  X i , n  . In other words, one can consider the following kernel ridge regression problem (  0 ) displaystyle (lambda 0) J ( f )  1  . Examples for K displaystyle K include the linear kernel  K ( P , Q )  P , Q H ( k )  displaystyle leftK(mu _P,mu _Q)langle mu _P,mu _Qrangle _mathcal H(k)right , the Gaussian kernel  K ( P , Q )  e P Q H ( k ) 2 / ( 2 2 )  displaystyle leftK(mu _P,mu _Q)e-leftmu _P-mu _Qright_H(k)2/(2sigma 2)right , the exponential kernel  K ( P , Q )  e P Q H ( k ) / ( 2 2 )  displaystyle leftK(mu _P,mu _Q)e-leftmu _P-mu _Qright_H(k)/(2sigma 2)right , the Cauchy kernel  K ( P , Q )  ( 1  P Q H ( k ) 2 / 2 ) 1  displaystyle leftK(mu _P,mu _Q)left(1leftmu _P-mu _Qright_H(k)2/sigma 2right)-1right , the generalized t-student kernel  K ( P , Q )  ( 1  P Q H ( k ) ) 1 , ( 2 )  displaystyle leftK(mu _P,mu _Q)left(1leftmu _P-mu _Qright_H(k)sigma right)-1,(sigma leq 2)right , or the inverse multiquadrics kernel  K ( P , Q )  ( P Q H ( k ) 2  2 ) 1 2  displaystyle leftK(mu _P,mu _Q)left(leftmu _P-mu _Qright_H(k)2sigma 2right)-frac 12right . The prediction on a new distribution ( X  ) displaystyle (hat X) takes the simple, analytical form y  ( X  )  k  G   1 y , displaystyle hat ybig (hat Xbig )mathbf k mathbf G lambda ell -1mathbf y , where   . Under mild regularity conditions this estimator can be shown to be consistent and it can achieve the one-stage sampled (as if one had access to the true X i displaystyle X_i -s) minimax optimal rate. In the J displaystyle J objective function y i displaystyle y_i -s are real numbers the results can also be extended to the case when y i displaystyle y_i -s are d displaystyle d -dimensional vectors, or more generally elements of a separable Hilbert space using operator-valued K displaystyle K kernels. Example In this simple example, which is taken from Song et al., X , Y displaystyle X,Y are assumed to be discrete random variables which take values in the set  1 , , K  displaystyle 1,ldots ,K and the kernel is chosen to be the Kronecker delta function, so k ( x , x )  ( x , x ) displaystyle k(x,x)delta (x,x) . The feature map corresponding to this kernel is the standard basis vector ( x )  e x displaystyle varphi (x)mathbf e _x . The kernel embeddings of such a distributions are thus vectors of marginal probabilities while the embeddings of joint distributions in this setting are K K displaystyle Ktimes K matrices specifying joint probability tables, and the explicit form of these embeddings is ). Title Knowledge graph embedding URL https//en.wikipedia.org/wiki/Knowledge_graph_embedding Content In representation learning, knowledge graph embedding (KGE), also called knowledge representation learning (KRL), or multi-relation learning, is a machine learning task of learning a low-dimensional representation of a knowledge graphs entities and relations while preserving their semantic meaning. Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction. Definition A knowledge graph  . A fact is a triple ( h , r , t ) F displaystyle (h,r,t)in F that denotes a link r R displaystyle rin R between the head h E displaystyle hin E and the tail t E displaystyle tin E of the triple. Another notation that is often used in the literature to represent a triple (or fact) is  h e a d , r e l a t i o n , t a i l  displaystyle head,relation,tail . This notation is called resource description framework (RDF). A knowledge graph represents the knowledge related to a specific domain leveraging this structured representation, it is possible to infer a piece of new knowledge from it after some refinement steps. However, nowadays, people have to deal with the sparsity of data and the computational inefficiency to use them in a real-world application. The embedding of a knowledge graph is a function that translates each entity and each relation into a vector of a given dimension d displaystyle d , called embedding dimension. It is even possible to embed the entities and relations with different dimensions. The embedding vectors can then be used for other tasks. A knowledge graph embedding is characterized by four aspects Representation space The low-dimensional space in which the entities and relations are represented. Scoring function A measure of the goodness of a triple embedded representation. Encoding models The modality in which the embedded representation of the entities and relations interact with each other. Additional information Any additional information coming from the knowledge graph that can enrich the embedded representation. Usually, an ad hoc scoring function is integrated into the general scoring function for each additional information. Embedding procedure All algorithms for creating a knowledge graph embedding follow the same approach. First, the embedding vectors are initialized to random values. Then, they are iteratively optimized using a training set of triples. In each iteration, a batch of size b displaystyle b triples is sampled from the training set, and a triple from it is sampled and corrupted i.e., a triple that does not represent a true fact in the knowledge graph. The corruption of a triple involves substituting the head or the tail (or both) of the triple with another entity that makes the fact false. The original triple and the corrupted triple are added in the training batch, and then the embeddings are updated, optimizing a scoring function. Iteration stops when a stop condition is reached. Usually, the stop condition depends on the overfitting of the training set. At the end, the learned embeddings should have extracted semantic meaning from the training triples and should correctly predict unseen true facts in the knowledge graph. Pseudocode The following is the pseudocode for the general embedding procedure. algorithm Compute entity and relation embeddings input The training set . The simplicity of the indexes makes them very suitable for evaluating the performance of an embedding algorithm even on a large scale. Given Q displaystyle ce Q as the set of all ranked predictions of a model, it is possible to define three different performance indexes HitsK, MR, and MRR. HitsK HitsK or in short, HK, is a performance index that measures the probability to find the correct prediction in the first top K model predictions. Usually, it is used  . HitsK reflects the accuracy of an embedding model to predict the relation between two given triples correctly. Hits. Mean rank (MR) Mean rank is the average ranking position of the items predicted by the model among all the possible items. M . Mean reciprocal rank (MRR) Mean reciprocal rank measures the number of triples predicted correctly. If the first predicted triple is correct, then 1 is added, if the second is correct 1 2 displaystyle frac 12 is summed, and so on. Mean reciprocal rank is generally used to quantify the effect of search algorithms. M R . Applications Machine learning tasks Knowledge graph completion (KGC) is a collection of techniques to infer knowledge from an embedded knowledge graph representation. In particular, this technique completes a triple inferring the missing entity or relation. The corresponding sub-tasks are named link or entity prediction (i.e., guessing an entity from the embedding given the other entity of the triple and the relation), and relation prediction (i.e., forecasting the most plausible relation that connects two entities). Triple Classification is a binary classification problem. Given a triple, the trained model evaluates the plausibility of the triple using the embedding to determine if a triple is true or false. The decision is made with the model score function and a given threshold. Clustering is another application that leverages the embedded representation of a sparse knowledge graph to condense the representation of similar semantic entities close in a 2D space. Real world applications The use of knowledge graph embedding is increasingly pervasive in many applications. In the case of recommender systems, the use of knowledge graph embedding can overcome the limitations of the usual reinforcement learning. Training this kind of recommender system requires a huge amount of information from the users however, knowledge graph techniques can address this issue by using a graph already constructed over a prior knowledge of the item correlation and using the embedding to infer from it the recommendation. Drug repurposing is the use of an already approved drug, but for a therapeutic purpose different from the one for which it was initially designed. It is possible to use the task of link prediction to infer a new connection between an already existing drug and a disease by using a biomedical knowledge graph built leveraging the availability of massive literature and biomedical databases. Knowledge graph embedding can also be used in the domain of social politics. Models Given a collection of triples (or facts) . ( h , r , t ) displaystyle (h,r,t) is the corresponding embedding of a triple with h , t I R d displaystyle h,tin rm I!Rd and r I R k displaystyle rin rm I!Rk , where d displaystyle d is the embedding dimension for the entities, and k displaystyle k for the relations. The score function of a given model is denoted by f r ( h , t ) displaystyle mathcal f_r(h,t) and measures the distance of the embedding of the head from the embedding of tail given the embedding of the relation. In other words, it quantifies the plausibility of the embedded representation of a given fact. Rossi et al. propose a taxonomy of the embedding models and identifies three main families of models tensor decomposition models, geometric models, and deep learning models. Tensor decomposition model The tensor decomposition is a family of knowledge graph embedding models that use a multi-dimensional matrix to represent a knowledge graph, that is partially knowable due to gaps of the graph describing a particular domain thoroughly. In particular, these models use a third-order (3D) tensor, which is then factorized into low-dimensional vectors that are the embeddings. A third-order tensor is suitable for representing a knowledge graph because it records only the existence or absence of a relation between entities, and so is simple, and there is no need to know a priori the network structure, making this class of embedding models light, and easy to train even if they suffer from high-dimensionality and sparsity of data. Bilinear models This family of models uses a linear equation to embed the connection between the entities through a relation. In particular, the embedded representation of the relations is a bidimensional matrix. These models, during the embedding procedure, only use the single facts to compute the embedded representation and ignore the other associations to the same entity or relation. DistMult Since the embedding matrix of the relation is a diagonal matrix, the scoring function can not distinguish asymmetric facts. ComplEx As DistMult uses a diagonal matrix to represent the relations embedding but adds a representation in the complex vector space and the hermitian product, it can distinguish symmetric and asymmetric facts. This approach is scalable to a large knowledge graph in terms of time and space cost. ANALOGY This model encodes in the embedding the analogical structure of the knowledge graph to simulate inductive reasoning. Using a differentiable objective function, ANALOGY has good theoretical generality and computational scalability. It is proven that the embedding produced by ANALOGY fully recovers the embedding of DistMult, ComplEx, and HolE. SimplE This model is the improvement of canonical polyadic decomposition (CP), in which an embedding vector for the relation and two independent embedding vectors for each entity are learned, depending on whether it is a head or a tail in the knowledge graph fact. SimplE resolves the problem of independent learning of the two entity embeddings using an inverse relation and average the CP score of ( h , r , t ) displaystyle (h,r,t) and ( t , r 1 , h ) displaystyle (t,r-1,h) . In this way, SimplE collects the relation between entities while they appear in the role of subject or object inside a fact, and it is able to embed asymmetric relations. Non-bilinear models HolE HolE uses circular correlation to create an embedded representation of the knowledge graph, which can be seen as a compression of the matrix product, but is more computationally efficient and scalable while keeping the capabilities to express asymmetric relation since the circular correlation is not commutative. HolE links holographic and complex embeddings since, if used together with Fourier, can be seen as a special case of ComplEx. TuckER TuckER sees the knowledge graph as a tensor that could be decomposed using the Tucker decomposition in a collection of vectors i.e., the embeddings of entities and relations with a shared core. The weights of the core tensor are learned together with the embeddings and represent the level of interaction of the entries. Each entity and relation has its own embedding dimension, and the size of the core tensor is determined by the shape of the entities and relations that interact. The embedding of the subject and object of a fact are summed in the same way, making TuckER fully expressive, and other embedding models such as RESCAL, DistMult, ComplEx, and SimplE can be expressed as a special formulation of TuckER. MEI MEI introduces the multi-partition embedding interaction technique with the block term tensor format, which is a generalization of CP decomposition and Tucker decomposition. It divides the embedding vector into multiple partitions and learns the local interaction patterns from data instead of using fixed special patterns as in ComplEx or SimplE models. This enables MEI to achieve optimal efficiency expressiveness trade-off, not just being fully expressive. Previous models such as TuckER, RESCAL, DistMult, ComplEx, and SimplE are suboptimal restricted special cases of MEI. MEIM MEIM goes beyond the block term tensor format to introduce the independent core tensor for ensemble boosting effects and the soft orthogonality for max-rank relational mapping, in addition to multi-partition embedding interaction. MEIM generalizes several previous models such as MEI and its subsumed models, RotaE, and QuatE. MEIM improves expressiveness while still being highly efficient in practice, helping it achieve good results using fairly small model sizes. Geometric models The geometric space defined by this family of models encodes the relation as a geometric transformation between the head and tail of a fact. For this reason, to compute the embedding of the tail, it is necessary to apply a transformation displaystyle tau  to the head embedding, and a distance function displaystyle delta  is used to measure the goodness of the embedding or to score the reliability of a fact. f r ( h , t )  ( ( h , r ) , t ) displaystyle mathcal f_r(h,t)delta (tau (h,r),t) Geometric models are similar to the tensor decomposition model, but the main difference between the two is that they have to preserve the applicability of the transformation displaystyle tau  in the geometric space in which it is defined. Pure translational models This class of models is inspired by the idea of translation invariance introduced in word2vec. A pure translational model relies on the fact that the embedding vector of the entities are close to each other after applying a proper relational translation in the geometric space in which they are defined. In other words, given a fact, the embedding of the head plus the embedding of the relation should equal the embedding of the tail. The closeness of the entities embedding is given by some distance measure and quantifies the reliability of a fact. TransE Uses a scoring function that forces the embeddings to satisfy a simple vector sum equation in each fact in which they appear h   . The embedding will be exact if each entity and relation appears in only one fact, and so in practice is poor at representing one-to-many, many-to-one, and asymmetric relations. TransH A modification of TransE for representing types of relations, by using a hyperplane as a geometric space. In TransH, the relation embedding is on a different hyperplane depending on the entities it interacts with. So, to compute, for example, the score function of a fact, the embedded representation of the head and tail need to be projected using a relational projection matrix on the correct hyperplane of the relation. TransR A modification of TransH that uses different spaces embedding entities versus relations, thus separating the semantic spaces of entities and relations. TransR also uses a relational projection matrix to translate the embedding of the entities to the relation space. TransD In TransR, the head and the tail of a given fact could belong to two different types of entities. For example, in the fact ( O b a m a , p r e s i d e n t _ o f , U S A ) displaystyle (Obama,president_of,USA) , Obama is a person and USA is a country. Matrix multiplication is an expensive procedure in TransR to compute the projection. In this context, TransD uses two vectors for each entity-relation pair to compute a dynamic mapping that substitutes the projection matrix while reducing the dimensional complexity. The first vector is used to represent the semantic meaning of the entities and relations, the second to compute the mapping matrix. TransA All the translational models define a score function in their representation space, but they oversimplify this metric loss. Since the vector representation of the entities and relations is not perfect, a pure translation of h  r displaystyle hr could be distant from t displaystyle t , and a spherical equipotential Euclidean distance makes it hard to distinguish which is the closest entity. TransA, instead, introduces an adaptive Mahalanobis distance to weights the embedding dimensions, together with elliptical surfaces to remove the ambiguity. Translational models with additional embeddings It is possible to associate additional information to each element in the knowledge graph and their common representation facts. Each entity and relation can be enriched with text descriptions, weights, constraints, and others in order to improve the overall description of the domain with a knowledge graph. During the embedding of the knowledge graph, this information can be used to learn specialized embeddings for these characteristics together with the usual embedded representation of entities and relations, with the cost of learning a more significant number of vectors. STransE This model is the result of the combination of TransE and of the structure embedding in such a way it is able to better represent the one-to-many, many-to-one, and many-to-many relations. To do so, the model involves two additional independent matrix W r h displaystyle W_rh and W r t displaystyle W_rt for each embedded relation r displaystyle r in the KG. Each additional matrix is used based on the fact the specific relation interact with the head or the tail of the fact. In other words, given a fact ( h , r , t ) displaystyle (h,r,t) , before applying the vector translation, the head h displaystyle h is multiplied by W r h displaystyle W_rh and the tail is multiplied by W r t displaystyle W_rt . CrossE Crossover interactions can be used for related information selection, and could be very useful for the embedding procedure. Crossover interactions provide two distinct contributions in the information selection interactions from relations to entities and interactions from entities to relations. This means that a relation, e.g.president_of automatically selects the types of entities that are connecting the subject to the object of a fact. In a similar way, the entity of a fact inderectly determine which is inference path that has to be choose to predict the object of a related triple. CrossE, to do so, learns an additional interaction matrix C displaystyle C , uses the element-wise product to compute the interaction between h displaystyle h and r displaystyle r . Even if, CrossE, does not rely on a neural network architecture, it is shown that this methodology can be encoded in such architecture. Roto-translational models This family of models, in addition or in substitution of a translation they employ a rotation-like transformation. TorusE The regularization term of TransE makes the entity embedding to build a spheric space, and consequently loses the translation properties of the geometric space. To address this problem, TorusE leverages the use of a compact Lie group that in this specific case is n-dimensional torus space, and avoid the use of regularization. TorusE defines the distance functions to substitute the and norm of TransE. RotatE RotatE is inspired by the Eulers identity and involves the use of Hadamard product to represent a relation r displaystyle r as a rotation from the head h displaystyle h to the tail t displaystyle t in the complex space. For each element of the triple, the complex part of the embedding describes a counterclockwise rotation respect to an axis, that can be describe with the Eulers identity, whereas the modulus of the relation vector is 1. It is shown that the model is capable of embedding symmetric, asymmetric, inversion, and composition relations from the knowledge graph. Deep learning models This group of embedding models uses deep neural network to learn patterns from the knowledge graph that are the input data. These models have the generality to distinguish the type of entity and relation, temporal information, path information, underlay structured information, and resolve the limitations of distance-based and semantic-matching-based models in representing all the features of a knowledge graph. The use of deep learning for knowledge graph embedding has shown good predictive performance even if they are more expensive in the training phase, data-hungry, and often required a pre-trained embedding representation of knowledge graph coming from a different embedding model. Convolutional neural networks This family of models, instead of using fully connected layers, employs one or more convolutional layers that convolve the input data applying a low-dimensional filter capable of embedding complex structures with few parameters by learning nonlinear features. ConvE ConvE is an embedding model that represents a good tradeoff expressiveness of deep learning models and computational expensiveness, in fact it is shown that it used 8x less parameters, when compared to DistMult. ConvE uses a one-dimensional d displaystyle d -sized embedding to represent the entities and relations of a knowledge graph. To compute the score function of a triple, ConvE apply a simple procedure first concatenes and merge the embeddings of the head of the triple and the relation in a single data  h  r  displaystyle ce hmathcal r , then this matrix is used as input for the 2D convolutional layer. The result is then passed through a dense layer that apply a linear transformation parameterized by the matrix W displaystyle mathcal W and at the end, with the inner product is linked to the tail triple. ConvE is also particularly efficient in the evaluation procedure using a 1-N scoring, the model matches, given a head and a relation, all the tails at the same time, saving a lot of evaluation time when compared to the 1-1 evaluation program of the other models. ConvR ConvR is an adaptive convolutional network aimed to deeply represent all the possible interactions between the entities and the relations. For this task, ConvR, computes convolutional filter for each relation, and, when required, applies these filters to the entity of interest to extract convoluted features. The procedure to compute the score of triple is the same as ConvE. ConvKB ConvKB, to compute score function of a given triple ( h , r , t ) displaystyle (h,r,t) , it produces an input  h  r  t  displaystyle ce hmathcal rt of dimension d 3 displaystyle dtimes 3 without reshaping and passes it to series of convolutional filter of size 1 3 displaystyle 1times 3 . This result feeds a dense layer with only one neuron that produces the final score. The single final neuron makes this architecture as a binary classifier in which the fact could be true or false. A difference with ConvE is that the dimensionality of the entities is not changed. Capsule neural networks This family of models uses capsule neural networks to create a more stable representation that is able to recognize a feature in the input without losing spatial information. The network is composed of convolutional layers, but they are organized in capsules, and the overall result of a capsule is sent to a higher-capsule decided by a dynamic process routine. CapsE CapsE implements a capsule network to model a fact ( h , r , t ) displaystyle (h,r,t) . As in ConvKB, each triple element is concatenated to build a matrix  h  r  t  displaystyle ce hmathcal rt and is used to feed to a convolutional layer to extract the convolutional features. These features are then redirected to a capsule to produce a continuous vector, more the vector is long, more the fact is true. Recurrent neural networks This class of models leverages the use of recurrent neural network. The advantage of this architecture is to memorize a sequence of fact, rather than just elaborate single events. RSN During the embedding procedure is commonly assumed that, similar entities has similar relations. In practice, this type of information is not leveraged, because the embedding is computed just on the undergoing fact rather than a history of facts. Recurrent skipping networks (RSN) uses a recurrent neural network to learn relational path using a random walk sampling. Model performance The machine learning task for knowledge graph embedding that is more often used to evaluate the embedding accuracy of the models is the link prediction. Rossi et al. produced an extensive benchmark of the models, but also other surveys produces similar results. The benchmark involves five datasets FB15k, , FB15k-237, WN18RR, and -10. More recently, it has been discussed that these datasets are far away from real-world applications, and other datasets should be integrated as a standard benchmark. Libraries KGE on GitHub MEI-KGE on GitHub Pykg2vec on GitHub DGL-KE on GitHub PyKEEN on GitHub TorchKGE on GitHub AmpliGraph on GitHub OpenKE on GitHub scikit-kge on GitHub Fast-TransX on GitHub MEIM-KGE on GitHub DICEE on GitHub See also Knowledge graph Embedding Machine learning Knowledge base Knowledge extraction Statistical relational learning Representation learning Graph embedding References External links Open Graph Benchmark - Stanford WordNet - Princeton Title Knowledge integration URL https//en.wikipedia.org/wiki/Knowledge_integration Content Knowledge integration is the process of synthesizing multiple knowledge models (or representations) into a common model (representation). Compared to information integration, which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives. For example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index. The Web-based Inquiry Science Environment (WISE), from the University of California at Berkeley has been developed along the lines of knowledge integration theory. Knowledge integration has also been studied as the process of incorporating new information into a body of existing knowledge with an interdisciplinary approach. This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge. A learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities e.g., to resolve knowledge conflicts and to fill knowledge gaps. By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information. The machine learning program KI, developed by Murray and Porter at the University of Texas at Austin, was created to study the use of automated and semi-automated knowledge integration to assist knowledge engineers constructing a large knowledge base. A possible technique which can be used is semantic matching. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on Minimal Mappings. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i). The University of Waterloo operates a Bachelor of Knowledge Integration undergraduate degree program as an academic major or minor. The program started in 2008. See also Data integration Knowledge value chain References Further reading Linn, M. C. (2006) The Knowledge Integration Perspective on Learning and Instruction. R. Sawyer (Ed.). In The Cambridge Handbook of the Learning Sciences. Cambridge, MA. Cambridge University Press Murray, K. S. (1996) KI A tool for Knowledge Integration. Proceedings of the Thirteenth National Conference on Artificial Intelligence Murray, K. S. (1995) Learning as Knowledge Integration, Technical Report TR-95-41, The University of Texas at Austin Murray, K. S. (1990) Improving Explanatory Competence, Proceedings of the Twelfth Annual Conference of the Cognitive Science Society Murray, K. S., Porter, B. W. (1990) Developing a Tool for Knowledge Integration Initial Results. International Journal for Man-Machine Studies, volume 33 Murray, K. S., Porter, B. W. (1989) Controlling Search for the Consequences of New Information during Knowledge Integration. Proceedings of the Sixth International Machine Learning Conference Shen, J., Sung, S.,  Zhang, D.M. (2016) Toward an analytic framework of interdisciplinary reasoning and communication (IRC) processes in science. International Journal of Science Education, 37 (17), 2809 2835. Shen, J., Liu, O.,  Sung, S. (2014). Designing interdisciplinary assessments in science for college students An example on osmosis. International Journal of Science Education, 36 (11), 1773 1793. Title Labeled data URL https//en.wikipedia.org/wiki/Labeled_data Content Labeled data is a group of samples that have been tagged with one or more labels. Labeling typically takes a set of unlabeled data and augments each piece of it with informative tags. For example, a data label might indicate whether a photo contains a horse or a cow, which words were uttered in an audio recording, what type of action is being performed in a video, what the topic of a news article is, what the overall sentiment of a tweet is, or whether a dot in an X-ray is a tumor. Labels can be obtained by asking humans to make judgments about a given piece of unlabeled data. Labeled data is significantly more expensive to obtain than the raw unlabeled data. The quality of labeled data directly influences the performance of supervised machine learning models in operation, as these models learn from the provided labels. Crowdsourced labeled data In 2006, Fei-Fei Li, the co-director of the Stanford Human-Centered AI Institute, initiated research to improve the artificial intelligence models and algorithms for image recognition by significantly enlarging the training data. The researchers downloaded millions of images from the World Wide Web and a team of undergraduates started to apply labels for objects to each image. In 2007, Li outsourced the data labeling work on Amazon Mechanical Turk, an online marketplace for digital piece work. The 3.2 million images that were labeled by more than 49,000 workers formed the basis for ImageNet, one of the largest hand-labeled database for outline of object recognition. Automated data labelling After obtaining a labeled dataset, machine learning models can be applied to the data so that new unlabeled data can be presented to the model and a likely label can be guessed or predicted for that piece of unlabeled data. Challenges with Labeled Data Data-driven bias Algorithmic decision-making is subject to programmer-driven bias as well as data-driven bias. Training data that relies on bias labeled data will result in prejudices and omissions in a predictive model, despite the machine learning algorithm being legitimate. The labeled data used to train a specific machine learning algorithm needs to be a statistically representative sample to not bias the results. For example, in facial recognition systems underrepresented groups are subsequently often misclassified if the labeled data available to train has not been representative of the population,. In 2018, a study by Joy Buolamwini and Timnit Gebru demonstrated that two facial analysis datasets that have been used to train facial recognition algorithms, IJB-A and Adience, are composed of 79.6 and 86.2 lighter skinned humans respectively. Human Error and Inconsistency Human annotators are prone to errors and biases when labeling data. This can lead to inconsistent labels and affect the quality of the data set. The inconsistency can affect the machine learning models ability to generalize well. Domain Expertise Certain fields, such as legal document analysis or medical imaging, require annotators with specialized domain knowledge. Without the expertise, the annotations or labeled data may be inaccurate, negatively impacting the machine learning models performance in a real-world scenario. Title Lazy learning URL https//en.wikipedia.org/wiki/Lazy_learning Content (Not to be confused with the lazy learning regime, see Neural tangent kernel). In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries. The primary motivation for employing lazy learning, as in the K-nearest neighbors algorithm, used by online recommendation systems (people who viewed/purchased/listened to this movie/item/tune also ...) is that the data set is continuously updated with new entries (e.g., new items for sale at Amazon, new movies to view at Netflix, new clips at YouTube, new music at Spotify or Pandora). Because of the continuous update, the training data would be rendered obsolete in a relatively short time especially in areas like books and movies, where new best-sellers or hit movies/music are published/released continuously. Therefore, one cannot really talk of a training phase. Lazy classifiers are most useful for large, continuously changing datasets with few attributes that are commonly queried. Specifically, even if a large set of attributes exist - for example, books have a year of publication, author/s, publisher, title, edition, ISBN, selling price, etc. - recommendation queries rely on far fewer attributes - e.g., purchase or viewing co-occurrence data, and user ratings of items purchased/viewed. Advantages The main advantage gained in employing a lazy learning method is that the target function will be approximated locally, such as in the k-nearest neighbor algorithm. Because the target function is approximated locally for each query to the system, lazy learning systems can simultaneously solve multiple problems and deal successfully with changes in the problem domain. At the same time they can reuse a lot of theoretical and applied results from linear regression modelling (notably PRESS statistic) and control. It is said that the advantage of this system is achieved if the predictions using a single training set are only developed for few objects. This can be demonstrated in the case of the k-NN technique, which is instance-based and function is only estimated locally. Disadvantages Theoretical disadvantages with lazy learning include The large space requirement to store the entire training dataset. In practice, this is not an issue because of advances in hardware and the relatively small number of attributes (e.g., as co-occurrence frequency) that need to be stored. Particularly noisy training data increases the case base unnecessarily, because no abstraction is made during the training phase. In practice, as stated earlier, lazy learning is applied to situations where any learning performed in advance soon becomes obsolete because of changes in the data. Also, for the problems for which lazy learning is optimal, noisy data does not really occur - the purchaser of a book has either bought another book or hasnt. Lazy learning methods are usually slower to evaluate. In practice, for very large databases with high concurrency loads, the queries are not postponed until actual query time, but recomputed in advance on a periodic basis - e.g., nightly, in anticipation of future queries, and the answers stored. This way, the next time new queries are asked about existing entries in the database, the answers are merely looked up rapidly instead of having to be computed on the fly, which would almost certainly bring a high-concurrency multi-user system to its knees. Larger training data also entail increased cost. Particularly, there is the fixed amount of computational cost, where a processor can only process a limited amount of training data points. There are standard techniques to improve re-computation efficiency so that a particular answer is not recomputed unless the data that impact this answer has changed (e.g., new items, new purchases, new views). In other words, the stored answers are updated incrementally. This approach, used by large e-commerce or media sites, has long been used in the Entrez portal of the National Center for Biotechnology Information (NCBI) to precompute similarities between the different items in its large datasets biological sequences, 3-D protein structures, published-article abstracts, etc. Because find similar queries are asked so frequently, the NCBI uses highly parallel hardware to perform nightly recomputation. The recomputation is performed only for new entries in the datasets against each other and against existing entries the similarity between two existing entries need not be recomputed. Examples of Lazy Learning Methods K-nearest neighbors, which is a special case of instance-based learning. Local regression. Lazy naive Bayes rules, which are extensively used in commercial spam detection software. Here, the spammers keep getting smarter and revising their spamming strategies, and therefore the learning rules must also be continually updated. References Further reading lazy Lazy Learning for Local Regression, R package with reference manual The Lazy Learning Package. Archived from the original on 16 February 2012. Webb G.I. (2011) Lazy Learning. In Sammut C., Webb G.I. (eds) Encyclopedia of Machine Learning. Springer, Boston, MA David W. Aha Lazy learning. Kluwer Academic Publishers, Norwell 1997, ISBN 0-7923-4584-3. Atkeson, Christopher G. Moore, Andrew W. Schaal, Stefan (1 February 1997). Locally Weighted Learning for Control. Artificial Intelligence Review. 11 (1) 75 113. doi10.1023/A1006511328852. S2CID 3694612. Bontempi, Gianluca Birattari, Mauro Bersini, Hugues. Lazy Learning for Local Modeling and Control Design. International Journal of Control. 72 (7) 643 658. doi10.1080/002071799220830. Aha, David W. Kibler, Dennis Albert, Marc K. (1 January 1991). Instance-based learning algorithms. Machine Learning. 6 (1) 37 66. doi10.1007/. Title Leakage (machine learning) URL https//en.wikipedia.org/wiki/Leakage_(machine_learning) Content In statistics and machine learning, leakage (also known as data leakage or target leakage) is the use of information in the model training process which would not be expected to be available at prediction time, causing the predictive scores (metrics) to overestimate the models utility when run in a production environment. Leakage is often subtle and indirect, making it hard to detect and eliminate. Leakage can cause a statistician or modeler to select a suboptimal model, which could be outperformed by a leakage-free model. Leakage modes Leakage can occur in many steps in the machine learning process. The leakage causes can be sub-classified into two possible sources of leakage for a model features and training examples. Feature leakage Feature or column-wise leakage is caused by the inclusion of columns which are one of the following a duplicate label, a proxy for the label, or the label itself. These features, known as anachronisms, will not be available when the model is used for predictions, and result in leakage if included when the model is trained. For example, including a MonthlySalary column when predicting YearlySalary or MinutesLate when predicting IsLate. Training example leakage Row-wise leakage is caused by improper sharing of information between rows of data. Types of row-wise leakage include Premature featurization leaking from premature featurization before Cross-validation/Train/Test split (must fit MinMax/ngrams/etc on only the train split, then transform the test set) Duplicate rows between train/validation/test (e.g. oversampling a dataset to pad its size before splitting e.g. different rotations/augmentations of a single image bootstrap sampling before splitting or duplicating rows to up sample the minority class) Non-i.i.d. data Time leakage (e.g. splitting a time-series dataset randomly instead of newer data in test set using a TrainTest split or rolling-origin cross validation) Group leakage not including a grouping split column (e.g. Andrew Ngs group had 100k x-rays of 30k patients, meaning 3 images per patient. The paper used random splitting instead of ensuring that all images of a patient were in the same split. Hence the model partially memorized the patients instead of learning to recognize pneumonia in chest x-rays.) A 2023 review found data leakage to be a widespread failure mode in machine-learning (ML)-based science, having affected at least 294 academic publications across 17 disciplines, and causing a potential reproducibility crisis. Detection Data leakage in machine learning can be detected through various methods, focusing on performance analysis, feature examination, data auditing, and model behavior analysis. Performance-wise, unusually high accuracy or significant discrepancies between training and test results often indicate leakage. Inconsistent cross-validation outcomes may also signal issues. Feature examination involves scrutinizing feature importance rankings and ensuring temporal integrity in time series data. A thorough audit of the data pipeline is crucial, reviewing pre-processing steps, feature engineering, and data splitting processes. Detecting duplicate entries across dataset splits is also important. For language models, the Min-K method can detect the presence of data in a pretraining dataset. It presents a sentence suspected to be present in the pretraining dataset, and computes the log-likelihood of each token, then compute the average of the lowest K of these. If this exceeds a threshold, then the sentence is likely present. This method is improved by comparing against a baseline of the mean and variance. Analyzing model behavior can reveal leakage. Models relying heavily on counter-intuitive features or showing unexpected prediction patterns warrant investigation. Performance degradation over time when tested on new data may suggest earlier inflated metrics due to leakage. Advanced techniques include backward feature elimination, where suspicious features are temporarily removed to observe performance changes. Using a separate hold-out dataset for final validation before deployment is advisable. See also AutoML Concept drift (where the structure of the system being studied evolves over time, invalidating the model) Overfitting Resampling (statistics) Supervised learning Training, validation, and test sets Title Learnable function class URL https//en.wikipedia.org/wiki/Learnable_function_class Content In statistical learning theory, a learnable function class is a set of functions for which an algorithm can be devised to asymptotically minimize the expected risk, uniformly over all probability distributions. The concept of learnable classes are closely related to regularization in machine learning, and provides large sample justifications for certain learning algorithms. Definition Background ).  . L  Y Y R displaystyle Lmathcal Ytimes mathcal Ymapsto mathbb R  is a pre-given loss function (usually non-negative). Given a probability distribution P ( x , y ) displaystyle P(x,y) on displaystyle Omega  , define the expected risk I P ( f ) displaystyle I_P(f) to be I P ( f )  L ( f ( x ) , y ) d P ( x , y ) displaystyle I_P(f)int L(f(x),y)dP(x,y) The general goal in statistical learning is to find the function in F displaystyle mathcal F that minimizes the expected risk. That is, to find solutions to the following problem f   arg min f F I P ( f ) displaystyle hat farg min _fin mathcal FI_P(f) But in practice the distribution P displaystyle P is unknown, and any learning task can only be based on finite samples. Thus we seek instead to find an algorithm that asymptotically minimizes the empirical risk, i.e., to find a sequence of functions  f  n  . Learnable function class We can make the condition given in the above equation stronger by requiring that the convergence is uniform for all probability distributions. That is The intuition behind the more strict requirement is as such the rate at which sequence  f  n  displaystyle hat f_n converges to the minimizer of the expected risk can be very different for different P ( x , y ) displaystyle P(x,y) . Because in real world the true distribution P displaystyle P is always unknown, we would want to select a sequence that performs well under all cases. However, by the no free lunch theorem, such a sequence that satisfies (1) does not exist if F displaystyle mathcal F is too complex. This means we need to be careful and not allow too many functions in F displaystyle mathcal F if we want (1) to be a meaningful requirement. Specifically, function classes that ensure the existence of a sequence  f  n  displaystyle hat f_n that satisfies (1) are known as learnable classes. It is worth noting that at least for supervised classification and regression problems, if a function class is learnable, then the empirical risk minimization automatically satisfies (1). Thus in these settings not only do we know that the problem posed by (1) is solvable, we also immediately have an algorithm that gives the solution. Interpretations If the true relationship between y displaystyle y and x displaystyle x is y f ( x ) displaystyle ysim f(x) , then by selecting the appropriate loss function, f displaystyle f can always be expressed as the minimizer of the expected loss across all possible functions. That is,  . f displaystyle f can be interpreted as the actual data generating mechanism. However, the no free lunch theorem tells us that in practice, with finite samples we cannot hope to search for the expected risk minimizer over F displaystyle mathcal F . Thus we often consider a subset of F displaystyle mathcal F , F displaystyle mathcal F , to carry out searches on. By doing so, we risk that f displaystyle f might not be an element of F displaystyle mathcal F . This tradeoff can be mathematically expressed as In the above decomposition, part ( b ) displaystyle (b) does not depend on the data and is non-stochastic. It describes how far away our assumptions ( F displaystyle mathcal F ) are from the truth ( F displaystyle mathcal F ). ( b ) displaystyle (b) will be strictly greater than 0 if we make assumptions that are too strong ( F displaystyle mathcal F too small). On the other hand, failing to put enough restrictions on F displaystyle mathcal F will cause it to be not learnable, and part ( a ) displaystyle (a) will not stochastically converge to 0. This is the well-known overfitting problem in statistics and machine learning literature. Example Tikhonov regularization A good example where learnable classes are used is the so-called Tikhonov regularization in reproducing kernel Hilbert space (RKHS). Specifically, let F displaystyle mathcal F be an RKHS, and     2 displaystyle cdot _2 be the norm on F displaystyle mathcal F given by its inner product. It is shown in that   . The empirical minimization algorithm to the dual form of this problem is arg min f F  . Many statistical learning algorithms can be expressed in such a form (for example, the well-known ridge regression). The tradeoff between ( a ) displaystyle (a) and ( b ) displaystyle (b) in (2) is geometrically more intuitive with Tikhonov regularization in RKHS. We can consider a sequence of  F  displaystyle mathcal F_gamma  , which are essentially balls in F displaystyle mathcal F with centers at 0. As displaystyle gamma  gets larger, F displaystyle mathcal F_gamma  gets closer to the entire space, and ( b ) displaystyle (b) is likely to become smaller. However we will also suffer smaller convergence rates in ( a ) displaystyle (a) . The way to choose an optimal displaystyle gamma  in finite sample settings is usually through cross-validation. Relationship to empirical process theory Part ( a ) displaystyle (a) in (2) is closely linked to empirical process theory in statistics, where the empirical risk  . In this field, the function class F displaystyle mathcal F that satisfies the stochastic convergence are known as uniform Glivenko Cantelli classes. It has been shown that under certain regularity conditions, learnable classes and uniformly Glivenko-Cantelli classes are equivalent. Interplay between ( a ) displaystyle (a) and ( b ) displaystyle (b) in statistics literature is often known as the bias-variance tradeoff. However, note that in the authors gave an example of stochastic convex optimization for General Setting of Learning where learnability is not equivalent with uniform convergence. Title Learning automaton URL https//en.wikipedia.org/wiki/Learning_automaton Content A learning automaton is one type of machine learning algorithm studied since 1970s. Learning automata select their current action based on past experiences from the environment. It will fall into the range of reinforcement learning if the environment is stochastic and a Markov decision process (MDP) is used. History Research in learning automata can be traced back to the work of Michael Lvovitch Tsetlin in the early 1960s in the Soviet Union. Together with some colleagues, he published a collection of papers on how to use matrices to describe automata functions. Additionally, Tsetlin worked on reasonable and collective automata behaviour, and on automata games. Learning automata were also investigated by researches in the United States in the 1960s. However, the term learning automaton was not used until Narendra and Thathachar introduced it in a survey paper in 1974. Definition A learning automaton is an adaptive decision-making unit situated in a random environment that learns the optimal action through repeated interactions with its environment. The actions are chosen according to a specific probability distribution which is updated based on the environment response the automaton obtains by performing a particular action. With respect to the field of reinforcement learning, learning automata are characterized as policy iterators. In contrast to other reinforcement learners, policy iterators directly manipulate the policy . Another example for policy iterators are evolutionary algorithms. Formally, Narendra and Thathachar define a stochastic automaton to consist of a set X of possible inputs, a , ..., s  of possible internal states, a , ..., r  of possible outputs, or actions, with r s, an initial state probability vector p(0)  (0), ..., ps(0) , a computable function A which after each time step t generates p(t1) from p(t), the current input, and the current state, and a function G which generates the output at each time step. In their paper, they investigate only stochastic automata with . The states of such an automaton correspond to the states of a discrete-state discrete-parameter Markov process. At each time step ,..., the automaton reads an input from its environment, updates p(t) to p(t1) by A, randomly chooses a successor state according to the probabilities p(t1) and outputs the corresponding action. The automatons environment, in turn, reads the action and sends the next input to the automaton. Frequently, the input set . More generally, a Q-model allows an arbitrary finite input set X, and an S-model uses the interval 0,1 of real numbers as X. A visualised demo/ Art Work of a single Learning Automaton had been developed by Systems (microSystems) Research Group at Newcastle University. Finite action-set learning automata Finite action-set learning automata (FALA) are a class of learning automata for which the number of possible actions is finite or, in more mathematical terms, for which the size of the action-set is finite. See also Reinforcement learning Game theory Automata theory Literature Philip Aranzulla and John Mellor (Home page) Mellor J and Aranzulla P (2000) Using an S-Model Response Environment with Learnng sic Automata Based Routing Schemes for IP Networks , Proc. Eighth IFIP Workshop on Performance Modelling and Evaluation of ATM and IP Networks, pp 56/1-56/12, Ilkley, UK. Aranzulla P and Mellor J (1997) Comparing two routing algorithms requiring reduced signalling when applied to ATM networks, Proc. Fourteenth UK Teletraffic Symposium on Performance Engineering in Information Systems, pp 20/1-20/4, UMIST, Manchester, UK. Narendra K. Thathachar M.A.L. (July 1974). Learning automata a survey (PDF). IEEE Transactions on Systems, Man, and Cybernetics. SMC-4 (4) 323 334. CiteSeerX 10.1.1.295.2280. doi10.1109/tsmc.1974.5408453. Tsetlin M.L. Automation theory and modeling of biological systems. Academic Press 1973. Title Learning curve (machine learning) URL https//en.wikipedia.org/wiki/Learning_curve_(machine_learning) Content In machine learning (ML), a learning curve (or training curve) is a graphical representation that shows how a models performance on a training set (and usually a validation set) changes with the number of training iterations (epochs) or the amount of training data. Typically, the number of training epochs or training set size is plotted on the x-axis, and the value of the loss function (and possibly some other metric such as the cross-validation score) on the y-axis. Synonyms include error curve, experience curve, improvement curve and generalization curve. More abstractly, learning curves plot the difference between learning effort and predictive performance, where learning effort usually means the number of training samples, and predictive performance means accuracy on testing samples. Learning curves have many useful purposes in ML, including choosing model parameters during design, adjusting optimization to improve convergence, and diagnosing problems such as overfitting (or underfitting). Learning curves can also be tools for determining how much a model benefits from adding more training data, and whether the model suffers more from a variance error or a bias error. If both the validation score and the training score converge to a certain value, then the model will no longer significantly benefit from more training data. Formal definition When creating a function to approximate the distribution of some data, it is necessary to define a loss function L ( f ( X ) , Y ) displaystyle L(f_theta (X),Y) to measure how good the model output is (e.g., accuracy for classification tasks or mean squared error for regression). We then define an optimization process which finds model parameters displaystyle theta  such that L ( f ( X ) , Y ) displaystyle L(f_theta (X),Y) is minimized, referred to as displaystyle theta  . Training curve for amount of data If the training data is  x 1 , x 2 , , x n  ,  y 1 , y 2 , y n  displaystyle x_1,x_2,dots ,x_n,y_1,y_2,dots y_n and the validation data is  x 1 , x 2 , x m  ,  y 1 , y 2 , y m  displaystyle x_1,x_2,dots x_m,y_1,y_2,dots y_m , a learning curve is the plot of the two curves i L ( f ( X i , Y i ) ( X i ) , Y i ) displaystyle imapsto L(f_theta (X_i,Y_i)(X_i),Y_i) i L ( f ( X i , Y i ) ( X i ) , Y i ) displaystyle imapsto L(f_theta (X_i,Y_i)(X_i),Y_i) where X . Gradient descent is one such algorithm. If i displaystyle theta _i is the approximation of the optimal displaystyle theta  after i displaystyle i steps, a learning curve is the plot of i L ( f i ( X , Y ) ( X ) , Y ) displaystyle imapsto L(f_theta _i(X,Y)(X),Y) i L ( f i ( X , Y ) ( X ) , Y ) displaystyle imapsto L(f_theta _i(X,Y)(X),Y) See also Overfitting Bias variance tradeoff Model selection Cross-validation (statistics) Validity (statistics) Verification and validation Double descent Title Learning rate URL https//en.wikipedia.org/wiki/Learning_rate Content In machine learning and statistics, the learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Since it influences to what extent newly acquired information overrides old information, it metaphorically represents the speed at which a machine learning model learns. In the adaptive control literature, the learning rate is commonly referred to as gain. In setting a learning rate, there is a trade-off between the rate of convergence and overshooting. While the descent direction is usually determined from the gradient of the loss function, the learning rate determines how big a step is taken in that direction. A too high learning rate will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum. In order to achieve faster convergence, prevent oscillations and getting stuck in undesirable local minima the learning rate is often varied during training either in accordance to a learning rate schedule or by using an adaptive learning rate. The learning rate and its adjustments may also differ per parameter, in which case it is a diagonal matrix that can be interpreted as an approximation to the inverse of the Hessian matrix in Newtons method. The learning rate is related to the step length determined by inexact line search in quasi-Newton methods and related optimization algorithms. Learning rate schedule Initial rate can be left as system default or can be selected using a range of techniques. A learning rate schedule changes the learning rate during learning and is most often changed between epochs/iterations. This is mainly done with two parameters decay and momentum. There are many different learning rate schedules but the most common are time-based, step-based and exponential. Decay serves to settle the learning in a nice place and avoid oscillations, a situation that may arise when a too high constant learning rate makes the learning jump back and forth over a minimum, and is controlled by a hyperparameter. Momentum is analogous to a ball rolling down a hill we want the ball to settle at the lowest point of the hill (corresponding to the lowest error). Momentum both speeds up the learning (increasing the learning rate) when the error cost gradient is heading in the same direction for a long time and also avoids local minima by rolling over small bumps. Momentum is controlled by a hyperparameter analogous to a balls mass which must be chosen manually too high and the ball will roll over minima which we wish to find, too low and it will not fulfil its purpose. The formula for factoring in the momentum is more complex than for decay but is most often built in with deep learning libraries such as Keras. Time-based learning schedules alter the learning rate depending on the learning rate of the previous time iteration. Factoring in the decay the mathematical formula for the learning rate is n  1  n 1  d n displaystyle eta _n1frac eta _n1dn where displaystyle eta  is the learning rate, d displaystyle d is a decay parameter and n displaystyle n is the iteration step. Step-based learning schedules changes the learning rate according to some predefined steps. The decay application formula is here defined as .5 corresponds to a halving) and r displaystyle r corresponds to the drop rate, or how often the rate should be dropped (10 corresponds to a drop every 10 iterations). The floor function ( displaystyle lfloor dots rfloor  ) here drops the value of its input to 0 for all values smaller than 1. Exponential learning schedules are similar to step-based, but instead of steps, a decreasing exponential function is used. The mathematical formula for factoring in the decay is . Adaptive learning rate The issue with learning rate schedules is that they all depend on hyperparameters that must be manually chosen for each given learning session and may vary greatly depending on the problem at hand or the model used. To combat this, there are many different types of adaptive gradient descent algorithms such as Adagrad, Adadelta, RMSprop, and Adam which are generally built into deep learning libraries such as Keras. See also References Further reading G ron, Aur lien (2017). Gradient Descent. Hands-On Machine Learning with Scikit-Learn and TensorFlow. OReilly. pp. 113 124. ISBN 978-1-4919-6229-9. Plagianakos, V. P. Magoulas, G. D. Vrahatis, M. N. (2001). Learning Rate Adaptation in Stochastic Gradient Descent. Advances in Convex Analysis and Global Optimization. Kluwer. pp. 433 444. ISBN 0-7923-6942-4. External links de Freitas, Nando (February 12, 2015). Optimization. Deep Learning Lecture 6. University of Oxford via YouTube. Title Learning to rank URL https//en.wikipedia.org/wiki/Learning_to_rank Content Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data may, for example, consist of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. relevant or not relevant) for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data. Applications In information retrieval Ranking is a central part of many information retrieval problems, such as document retrieval, collaborative filtering, sentiment analysis, and online advertising. A possible architecture of a machine-learned search engine is shown in the accompanying figure. Training data consists of queries and documents matching them together with the relevance degree of each match. It may be prepared manually by human assessors (or raters, as Google calls them), who check results for some queries and determine relevance of each result. It is not feasible to check the relevance of all documents, and so typically a technique called pooling is used only the top few documents, retrieved by some existing ranking models are checked. This technique may introduce selection bias. Alternatively, training data may be derived automatically by analyzing clickthrough logs (i.e. search results which got clicks from users), query chains, or such search engines features as Googles (since-replaced) SearchWiki. Clickthrough logs can be biased by the tendency of users to click on the top search results on the assumption that they are already well-ranked. Training data is used by a learning algorithm to produce a ranking model which computes the relevance of documents for actual queries. Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used. First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the vector space model, Boolean model, weighted AND, or . This phase is called top- k displaystyle k document retrieval and many heuristics were proposed in the literature to accelerate it, such as using a documents static quality score and tiered indexes. In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents. In other areas Learning to rank algorithms have been applied in areas other than information retrieval In machine translation for ranking a set of hypothesized translations In computational biology for ranking candidate 3-D structures in protein structure prediction problems In recommender systems for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article. Feature vectors For the convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called feature vectors. Such an approach is sometimes called bag of features and is analogous to the bag of words model and vector space model used in information retrieval for representation of documents. Components of such vectors are called features, factors or ranking signals. They may be divided into three groups (features from document retrieval are shown as examples) Query-independent or static features those features, which depend only on the document, but not on the query. For example, PageRank or documents length. Such features can be precomputed in off-line mode during indexing. They may be used to compute documents static quality score (or static rank), which is often used to speed up search query evaluation. Query-dependent or dynamic features those features, which depend both on the contents of the document and the query, such as TF-IDF score or other non-machine-learned ranking functions. Query-level features or query features, which depend only on the query. For example, the number of words in a query. Some examples of features, which were used in the well-known LETOR dataset TF, TF-IDF, , and language modeling scores of documents zones (title, body, anchors text, URL) for a given query Lengths and IDF sums of documents zones Documents PageRank, HITS ranks and their variants. Selecting and designing good features is an important area in machine learning, which is called feature engineering. Evaluation measures There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare the performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics. Examples of ranking quality measures Mean average precision (MAP) DCG and NDCG Precisionn, NDCGn, where n denotes that the metrics are evaluated only on top n documents Mean reciprocal rank Kendalls tau Spearmans rho. DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used. Other metrics such as MAP, MRR and precision, are defined only for binary judgments. Recently, there have been proposed several new evaluation metrics which claim to model users satisfaction with search results better than the DCG metric Expected reciprocal rank (ERR) Yandexs pfound. Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document. Approaches Learning to Rank approaches are often categorized using one of three approaches pointwise (where individual documents are ranked), pairwise (where pairs of documents are ranked into a relative order), and listwise (where an entire list of documents are ordered). Tie-Yan Liu of Microsoft Research Asia has analyzed existing algorithms for learning to rank problems in his book Learning to Rank for Information Retrieval. He categorized them into three groups by their input spaces, output spaces, hypothesis spaces (the core function of the model) and loss functions the pointwise, pairwise, and listwise approach. In practice, listwise approaches often outperform pairwise approaches and pointwise approaches. This statement was further supported by a large scale experiment on the performance of different learning-to-rank methods on a large collection of benchmark data sets. In this section, without further notice, x displaystyle x denotes an object to be evaluated, for example, a document or an image, f ( x ) displaystyle f(x) denotes a single-value hypothesis, h ( ) displaystyle h(cdot ) denotes a bi-variate or multi-variate function and L ( ) displaystyle L(cdot ) denotes the loss function. Pointwise approach In this case, it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then the learning-to-rank problem can be approximated by a regression problem given a single query-document pair, predict its score. Formally speaking, the pointwise approach aims at learning a function f ( x ) displaystyle f(x) predicting the real-value or ordinal score of a document x displaystyle x using the loss function L ( f  x j , y j ) displaystyle L(fx_j,y_j) . A number of existing supervised machine learning algorithms can be readily used for this purpose. Ordinal regression and classification algorithms can also be used in pointwise approach when they are used to predict the score of a single query-document pair, and it takes a small, finite number of values. Pairwise approach In this case, the learning-to-rank problem is approximated by a classification problem learning a binary classifier h ( x u , x v ) displaystyle h(x_u,x_v) that can tell which document is better in a given pair of documents. The classifier shall take two documents as its input and the goal is to minimize a loss function L ( h  x u , x v , y u , v ) displaystyle L(hx_u,x_v,y_u,v) . The loss function typically reflects the number and magnitude of inversions in the induced ranking. In many cases, the binary classifier h ( x u , x v ) displaystyle h(x_u,x_v) is implemented with a scoring function f ( x ) displaystyle f(x) . As an example, RankNet adapts a probability model and defines h ( x u , x v ) displaystyle h(x_u,x_v) as the estimated probability of the document x u displaystyle x_u has higher quality than x v displaystyle x_v  P u , v ( f )  CDF ( f ( x u ) f ( x v ) ) , displaystyle P_u,v(f)textCDF(f(x_u)-f(x_v)), where CDF ( ) displaystyle textCDF(cdot ) is a cumulative distribution function, for example, the standard logistic CDF, i.e. CDF ( x )  1 1  exp  x  . displaystyle textCDF(x)frac 11exp left-xright. Listwise approach These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is often difficult in practice because most evaluation measures are not continuous functions with respect to ranking models parameters, and so continuous approximations or bounds on evaluation measures have to be used. For example the SoftRank algorithm. LambdaMART is a pairwise algorithm which has been empirically shown to approximate listwise objective functions. List of methods A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method Note as most supervised learning-to-rank algorithms can be applied to pointwise, pairwise and listwise case, only those methods which are specifically designed with ranking in mind are shown above. History Norbert Fuhr introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation a specific variant of this approach (using polynomial regression) had been published by him three years earlier. Bill Cooper proposed logistic regression for the same purpose in 1992 and used it with his Berkeley research group to train a successful ranking function for TREC. Manning et al. suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques. Several conferences, such as NeurIPS, SIGIR and ICML have had workshops devoted to the learning-to-rank problem since the mid-2000s (decade). Practical usage by search engines Commercial web search engines began using machine-learned ranking systems since the 2000s (decade). One of the first search engines to start using it was AltaVista (later its technology was acquired by Overture, and then Yahoo), which launched a gradient boosting-trained ranking function in April 2003. Bings search is said to be powered by RankNet algorithm, which was invented at Microsoft Research in 2005. In November 2009 a Russian search engine Yandex announced that it had significantly increased its search quality due to deployment of a new proprietary MatrixNet algorithm, a variant of gradient boosting method which uses oblivious decision trees. Recently they have also sponsored a machine-learned ranking competition Internet Mathematics 2009 based on their own search engines production data. Yahoo has announced a similar competition in 2010. As of 2008, Googles Peter Norvig denied that their search engine exclusively relies on machine-learned ranking. Cuils CEO, Tom Costello, suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models learn what people say they like, not what people actually like. In January 2017, the technology was included in the open source search engine Apache Solr. It is also available in the open source OpenSearch and the source-available Elasticsearch. These implementations make learning to rank widely accessible for enterprise search. Vulnerabilities Similar to recognition applications in computer vision, recent neural network based ranking algorithms are also found to be susceptible to covert adversarial attacks, both on the candidates and the queries. With small perturbations imperceptible to human beings, ranking order could be arbitrarily altered. In addition, model-agnostic transferable adversarial examples are found to be possible, which enables black-box adversarial attacks on deep ranking systems without requiring access to their underlying implementations. Conversely, the robustness of such ranking systems can be improved via adversarial defenses such as the Madry defense. See also Content-based image retrieval Multimedia information retrieval Image retrieval Triplet loss References External links Competitions and public datasets LETOR A Benchmark Collection for Research on Learning to Rank for Information Retrieval Yandexs Internet Mathematics 2009 Yahoo! Learning to Rank Challenge Microsoft Learning to Rank Datasets Title Learning with errors URL https//en.wikipedia.org/wiki/Learning_with_errors Content In cryptography, learning with errors (LWE) is a mathematical problem that is widely used to create secure encryption algorithms. It is based on the idea of representing secret information as a set of equations with errors. In other words, LWE is a way to hide the value of a secret by introducing noise to it. In more technical terms, it refers to the computational problem of inferring a linear n displaystyle n -ary function f displaystyle f over a finite ring from given samples y . The LWE problem is conjectured to be hard to solve, and thus to be useful in cryptography. More precisely, the LWE problem is defined as follows. Let Z q displaystyle mathbb Z _q denote the ring of integers modulo q displaystyle q and let Z q n displaystyle mathbb Z _qn denote the set of n displaystyle n -vectors over Z q displaystyle mathbb Z _q . There exists a certain unknown linear function f  Z q n Z q displaystyle fmathbb Z _qnrightarrow mathbb Z _q , and the input to the LWE problem is a sample of pairs ( x , y ) displaystyle (mathbf x ,y) , where x Z q n displaystyle mathbf x in mathbb Z _qn and y Z q displaystyle yin mathbb Z _q , so that with high probability  ) . Furthermore, the deviation from the equality is according to some known noise model. The problem calls for finding the function f displaystyle f , or some close approximation thereof, with high probability. The LWE problem was introduced by Oded Regev in 2005 (who won the 2018 G del Prize for this work) it is a generalization of the parity learning problem. Regev showed that the LWE problem is as hard to solve as several worst-case lattice problems. Subsequently, the LWE problem has been used as a hardness assumption to create public-key cryptosystems, such as the ring learning with errors key exchange by Peikert. Definition Denote by . Let s Z q n displaystyle mathbf s in mathbb Z _qn be a fixed vector. Let displaystyle phi  be a fixed probability distribution over T displaystyle mathbb T  . Denote by A s , displaystyle A_mathbf s ,phi  the distribution on Z q n T displaystyle mathbb Z _qntimes mathbb T  obtained as follows. Pick a vector a Z q n displaystyle mathbf a in mathbb Z _qn from the uniform distribution over Z q n displaystyle mathbb Z _qn , Pick a number e T displaystyle ein mathbb T  from the distribution displaystyle phi  , Evaluate   . Output the pair ( a , t ) displaystyle (mathbf a ,t) . The learning with errors problem L W E q , displaystyle mathrm LWE _q,phi  is to find s Z q n displaystyle mathbf s in mathbb Z _qn , given access to polynomially many samples of choice from A s , displaystyle A_mathbf s ,phi  . For every  0 displaystyle alpha 0 , denote by D displaystyle D_alpha  the one-dimensional Gaussian with zero mean and variance 2 / ( 2 ) displaystyle alpha 2/(2pi ) , that is, the density function is D ( x )  ( x ) / displaystyle D_alpha (x)rho _alpha (x)/alpha  where ( x )  e (  x  / ) 2 displaystyle rho _alpha (x)e-pi (x/alpha )2 , and let displaystyle Psi _alpha  be the distribution on T displaystyle mathbb T  obtained by considering D displaystyle D_alpha  modulo one. The version of LWE considered in most of the results would be L W E q , displaystyle mathrm LWE _q,Psi _alpha  Decision version The LWE problem described above is the search version of the problem. In the decision version (DLWE), the goal is to distinguish between noisy inner products and uniformly random samples from Z q n T displaystyle mathbb Z _qntimes mathbb T  (practically, some discretized version of it). Regev showed that the decision and search versions are equivalent when q displaystyle q is a prime bounded by some polynomial in n displaystyle n . Solving decision assuming search Intuitively, if we have a procedure for the search problem, the decision version can be solved easily just feed the input samples for the decision problem to the solver for the search problem. Denote the given samples by  ( a i , b i )  Z q n T displaystyle (mathbf a _i,mathbf b _i)subset mathbb Z _qntimes mathbb T  . If the solver returns a candidate s displaystyle mathbf s  , for all i displaystyle i , calculate  a i , s b i  displaystyle langle mathbf a _i,mathbf s rangle -mathbf b _i . If the samples are from an LWE distribution, then the results of this calculation will be distributed according displaystyle chi  , but if the samples are uniformly random, these quantities will be distributed uniformly as well. Solving search assuming decision For the other direction, given a solver for the decision problem, the search version can be solved as follows Recover s displaystyle mathbf s  one coordinate at a time. To obtain the first coordinate, s 1 displaystyle mathbf s _1 , make a guess k Z q displaystyle kin mathbb Z _q , and do the following. Choose a number r Z q displaystyle rin mathbb Z _q uniformly at random. Transform the given samples  ( a i , b i )  Z q n T displaystyle (mathbf a _i,mathbf b _i)subset mathbb Z _qntimes mathbb T  as follows. Calculate  ( a i  ( r , 0 , , 0 ) , b i  ( r k ) / q )  displaystyle (mathbf a _i(r,0,ldots ,0),mathbf b _i(rk)/q) . Send the transformed samples to the decision solver. If the guess k displaystyle k was correct, the transformation takes the distribution A s , displaystyle A_mathbf s ,chi  to itself, and otherwise, since q displaystyle q is prime, it takes it to the uniform distribution. So, given a polynomial-time solver for the decision problem that errs with very small probability, since q displaystyle q is bounded by some polynomial in n displaystyle n , it only takes polynomial time to guess every possible value for k displaystyle k and use the solver to see which one is correct. After obtaining s 1 displaystyle mathbf s _1 , we follow an analogous procedure for each other coordinate s j displaystyle mathbf s _j . Namely, we transform our b i displaystyle mathbf b _i samples the same way, and transform our a i displaystyle mathbf a _i samples by calculating a i  ( 0 , , r , , 0 ) displaystyle mathbf a _i(0,ldots ,r,ldots ,0) , where the r displaystyle r is in the j th displaystyle jtextth coordinate. Peikert showed that this reduction, with a small modification, works for any q displaystyle q that is a product of distinct, small (polynomial in n displaystyle n ) primes. The main idea is if  . Average case hardness Regev showed the random self-reducibility of the LWE and DLWE problems for arbitrary q displaystyle q and displaystyle chi  . Given samples  ( a i , b i )  displaystyle (mathbf a _i,mathbf b _i) from A s , displaystyle A_mathbf s ,chi  , it is easy to see that  ( a i , b i  a i , t ) / q  displaystyle (mathbf a _i,mathbf b _ilangle mathbf a _i,mathbf t rangle )/q are samples from A s  t , displaystyle A_mathbf s mathbf t ,chi  . So, suppose there was some set S Z q n displaystyle mathcal Ssubset mathbb Z _qn such that  S  /  Z q n   1 / poly ( n ) displaystyle mathcal S/mathbb Z _qn1/operatorname poly (n) , and for distributions A s , displaystyle A_mathbf s ,chi  , with s S displaystyle mathbf s leftarrow mathcal S , DLWE was easy. Then there would be some distinguisher A displaystyle mathcal A , who, given samples  ( a i , b i )  displaystyle (mathbf a _i,mathbf b _i) , could tell whether they were uniformly random or from A s , displaystyle A_mathbf s ,chi  . If we need to distinguish uniformly random samples from A s , displaystyle A_mathbf s ,chi  , where s displaystyle mathbf s  is chosen uniformly at random from Z q n displaystyle mathbb Z _qn , we could simply try different values t displaystyle mathbf t  sampled uniformly at random from Z q n displaystyle mathbb Z _qn , calculate  ( a i , b i  a i , t ) / q  displaystyle (mathbf a _i,mathbf b _ilangle mathbf a _i,mathbf t rangle )/q and feed these samples to A displaystyle mathcal A . Since S displaystyle mathcal S comprises a large fraction of Z q n displaystyle mathbb Z _qn , with high probability, if we choose a polynomial number of values for t displaystyle mathbf t  , we will find one such that s  t S displaystyle mathbf s mathbf t in mathcal S , and A displaystyle mathcal A will successfully distinguish the samples. Thus, no such S displaystyle mathcal S can exist, meaning LWE and DLWE are (up to a polynomial factor) as hard in the average case as they are in the worst case. Hardness results Regevs result For a n-dimensional lattice L displaystyle L , let smoothing parameter ( L ) displaystyle eta _varepsilon (L) denote the smallest s displaystyle s such that 1 / s ( L  0  ) displaystyle rho _1/s(Lsetminus mathbf 0 )leq varepsilon  where L displaystyle L is the dual of L displaystyle L and ( x )  e (  x  / ) 2 displaystyle rho _alpha (x)e-pi (x/alpha )2 is extended to sets by summing over function values at each element in the set. Let D L , r displaystyle D_L,r denote the discrete Gaussian distribution on L displaystyle L of width r displaystyle r for a lattice L displaystyle L and real r  0 displaystyle r0 . The probability of each x L displaystyle xin L is proportional to r ( x ) displaystyle rho _r(x) . The discrete Gaussian sampling problem(DGS) is defined as follows An instance of D G S displaystyle DGS_phi  is given by an n displaystyle n -dimensional lattice L displaystyle L and a number r ( L ) displaystyle rgeq phi (L) . The goal is to output a sample from D L , r displaystyle D_L,r . Regev shows that there is a reduction from GapSVP 100 n ( n ) displaystyle operatorname GapSVP _100sqrt ngamma (n) to D G S n ( n ) / ( L ) displaystyle DGS_sqrt ngamma (n)/lambda (L) for any function ( n ) 1 displaystyle gamma (n)geq 1 . Regev then shows that there exists an efficient quantum algorithm for D G S 2 n ( L ) / displaystyle DGS_sqrt 2neta _varepsilon (L)/alpha  given access to an oracle for L W E q , displaystyle mathrm LWE _q,Psi _alpha  for integer q displaystyle q and ( 0 , 1 ) displaystyle alpha in (0,1) such that q  2 n displaystyle alpha q2sqrt n . This implies the hardness for LWE. Although the proof of this assertion works for any q displaystyle q , for creating a cryptosystem, the modulus q displaystyle q has to be polynomial in n displaystyle n . Peikerts result Peikert proves that there is a probabilistic polynomial time reduction from the GapSVP , displaystyle operatorname GapSVP _zeta ,gamma  problem in the worst case to solving L W E q , displaystyle mathrm LWE _q,Psi _alpha  using poly ( n ) displaystyle operatorname poly (n) samples for parameters ( 0 , 1 ) displaystyle alpha in (0,1) , ( n ) n / ( log n ) displaystyle gamma (n)geq n/(alpha sqrt log n) , ( n ) ( n ) displaystyle zeta (n)geq gamma (n) and q ( / n ) log n ) displaystyle qgeq (zeta /sqrt n)omega sqrt log n) . Use in cryptography The LWE problem serves as a versatile problem used in construction of several cryptosystems. In 2005, Regev showed that the decision version of LWE is hard assuming quantum hardness of the lattice problems G a p S V P displaystyle mathrm GapSVP _gamma  (for displaystyle gamma  as above) and S I V P t displaystyle mathrm SIVP _t with  ) ). In 2009, Peikert proved a similar result assuming only the classical hardness of the related problem G a p S V P , displaystyle mathrm GapSVP _zeta ,gamma  . The disadvantage of Peikerts result is that it bases itself on a non-standard version of an easier (when compared to SIVP) problem GapSVP. Public-key cryptosystem Regev proposed a public-key cryptosystem based on the hardness of the LWE problem. The cryptosystem as well as the proof of security and correctness are completely classical. The system is characterized by m , q displaystyle m,q and a probability distribution displaystyle chi  on T displaystyle mathbb T  . The setting of the parameters used in proofs of correctness and security is q 2 displaystyle qgeq 2 , usually a prime number between n 2 displaystyle n2 and 2 n 2 displaystyle 2n2 .  . The cryptosystem is then defined by Private key Private key is an s Z q n displaystyle mathbf s in mathbb Z _qn chosen uniformly at random. Public key Choose m displaystyle m vectors a 1 , , a m Z q n displaystyle mathbf a _1,ldots ,mathbf a _min mathbb Z _qn uniformly and independently. Choose error offsets e 1 , , e m T displaystyle e_1,ldots ,e_min mathbb T  independently according to displaystyle chi  . The public key consists of ( a i , b . The proof of correctness follows from choice of parameters and some probability analysis. The proof of security is by reduction to the decision version of LWE an algorithm for distinguishing between encryptions (with above parameters) of 0 displaystyle 0 and 1 displaystyle 1 can be used to distinguish between A s , displaystyle A_s,chi  and the uniform distribution over Z q n T displaystyle mathbb Z _qntimes mathbb T  CCA-secure cryptosystem Peikert proposed a system that is secure even against any chosen-ciphertext attack. Key exchange The idea of using LWE and Ring LWE for key exchange was proposed and filed at the University of Cincinnati in 2011 by Jintai Ding. The idea comes from the associativity of matrix multiplications, and the errors are used to provide the security. The paper appeared in 2012 after a provisional patent application was filed in 2012. The security of the protocol is proven based on the hardness of solving the LWE problem. In 2014, Peikert presented a key-transport scheme following the same basic idea of Dings, where the new idea of sending an additional 1-bit signal for rounding in Dings construction is also used. The new hope implementation selected for Googles post-quantum experiment, uses Peikerts scheme with variation in the error distribution. Ring learning with errors signature (RLWE-SIG) Main article Ring learning with errors signature A RLWE version of the classic Feige Fiat Shamir Identification protocol was created and converted to a digital signature in 2011 by Lyubashevsky. The details of this signature were extended in 2012 by Gunesyu, Lyubashevsky, and Popplemann in 2012 and published in their paper Practical Lattice Based Cryptography A Signature Scheme for Embedded Systems. These papers laid the groundwork for a variety of recent signature algorithms some based directly on the ring learning with errors problem and some which are not tied to the same hard RLWE problems. See also Post-quantum cryptography Ring learning with errors Lattice-based cryptography Ring learning with errors key exchange Short integer solution (SIS) problem Kyber Title Life-time of correlation URL https//en.wikipedia.org/wiki/Life-time_of_correlation Content In probability theory and related fields, the life-time of correlation measures the timespan over which there is appreciable autocorrelation or cross-correlation in stochastic processes. Definition The correlation coefficient , expressed as an autocorrelation function or cross-correlation function, depends on the lag-time between the times being considered. Typically such functions, (t), decay to zero with increasing lag-time, but they can assume values across all levels of correlations strong and weak, and positive and negative as in the table. The life-time of a correlation is defined as the length of time when the correlation coefficient is at the strong level. The durability of correlation is determined by signal (the strong level of correlation is separated from weak and negative levels). The mean life-time of correlation could measure how the durability of correlation depends on the window width size (the window is the length of time series used to calculate correlation). Title Linear predictor function URL https//en.wikipedia.org/wiki/Linear_predictor_function Content In statistics and in machine learning, a linear predictor function is a linear function (linear combination) of a set of coefficients and explanatory variables (independent variables), whose value is used to predict the outcome of a dependent variable. This sort of function usually comes in linear regression, where the coefficients are called regression coefficients. However, they also occur in various types of linear classifiers (e.g. logistic regression, perceptrons, support vector machines, and linear discriminant analysis), as well as in various other models, such as principal component analysis and factor analysis. In many of these models, the coefficients are referred to as weights. Definition The basic form of a linear predictor function f ( i ) displaystyle f(i) for data point i (consisting of p explanatory variables), for , ..., n, is f ( i )  0  1 x i 1   p x i p , displaystyle f(i)beta _0beta _1x_cdots beta _px_ip, where x i k displaystyle x_ik , for , ..., p, is the value of the k-th explanatory variable for data point i, and 0 , , p displaystyle beta _0,ldots ,beta _p are the coefficients (regression coefficients, weights, etc.) indicating the relative effect of a particular explanatory variable on the outcome. Notations It is common to write the predictor function in a more compact form as follows The coefficients 0, 1, ..., p are grouped into a single vector of size p  1. For each data point i, an additional explanatory pseudo-variable is added, with a fixed value of 1, corresponding to the intercept coefficient 0. The resulting explanatory variables ( 1), , ..., xip are then grouped into a single vector xi of size p  1. Vector Notation This makes it possible to write the linear predictor function as follows f ( i )  x i displaystyle f(i)boldsymbol beta cdot mathbf x _i using the notation for a dot product between two vectors. Matrix Notation An equivalent form using matrix notation is as follows f ( i )  T x . Linear regression An example of the usage of a linear predictor function is in linear regression, where each data point is associated with a continuous outcome yi, and the relationship written y . Stacking In some models (standard linear regression, in particular), the equations for each of the data points , ..., n are stacked together and written in vector form as  ) . displaystyle mathbf y beginpmatrixy_1y_2vdots y_nendpmatrix,quad mathbf X beginpmatrixmathbf x _1mathbf x _2vdots mathbf x _nendpmatrixbeginpmatrixx_11cdots x_1px_21cdots x_2pvdots ddots vdots x_cdots x_npendpmatrix,quad boldsymbol beta beginpmatrixbeta _1vdots beta _pendpmatrix,quad boldsymbol varepsilon beginpmatrixvarepsilon _1varepsilon _2vdots varepsilon _nendpmatrix. The matrix X is known as the design matrix and encodes all known information about the independent variables. The variables i displaystyle varepsilon _i are random variables, which in standard linear regression are distributed according to a standard normal distribution they express the influence of any unknown factors on the outcome. This makes it possible to find optimal coefficients through the method of least squares using simple matrix operations. In particular, the optimal coefficients  displaystyle boldsymbol hat beta  as estimated by least squares can be written as follows   ( X T X ) 1 X T y . displaystyle boldsymbol hat beta (Xmathrm T X)-1Xmathrm T mathbf y . The matrix ( X T X ) 1 X T displaystyle (Xmathrm T X)-1Xmathrm T  is known as the Moore Penrose pseudoinverse of X. The use of the matrix inverse in this formula requires that X is of full rank, i.e. there is not perfect multicollinearity among different explanatory variables (i.e. no explanatory variable can be perfectly predicted from the others). In such cases, the singular value decomposition can be used to compute the pseudoinverse. Preprocessing of explanatory variables When a fixed set of nonlinear functions are used to transform the value(s) of a data point, these functions are known as basis functions. An example is polynomial regression, which uses a linear predictor function to fit an arbitrary degree polynomial relationship (up to a given order) between two sets of data points (i.e. a single real-valued explanatory variable and a related real-valued dependent variable), by adding multiple explanatory variables corresponding to various powers of the existing explanatory variable. Mathematically, the form looks like this y  . displaystyle y_ibeta _0beta _1x_ibeta _2x_i2cdots beta _px_ip. In this case, for each data point i, a set of explanatory variables is created as follows ( x i 1  x i , x i 2  x i 2 , , x i . The basis functions in this example would be ( x )  ( 1 ( x ) , 2 ( x ) , , p ( x ) )  ( x , x 2 , , x p ) . displaystyle boldsymbol phi (x)(phi _1(x),phi _2(x),ldots ,phi _p(x))(x,x2,ldots ,xp). This example shows that a linear predictor function can actually be much more powerful than it first appears It only really needs to be linear in the coefficients. All sorts of non-linear functions of the explanatory variables can be fit by the model. There is no particular need for the inputs to basis functions to be univariate or single-dimensional (or their outputs, for that matter, although in such a case, a K-dimensional output value is likely to be treated as K separate scalar-output basis functions). An example of this is radial basis functions (RBFs), which compute some transformed version of the distance to some fixed point ( x  c )  (   x c   )  ( ( x 1 c 1 ) 2   ( x K c K ) 2 ) displaystyle phi (mathbf x mathbf c )phi (mathbf x -mathbf c )phi (sqrt (x_1-c_1)2ldots (x_K-c_K)2) An example is the Gaussian RBF, which has the same functional form as the normal distribution ( x  c )  e b   x c   2 displaystyle phi (mathbf x mathbf c )e-bmathbf x -mathbf c 2 which drops off rapidly as the distance from c increases. A possible usage of RBFs is to create one for every observed data point. This means that the result of an RBF applied to a new data point will be close to 0 unless the new point is near to the point around which the RBF was applied. That is, the application of the radial basis functions will pick out the nearest point, and its regression coefficient will dominate. The result will be a form of nearest neighbor interpolation, where predictions are made by simply using the prediction of the nearest observed data point, possibly interpolating between multiple nearby data points when they are all similar distances away. This type of nearest neighbor method for prediction is often considered diametrically opposed to the type of prediction used in standard linear regression But in fact, the transformations that can be applied to the explanatory variables in a linear predictor function are so powerful that even the nearest neighbor method can be implemented as a type of linear regression. It is even possible to fit some functions that appear non-linear in the coefficients by transforming the coefficients into new coefficients that do appear linear. For example, a function of the form a  b 2 x i 1  c x i 2 displaystyle ab2x_sqrt cx_ for coefficients a , b , c displaystyle a,b,c could be transformed into the appropriate linear function by applying the substitutions . Linear regression and similar techniques could be applied and will often still find the optimal coefficients, but their error estimates and such will be wrong. The explanatory variables may be of any type real-valued, binary, categorical, etc. The main distinction is between continuous variables (e.g. income, age, blood pressure, etc.) and discrete variables (e.g. sex, race, political party, etc.). Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), i.e. separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning variable does have the given value and a 0 meaning variable does not have the given value. For example, a four-way discrete variable of blood type with the possible values A, B, AB, O would be converted to separate two-way dummy variables, is-A, is-B, is-AB, is-O, where only one of them has the value 1 and all the rest have the value 0. This allows for separate regression coefficients to be matched for each possible value of the discrete variable. Note that, for K categories, not all K dummy variables are independent of each other. For example, in the above blood type example, only three of the four dummy variables are independent, in the sense that once the values of three of the variables are known, the fourth is automatically determined. Thus, its really only necessary to encode three of the four possibilities as dummy variables, and in fact if all four possibilities are encoded, the overall model becomes non-identifiable. This causes problems for a number of methods, such as the simple closed-form solution used in linear regression. The solution is either to avoid such cases by eliminating one of the dummy variables, and/or introduce a regularization constraint (which necessitates a more powerful, typically iterative, method for finding the optimal coefficients). See also Linear model Linear regression Title Linear separability URL https//en.wikipedia.org/wiki/Linear_separability Content In Euclidean geometry, linear separability is a property of two sets of points. This is most easily visualized in two dimensions (the Euclidean plane) by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side. This idea immediately generalizes to higher-dimensional Euclidean spaces if the line is replaced by a hyperplane. The problem of determining if a pair of sets is linearly separable and finding a separating hyperplane if they are, arises in several areas. In statistics and machine learning, classifying certain types of data is a problem for which good algorithms exist that are based on this concept. Mathematical definition Let X 0 displaystyle X_0 and X 1 displaystyle X_1 be two sets of points in an n-dimensional Euclidean space. Then X 0 displaystyle X_0 and X 1 displaystyle X_1 are linearly separable if there exist n  1 real numbers w 1 , w 2 , . . , w n , k displaystyle w_1,w_2,..,w_n,k , such that every point x X 0 displaystyle xin X_0 satisfies  . Equivalently, two sets are linearly separable precisely when their respective convex hulls are disjoint (colloquially, do not overlap). In simple 2D, it can also be imagined that the set of points under a linear transformation collapses into a line, on which there exists a value, k, greater than which one set of points will fall into, and lesser than which the other set of points fall. Examples Three non-collinear points in two classes ( and -) are always linearly separable in two dimensions. This is illustrated by the three examples in the following figure (the all  case is not shown, but is similar to the all - case) However, not all sets of four points, no three collinear, are linearly separable in two dimensions. The following example would need two straight lines and thus is not linearly separable Notice that three points which are collinear and of the form   are also not linearly separable. Number of linear separations Let T ( N , K ) displaystyle T(N,K) be the number of ways to linearly separate N points (in general position) in K dimensions, then T ( N , K )   2 N K N 2 . When K is large, T ( N , K ) / 2 N displaystyle T(N,K)/2N is very close to one when N 2 K displaystyle Nleq 2K , but very close to zero when N  2 K displaystyle N2K . In words, one perceptron unit can almost certainly memorize a random assignment of binary labels on N points when N 2 K displaystyle Nleq 2K , but almost certainly not when N  2 K displaystyle N2K . Linear separability of Boolean functions in n variables A Boolean function in n variables can be thought of as an assignment of 0 or 1 to each vertex of a Boolean hypercube in n dimensions. This gives a natural division of the vertices into two sets. The Boolean function is said to be linearly separable provided these two sets of points are linearly separable. The number of distinct Boolean functions is 2 2 n displaystyle 22n where n is the number of variables passed into the function. Such functions are also called linear threshold logic, or perceptrons. The classical theory is summarized in, as Knuth claims. The value is only known exactly up to ) . It is co-NP-complete to decide whether a Boolean function given in disjunctive or conjunctive normal form is linearly separable. Support vector machines Classifying data is a common task in machine learning. Suppose some data points, each belonging to one of two sets, are given and we wish to create a model that will decide which set a new data point will be in. In the case of support vector machines, a data point is viewed as a p-dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p 1)-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify (separate) the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two sets. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum margin classifier. More formally, given some training data D displaystyle mathcal D , a set of n points of the form . Each x i displaystyle mathbf x _i is a p-dimensional real vector. We want to find the maximum-margin hyperplane that divides the points having y  . Any hyperplane can be written as the set of points x displaystyle mathbf x  satisfying w x . The parameter b w displaystyle tfrac bmathbf w  determines the offset of the hyperplane from the origin along the normal vector w displaystyle mathbf w  . If the training data are linearly separable, we can select two hyperplanes in such a way that they separate the data and there are no points between them, and then try to maximize their distance. See also Clustering (statistics) Hyperplane separation theorem Kirchbergers theorem Perceptron Vapnik Chervonenkis dimension Title Local case-control sampling URL https//en.wikipedia.org/wiki/Local_case-control_sampling Content In machine learning, local case-control sampling is an algorithm used to reduce the complexity of training a logistic regression classifier. The algorithm reduces the training complexity by selecting a small subsample of the original dataset for training. It assumes the availability of a (unreliable) pilot estimation of the parameters. It then performs a single pass over the entire dataset using the pilot estimation to identify the most surprising samples. In practice, the pilot may come from prior knowledge or training using a subsample of the dataset. The algorithm is most effective when the underlying dataset is imbalanced. It exploits the structures of conditional imbalanced datasets more efficiently than alternative methods, such as case control sampling and weighted case control sampling. Imbalanced datasets In classification, a dataset is a set of N data points ( x i , y i ) . Intuitively, a dataset is imbalanced when certain important statistical patterns are rare. The lack of observations of certain patterns does not always imply their irrelevance. For example, in medical studies of rare diseases, the small number of infected patients (cases) conveys the most valuable information for diagnosis and treatments. Formally, an imbalanced dataset exhibits one or more of the following properties Marginal Imbalance. A dataset is marginally imbalanced if one class is rare compared to the other class. In other words, P (  . Conditional Imbalance. A dataset is conditionally imbalanced when it is easy to predict the correct labels in most cases. For example, if X  0 , 1  displaystyle Xin 0,1 , the dataset is conditionally imbalanced if P (  . Algorithm outline In logistic regression, given the ) . The local-case control sampling algorithm assumes the availability of a pilot model   (  ,  ) displaystyle tilde theta (tilde alpha ,tilde beta ) . Given the pilot model, the algorithm performs a single pass over the entire dataset to select the subset of samples to include in training the logistic regression model. For a sample ( x , y ) displaystyle (x,y) , define the acceptance probability as a ( x , y )   y p   ( x )  displaystyle a(x,y)y-tilde p_tilde theta (x) . The algorithm proceeds as follows Generate independent z i Bernoulli ( a ( x i , y i ) ) displaystyle z_isim textBernoulli(a(x_i,y_i)) for i  1 , , N  displaystyle iin 1,ldots ,N . Fit a logistic regression model to the subsample ) . The output model is   (  ,  ) displaystyle hat theta (hat alpha ,hat beta ) , where   S   displaystyle hat alpha leftarrow hat alpha _Stilde alpha  and   S   displaystyle hat beta leftarrow hat beta _Stilde beta  . The algorithm can be understood as selecting samples that surprises the pilot model. Intuitively these samples are closer to the decision boundary of the classifier and is thus more informative. Obtaining the pilot model In practice, for cases where a pilot model is naturally available, the algorithm can be applied directly to reduce the complexity of training. In cases where a natural pilot is nonexistent, an estimate using a subsample selected through another sampling technique can be used instead. In the original paper describing the algorithm, the authors propose to use weighted case-control sampling with half the assigned sampling budget. For example, if the objective is to use a subsample with size . Larger or smaller sample size It is possible to control the sample size by multiplying the acceptance probability with a constant c displaystyle c . For a larger sample size, pick c  1 displaystyle c1 and adjust the acceptance probability to min ( c a ( x i , y i ) , 1 ) displaystyle min(ca(x_i,y_i),1) . For a smaller sample size, the same strategy applies. In cases where the number of samples desired is precise, a convenient alternative method is to uniformly downsample from a larger subsample selected by local case-control sampling. Properties The algorithm has the following properties. When the pilot is consistent, the estimates using the samples from local case-control sampling is consistent even under model misspecification. If the model is correct then the algorithm has exactly twice the asymptotic variance of logistic regression on the full data set. For a larger sample size with c  1 displaystyle c1 , the factor 2 is improved to 1  1 c displaystyle 1frac 1c . Title Lottery ticket hypothesis URL https//en.wikipedia.org/wiki/Lottery_ticket_hypothesis Content In machine learning, the lottery ticket hypothesis is that artificial neural networks with random weights can contain subnetworks which entirely by chance can be tuned to a similar level of performance as the complete network. The term derived from considering the tunable subnetwork as the equivalent of a winning lottery ticket the chance of any given ticket winning is tiny, but if you buy enough of them you are certain to win, and the number of possible subnetworks increases exponentially as the power set of the set of connections, making the number of possible subnetworks astronomical for any reasonsably large network. Malach et. al. have proved a stronger version of the hypothesis, which is that a sufficiently overparameterized untuned network will typically contain a subnetwork that is already an approximation to the given goal, even before tuning. A similar result has been proven for the special case of convolutional neural networks. See also Grokking (machine learning) Pruning (artificial neural network) Title Lyra (codec) URL https//en.wikipedia.org/wiki/Lyra_(codec) Content Lyra is a lossy audio codec developed by Google that is designed for compressing speech at very low bitrates. Unlike most other audio formats, it compresses data using a machine learning-based algorithm. Features The Lyra codec is designed to transmit speech in real-time when bandwidth is severely restricted, such as over slow or unreliable network connections. It runs at fixed bitrates of 3.2, 6, and 9 kbit/s and it is intended to provide better quality than codecs that use traditional waveform-based algorithms at similar bitrates. Instead, compression is achieved via a machine learning algorithm that encodes the input with feature extraction, and then reconstructs an approximation of the original using a generative model. This model was trained on thousands of hours of speech recorded in over 70 languages to function with various speakers. Because generative models are more computationally complex than traditional codecs, a simple model that processes different frequency ranges in parallel is used to obtain acceptable performance. Lyra imposes 20 ms of latency due to its frame size. Googles reference implementation is available for Android and Linux. Quality Lyras initial version performed significantly better than traditional codecs at similar bitrates. Ian Buckley at MakeUseOf said, It succeeds in creating almost eerie levels of audio reproduction with bitrates as low as 3 kbps. Google claims that it reproduces natural-sounding speech, and that Lyra at 3 kbit/s beats Opus at 8 kbit/s. Tsahi Levent-Levi writes that Satin, Microsofts AI-based codec, outperforms it at higher bitrates. History In December 2017, Google researchers published a preprint paper on replacing the Codec 2 decoder with a WaveNet neural network. They found that a neural network is able to extrapolate features of the voice not described in the Codec 2 bitstream and give better audio quality, and that the use of conventional features makes the neural network calculation simpler compared to a purely waveform-based network. Lyra version 1 would reuse this overall framework of feature extraction, quantization, and neural synthesis. Lyra was first announced in February 2021, and in April, Google released the source code of their reference implementation. The initial version had a fixed bitrate of 3 kbit/s and around 90 ms latency. The encoder calculates a log mel spectrogram and performs vector quantization to store the spectrogram in a data stream. The decoder is a WaveNet neural network that takes the spectrogram and reconstructs the input audio. A second version (/1.2.0), released in September 2022, improved sound quality, latency, and performance, and permitted multiple bitrates. uses a SoundStream structure where both the encoder and decoder are neural networks, a kind of autoencoder. A residual vector quantizer is used to turn the feature values into transferrable data. Support Implementations Googles implementation is available on GitHub under the Apache License. Written in C, it is optimized for 64-bit ARM but also runs on , on either Android or Linux. Applications Google Meet uses Lyra to transmit sound for video chats when bandwidth is limited. References External links Lyra A New Very Low-Bitrate Codec for Speech Compression Google blog post with a demonstration comparing codecs See also Satin (codec), an AI-based codec developed by Microsoft Comparison of audio coding formats Speech coding Videotelephony Title M-theory (learning framework) URL https//en.wikipedia.org/wiki/M-theory_(learning_framework) Content In machine learning and computer vision, M-theory is a learning framework inspired by feed-forward processing in the ventral stream of visual cortex and originally developed for recognition and classification of objects in visual scenes. M-theory was later applied to other areas, such as speech recognition. On certain image recognition tasks, algorithms based on a specific instantiation of M-theory, HMAX, achieved human-level performance. The core principle of M-theory is extracting representations invariant under various transformations of images (translation, scale, 2D and 3D rotation and others). In contrast with other approaches using invariant representations, in M-theory they are not hardcoded into the algorithms, but learned. M-theory also shares some principles with compressed sensing. The theory proposes multilayered hierarchical learning architecture, similar to that of visual cortex. Intuition Invariant representations A great challenge in visual recognition tasks is that the same object can be seen in a variety of conditions. It can be seen from different distances, different viewpoints, under different lighting, partially occluded, etc. In addition, for particular classes objects, such as faces, highly complex specific transformations may be relevant, such as changing facial expressions. For learning to recognize images, it is greatly beneficial to factor out these variations. It results in much simpler classification problem and, consequently, in great reduction of sample complexity of the model. A simple computational experiment illustrates this idea. Two instances of a classifier were trained to distinguish images of planes from those of cars. For training and testing of the first instance, images with arbitrary viewpoints were used. Another instance received only images seen from a particular viewpoint, which was equivalent to training and testing the system on invariant representation of the images. One can see that the second classifier performed quite well even after receiving a single example from each category, while performance of the first classifier was close to random guess even after seeing 20 examples. Invariant representations has been incorporated into several learning architectures, such as neocognitrons. Most of these architectures, however, provided invariance through custom-designed features or properties of architecture itself. While it helps to take into account some sorts of transformations, such as translations, it is very nontrivial to accommodate for other sorts of transformations, such as 3D rotations and changing facial expressions. M-theory provides a framework of how such transformations can be learned. In addition to higher flexibility, this theory also suggests how human brain may have similar capabilities. Templates Another core idea of M-theory is close in spirit to ideas from the field of compressed sensing. An implication from Johnson Lindenstrauss lemma says that a particular number of images can be embedded into a low-dimensional feature space with the same distances between images by using random projections. This result suggests that dot product between the observed image and some other image stored in memory, called template, can be used as a feature helping to distinguish the image from other images. The template need not to be anyhow related to the image, it could be chosen randomly. Combining templates and invariant representations The two ideas outlined in previous sections can be brought together to construct a framework for learning invariant representations. The key observation is how dot product between image I displaystyle I and a template t displaystyle t behaves when image is transformed (by such transformations as translations, rotations, scales, etc.). If transformation g displaystyle g is a member of a unitary group of transformations, then the following holds g I , . For instance, for image rotated by 90 degrees, the inversely transformed template would be rotated by 90 degrees. Consider the set of dot products of an image I displaystyle I to all possible transformations of template  I , g t g G  displaystyle lbrace langle I,gprime trangle mid gprime in Grbrace  . If one applies a transformation g displaystyle g to I displaystyle I , the set would become  g I , g t g G  displaystyle lbrace langle gI,gprime trangle mid gprime in Grbrace  . But because of the property (1), this is equal to  I , g 1 g t g G  displaystyle lbrace langle I,g-1gprime trangle mid gprime in Grbrace  . The set  g 1 g g G  displaystyle lbrace g-1gprime mid gprime in Grbrace  is equal to just the set of all elements in G displaystyle G . To see this, note that every g 1 g displaystyle g-1gprime  is in G displaystyle G due to the closure property of groups, and for every g displaystyle gprime prime  in G there exist its prototype g displaystyle gprime  such as   ). Thus,  I , g 1 g t g G    I , g t g G  displaystyle lbrace langle I,g-1gprime trangle mid gprime in G  . One can see that the set of dot products remains the same despite that a transformation was applied to the image! This set by itself may serve as a (very cumbersome) invariant representation of an image. More practical representations can be derived from it. In the introductory section, it was claimed that M-theory allows to learn invariant representations. This is because templates and their transformed versions can be learned from visual experience by exposing the system to sequences of transformations of objects. It is plausible that similar visual experiences occur in early period of human life, for instance when infants twiddle toys in their hands. Because templates may be totally unrelated to images that the system later will try to classify, memories of these visual experiences may serve as a basis for recognizing many different kinds of objects in later life. However, as it is shown later, for some kinds of transformations, specific templates are needed. Theoretical aspects From orbits to distribution measures To implement the ideas described in previous sections, one need to know how to derive a computationally efficient invariant representation of an image. Such unique representation for each image can be characterized as it appears by a set of one-dimensional probability distributions (empirical distributions of the dot-products between image and a set of templates stored during unsupervised learning). These probability distributions in their turn can be described by either histograms or a set of statistical moments of it, as it will be shown below. Orbit O I displaystyle O_I is a set of images g I displaystyle gI generated from a single image I displaystyle I under the action of the group G , g G displaystyle G,forall gin G . In other words, images of an object and of its transformations correspond to an orbit O I displaystyle O_I . If two orbits have a point in common they are identical everywhere, i.e. an orbit is an invariant and unique representation of an image. So, two images are called equivalent when they belong to the same orbit I I displaystyle Isim Iprime  if g G displaystyle exists gin G such that  . Conversely, two orbits are different if none of the images in one orbit coincide with any image in the other. A natural question arises how can one compare two orbits? There are several possible approaches. One of them employs the fact that intuitively two empirical orbits are the same irrespective of the ordering of their points. Thus, one can consider a probability distribution P I displaystyle P_I induced by the groups action on images I displaystyle I ( g I displaystyle gI can be seen as a realization of a random variable). This probability distribution P I displaystyle P_I can be almost uniquely characterized by K displaystyle K one-dimensional probability distributions P I , t k displaystyle P_langle I,tkrangle  induced by the (one-dimensional) results of projections I , t k displaystyle langle I,tkrangle  , where t k , ). Consider n displaystyle n images X n X displaystyle X_nin X . Let K 2 c 2 log n displaystyle Kgeq frac 2cvarepsilon 2log frac ndelta  , where c displaystyle c is a universal constant. Then  d ( P I , P I ) d K ( P I , P I )  , displaystyle d(P_I,P_Iprime )-dK(P_I,P_Iprime )leq varepsilon , with probability 1 2 displaystyle 1-delta 2 , for all I , I displaystyle I,Iprime  displaystyle in  X n displaystyle X_n . This result (informally) says that an approximately invariant and unique representation of an image I displaystyle I can be obtained from the estimates of K displaystyle K 1-D probability distributions P I , t k displaystyle P_langle I,tkrangle  for  . The number K displaystyle K of projections needed to discriminate n displaystyle n orbits, induced by n displaystyle n images, up to precision displaystyle varepsilon  (and with confidence 1 2 displaystyle 1-delta 2 ) is K 2 c 2 log n displaystyle Kgeq frac 2cvarepsilon 2log frac ndelta  , where c displaystyle c is a universal constant. To classify an image, the following recipe can be used Memorize a set of images/objects called templates Memorize observed transformations for each template Compute dot products of its transformations with image Compute histogram of the resulting values, called signature of the image Compare the obtained histogram with signatures stored in memory. Estimates of such one-dimensional probability density functions (PDFs) P I , t k displaystyle P_langle I,tkrangle  can be written in terms of histograms as n k ( I )  1 /  G  . These 1-D probability distributions can be characterized with N-bin histograms or set of statistical moments. For example, HMAX represents an architecture in which pooling is done with a max operation. Non-compact groups of transformations In the recipe for image classification, groups of transformations are approximated with finite number of transformations. Such approximation is possible only when the group is compact. Such groups as all translations and all scalings of the image are not compact, as they allow arbitrarily big transformations. However, they are locally compact. For locally compact groups, invariance is achievable within certain range of transformations. Assume that G 0 displaystyle G_0 is a subset of transformations from G displaystyle G for which the transformed patterns exist in memory. For an image I displaystyle I and template t k displaystyle t_k , assume that I , g 1 t k displaystyle langle I,g-1t_krangle  is equal to zero everywhere except some subset of G 0 displaystyle G_0 . This subset is called support of I , g 1 t k displaystyle langle I,g-1t_krangle  and denoted as supp ( I , g 1 t k ) displaystyle operatorname supp (langle I,g-1t_krangle ) . It can be proven that if for a transformation g displaystyle gprime  , support set will also lie within g G 0 displaystyle gprime G_0 , then signature of I displaystyle I is invariant with respect to g displaystyle gprime  . This theorem determines the range of transformations for which invariance is guaranteed to hold. One can see that the smaller is supp ( I , g 1 t k ) displaystyle operatorname supp (langle I,g-1t_krangle ) , the larger is the range of transformations for which invariance is guaranteed to hold. It means that for a group that is only locally compact, not all templates would work equally well anymore. Preferable templates are those with a reasonably small supp ( g I , t k ) displaystyle operatorname supp (langle gI,t_krangle ) for a generic image. This property is called localization templates are sensitive only to images within a small range of transformations. Although minimizing supp ( g I , t k ) displaystyle operatorname supp (langle gI,t_krangle ) is not absolutely necessary for the system to work, it improves approximation of invariance. Requiring localization simultaneously for translation and scale yields a very specific kind of templates Gabor functions. The desirability of custom templates for non-compact group is in conflict with the principle of learning invariant representations. However, for certain kinds of regularly encountered image transformations, templates might be the result of evolutionary adaptations. Neurobiological data suggests that there is Gabor-like tuning in the first layer of visual cortex. The optimality of Gabor templates for translations and scales is a possible explanation of this phenomenon. Non-group transformations Many interesting transformations of images do not form groups. For instance, transformations of images associated with 3D rotation of corresponding 3D object do not form a group, because it is impossible to define an inverse transformation (two objects may looks the same from one angle but different from another angle). However, approximate invariance is still achievable even for non-group transformations, if localization condition for templates holds and transformation can be locally linearized. As it was said in the previous section, for specific case of translations and scaling, localization condition can be satisfied by use of generic Gabor templates. However, for general case (non-group) transformation, localization condition can be satisfied only for specific class of objects. More specifically, in order to satisfy the condition, templates must be similar to the objects one would like to recognize. For instance, if one would like to build a system to recognize 3D rotated faces, one need to use other 3D rotated faces as templates. This may explain the existence of such specialized modules in the brain as one responsible for face recognition. Even with custom templates, a noise-like encoding of images and templates is necessary for localization. It can be naturally achieved if the non-group transformation is processed on any layer other than the first in hierarchical recognition architecture. Hierarchical architectures The previous section suggests one motivation for hierarchical image recognition architectures. However, they have other benefits as well. Firstly, hierarchical architectures best accomplish the goal of parsing a complex visual scene with many objects consisting of many parts, whose relative position may greatly vary. In this case, different elements of the system must react to different objects and parts. In hierarchical architectures, representations of parts at different levels of embedding hierarchy can be stored at different layers of hierarchy. Secondly, hierarchical architectures which have invariant representations for parts of objects may facilitate learning of complex compositional concepts. This facilitation may happen through reusing of learned representations of parts that were constructed before in process of learning of other concepts. As a result, sample complexity of learning compositional concepts may be greatly reduced. Finally, hierarchical architectures have better tolerance to clutter. Clutter problem arises when the target object is in front of a non-uniform background, which functions as a distractor for the visual task. Hierarchical architecture provides signatures for parts of target objects, which do not include parts of background and are not affected by background variations. In hierarchical architectures, one layer is not necessarily invariant to all transformations that are handled by the hierarchy as a whole. Some transformations may pass through that layer to upper layers, as in the case of non-group transformations described in the previous section. For other transformations, an element of the layer may produce invariant representations only within small range of transformations. For instance, elements of the lower layers in hierarchy have small visual field and thus can handle only a small range of translation. For such transformations, the layer should provide covariant rather than invariant, signatures. The property of covariance can be written as distr ( l ( g I ) , l ( t ) )  distr ( l ( I ) , l ( g 1 t ) ) displaystyle operatorname distr (langle mu _l(gI),mu _l(t)rangle )operatorname distr (langle mu _l(I),mu _l(g-1t)rangle ) , where l displaystyle l is a layer, l ( I ) displaystyle mu _l(I) is the signature of image on that layer, and distr displaystyle operatorname distr  stands for distribution of values of the expression for all g G displaystyle gin G . Relation to biology M-theory is based on a quantitative theory of the ventral stream of visual cortex. Understanding how visual cortex works in object recognition is still a challenging task for neuroscience. Humans and primates are able to memorize and recognize objects after seeing just couple of examples unlike any state-of-the art machine vision systems that usually require a lot of data in order to recognize objects. Prior to the use of visual neuroscience in computer vision has been limited to early vision for deriving stereo algorithms (e.g.,) and to justify the use of DoG (derivative-of-Gaussian) filters and more recently of Gabor filters. No real attention has been given to biologically plausible features of higher complexity. While mainstream computer vision has always been inspired and challenged by human vision, it seems to have never advanced past the very first stages of processing in the simple cells in and . Although some of the systems inspired to various degrees by neuroscience, have been tested on at least some natural images, neurobiological models of object recognition in cortex have not yet been extended to deal with real-world image databases. M-theory learning framework employs a novel hypothesis about the main computational function of the ventral stream the representation of new objects/images in terms of a signature, which is invariant to transformations learned during visual experience. This allows recognition from very few labeled examples in the limit, just one. Neuroscience suggests that natural functionals for a neuron to compute is a high-dimensional dot product between an image patch and another image patch (called template) which is stored in terms of synaptic weights (synapses per neuron). The standard computational model of a neuron is based on a dot product and a threshold. Another important feature of the visual cortex is that it consists of simple and complex cells. This idea was originally proposed by Hubel and Wiesel. M-theory employs this idea. Simple cells compute dot products of an image and transformations of templates I , g i t k displaystyle langle I,g_itkrangle  for ). Complex cells are responsible for pooling and computing empirical histograms or statistical moments of it. The following formula for constructing histogram can be computed by neurons 1  G  . Applications Applications to computer vision In authors applied M-theory to unconstrained face recognition in natural photographs. Unlike the DAR (detection, alignment, and recognition) method, which handles clutter by detecting objects and cropping closely around them so that very little background remains, this approach accomplishes detection and alignment implicitly by storing transformations of training images (templates) rather than explicitly detecting and aligning or cropping faces at test time. This system is built according to the principles of a recent theory of invariance in hierarchical networks and can evade the clutter problem generally problematic for feedforward systems. The resulting end-to-end system achieves a drastic improvement in the state of the art on this end-to-end task, reaching the same level of performance as the best systems operating on aligned, closely cropped images (no outside training data). It also performs well on two newer datasets, similar to LFW, but more difficult significantly jittered (misaligned) version of LFW and SUFR-W (for example, the models accuracy in the LFW unaligned  no outside data used category is 87.55 1.41 compared to state-of-the-art APEM (adaptive probabilistic elastic matching) 81.70 1.78). The theory was also applied to a range of recognition tasks from invariant single object recognition in clutter to multiclass categorization problems on publicly available data sets (, , MIT-CBCL) and complex (street) scene understanding tasks that requires the recognition of both shape-based as well as texture-based objects (on StreetScenes data set). The approach performs really well It has the capability of learning from only a few training examples and was shown to outperform several more complex state-of-the-art systems constellation models, the hierarchical SVM-based face-detection system. A key element in the approach is a new set of scale and position-tolerant feature detectors, which are biologically plausible and agree quantitatively with the tuning properties of cells along the ventral stream of visual cortex. These features are adaptive to the training set, though we also show that a universal feature set, learned from a set of natural images unrelated to any categorization task, likewise achieves good performance. Applications to speech recognition This theory can also be extended for the speech recognition domain. As an example, in an extension of a theory for unsupervised learning of invariant visual representations to the auditory domain and empirically evaluated its validity for voiced speech sound classification was proposed. Authors empirically demonstrated that a single-layer, phone-level representation, extracted from base speech features, improves segment classification accuracy and decreases the number of training examples in comparison with standard spectral and cepstral features for an acoustic classification task on TIMIT dataset. Title Machine Learning (journal) URL https//en.wikipedia.org/wiki/Machine_Learning_(journal) Content Machine Learning is a peer-reviewed scientific journal, published since 1986. In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet. Following the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review. Selected articles J.R. Quinlan (1986). Induction of Decision Trees. Machine Learning. 1 81 106. doi10.1007/. Nick Littlestone (1988). Learning Quickly When Irrelevant Attributes Abound A New Linear-threshold Algorithm (PDF). Machine Learning. 2 (4) 285 318. doi10.1007/. John R. Anderson and Michael Matessa (1992). Explorations of an Incremental, Bayesian Algorithm for Categorization. Machine Learning. 9 (4) 275 308. doi10.1007/. David Klahr (1994). Children, Adults, and Machines as Discovery Systems. Machine Learning. 14 (3) 313 320. doi10.1007/. Thomas Dean and Dana Angluin and Kenneth Basye and Sean Engelson and Leslie Kaelbling and Evangelos Kokkevis and Oded Maron (1995). Inferring Finite Automata with Stochastic Output Functions and an Application to Map Learning. Machine Learning. 18 81 108. doi10.1007/. Luc De Raedt and Luc Dehaspe (1997). Clausal Discovery. Machine Learning. 26 (2/3) 99 146. doi10.1023/A1007361123060. C. de la Higuera (1997). Characteristic Sets for Grammatical Inference. Machine Learning. 27 1 14. Robert E. Schapire and Yoram Singer (1999). Improved Boosting Algorithms Using Confidence-rated Predictions. Machine Learning. 37 (3) 297 336. doi10.1023/A1007614523901. Robert E. Schapire and Yoram Singer (2000). BoosTexter A Boosting-based System for Text Categorization. Machine Learning. 39 (2/3) 135 168. doi10.1023/A1007649029923. P. Rossmanith and T. Zeugmann (2001). Stochastic Finite Learning of the Pattern Languages. Machine Learning. 44 (1 2) 67 91. doi10.1023/A1010875913047. Parekh, Rajesh Honavar, Vasant (2001). Learning DFA from Simple Examples. Machine Learning. 44 (1/2) 9 35. doi10.1023/A1010822518073. Ayhan Demiriz and Kristin P. Bennett and John Shawe-Taylor (2002). Linear Programming Boosting via Column Generation. Machine Learning. 46 225 254. doi10.1023/A1012470815092. Simon Colton and Stephen Muggleton (2006). Mathematical Applications of Inductive Logic Programming (PDF). Machine Learning. 64 (1 3) 25 64. doi10.1007/-006-8259-x. Will Bridewell and Pat Langley and Ljupco Todorovski and Saso Dzeroski (2008). Inductive Process Modeling. Machine Learning. Stephen Muggleton and Alireza Tamaddoni-Nezhad (2008). QG/GA a stochastic search for Progol. Machine Learning. 70 (2 3) 121 133. doi10.1007/-007-5029-3. Title Machine learning control URL https//en.wikipedia.org/wiki/Machine_learning_control Content Machine learning control (MLC) is a subfield of machine learning, intelligent control, and control theory which aims to solve optimal control problems with machine learning methods. Key applications are complex nonlinear systems for which linear control theory methods are not applicable. Types of problems and tasks Four types of problems are commonly encountered Control parameter identification MLC translates to a parameter identification if the structure of the control law is given but the parameters are unknown. One example is the genetic algorithm for optimizing coefficients of a PID controller or discrete-time optimal control. Control design as regression problem of the first kind MLC approximates a general nonlinear mapping from sensor signals to actuation commands, if the sensor signals and the optimal actuation command are known for every state. One example is the computation of sensor feedback from a known full state feedback. Neural networks are commonly used for such tasks. Control design as regression problem of the second kind MLC may also identify arbitrary nonlinear control laws which minimize the cost function of the plant. In this case, neither a model, the control law structure, nor the optimizing actuation command needs to be known. The optimization is only based on the control performance (cost function) as measured in the plant. Genetic programming is a powerful regression technique for this purpose. Reinforcement learning control The control law may be continually updated over measured performance changes (rewards) using reinforcement learning. Applications MLC has been successfully applied to many nonlinear control problems, exploring unknown and often unexpected actuation mechanisms. Example applications include spacecraft attitude control, thermal control of buildings, feedback control of turbulence, and remotely operated underwater vehicles. Many more engineering MLC application are summarized in the review article of PJ Fleming  RC Purshouse (2002). As is the case for all general nonlinear methods, MLC does not guarantee convergence, optimality, or robustness for a range of operating conditions. See also Reinforcement learning References Title Machine learning in bioinformatics URL https//en.wikipedia.org/wiki/Machine_learning_in_bioinformatics Content Machine learning in bioinformatics is the application of machine learning algorithms to bioinformatics, including genomics, proteomics, microarrays, systems biology, evolution, and text mining. Prior to the emergence of machine learning, bioinformatics algorithms had to be programmed by hand for problems such as protein structure prediction, this proved difficult. Machine learning techniques such as deep learning can learn features of data sets rather than requiring the programmer to define them individually. The algorithm can further learn how to combine low-level features into more abstract features, and so on. This multi-layered approach allows such systems to make sophisticated predictions when appropriately trained. These methods contrast with other computational biology approaches which, while exploiting existing datasets, do not allow the data to be interpreted and analyzed in unanticipated ways. Tasks Machine learning algorithms in bioinformatics can be used for prediction, classification, and feature selection. Methods to achieve this task are varied and span many disciplines most well known among them are machine learning and statistics. Classification and prediction tasks aim at building models that describe and distinguish classes or concepts for future prediction. The differences between them are the following Classification/recognition outputs a categorical class, while prediction outputs a numerical valued feature. The type of algorithm, or process used to build the predictive models from data using analogies, rules, neural networks, probabilities, and/or statistics. Due to the exponential growth of information technologies and applicable models, including artificial intelligence and data mining, in addition to the access ever-more comprehensive data sets, new and better information analysis techniques have been created, based on their ability to learn. Such models allow reach beyond description and provide insights in the form of testable models. Machine learning approaches Artificial neural networks Artificial neural networks in bioinformatics have been used for Comparing and aligning RNA, protein, and DNA sequences. Identification of promoters and finding genes from sequences related to DNA. Interpreting the expression-gene and micro-array data. Identifying the network (regulatory) of genes. Learning evolutionary relationships by constructing phylogenetic trees. Classifying and predicting protein structure. Molecular design and docking Feature engineering The way that features, often vectors in a many-dimensional space, are extracted from the domain data is an important component of learning systems. In genomics, a typical representation of a sequence is a vector of k-mers frequencies, which is a vector of dimension 4 k displaystyle 4k whose entries count the appearance of each subsequence of length k displaystyle k in a given sequence. Since for a value as small as .g. in this case the dimension is 4 12 16 10 6 displaystyle 412approx 16times 106 ), techniques such as principal component analysis are used to project the data to a lower dimensional space, thus selecting a smaller set of features from the sequences. Classification In this type of machine learning task, the output is a discrete variable. One example of this type of task in bioinformatics is labeling new genomic data (such as genomes of unculturable bacteria) based on a model of already labeled data. Hidden Markov models Hidden Markov models (HMMs) are a class of statistical models for sequential data (often related to systems evolving over time). An HMM is composed of two mathematical objects an observed state dependent process X 1 , X 2 , , X M displaystyle X_1,X_2,ldots ,X_M , and an unobserved (hidden) state process S 1 , S 2 , , S T displaystyle S_1,S_2,ldots ,S_T . In an HMM, the state process is not directly observed it is a hidden (or latent) variable but observations are made of a state dependent process (or observation process) that is driven by the underlying state process (and which can thus be regarded as a noisy measurement of the system states of interest). HMMs can be formulated in continuous time. HMMs can be used to profile and convert a multiple sequence alignment into a position-specific scoring system suitable for searching databases for homologous sequences remotely. Additionally, ecological phenomena can be described by HMMs. Convolutional neural networks Convolutional neural networks (CNN) are a class of deep neural network whose architecture is based on shared weights of convolution kernels or filters that slide along input features, providing translation-equivariant responses known as feature maps. CNNs take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns discovered via their filters. Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field. CNN uses relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This reduced reliance on prior knowledge of the analyst and on human intervention in manual feature extraction makes CNNs a desirable model. A phylogenetic convolutional neural network (Ph-CNN) is a convolutional neural network architecture proposed by Fioranti et al. in 2018 to classify metagenomics data. In this approach, phylogenetic data is endowed with patristic distance (the sum of the lengths of all branches connecting two operational taxonomic units OTU) to select k-neighborhoods for each OTU, and each OTU and its neighbors are processed with convolutional filters. Self-supervised learning Unlike supervised methods, self-supervised learning methods learn representations without relying on annotated data. That is well-suited for genomics, where high throughput sequencing techniques can create potentially large amounts of unlabeled data. Some examples of self-supervised learning methods applied on genomics include DNABERT and Self-GenomeNet. Random forest Random forests (RF) classify by constructing an ensemble of decision trees, and outputting the average prediction of the individual trees. This is a modification of bootstrap aggregating (which aggregates a large collection of decision trees) and can be used for classification or regression. As random forests give an internal estimate of generalization error, cross-validation is unnecessary. In addition, they produce proximities, which can be used to impute missing values, and which enable novel data visualizations. Computationally, random forests are appealing because they naturally handle both regression and (multiclass) classification, are relatively fast to train and to predict, depend only on one or two tuning parameters, have a built-in estimate of the generalization error, can be used directly for high-dimensional problems, and can easily be implemented in parallel. Statistically, random forests are appealing for additional features, such as measures of variable importance, differential class weighting, missing value imputation, visualization, outlier detection, and unsupervised learning. Clustering Clustering - the partitioning of a data set into disjoint subsets, so that the data in each subset are as close as possible to each other and as distant as possible from data in any other subset, according to some defined distance or similarity function - is a common technique for statistical data analysis. Clustering is central to much data-driven bioinformatics research and serves as a powerful computational method whereby means of hierarchical, centroid-based, distribution-based, density-based, and self-organizing maps classification, has long been studied and used in classical machine learning settings. Particularly, clustering helps to analyze unstructured and high-dimensional data in the form of sequences, expressions, texts, images, and so on. Clustering is also used to gain insights into biological processes at the genomic level, e.g. gene functions, cellular processes, subtypes of cells, gene regulation, and metabolic processes. Clustering algorithms used in bioinformatics Data clustering algorithms can be hierarchical or partitional. Hierarchical algorithms find successive clusters using previously established clusters, whereas partitional algorithms determine all clusters at once. Hierarchical algorithms can be agglomerative (bottom-up) or divisive (top-down). Agglomerative algorithms begin with each element as a separate cluster and merge them in successively larger clusters. Divisive algorithms begin with the whole set and proceed to divide it into successively smaller clusters. Hierarchical clustering is calculated using metrics on Euclidean spaces, the most commonly used is the Euclidean distance computed by finding the square of the difference between each variable, adding all the squares, and finding the square root of the said sum. An example of a hierarchical clustering algorithm is BIRCH, which is particularly good on bioinformatics for its nearly linear time complexity given generally large datasets. Partitioning algorithms are based on specifying an initial number of groups, and iteratively reallocating objects among groups to convergence. This algorithm typically determines all clusters at once. Most applications adopt one of two popular heuristic methods k-means algorithm or k-medoids. Other algorithms do not require an initial number of groups, such as affinity propagation. In a genomic setting this algorithm has been used both to cluster biosynthetic gene clusters in gene cluster families(GCF) and to cluster said GCFs. Workflow Typically, a workflow for applying machine learning to biological data goes through four steps Recording, including capture and storage. In this step, different information sources may be merged into a single set. Preprocessing, including cleaning and restructuring into a ready-to-analyze form. In this step, uncorrected data are eliminated or corrected, while missing data maybe imputed and relevant variables chosen. Analysis, evaluating data using either supervised or unsupervised algorithms. The algorithm is typically trained on a subset of data, optimizing parameters, and evaluated on a separate test subset. Visualization and interpretation, where knowledge is represented effectively using different methods to assess the significance and importance of the findings. Data errors Duplicate data is a significant issue in bioinformatics. Publicly available data may be of uncertain quality. Errors during experimentation. Erroneous interpretation. Typing mistakes. Non-standardized methods (3D structure in PDB from multiple sources, X-ray diffraction, theoretical modeling, nuclear magnetic resonance, etc.) are used in experiments. Applications In general, a machine learning system can usually be trained to recognize elements of a certain class given sufficient samples. For example, machine learning methods can be trained to identify specific visual features such as splice sites. Support vector machines have been extensively used in cancer genomic studies. In addition, deep learning has been incorporated into bioinformatic algorithms. Deep learning applications have been used for regulatory genomics and cellular imaging. Other applications include medical image classification, genomic sequence analysis, as well as protein structure classification and prediction. Deep learning has been applied to regulatory genomics, variant calling and pathogenicity scores. Natural language processing and text mining have helped to understand phenomena including protein-protein interaction, gene-disease relation as well as predicting biomolecule structures and functions. Precision/personalized medicine Natural language processing algorithms personalized medicine for patients who suffer genetic diseases, by combining the extraction of clinical information and genomic data available from the patients. Institutes such as Health-funded Pharmacogenomics Research Network focus on finding breast cancer treatments. Precision medicine considers individual genomic variability, enabled by large-scale biological databases. Machine learning can be applied to perform the matching function between (groups of patients) and specific treatment modalities. Computational techniques are used to solve other problems, such as efficient primer design for PCR, biological-image analysis and back translation of proteins (which is, given the degeneration of the genetic code, a complex combinatorial problem). Genomics While genomic sequence data has historically been sparse due to the technical difficulty of sequencing a piece of DNA, the number of available sequences is growing. On average, the number of bases available in the GenBank public repository has doubled every 18 months since 1982. However, while raw data was becoming increasingly available and accessible, As of 2002, biological interpretation of this data was occurring at a much slower pace. This made for an increasing need for developing computational genomics tools, including machine learning systems, that can automatically determine the location of protein-encoding genes within a given DNA sequence (i.e. gene prediction). Gene prediction is commonly performed through both extrinsic searches and intrinsic searches. For the extrinsic search, the input DNA sequence is run through a large database of sequences whose genes have been previously discovered and their locations annotated and identifying the target sequences genes by determining which strings of bases within the sequence are homologous to known gene sequences. However, not all the genes in a given input sequence can be identified through homology alone, due to limits in the size of the database of known and annotated gene sequences. Therefore, an intrinsic search is needed where a gene prediction program attempts to identify the remaining genes from the DNA sequence alone. Machine learning has also been used for the problem of multiple sequence alignment which involves aligning many DNA or amino acid sequences in order to determine regions of similarity that could indicate a shared evolutionary history. It can also be used to detect and visualize genome rearrangements. Proteomics Proteins, strings of amino acids, gain much of their function from protein folding, where they conform into a three-dimensional structure, including the primary structure, the secondary structure (alpha helices and beta sheets), the tertiary structure, and the quaternary structure. Protein secondary structure prediction is a main focus of this subfield as tertiary and quaternary structures are determined based on the secondary structure. Solving the true structure of a protein is expensive and time-intensive, furthering the need for systems that can accurately predict the structure of a protein by analyzing the amino acid sequence directly. Prior to machine learning, researchers needed to conduct this prediction manually. This trend began in 1951 when Pauling and Corey released their work on predicting the hydrogen bond configurations of a protein from a polypeptide chain. Automatic feature learning reaches an accuracy of 82-84. The current state-of-the-art in secondary structure prediction uses a system called DeepCNF (deep convolutional neural fields) which relies on the machine learning model of artificial neural networks to achieve an accuracy of approximately 84 when tasked to classify the amino acids of a protein sequence into one of three structural classes (helix, sheet, or coil). The theoretical limit for three-state protein secondary structure is 88 90. Machine learning has also been applied to proteomics problems such as protein side-chain prediction, protein loop modeling, and protein contact map prediction. Metagenomics Metagenomics is the study of microbial communities from environmental DNA samples. Currently, limitations and challenges predominate in the implementation of machine learning tools due to the amount of data in environmental samples. Supercomputers and web servers have made access to these tools easier. The high dimensionality of microbiome datasets is a major challenge in studying the microbiome this significantly limits the power of current approaches for identifying true differences and increases the chance of false discoveries. Despite their importance, machine learning tools related to metagenomics have focused on the study of gut microbiota and the relationship with digestive diseases, such as inflammatory bowel disease (IBD), Clostridioides difficile infection (CDI), colorectal cancer and diabetes, seeking better diagnosis and treatments. Many algorithms were developed to classify microbial communities according to the health condition of the host, regardless of the type of sequence data, e.g. 16S rRNA or whole-genome sequencing (WGS), using methods such as least absolute shrinkage and selection operator classifier, random forest, supervised classification model, and gradient boosted tree model. Neural networks, such as recurrent neural networks (RNN), convolutional neural networks (CNN), and Hopfield neural networks have been added. For example, in 2018, Fioravanti et al. developed an algorithm called Ph-CNN to classify data samples from healthy patients and patients with IBD symptoms (to distinguish healthy and sick patients) by using phylogenetic trees and convolutional neural networks. In addition, random forest (RF) methods and implemented importance measures help in the identification of microbiome species that can be used to distinguish diseased and non-diseased samples. However, the performance of a decision tree and the diversity of decision trees in the ensemble significantly influence the performance of RF algorithms. The generalization error for RF measures how accurate the individual classifiers are and their interdependence. Therefore, the high dimensionality problems of microbiome datasets pose challenges. Effective approaches require many possible variable combinations, which exponentially increases the computational burden as the number of features increases. For microbiome analysis in 2020 Dang  Kishino developed a novel analysis pipeline. The core of the pipeline is an RF classifier coupled with forwarding variable selection (RF-FVS), which selects a minimum-size core set of microbial species or functional signatures that maximize the predictive classifier performance. The framework combines identifying a few significant features by a massively parallel forward variable selection procedure mapping the selected species on a phylogenetic tree, and predicting functional profiles by functional gene enrichment analysis from metagenomic 16S rRNA data. They demonstrated performance by analyzing two published datasets from large-scale case-control studies 16S rRNA gene amplicon data for C. difficile infection (CDI) and shotgun metagenomics data for human colorectal cancer (CRC). The proposed approach improved the accuracy from 81 to 99.01 for CDI and from 75.14 to 90.17 for CRC. The use of machine learning in environmental samples has been less explored, maybe because of data complexity, especially from WGS. Some works show that it is possible to apply these tools in environmental samples. In 2021 Dhungel et al., designed an R package called MegaR. This package allows working with 16S rRNA and whole metagenomic sequences to make taxonomic profiles and classification models by machine learning models. MegaR includes a comfortable visualization environment to improve the user experience. Machine learning in environmental metagenomics can help to answer questions related to the interactions between microbial communities and ecosystems, e.g. the work of Xun et al., in 2021 where the use of different machine learning methods offered insights on the relationship among the soil, microbiome biodiversity, and ecosystem stability. Microarrays Microarrays, a type of lab-on-a-chip, are used for automatically collecting data about large amounts of biological material. Machine learning can aid in analysis, and has been applied to expression pattern identification, classification, and genetic network induction. This technology is especially useful for monitoring gene expression, aiding in diagnosing cancer by examining which genes are expressed. One of the main tasks is identifying which genes are expressed based on the collected data. In addition, due to the huge number of genes on which data is collected by the microarray, winnowing the large amount of irrelevant data to the task of expressed gene identification is challenging. Machine learning presents a potential solution as various classification methods can be used to perform this identification. The most commonly used methods are radial basis function networks, deep learning, Bayesian classification, decision trees, and random forest. Systems biology Systems biology focuses on the study of emergent behaviors from complex interactions of simple biological components in a system. Such components can include DNA, RNA, proteins, and metabolites. Machine learning has been used to aid in modeling these interactions in domains such as genetic networks, signal transduction networks, and metabolic pathways. Probabilistic graphical models, a machine learning technique for determining the relationship between different variables, are one of the most commonly used methods for modeling genetic networks. In addition, machine learning has been applied to systems biology problems such as identifying transcription factor binding sites using Markov chain optimization. Genetic algorithms, machine learning techniques which are based on the natural process of evolution, have been used to model genetic networks and regulatory structures. Other systems biology applications of machine learning include the task of enzyme function prediction, high throughput microarray data analysis, analysis of genome-wide association studies to better understand markers of disease, protein function prediction. Evolution This domain, particularly phylogenetic tree reconstruction, uses the features of machine learning techniques. Phylogenetic trees are schematic representations of the evolution of organisms. Initially, they were constructed using features such as morphological and metabolic features. Later, due to the availability of genome sequences, the construction of the phylogenetic tree algorithm used the concept based on genome comparison. With the help of optimization techniques, a comparison was done by means of multiple sequence alignment. Stroke diagnosis Machine learning methods for the analysis of neuroimaging data are used to help diagnose stroke. Historically multiple approaches to this problem involved neural networks. Multiple approaches to detect strokes used machine learning. As proposed by Mirtskhulava, feed-forward networks were tested to detect strokes using neural imaging. As proposed by Titano 3D-CNN techniques were tested in supervised classification to screen head CT images for acute neurologic events. Three-dimensional CNN and SVM methods are often used. Text mining The increase in biological publications increased the difficulty in searching and compiling relevant available information on a given topic. This task is known as knowledge extraction. It is necessary for biological data collection which can then in turn be fed into machine learning algorithms to generate new biological knowledge. Machine learning can be used for this knowledge extraction task using techniques such as natural language processing to extract the useful information from human-generated reports in a database. Text Nailing, an alternative approach to machine learning, capable of extracting features from clinical narrative notes was introduced in 2017. This technique has been applied to the search for novel drug targets, as this task requires the examination of information stored in biological databases and journals. Annotations of proteins in protein databases often do not reflect the complete known set of knowledge of each protein, so additional information must be extracted from biomedical literature. Machine learning has been applied to the automatic annotation of gene and protein function, determination of the protein subcellular localization, DNA-expression array analysis, large-scale protein interaction analysis, and molecule interaction analysis. Another application of text mining is the detection and visualization of distinct DNA regions given sufficient reference data. Clustering and abundance profiling of biosynthetic gene clusters Microbial communities are complex assembles of diverse microorganisms, where symbiont partners constantly produce diverse metabolites derived from the primary and secondary (specialized) metabolism, from which metabolism plays an important role in microbial interaction. Metagenomic and metatranscriptomic data are an important source for deciphering communications signals. Molecular mechanisms produce specialized metabolites in various ways. Biosynthetic Gene Clusters (BGCs) attract attention, since several metabolites are clinically valuable, anti-microbial, anti-fungal, anti-parasitic, anti-tumor and immunosuppressive agents produced by the modular action of multi-enzymatic, multi-domains gene clusters, such as Nonribosomal peptide synthetases (NRPSs) and polyketide synthases (PKSs). Diverse studies show that grouping BGCs that share homologous core genes into gene cluster families (GCFs) can yield useful insights into the chemical diversity of the analyzed strains, and can support linking BGCs to their secondary metabolites. GCFs have been used as functional markers in human health studies and to study the ability of soil to suppress fungal pathogens. Given their direct relationship to catalytic enzymes, and compounds produced from their encoded pathways, BGCs/GCFs can serve as a proxy to explore the chemical space of microbial secondary metabolism. Cataloging GCFs in sequenced microbial genomes yields an overview of the existing chemical diversity and offers insights into future priorities. Tools such as BiG-SLiCE and BIG-MAP have emerged with the sole purpose of unveiling the importance of BGCs in natural environments. Decodification of RiPPs chemical structures The increase of experimentally characterized ribosomally synthesized and post-translationally modified peptides (RiPPs), together with the availability of information on their sequence and chemical structure, selected from databases such as BAGEL, BACTIBASE, MIBIG, and THIOBASE, provide the opportunity to develop machine learning tools to decode the chemical structure and classify them. In 2017, researchers at the National Institute of Immunology of New Delhi, India, developed RiPPMiner software, a bioinformatics resource for decoding RiPP chemical structures by genome mining. The RiPPMiner web server consists of a query interface and the RiPPDB database. RiPPMiner defines 12 subclasses of RiPPs, predicting the cleavage site of the leader peptide and the final cross-link of the RiPP chemical structure. Mass spectral similarity scoring Many tandem mass spectrometry (MS/MS) based metabolomics studies, such as library matching and molecular networking, use spectral similarity as a proxy for structural similarity. Spec2vec algorithm provides a new way of spectral similarity score, based on Word2Vec. Spec2Vec learns fragmental relationships within a large set of spectral data, in order to assess spectral similarities between molecules and to classify unknown molecules through these comparisons. For systemic annotation, some metabolomics studies rely on fitting measured fragmentation mass spectra to library spectra or contrasting spectra via network analysis. Scoring functions are used to determine the similarity between pairs of fragment spectra as part of these processes. So far, no research has suggested scores that are significantly different from the commonly utilized cosine-based similarity. Databases An important part of bioinformatics is the management of big datasets, known as databases of reference. Databases exist for each type of biological data, for example for biosynthetic gene clusters and metagenomes. General databases by bioinformatics National Center for Biotechnology Information The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. Resources include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. All of these resources can be accessed through NCBI. Bioinformatics analysis for biosynthetic gene clusters antiSMASH antiSMASH allows the rapid genome-wide identification, annotation and analysis of secondary metabolite biosynthesis gene clusters in bacterial and fungal genomes. It integrates and cross-links with a large number of in silico secondary metabolite analysis tools. gutSMASH gutSMASH is a tool that systematically evaluates bacterial metabolic potential by predicting both known and novel anaerobic metabolic gene clusters (MGCs) from the gut microbiome. MIBiG MIBiG, the minimum information about a biosynthetic gene cluster specification, provides a standard for annotations and metadata on biosynthetic gene clusters and their molecular products. MIBiG is a Genomic Standards Consortium project that builds on the minimum information about any sequence (MIxS) framework. MIBiG facilitates the standardized deposition and retrieval of biosynthetic gene cluster data as well as the development of comprehensive comparative analysis tools. It empowers next-generation research on the biosynthesis, chemistry and ecology of broad classes of societally relevant bioactive secondary metabolites, guided by robust experimental evidence and rich metadata components. SILVA SILVA is an interdisciplinary project among biologists and computers scientists assembling a complete database of RNA ribosomal (rRNA) sequences of genes, both small (16S, 18S, SSU) and large (23S, 28S, LSU) subunits, which belong to the bacteria, archaea and eukarya domains. These data are freely available for academic and commercial use. Greengenes Greengenes is a full-length 16S rRNA gene database that provides chimera screening, standard alignment and a curated taxonomy based on de novo tree inference. Overview 1,012,863 RNA sequences from 92,684 organisms contributed to RNAcentral. The shortest sequence has 1,253 nucleotides, the longest 2,368. The average length is 1,402 nucleotides. Database version 13.5. Open Tree of Life Taxonomy Open Tree of Life Taxonomy (OTT) aims to build a complete, dynamic, and digitally available Tree of Life by synthesizing published phylogenetic trees along with taxonomic data. Phylogenetic trees have been classified, aligned, and merged. Taxonomies have been used to fill in sparse regions and gaps left by phylogenies. OTT is a base that has been little used for sequencing analyzes of the 16S region, however, it has a greater number of sequences classified taxonomically down to the genus level compared to SILVA and Greengenes. However, in terms of classification at the edge level, it contains a lesser amount of information Ribosomal Database Project Ribosomal Database Project (RDP) is a database that provides RNA ribosomal (rRNA) sequences of small subunits of domain bacterial and archaeal (16S) and fungal rRNA sequences of large subunits (28S). Title Machine learning in earth sciences URL https//en.wikipedia.org/wiki/Machine_learning_in_earth_sciences Content Applications of machine learning (ML) in earth sciences include geological mapping, gas leakage detection and geological feature identification. Machine learning is a subdiscipline of artificial intelligence aimed at developing programs that are able to classify, cluster, identify, and analyze vast and complex data sets without the need for explicit programming to do so. Earth science is the study of the origin, evolution, and future of the Earth. The earths system can be subdivided into four major components including the solid earth, atmosphere, hydrosphere, and biosphere. A variety of algorithms may be applied depending on the nature of the task. Some algorithms may perform significantly better than others for particular objectives. For example, convolutional neural networks (CNNs) are good at interpreting images, whilst more general neural networks may be used for soil classification, but can be more computationally expensive to train than alternatives such as support vector machines. The range of tasks to which ML (including deep learning) is applied has been ever-growing in recent decades, as has the development of other technologies such as unmanned aerial vehicles (UAVs), ultra-high resolution remote sensing technology, and high-performance computing. This has led to the availability of large high-quality datasets and more advanced algorithms. Significance Complexity of earth science Problems in earth science are often complex. It is difficult to apply well-known and described mathematical models to the natural environment, therefore machine learning is commonly a better alternative for such non-linear problems. Ecological data are commonly non-linear and consist of higher-order interactions, and together with missing data, traditional statistics may underperform as unrealistic assumptions such as linearity are applied to the model. A number of researchers found that machine learning outperforms traditional statistical models in earth science, such as in characterizing forest canopy structure, predicting climate-induced range shifts, and delineating geologic facies. Characterizing forest canopy structure enables scientists to study vegetation response to climate change. Predicting climate-induced range shifts enable policy makers to adopt suitable conversation method to overcome the consequences of climate change. Delineating geologic facies helps geologists to understand the geology of an area, which is essential for the development and management of an area. Inaccessible data In Earth Sciences, some data are often difficult to access or collect, therefore inferring data from data that are easily available by machine learning method is desirable. For example, geological mapping in tropical rainforests is challenging because the thick vegetation cover and rock outcrops are poorly exposed. Applying remote sensing with machine learning approaches provides an alternative way for rapid mapping without the need of manually mapping in the unreachable areas. Reduce time costs Machine learning can also reduce the efforts done by experts, as manual tasks of classification and annotation etc. are the bottlenecks in the workflow of the research of earth science. Geological mapping, especially in a vast, remote area is labour, cost and time-intensive with traditional methods. Incorporation of remote sensing and machine learning approaches can provide an alternative solution to eliminate some field mapping needs. Consistent and bias-free Consistency and bias-free is also an advantage of machine learning compared to manual works by humans. In research comparing the performance of human and machine learning in the identification of dinoflagellates, machine learning is found to be not as prone to systematic bias as humans. A recency effect that is present in humans is that the classification often biases towards the most recently recalled classes. In a labelling task of the research, if one kind of dinoflagellates occurs rarely in the samples, then expert ecologists commonly will not classify it correctly. The systematic bias strongly deteriorate the classification accuracies of humans. Optimal machine learning algorithm The extensive usage of machine learning in various fields has led to a wide range of algorithms of learning methods being applied. Choosing the optimal algorithm for a specific purpose can lead to a significant boost in accuracy for example, the lithological mapping of gold-bearing granite-greenstone rocks in Hutti, India with AVIRIS-NG hyperspectral data, shows more than 10 difference in overall accuracy between using support vector machines (SVMs) and random forest. Some algorithms can also reveal hidden important information white box models are transparent models, the outputs of which can be easily explained, while black box models are the opposite. For example, although an SVM yielded the best result in landslide susceptibility assessment accuracy, the result cannot be rewritten in the form of expert rules that explain how and why an area was classified as that specific class. In contrast, decision trees are transparent and easily understood, and the user can observe and fix the bias if any is present in such models. If computational resource is a concern, more computationally demanding learning methods such as deep neural networks are less preferred, despite the fact that they may outperform other algorithms, such as in soil classification. Usage Mapping Geological or lithological mapping and mineral prospectivity mapping Geological or lithological mapping produces maps showing geological features and geological units. Mineral prospectivity mapping utilizes a variety of datasets such as geological maps and aeromagnetic imagery to produce maps that are specialized for mineral exploration. Geological, lithological, and mineral prospectivity mapping can be carried out by processing data with ML techniques, with the input of spectral imagery obtained from remote sensing and geophysical data. Spectral imaging is also used the imaging of wavelength bands in the electromagnetic spectrum, while conventional imaging captures three wavelength bands (red, green, blue) in the electromagnetic spectrum. Random forests and SVMs are some algorithms commonly used with remotely-sensed geophysical data, while Simple Linear Iterative Clustering-Convolutional Neural Network (SLIC-CNN) and Convolutional Neural Networks (CNNs) are commonly applied to aerial imagery. Large scale mapping can be carried out with geophysical data from airborne and satellite remote sensing geophysical data, and smaller-scale mapping can be carried out with images from Unmanned Aerial Vehicles (UAVs) for higher resolution. Vegetation cover is one of the major obstacles for geological mapping with remote sensing, as reported in various research, both in large-scale and small-scale mapping. Vegetation affects the quality of spectral images, or obscures the rock information in aerial images. Landslide susceptibility and hazard mapping Landslide susceptibility refers to the probability of landslide of a certain geographical location, which is dependent on local terrain conditions. Landslide susceptibility mapping can highlight areas prone to landslide risks, which is useful for urban planning and disaster management. Such datasets for ML algorithms usually include topographic information, lithological information, satellite images, etc., and some may include land use, land cover, drainage information, and vegetation cover according to the study requirements. As usual, for training an ML model for landslide susceptibility mapping, training and testing datasets are required. There are two methods of allocating datasets for training and testing one is to randomly split the study area for the datasets another is to split the whole study into two adjacent parts for the two datasets. To test classification models, the common practice is to split the study area randomly however, it is more useful if the study area can be split into two adjacent parts so that an automation algorithm can carry out mapping of a new area with the input of expert-processed data of adjacent land. Feature identification and detection Discontinuity analyses Discontinuities such as fault planes and bedding planes have important implications in civil engineering. Rock fractures can be recognized automatically by machine learning through photogrammetric analysis, even with the presence of interfering objects such as vegetation. In ML training for classifying images, data augmentation is a common practice to avoid overfitting and increase the training dataset size and variability. For example, in a study of rock fracture recognition, 68 images for training and 23 images for testing were prepared via random splitting. Data augmentation was performed, increasing the training dataset size to 8704 images by flipping and random cropping. The approach was able to recognize rock fractures accurately in most cases. Both the negative prediction value (NPV) and the specificity were over 0.99. This demonstrated the robustness of discontinuity analyses with machine learning. Carbon dioxide leakage detection Quantifying carbon dioxide leakage from a geological sequestration site has gained increased attention as the public is interested in whether carbon dioxide is stored underground safely and effectively. Carbon dioxide leakage from a geological sequestration site can be detected indirectly with the aid of remote sensing and an unsupervised clustering algorithm such as Iterative Self-Organizing Data Analysis Technique (ISODATA). The increase in soil concentration causes a stress response for plants by inhibiting plant respiration, as oxygen is displaced by carbon dioxide. The vegetation stress signal can be detected with the Normalized Difference Red Edge Index (NDRE). The hyperspectral images are processed by the unsupervised algorithm, clustering pixels with similar plant responses. The hyperspectral information in areas with known leakage is extracted so that areas with leakage can be matched with the clustered pixels with spectral anomalies. Although the approach can identify leakage efficiently, there are some limitations that require further study. The NDRE may not be accurate due to reasons like higher chlorophyll absorption, variation in vegetation, and shadowing effects therefore, some stressed pixels can be incorrectly classed as healthy. Seasonality, groundwater table height may also affect the stress response to of the vegetation. Quantification of water inflow The rock mass rating (RMR) system is a widely adopted rock mass classification system by geomechanical means with the input of six parameters. The amount of water inflow is one of the inputs of the classification scheme, representing the groundwater condition. Quantification of the water inflow in the faces of a rock tunnel was traditionally carried out by visual observation in the field, which is labour and time-consuming, and fraught with safety concerns. Machine learning can determine water inflow by analyzing images taken on the construction site. The classification of the approach mostly follows the RMR system, but combining damp and wet states, as it is difficult to distinguish only by visual inspection. The images were classified into the non-damaged state, wet state, dripping state, flowing state, and gushing state. The accuracy of classifying the images was approximately 90. Classification Soil classification The most popular cost-effective method od soil investigation method is cone penetration testing (CPT). The test is carried out by pushing a metallic cone through the soil the force required to push at a constant rate is recorded as a quasi-continuous log. Machine learning can classify soil with the input of CPT data. In an attempt to classify with ML, there are two tasks required to analyze the data, namely segmentation and classification. Segmentation can be carried out with the Constraint Clustering and Classification (CONCC) algorithm to split a single series data into segments. Classification can then be carried out by algorithms such as decision trees, SVMs, or neural networks. Geological structure classification Exposed geological structures such as anticlines, ripple marks, and xenoliths can be identified automatically with deep learning models. Research has demonstrated that three-layer CNNs and transfer learning have strong accuracy (about 80 and 90 respectively), while others like k-nearest neighbors (k-NN), regular neural nets, and extreme gradient boosting (XGBoost) have low accuracies (ranging from 10 - 30). The grayscale images and colour images were both tested, with the accuracy difference being little, implying that colour is not very important in identifying geological structures. Forecast and predictions Earthquake early warning systems and forecasting Earthquake warning systems are often vulnerable to local impulsive noise, therefore giving out false alerts. False alerts can be eliminated by discriminating the earthquake waveforms from noise signals with the aid of ML methods. The method consists of two parts, the first being unsupervised learning with a generative adversarial network (GAN) to learn and extract features of first-arrival P-waves, and the second being use of a random forest to discriminate P-waves. This approach achieved 99.2 in recognizing P-waves, and can avoid false triggers by noise signals with 98.4 accuracy. Earthquakes can be produced in a laboratory settings to mimic real-world ones. With the help of machine learning, the patterns of acoustic signals as precursors for earthquakes can be identified. Predicting the time remaining before failure was demonstrated in a study with continuous acoustic time series data recorded from a fault. The algorithm applied was a random forest, trained with a set of slip events, performing strongly in predicting the time to failure. It identified acoustic signals to predict failures, with one of them being previously unidentified. Although this laboratory earthquake is not as complex as a natural one, progress was made that guides future earthquake prediction work. Streamflow discharge prediction Real-time streamflow data is integral for decision making (e.g., evacuations, or regulation of reservoir water levels during flooding). Streamflow data can be estimated by data provided by stream gauges, which measure the water level of a river. However, water and debris from flooding may damage stream gauges, resulting in lack of essential real-time data. The ability of machine learning to infer missing data enables it to predict streamflow with both historical stream gauge data and real-time data. Streamflow Hydrology Estimate using Machine Learning (SHEM) is a model that can serve this purpose. To verify its accuracies, the prediction result was compared with the actual recorded data, and the accuracies were found to be between 0.78 and 0.99. Challenge Inadequate training data An adequate amount of training and validation data is required for machine learning. However, some very useful products like satellite remote sensing data only have decades of data since the 1970s. If one is interested in the yearly data, then only less than 50 samples are available. Such amount of data may not be adequate. In a study of automatic classification of geological structures, the weakness of the model is the small training dataset, even though with the help of data augmentation to increase the size of the dataset. Another study of predicting streamflow found that the accuracies depend on the availability of sufficient historical data, therefore sufficient training data determine the performance of machine learning. Inadequate training data may lead to a problem called overfitting. Overfitting causes inaccuracies in machine learning as the model learns about the noise and undesired details. Limited by data input Machine learning cannot carry out some of the tasks as a human does easily. For example, in the quantification of water inflow in rock tunnel faces by images for Rock Mass Rating system (RMR), the damp and the wet state was not classified by machine learning because discriminating the two only by visual inspection is not possible. In some tasks, machine learning may not able to fully substitute manual work by a human. Black-box operation In many machine learning algorithms, for example, Artificial Neural Network (ANN), it is considered as black box approach as clear relationships and descriptions of how the results are generated in the hidden layers are unknown. White-box approach such as decision tree can reveal the algorithm details to the users. If one wants to investigate the relationships, such black-box approaches are not suitable. However, the performances of black-box algorithms are usually better. Title Machine learning in physics URL https//en.wikipedia.org/wiki/Machine_learning_in_physics Content Applying machine learning (ML) (including deep learning) methods to the study of quantum systems is an emergent area of physics research. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other examples include learning Hamiltonians, learning quantum phase transitions, and automatically generating new quantum experiments. ML is effective at processing large amounts of experimental or calculated data in order to characterize an unknown quantum system, making its application useful in contexts including quantum information theory, quantum technology development, and computational materials design. In this context, for example, it can be used as a tool to interpolate pre-calculated interatomic potentials, or directly solving the Schr dinger equation with a variational method. Applications of machine learning to physics Noisy data The ability to experimentally control and prepare increasingly complex quantum systems brings with it a growing need to turn large and noisy data sets into meaningful information. This is a problem that has already been studied extensively in the classical setting, and consequently, many existing machine learning techniques can be naturally adapted to more efficiently address experimentally relevant problems. For example, Bayesian methods and concepts of algorithmic learning can be fruitfully applied to tackle quantum state classification, Hamiltonian learning, and the characterization of an unknown unitary transformation. Other problems that have been addressed with this approach are given in the following list Identifying an accurate model for the dynamics of a quantum system, through the reconstruction of the Hamiltonian Extracting information on unknown states Learning unknown unitary transformations and measurements Engineering of quantum gates from qubit networks with pairwise interactions, using time dependent or independent Hamiltonians. Improving the extraction accuracy of physical observables from absorption images of ultracold atoms (degenerate Fermi gas), by the generation of an ideal reference frame. Calculated and noise-free data Quantum machine learning can also be applied to dramatically accelerate the prediction of quantum properties of molecules and materials. This can be helpful for the computational design of new molecules or materials. Some examples include Interpolating interatomic potentials Inferring molecular atomization energies throughout chemical compound space Accurate potential energy surfaces with restricted Boltzmann machines Automatic generation of new quantum experiments Solving the many-body, static and time-dependent Schr dinger equation Identifying phase transitions from entanglement spectra Generating adaptive feedback schemes for quantum metrology and quantum tomography. Variational circuits Variational circuits are a family of algorithms which utilize training based on circuit parameters and an objective function. Variational circuits are generally composed of a classical device communicating input parameters (random or pre-trained parameters) into a quantum device, along with a classical Mathematical optimization function. These circuits are very heavily dependent on the architecture of the proposed quantum device because parameter adjustments are adjusted based solely on the classical components within the device. Though the application is considerably infantile in the field of quantum machine learning, it has incredibly high promise for more efficiently generating efficient optimization functions. Sign problem Machine learning techniques can be used to find a better manifold of integration for path integrals in order to avoid the sign problem. Fluid dynamics Physics discovery and prediction A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants. Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems future dynamics from video recordings of their behavior. In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems. Beyond discovery and prediction, blank slate-type of learning of fundamental aspects of the physical world may have further applications such as improving adaptive and broad artificial general intelligence. In specific, prior machine learning models were highly specialised and lack a general understanding of the world. See also Quantum computing Quantum machine learning Quantum annealing Quantum neural network HHL Algorithm Title Machine learning in video games URL https//en.wikipedia.org/wiki/Machine_learning_in_video_games Content Artificial intelligence and machine learning techniques are used in video games for a wide variety of applications such as non-player character (NPC) control and procedural content generation (PCG). Machine learning is a subset of artificial intelligence that uses historical data to build predictive and analytical models. This is in sharp contrast to traditional methods of artificial intelligence such as search trees and expert systems. Information on machine learning techniques in the field of games is mostly known to public through research projects as most gaming companies choose not to publish specific information about their intellectual property. The most publicly known application of machine learning in games is likely the use of deep learning agents that compete with professional human players in complex strategy games. There has been a significant application of machine learning on games such as Atari/ALE, Doom, Minecraft, StarCraft, and car racing. Other games that did not originally exists as video games, such as chess and Go have also been affected by the machine learning. Overview of relevant machine learning techniques Deep learning Deep learning is a subset of machine learning which focuses heavily on the use of artificial neural networks (ANN) that learn to solve complex tasks. Deep learning uses multiple layers of ANN and other techniques to progressively extract information from an input. Due to this complex layered approach, deep learning models often require powerful machines to train and run on. Convolutional neural networks Convolutional neural networks (CNN) are specialized ANNs that are often used to analyze image data. These types of networks are able to learn translation invariant patterns, which are patterns that are not dependent on location. CNNs are able to learn these patterns in a hierarchy, meaning that earlier convolutional layers will learn smaller local patterns while later layers will learn larger patterns based on the previous patterns. A CNNs ability to learn visual data has made it a commonly used tool for deep learning in games. Recurrent neural network Recurrent neural networks are a type of ANN that are designed to process sequences of data in order, one part at a time rather than all at once. An RNN runs over each part of a sequence, using the current part of the sequence along with memory of previous parts of the current sequence to produce an output. These types of ANN are highly effective at tasks such as speech recognition and other problems that depend heavily on temporal order. There are several types of RNNs with different internal configurations the basic implementation suffers from a lack of long term memory due to the vanishing gradient problem, thus it is rarely used over newer implementations. Long short-term memory A long short-term memory (LSTM) network is a specific implementation of a RNN that is designed to deal with the vanishing gradient problem seen in simple RNNs, which would lead to them gradually forgetting about previous parts of an inputted sequence when calculating the output of a current part. LSTMs solve this problem with the addition of an elaborate system that uses an additional input/output to keep track of long term data. LSTMs have achieved very strong results across various fields, and were used by several monumental deep learning agents in games. Reinforcement learning Reinforcement learning is the process of training an agent using rewards and/or punishments. The way an agent is rewarded or punished depends heavily on the problem such as giving an agent a positive reward for winning a game or a negative one for losing. Reinforcement learning is used heavily in the field of machine learning and can be seen in methods such as Q-learning, policy search, Deep Q-networks and others. It has seen strong performance in both the field of games and robotics. Neuroevolution Neuroevolution involves the use of both neural networks and evolutionary algorithms. Instead of using gradient descent like most neural networks, neuroevolution models make use of evolutionary algorithms to update neurons in the network. Researchers claim that this process is less likely to get stuck in a local minimum and is potentially faster than state of the art deep learning techniques. Deep learning agents Machine learning agents have been used to take the place of a human player rather than function as NPCs, which are deliberately added into video games as part of designed gameplay. Deep learning agents have achieved impressive results when used in competition with both humans and other artificial intelligence agents. Chess Chess is a turn-based strategy game that is considered a difficult AI problem due to the computational complexity of its board space. Similar strategy games are often solved with some form of a Minimax Tree Search. These types of AI agents have been known to beat professional human players, such as the historic 1997 Deep Blue versus Garry Kasparov match. Since then, machine learning agents have shown ever greater success than previous AI agents. Go Go is another turn-based strategy game which is considered an even more difficult AI problem than chess. The state space of is Go is around 10170 possible board states compared to the 10120 board states for Chess. Prior to recent deep learning models, AI Go agents were only able to play at the level of a human amateur. AlphaGo Googles 2015 AlphaGo was the first AI agent to beat a professional Go player. AlphaGo used a deep learning model to train the weights of a Monte Carlo tree search (MCTS). The deep learning model consisted of 2 ANN, a policy network to predict the probabilities of potential moves by opponents, and a value network to predict the win chance of a given state. The deep learning model allows the agent to explore potential game states more efficiently than a vanilla MCTS. The network were initially trained on games of humans players and then were further trained by games against itself. AlphaGo Zero AlphaGo Zero, another implementation of AlphaGo, was able to train entirely by playing against itself. It was able to quickly train up to the capabilities of the previous agent. StarCraft series StarCraft and its sequel StarCraft II are real-time strategy (RTS) video games that have become popular environments for AI research. Blizzard and DeepMind have worked together to release a public StarCraft 2 environment for AI research to be done on. Various deep learning methods have been tested on both games, though most agents usually have trouble outperforming the default AI with cheats enabled or skilled players of the game. Alphastar Alphastar was the first AI agent to beat professional StarCraft 2 players without any in-game advantages. The deep learning network of the agent initially received input from a simplified zoomed out version of the gamestate, but was later updated to play using a camera like other human players. The developers have not publicly released the code or architecture of their model, but have listed several state of the art machine learning techniques such as relational deep reinforcement learning, long short-term memory, auto-regressive policy heads, pointer networks, and centralized value baseline. Alphastar was initially trained with supervised learning, it watched replays of many human games in order to learn basic strategies. It then trained against different versions of itself and was improved through reinforcement learning. The final version was hugely successful, but only trained to play on a specific map in a protoss mirror matchup. Dota 2 Dota 2 is a multiplayer online battle arena (MOBA) game. Like other complex games, traditional AI agents have not been able to compete on the same level as professional human player. The only widely published information on AI agents attempted on Dota 2 is OpenAIs deep learning Five agent. OpenAI Five OpenAI Five utilized separate LSTM networks to learn each hero. It trained using a reinforcement learning technique known as Proximal Policy Learning running on a system containing 256 GPUs and 128,000 CPU cores. Five trained for months, accumulating 180 years of game experience each day, before facing off with professional players. It was eventually able to beat the 2018 Dota 2 esports champion team in a 2019 series of games. Planetary Annihilation Planetary Annihilation is a real-time strategy game which focuses on massive scale war. The developers use ANNs in their default AI agent. Supreme Commander 2 Supreme Commander 2 is a real-time strategy (RTS) video game. The game uses Multilayer Perceptrons (MLPs) to control a platoon s reaction to encountered enemy units. Total of four MLPs are used, one for each platoon type land, naval, bomber, and fighter. Generalized games There have been attempts to make machine learning agents that are able to play more than one game. These general gaming agents are trained to understand games based on shared properties between them. AlphaZero AlphaZero is a modified version of AlphaGo Zero which is able to play Shogi, chess, and Go. The modified agent starts with only basic rules of the game, and is also trained entirely through self-learning. DeepMind was able to train this generalized agent to be competitive with previous versions of itself on Go, as well as top agents in the other two games. Strengths and weaknesses of deep learning agents Machine learning agents are often not covered in many game design courses. Previous use of machine learning agents in games may not have been very practical, as even the 2015 version of AlphaGo took hundreds of CPUs and GPUs to train to a strong level. This potentially limits the creation of highly effective deep learning agents to large corporations or extremely wealthy individuals. The extensive training time of neural network based approaches can also take weeks on these powerful machines. The problem of effectively training ANN based models extends beyond powerful hardware environments finding a good way to represent data and learn meaningful things from it is also often a difficult problem. ANN models often overfit to very specific data and perform poorly in more generalized cases. AlphaStar shows this weakness, despite being able to beat professional players, it is only able to do so on a single map when playing a mirror protoss matchup. OpenAI Five also shows this weakness, it was only able to beat professional player when facing a very limited hero pool out of the entire game. This example show how difficult it can be to train a deep learning agent to perform in more generalized situations. Machine learning agents have shown great success in a variety of different games. However, agents that are too competent also risk making games too difficult for new or casual players. Research has shown that challenge that is too far above a players skill level will ruin lower player enjoyment. These highly trained agents are likely only desirable against very skilled human players who have many of hours of experience in a given game. Given these factors, highly effective deep learning agents are likely only a desired choice in games that have a large competitive scene, where they can function as an alternative practice option to a skilled human player. Computer vision-based players Computer vision focuses on training computers to gain a high-level understanding of digital images or videos. Many computer vision techniques also incorporate forms of machine learning, and have been applied on various video games. This application of computer vision focuses on interpreting game events using visual data. In some cases, artificial intelligence agents have used model-free techniques to learn to play games without any direct connection to internal game logic, solely using video data as input. Pong Andrej Karpathy has demonstrated that relatively trivial neural network with just one hidden layer is capable of being trained to play Pong based on screen data alone. Atari games In 2013, a team at DeepMind demonstrated the use of deep Q-learning to play a variety of Atari video games Beamrider, Breakout, Enduro, Pong, Qbert, Seaquest, and Space Invaders from screen data. The team expanded their work to create a learning algorithm called MuZero that was able to learn the rules and develop winning strategies for over 50 different Atari games based on screen data. Doom Doom (1993) is a first-person shooter (FPS) game. Student researchers from Carnegie Mellon University used computer vision techniques to create an agent that could play the game using only image pixel input from the game. The students used convolutional neural network (CNN) layers to interpret incoming image data and output valid information to a recurrent neural network which was responsible for outputting game moves. Super Mario Other uses of vision-based deep learning techniques for playing games have included playing Super Mario Bros. only using image input, using deep Q-learning for training. Minecraft Researchers with OpenAI created about 2000 hours of video plays of Minecraft coded with the necessary human inputs, and then trained a machine learning model to comprehend the video feedback from the input. The researchers then used that model with 70,000 hours of Minecraft playthroughs offered on YouTube to see how well the model could create the input to match that behavior and learn further from it, such as being able to learn the steps and process of creating a diamond pickaxe tool. Machine learning for procedural content generation in games Machine learning has seen research for use in content recommendation and generation. Procedural content generation is the process of creating data algorithmically rather than manually. This type of content is used to add replayability to games without relying on constant additions by human developers. PCG has been used in various games for different types of content generation, examples of which include weapons in Borderlands 2, all world layouts in Minecraft and entire universes in No Mans Sky. Common approaches to PCG include techniques that involve grammars, search-based algorithms, and logic programming. These approaches require humans to manually define the range of content possible, meaning that a human developer decides what features make up a valid piece of generated content. Machine learning is theoretically capable of learning these features when given examples to train off of, thus greatly reducing the complicated step of developers specifying the details of content design. Machine learning techniques used for content generation include Long Short-Term Memory (LSTM) Recurrent Neural Networks (RNN), Generative Adversarial networks (GAN), and K-means clustering. Not all of these techniques make use of ANNs, but the rapid development of deep learning has greatly increased the potential of techniques that do. Galactic Arms Race Galactic Arms Race is a space shooter video game that uses neuroevolution powered PCG to generate unique weapons for the player. This game was a finalist in the 2010 Indie Game Challenge and its related research paper won the Best Paper Award at the 2009 IEEE Conference on Computational Intelligence and Games. The developers use a form of neuroevolution called cgNEAT to generate new content based on each players personal preferences. Each generated item is represented by a special ANN known as a Compositional Pattern Producing Network (CPPNs). During the evolutionary phase of the game cgNEAT calculates the fitness of current items based on player usage and other gameplay metrics, this fitness score is then used decide which CPPNs will reproduce to create a new item. The ending result is the generation of new weapon effects based on the players preference. Super Mario Bros. Super Mario Bros. has been used by several researchers to simulate PCG level creation. Various attempts having used different methods. A version in 2014 used n-grams to generate levels similar to the ones it trained on, which was later improved by making use of MCTS to guide generation. These generations were often not optimal when taking gameplay metrics such as player movement into account, a separate research project in 2017 tried to resolve this problem by generating levels based on player movement using Markov Chains. These projects were not subjected to human testing and may not meet human playability standards. The Legend of Zelda PCG level creation for The Legend of Zelda has been attempted by researchers at the University of California, Santa Cruz. This attempt made use of a Bayesian Network to learn high level knowledge from existing levels, while Principal Component Analysis (PCA) was used to represent the different low level features of these levels. The researchers used PCA to compare generated levels to human made levels and found that they were considered very similar. This test did not include playability or human testing of the generated levels. Music generation Music is often seen in video games and can be a crucial element for influencing the mood of different situations and story points. Machine learning has seen use in the experimental field of music generation it is uniquely suited to processing raw unstructured data and forming high level representations that could be applied to the diverse field of music. Most attempted methods have involved the use of ANN in some form. Methods include the use of basic feedforward neural networks, autoencoders, restricted boltzmann machines, recurrent neural networks, convolutional neural networks, generative adversarial networks (GANs), and compound architectures that use multiple methods. VRAE video game melody symbolic music generation system The 2014 research paper on Variational Recurrent Auto-Encoders attempted to generate music based on songs from 8 different video games. This project is one of the few conducted purely on video game music. The neural network in the project was able to generate data that was very similar to the data of the games it trained off of. The generated data did not translate into good quality music. References Title Machine unlearning URL https//en.wikipedia.org/wiki/Machine_unlearning Content Machine unlearning is a branch of machine learning focused on removing specific undesired element, such as private data, outdated information, copyrighted material, harmful content, dangerous abilities, or misinformation, without needing to rebuild models from the ground up. Large language models, like the ones powering ChatGPT, may be asked not just to remove specific elements but also to unlearn a concept, fact, or knowledge, which arent easily linked to specific examples. New terms such as model editing, concept editing, and knowledge unlearning have emerged to describe this process. History Early research efforts were largely motivated by Article 17 of the GDPR, the European Unions privacy regulation commonly known as the right to be forgotten (RTBF), introduced in 2014. Present The GDPR did not anticipate that the development of large language models would make data erasure a complex task. This issue has since led to research on machine unlearning, with a growing focus on removing copyrighted material, harmful content, dangerous capabilities, and misinformation. Just as early experiences in humans shape later ones, some concepts are more fundamental and harder to unlearn. A piece of knowledge may be so deeply embedded in the model s knowledge graph that unlearning it could cause internal contradictions, requiring adjustments to other parts of the graph to resolve them. Title Machine-learned interatomic potential URL https//en.wikipedia.org/wiki/Machine-learned_interatomic_potential Content Machine-learned interatomic potentials (MLIPs), or simply machine learning potentials (MLPs), are interatomic potentials constructed by machine learning programs. Beginning in the 1990s, researchers have employed such programs to construct interatomic potentials by mapping atomic structures to their potential energies. These potentials are referred to as MLIPs or MLPs. Such machine learning potentials promised to fill the gap between density functional theory, a highly accurate but computationally intensive modelling method, and empirically derived or intuitively-approximated potentials, which were far lighter computationally but substantially less accurate. Improvements in artificial intelligence technology heightened the accuracy of MLPs while lowering their computational cost, increasing the role of machine learning in fitting potentials. Machine learning potentials began by using neural networks to tackle low-dimensional systems. While promising, these models could not systematically account for interatomic energy interactions they could be applied to small molecules in a vacuum, or molecules interacting with frozen surfaces, but not much else and even in these applications, the models often relied on force fields or potentials derived empirically or with simulations. These models thus remained confined to academia. Modern neural networks construct highly accurate and computationally light potentials, as theoretical understanding of materials science was increasingly built into their architectures and preprocessing. Almost all are local, accounting for all interactions between an atom and its neighbor up to some cutoff radius. There exist some nonlocal models, but these have been experimental for almost a decade. For most systems, reasonable cutoff radii enable highly accurate results. Almost all neural networks intake atomic coordinates and output potential energies. For some, these atomic coordinates are converted into atom-centered symmetry functions. From this data, a separate atomic neural network is trained for each element each atomic network is evaluated whenever that element occurs in the given structure, and then the results are pooled together at the end. This process in particular, the atom-centered symmetry functions which convey translational, rotational, and permutational invariances has greatly improved machine learning potentials by significantly constraining the neural network search space. Other models use a similar process but emphasize bonds over atoms, using pair symmetry functions and training one network per atom pair. Other models to learn their own descriptors rather than using predetermined symmetry-dictating functions. These models, called message-passing neural networks (MPNNs), are graph neural networks. Treating molecules as three-dimensional graphs (where atoms are nodes and bonds are edges), the model takes feature vectors describing the atoms as input, and iteratively updates these vectors as information about neighboring atoms is processed through message functions and convolutions. These feature vectors are then used to predict the final potentials. The flexibility of this method often results in stronger, more generalizable models. In 2017, the first-ever MPNN model (a deep tensor neural network) was used to calculate the properties of small organic molecules. Such technology was commercialized, leading to the development of Matlantis in 2022, which extracts properties through both the forward and backward passes. Gaussian Approximation Potential (GAP) One popular class of machine-learned interatomic potential is the Gaussian Approximation Potential (GAP), which combines compact descriptors of local atomic environments with Gaussian process regression to machine learn the potential energy surface of a given system. To date, the GAP framework has been used to successfully develop a number of MLIPs for various systems, including for elemental systems such as Carbon, Silicon, Phosphorus, and Tungsten, as well as for multicomponent systems such as Ge2Sb2Te5 and austenitic stainless steel, Fe7Cr2Ni. Title Manifold hypothesis URL https//en.wikipedia.org/wiki/Manifold_hypothesis Content The manifold hypothesis posits that many high-dimensional data sets that occur in the real world actually lie along low-dimensional latent manifolds inside that high-dimensional space. As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, likened to the local coordinate system of the underlying manifold. It is suggested that this principle underpins the effectiveness of machine learning algorithms in describing high-dimensional data sets by considering a few common features. The manifold hypothesis is related to the effectiveness of nonlinear dimensionality reduction techniques in machine learning. Many techniques of dimensional reduction make the assumption that data lies along a low-dimensional submanifold, such as manifold sculpting, manifold alignment, and manifold regularization. The major implications of this hypothesis is that Machine learning models only have to fit relatively simple, low-dimensional, highly structured subspaces within their potential input space (latent manifolds). Within one of these manifolds, it s always possible to interpolate between two inputs, that is to say, morph one into another via a continuous path along which all points fall on the manifold. The ability to interpolate between samples is the key to generalization in deep learning. The information geometry of statistical manifolds An empirically-motivated approach to the manifold hypothesis focuses on its correspondence with an effective theory for manifold learning under the assumption that robust machine learning requires encoding the dataset of interest using methods for data compression. This perspective gradually emerged using the tools of information geometry thanks to the coordinated effort of scientists working on the efficient coding hypothesis, predictive coding and variational Bayesian methods. The argument for reasoning about the information geometry on the latent space of distributions rests upon the existence and uniqueness of the Fisher information metric. In this general setting, we are trying to find a stochastic embedding of a statistical manifold. From the perspective of dynamical systems, in the big data regime this manifold generally exhibits certain properties such as homeostasis We can sample large amounts of data from the underlying generative process. Machine Learning experiments are reproducible, so the statistics of the generating process exhibit stationarity. In a sense made precise by theoretical neuroscientists working on the free energy principle, the statistical manifold in question possesses a Markov blanket. References Title Manifold regularization URL https//en.wikipedia.org/wiki/Manifold_regularization Content In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition. Manifold regularizer Motivation Manifold regularization is a type of regularization, a family of techniques that reduces overfitting and ensures that a problem is well-posed by penalizing complex solutions. In particular, manifold regularization extends the technique of Tikhonov regularization as applied to Reproducing kernel Hilbert spaces (RKHSs). Under standard Tikhonov regularization on RKHSs, a learning algorithm attempts to learn a function f displaystyle f from among a hypothesis space of functions H displaystyle mathcal H . The hypothesis space is an RKHS, meaning that it is associated with a kernel K displaystyle K , and so every candidate function f displaystyle f has a norm f K displaystyle leftfright_K , which represents the complexity of the candidate function in the hypothesis space. When the algorithm considers a candidate function, it takes its norm into account in order to penalize complex functions. Formally, given a set of labeled training data ( x 1 , y 1 ) , , ( x , y ) displaystyle (x_1,y_1),ldots ,(x_ell ,y_ell ) with x i X , y i Y displaystyle x_iin X,y_iin Y and a loss function V displaystyle V , a learning algorithm using Tikhonov regularization will attempt to solve the expression arg min f H 1 . Manifold regularization adds a second regularization term, the intrinsic regularizer, to the ambient regularizer used in standard Tikhonov regularization. Under the manifold assumption in machine learning, the data in question do not come from the entire input space X displaystyle X , but instead from a nonlinear manifold M X displaystyle Msubset X . The geometry of this manifold, the intrinsic space, is used to determine the regularization norm. Laplacian norm There are many possible choices for the intrinsic regularizer f I displaystyle leftfright_I . Many natural choices involve the gradient on the manifold M displaystyle nabla _M , which can provide a measure of how smooth a target function is. A smooth function should change slowly where the input data are dense that is, the gradient M f ( x ) displaystyle nabla _Mf(x) should be small where the marginal probability density P X ( x ) displaystyle mathcal P_X(x) , the probability density of a randomly drawn data point appearing at x displaystyle x , is large. This gives one appropriate choice for the intrinsic regularizer f I 2  x M M f ( x ) 2 d P X ( x ) displaystyle leftfright_I2int _xin Mleftnabla _Mf(x)right2,dmathcal P_X(x) In practice, this norm cannot be computed directly because the marginal distribution P X displaystyle mathcal P_X is unknown, but it can be estimated from the provided data. Graph-based approach of the Laplacian norm When the distances between input points are interpreted as a graph, then the Laplacian matrix of the graph can help to estimate the marginal distribution. Suppose that the input data include displaystyle ell  labeled examples (pairs of an input x displaystyle x and a label y displaystyle y ) and u displaystyle u unlabeled examples (inputs without associated labels). Define W displaystyle W to be a matrix of edge weights for a graph, where W i j displaystyle W_ij is a distance measure between the data points x i displaystyle x_i and x j displaystyle x_j . Define D displaystyle D to be a diagonal matrix with D i  . Then, as the number of data points  u displaystyle ell u increases, L displaystyle L converges to the Laplace Beltrami operator M displaystyle Delta _M , which is the divergence of the gradient M displaystyle nabla _M . Then, if f displaystyle mathbf f  is a vector of the values of f displaystyle f at the data, . Solving the regularization problem with graph-based approach Using the weights A displaystyle gamma _A and I displaystyle gamma _I for the ambient and intrinsic regularizers, the final expression to be solved becomes arg min f H 1 . Instead, a representer theorem shows that under certain conditions on the choice of the norm f I displaystyle leftfright_I , the optimal solution f displaystyle f must be a linear combination of the kernel centered at each of the input points for some weights i displaystyle alpha _i , f ( x )   . Functional approach of the Laplacian norm The idea beyond graph-Laplacian is to use neighbors to estimate Laplacian. This method is akin local averaging methods, that are known to scale poorly in high-dimensional problem. Indeed, graph Laplacian is known to suffer from the curse of dimensionality. Luckily, it is possible to leverage expected smoothness of the function to estimate thanks to more advanced functional analysis. This method consists in estimating the Laplacian operator thanks to derivatives of the kernel reading 1 , j K ( x i , x ) displaystyle partial _1,jK(x_i,x) where 1 , j displaystyle partial _1,j denotes the partial derivatives according to the j-th coordinate of the first variable. This second approach of the Laplacian norm is to put in relation with meshfree methods, that contrast with the finite difference method in PDE. Applications Manifold regularization can extend a variety of algorithms that can be expressed using Tikhonov regularization, by choosing an appropriate loss function V displaystyle V and hypothesis space H displaystyle mathcal H . Two commonly used examples are the families of support vector machines and regularized least squares algorithms. (Regularized least squares includes the ridge regression algorithm the related algorithms of LASSO and elastic net regularization can be expressed as support vector machines.) The extended versions of these algorithms are called Laplacian Regularized Least Squares (abbreviated LapRLS) and Laplacian Support Vector Machines (LapSVM), respectively. Laplacian Regularized Least Squares (LapRLS) Regularized least squares (RLS) is a family of regression algorithms algorithms that predict a value . In particular, RLS is designed to minimize the mean squared error between the predicted values and the true labels, subject to regularization. Ridge regression is one form of RLS in general, RLS is the same as ridge regression combined with the kernel method. The problem statement for RLS results from choosing the loss function V displaystyle V in Tikhonov regularization to be the mean squared error . Adding a Laplacian term for manifold regularization gives the Laplacian RLS statement   . Letting K displaystyle K be the kernel matrix as above, Y displaystyle Y be the vector of data labels, and J displaystyle J be the (  u ) (  u ) displaystyle (ell u)times (ell u) block matrix  I 0 0 0 u  displaystyle beginbmatrixI_ell 000_uendbmatrix   arg min R  u 1 ( Y J K ) T ( Y J K )  A T K  I (  u ) 2 T K L K displaystyle alpha underset alpha in mathbf R ell uarg !min frac 1ell (Y-JKalpha )mathrm T (Y-JKalpha )gamma _Aalpha mathrm T Kalpha frac gamma _I(ell u)2alpha mathrm T KLKalpha  with a solution . Laplacian Support Vector Machines (LapSVM) Support vector machines (SVMs) are a family of algorithms often used for classifying data into two or more groups, or classes. Intuitively, an SVM draws a boundary between classes so that the closest labeled examples to the boundary are as far away as possible. This can be directly expressed as a linear program, but it is also equivalent to Tikhonov regularization with the hinge loss function, V ( f ( x ) , y )  max ( 0 , 1 y f ( x ) ) displaystyle V(f(x),y)max(0,1-yf(x))  . Again letting K displaystyle K be the kernel matrix and J displaystyle J be the block matrix  I 0 0 0 u  displaystyle beginbmatrixI_ell 000_uendbmatrix , the solution can be shown to . Limitations Manifold regularization assumes that data with different labels are not likely to be close together. This assumption is what allows the technique to draw information from unlabeled data, but it only applies to some problem domains. Depending on the structure of the data, it may be necessary to use a different semi-supervised or transductive learning algorithm. In some datasets, the intrinsic norm of a function f I displaystyle leftfright_I can be very close to the ambient norm f K displaystyle leftfright_K  for example, if the data consist of two classes that lie on perpendicular lines, the intrinsic norm will be equal to the ambient norm. In this case, unlabeled data have no effect on the solution learned by manifold regularization, even if the data fit the algorithms assumption that the separator should be smooth. Approaches related to co-training have been proposed to address this limitation. If there are a very large number of unlabeled examples, the kernel matrix K displaystyle K becomes very large, and a manifold regularization algorithm may become prohibitively slow to compute. Online algorithms and sparse approximations of the manifold may help in this case. See also Manifold learning Manifold hypothesis Semi-supervised learning Transduction (machine learning) Spectral graph theory Reproducing kernel Hilbert space Tikhonov regularization Differential geometry References External links Software The ManifoldLearn library and the Primal LapSVM library implement LapRLS and LapSVM in MATLAB. The Dlib library for C includes a linear manifold regularization function. Title The Master Algorithm URL https//en.wikipedia.org/wiki/The_Master_Algorithm Content The Master Algorithm How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field. Overview The book outlines five approaches of machine learning inductive reasoning, connectionism, evolutionary computation, Bayes theorem and analogical modelling. The author explains these tribes to the reader by referring to more understandable processes of logic, connections made in the brain, natural selection, probability and similarity judgments. Throughout the book, it is suggested that each different tribe has the potential to contribute to a unifying master algorithm. Towards the end of the book the author pictures a master algorithm in the near future, where machine learning algorithms asymptotically grow to a perfect understanding of how the world and people in it work. Although the algorithm doesnt yet exist, he briefly reviews his own invention of the Markov logic network. In the media In 2016 Bill Gates recommended the book, alongside Nick Bostroms Superintelligence, as one of two books everyone should read to understand AI. In 2018 the book was noted to be on Chinese Communist Party general secretary Xi Jinpings bookshelf. Reception A computer science educator stated in Times Higher Education that the examples are clear and accessible. In contrast, The Economist agreed Domingos does a good job but complained that he constantly invents metaphors that grate or confuse. Kirkus Reviews praised the book, stating that Readers unfamiliar with logic and computer theory will have a difficult time, but those who persist will discover fascinating insights. A New Scientist review called it compelling but rather unquestioning. References Further reading https//www.wsj.com/articles/the-sum-of-human-knowledge-1442610803 http//www.kdnuggets.com/2015/09/book-master-algorithm-pedro-domingos.html http//www.kdnuggets.com/2014/08/interview-pedro-domingos-master-algorithm-new-deep-learning.html (interview) External links Official website Title Matchbox Educable Noughts and Crosses Engine URL https//en.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine Content The Matchbox Educable Noughts and Crosses Engine (sometimes called the Machine Educable Noughts and Crosses Engine or MENACE) was a mechanical computer made from 304 matchboxes designed and built by artificial intelligence researcher Donald Michie in 1961. It was designed to play human opponents in games of noughts and crosses (tic-tac-toe) by returning a move for any given state of play and to refine its strategy through reinforcement learning. This was one of the first types of artificial intelligence. Michie did not have a computer readily available, so he worked around this restriction by building it out of matchboxes. The matchboxes used by Michie each represented a single possible layout of a noughts and crosses grid. When the computer first played, it would randomly choose moves based on the current layout. As it played more games, through a reinforcement loop, it disqualified strategies that led to losing games, and supplemented strategies that led to winning games. Michie held a tournament against MENACE in 1961, wherein he experimented with different openings. Following MENACEs maiden tournament against Michie, it demonstrated successful artificial intelligence in its strategy. Michies essays on MENACEs weight initialisation and the BOXES algorithm used by MENACE became popular in the field of computer science research. Michie was honoured for his contribution to machine learning research, and was twice commissioned to program a MENACE simulation on an actual computer. Origin Donald Michie (1923 2007) had been on the team decrypting the German Tunny Code during World War II. Fifteen years later, he wanted to further display his mathematical and computational prowess with an early convolutional neural network. Since computer equipment was not obtainable for such uses, and Michie did not have a computer readily available, he decided to display and demonstrate artificial intelligence in a more esoteric format and constructed a functional mechanical computer out of matchboxes and beads. MENACE was constructed as the result of a bet with a computer science colleague who postulated that such a machine was impossible. Michie undertook the task of collecting and defining each matchbox as a fun project, later turned into a demonstration tool. Michie completed his essay on MENACE in 1963, Experiments on the mechanization of game-learning, as well as his essay on the BOXES Algorithm, written with R. A. Chambers and had built up an AI research unit in Hope Park Square, Edinburgh, Scotland. MENACE learned by playing successive matches of noughts and crosses. Each time, it would eliminate a losing strategy by the human player confiscating the beads that corresponded to each move. It reinforced winning strategies by making the moves more likely, by supplying extra beads. This was one of the earliest versions of the Reinforcement Loop, the schematic algorithm of looping the algorithm, dropping unsuccessful strategies until only the winning ones remain. This model starts as completely random, and gradually learns. Composition MENACE was made from 304 matchboxes glued together in an arrangement similar to a chest of drawers. Each box had a code number, which was keyed into a chart. This chart had drawings of tic-tac-toe game grids with various configurations of X, O, and empty squares, corresponding to all possible permutations a game could go through as it progressed. After removing duplicate arrangements (ones that were simply rotations or mirror images of other configurations), MENACE used 304 permutations in its chart and thus that many matchboxes. Each individual matchbox tray contained a collection of coloured beads. Each colour represented a move on a square on the game grid, and so matchboxes with arrangements where positions on the grid were already taken would not have beads for that position. Additionally, at the front of the tray were two extra pieces of card in a V shape, the point of the V pointing at the front of the matchbox. Michie and his artificial intelligence team called MENACEs algorithm Boxes, after the apparatus used for the machine. The first stage Boxes operated in five phases, each setting a definition and a precedent for the rules of the algorithm in relation to the game. Operation MENACE played first, as O, since all matchboxes represented permutations only relevant to the X player. To retrieve MENACEs choice of move, the opponent or operator located the matchbox that matched the current game state, or a rotation or mirror image of it. For example, at the start of a game, this would be the matchbox for an empty grid. The tray would be removed and lightly shaken so as to move the beads around. Then, the bead that had rolled into the point of the V shape at the front of the tray was the move MENACE had chosen to make. Its colour was then used as the position to play on, and, after accounting for any rotations or flips needed based on the chosen matchbox configurations relation to the current grid, the O would be placed on that square. Then the player performed their move, the new state was located, a new move selected, and so on, until the game was finished. When the game had finished, the human player observed the games outcome. As a game was played, each matchbox that was used for MENACEs turn had its tray returned to it ajar, and the bead used kept aside, so that MENACEs choice of moves and the game states they belonged to were recorded. Michie described his reinforcement system with reward and punishment. Once the game was finished, if MENACE had won, it would then receive a reward for its victory. The removed beads showed the sequence of the winning moves. These were returned to their respective trays, easily identifiable since they were slightly open, as well as three bonus beads of the same colour. In this way, in future games MENACE would become more likely to repeat those winning moves, reinforcing winning strategies. If it lost, the removed beads were not returned, punishing MENACE, and meaning that in future it would be less likely, and eventually incapable if that colour of bead became absent, to repeat the moves that cause a loss. If the game was a draw, one additional bead was added to each box. Results in practice Optimal strategy Noughts and crosses has a well-known optimal strategy. A player must place their symbol in a way that blocks the other player from achieving any rows while simultaneously making a row themself. However, if both players use this strategy, the game always ends in a draw. If the human player is familiar with the optimal strategy, and MENACE can quickly learn it, then the games will eventually only end in draws. The likelihood of the computer winning increases quickly when the computer plays against a random-playing opponent. When playing against a player using optimal strategy, the odds of a draw grow to 100. In Donald Michies official tournament against MENACE in 1961 he used optimal strategy, and he and the computer began to draw consistently after twenty games. Michies tournament had the following milestones Michie began by consistently opening with Variant 0, the middle square. At 15 games, MENACE abandoned all non-corner openings. At just over 20, Michie switched to consistently using Variant 1, the bottom-right square. At 60, he returned to Variant 0. As he neared 80 games, he moved to Variant 2, the top-middle. At 110, he switched to Variant 3, the top right. At 135, he switched to Variant 4, middle-right. At 190, he returned to Variant 1, and at 210, he returned to Variant 0. The trend in changes of beads in the 2 boxes runs Correlation Depending on the strategy employed by the human player, MENACE produces a different trend on scatter graphs of wins. Using a random turn from the human player results in an almost-perfect positive trend. Playing the optimal strategy returns a slightly slower increase. The reinforcement does not create a perfect standard of wins the algorithm will draw random uncertain conclusions each time. After the j-th round, the correlation of near-perfect play runs 1 D D D ( j  2 ) ). Below, Mn is the multiplier for the n-th round of the game. Legacy Donald Michies MENACE proved that a computer could learn from failure and success to become good at a task. It used what would become core principles within the field of machine learning before they had been properly theorised. For example, the combination of how MENACE starts with equal numbers of types of beads in each matchbox, and how these are then selected at random, creates a learning behaviour similar to weight initialisation in modern artificial neural networks. In 1968, Donald Michie and R.A Chambers made another BOXES-based algorithm called GLEE (Game Learning Expectimaxing Engine) which had to learn how to balance a pole on a cart. After the resounding reception of MENACE, Michie was invited to the US Office of Naval Research, where he was commissioned to build a BOXES-running program for an IBM Computer for use at Stanford University. Michie created a simulation program of MENACE on a Pegasus 2 computer with the aid of D. Martin. There have been multiple recreations of MENACE in more recent years, both in its original physical form and as a computer program. Its algorithm was later converged into Christopher Watkins Q-Learning algorithm. Although not as a functional computer, in examples of demonstration, MENACE has been used as a teaching aid for various neural network classes, including a public demonstration from University College London researcher Matthew Scroggs. A copy of MENACE built by Scroggs was featured in the 2019 Royal Institution Christmas Lectures, and in a 2023 episode of QI XL. MENACE in Popular Culture MENACE is referenced in Fred Saberhagens 1963 short story Without A Thought, and Thomas J Ryans 1977 novel The Adolescence of P-1. In her 2023 book The Future, author Naomi Alderman includes a fictional lecture with a detailed overview of MENACE. See also Hexapawn References Sources Michie, D. Chambers, R. A. (1968), BOXES An Experiment in Adaptive Control, Machine Intelligence, Edinburgh, UK Oliver and Boyd, S2CID 18229198 via Semantic Scholar, Michie and R. A Chambers paper on the AI implications of BOXES and MENACE. Russell, David W. (2012), The BOXES Methodology Black Box Dynamic Control, Springer London, ISBN 978-1849965286, a book on the Boxes algorithm employed by MENACE. External links Online simulation of MENACE Title Matrix regularization URL https//en.wikipedia.org/wiki/Matrix_regularization Content In the field of statistical learning theory, matrix regularization generalizes notions of vector regularization to cases where the object to be learned is a matrix. The purpose of regularization is to enforce conditions, for example sparsity or smoothness, that can produce stable predictive functions. For example, in the more common vector framework, Tikhonov regularization optimizes over min x A x y 2  x 2 displaystyle min _xleftAx-yright2lambda leftxright2 to find a vector x displaystyle x that is a stable solution to the regression problem. When the system is described by a matrix rather than a vector, this problem can be written as min X A X Y 2  X 2 , displaystyle min _XleftAX-Yright2lambda leftXright2, where the vector norm enforcing a regularization penalty on x displaystyle x has been extended to a matrix norm on X displaystyle X . Matrix regularization has applications in matrix completion, multivariate regression, and multi-task learning. Ideas of feature and group selection can also be extended to matrices, and these can be generalized to the nonparametric case of multiple kernel learning. Basic definition Consider a matrix W displaystyle W to be learned from a set of examples,  . Let each input matrix X i displaystyle X_i be R D T displaystyle in mathbb R DT , and let W displaystyle W be of size D T displaystyle Dtimes T . A general model for the output y displaystyle y can be posed as y i . For different applications the matrices X i displaystyle X_i will have different forms, but for each of these the optimization problem to infer W displaystyle W can be written as min W H E ( W )  R ( W ) , displaystyle min _Win mathcal HE(W)R(W), where E displaystyle E defines the empirical error for a given W displaystyle W , and R ( W ) displaystyle R(W) is a matrix regularization penalty. The function R ( W ) displaystyle R(W) is typically chosen to be convex and is often selected to enforce sparsity (using 1 displaystyle ell 1 -norms) and/or smoothness (using 2 displaystyle ell 2 -norms). Finally, W displaystyle W is in the space of matrices H displaystyle mathcal H with Frobenius inner product F displaystyle langle dots rangle _F . General applications Matrix completion In the problem of matrix completion, the matrix X i t displaystyle X_it takes the form X i  . In this case the role of the Frobenius inner product is to select individual elements w i t displaystyle w_it from the matrix W displaystyle W . Thus, the output y displaystyle y is a sampling of entries from the matrix W displaystyle W . The problem of reconstructing W displaystyle W from a small set of sampled entries is possible only under certain restrictions on the matrix, and these restrictions can be enforced by a regularization function. For example, it might be assumed that W displaystyle W is low-rank, in which case the regularization penalty can take the form of a nuclear norm. R ( W )   . Multivariate regression Models used in multivariate regression are parameterized by a matrix of coefficients. In the Frobenius inner product above, each matrix X displaystyle X is X i . The familiar form of such models is . One example is the squared Frobenius norm, which can be viewed as an 2 displaystyle ell 2 -norm acting either entrywise, or on the singular values of the matrix R ( W )  W F 2  i j  w i j  2  Tr ( W W )  i i 2 . displaystyle R(W)lambda leftWright_F2lambda sum _isum _jleftw_ijright2lambda operatorname Tr left(WWright)lambda sum _isigma _i2. In the multivariate case the effect of regularizing with the Frobenius norm is the same as the vector case very complex models will have larger norms, and, thus, will be penalized more. Multi-task learning The setup for multi-task learning is almost the same as the setup for multivariate regression. The primary difference is that the input variables are also indexed by task (columns of Y displaystyle Y ). The representation with the Frobenius inner product is then X i  . displaystyle X_ite_totimes x_it. The role of matrix regularization in this setting can be the same as in multivariate regression, but matrix norms can also be used to couple learning problems across tasks. In particular, note that for the optimization problem min W X W Y 2 2  W 2 2 displaystyle min _WleftXW-Yright_22lambda leftWright_22 the solutions corresponding to each column of Y displaystyle Y are decoupled. That is, the same solution can be found by solving the joint problem, or by solving an isolated regression problem for each column. The problems can be coupled by adding an additional regularization penalty on the covariance of solutions min W , X W Y 2 2  1 W 2 2  2 Tr ( W T 1 W ) displaystyle min _W,Omega leftXW-Yright_22lambda _1leftWright_22lambda _2operatorname Tr left(WTOmega -1Wright) where displaystyle Omega  models the relationship between tasks. This scheme can be used to both enforce similarity of solutions across tasks, and to learn the specific structure of task similarity by alternating between optimizations of W displaystyle W and displaystyle Omega  . When the relationship between tasks is known to lie on a graph, the Laplacian matrix of the graph can be used to couple the learning problems. Spectral regularization Regularization by spectral filtering has been used to find stable solutions to problems such as those discussed above by addressing ill-posed matrix inversions (see for example Filter function for Tikhonov regularization). In many cases the regularization function acts on the input (or kernel) to ensure a bounded inverse by eliminating small singular values, but it can also be useful to have spectral norms that act on the matrix that is to be learned. There are a number of matrix norms that act on the singular values of the matrix. Frequently used examples include the Schatten p-norms, with . For example, matrix regularization with a Schatten 1-norm, also called the nuclear norm, can be used to enforce sparsity in the spectrum of a matrix. This has been used in the context of matrix completion when the matrix in question is believed to have a restricted rank. In this case the optimization problem becomes min W subject to W i ,  . displaystyle min leftWright_text subject to W_i,jY_ij. Spectral Regularization is also used to enforce a reduced rank coefficient matrix in multivariate regression. In this setting, a reduced rank coefficient matrix can be found by keeping just the top n displaystyle n singular values, but this can be extended to keep any reduced set of singular values and vectors. Structured sparsity Sparse optimization has become the focus of much research interest as a way to find solutions that depend on a small number of variables (see e.g. the Lasso method). In principle, entry-wise sparsity can be enforced by penalizing the entry-wise 0 displaystyle ell 0 -norm of the matrix, but the 0 displaystyle ell 0 -norm is not convex. In practice this can be implemented by convex relaxation to the 1 displaystyle ell 1 -norm. While entry-wise regularization with an 1 displaystyle ell 1 -norm will find solutions with a small number of nonzero elements, applying an 1 displaystyle ell 1 -norm to different groups of variables can enforce structure in the sparsity of solutions. The most straightforward example of structured sparsity uses the p , q displaystyle ell _p,q norm with  . displaystyle leftWright_2,1sum _ileftw_iright_2. For example, the 2 , 1 displaystyle ell _2,1 norm is used in multi-task learning to group features across tasks, such that all the elements in a given row of the coefficient matrix can be forced to zero as a group. The grouping effect is achieved by taking the 2 displaystyle ell 2 -norm of each row, and then taking the total penalty to be the sum of these row-wise norms. This regularization results in rows that will tend to be all zeros, or dense. The same type of regularization can be used to enforce sparsity column-wise by taking the 2 displaystyle ell 2 -norms of each column. More generally, the 2 , 1 displaystyle ell _2,1 norm can be applied to arbitrary groups of variables R ( W )  g G j  G g   w g j  2  g G w g g displaystyle R(W)lambda sum _gGsqrt sum _jG_gleftw_gjright2lambda sum _gGleftw_gright_g where the index g displaystyle g is across groups of variables, and  G g  displaystyle G_g indicates the cardinality of group g displaystyle g . Algorithms for solving these group sparsity problems extend the more well-known Lasso and group Lasso methods by allowing overlapping groups, for example, and have been implemented via matching pursuit and proximal gradient methods. By writing the proximal gradient with respect to a given coefficient, w g i displaystyle w_gi , it can be seen that this norm enforces a group-wise soft threshold prox , R g ( w g )  . displaystyle operatorname prox _lambda ,R_gleft(w_gright)ileft(w_gi-lambda frac w_gileftw_gright_gright)mathbf 1 _w_g_ggeq lambda . where 1 w g g displaystyle mathbf 1 _w_g_ggeq lambda  is the indicator function for group norms displaystyle geq lambda  . Thus, using 2 , 1 displaystyle ell _2,1 norms it is straightforward to enforce structure in the sparsity of a matrix either row-wise, column-wise, or in arbitrary blocks. By enforcing group norms on blocks in multivariate or multi-task regression, for example, it is possible to find groups of input and output variables, such that defined subsets of output variables (columns in the matrix Y displaystyle Y ) will depend on the same sparse set of input variables. Multiple kernel selection The ideas of structured sparsity and feature selection can be extended to the nonparametric case of multiple kernel learning. This can be useful when there are multiple types of input data (color and texture, for example) with different appropriate kernels for each, or when the appropriate kernel is unknown. If there are two kernels, for example, with feature maps A displaystyle A and B displaystyle B that lie in corresponding reproducing kernel Hilbert spaces H A , H B displaystyle mathcal H_A,mathcal H_B , then a larger space, H D displaystyle mathcal H_D , can be created as the sum of two spaces H D   . In this case the 2 , 1 displaystyle ell _2,1 -norm is again the sum of norms f H D , 1  h H A  h H B displaystyle leftfright_mathcal H_D,1lefthright_mathcal H_Alefthright_mathcal H_B Thus, by choosing a matrix regularization function as this type of norm, it is possible to find a solution that is sparse in terms of which kernels are used, but dense in the coefficient of each used kernel. Multiple kernel learning can also be used as a form of nonlinear variable selection, or as a model aggregation technique (e.g. by taking the sum of squared norms and relaxing sparsity constraints). For example, each kernel can be taken to be the Gaussian kernel with a different width. See also Regularization (mathematics) Title Maximum inner-product search URL https//en.wikipedia.org/wiki/Maximum_inner-product_search Content Maximum inner-product search (MIPS) is a search problem, with a corresponding class of search algorithms which attempt to maximise the inner product between a query and the data items to be retrieved. MIPS algorithms are used in a wide variety of big data applications, including recommendation algorithms and machine learning. Formally, for a database of vectors x i displaystyle x_i defined over a set of labels S displaystyle S in an inner product space with an inner product , displaystyle langle cdot ,cdot rangle  defined on it, MIPS search can be defined as the problem of determining a r g m a x i S x i , q displaystyle underset iin Soperatorname arg,max  langle x_i,qrangle  for a given query q displaystyle q . Although there is an obvious linear-time implementation, it is generally too slow to be used on practical problems. However, efficient algorithms exist to speed up MIPS search. Under the assumption of all vectors in the set having constant norm, MIPS can be viewed as equivalent to a nearest neighbor search (NNS) problem in which maximizing the inner product is equivalent to minimizing the corresponding distance metric in the NNS problem. Like other forms of NNS, MIPS algorithms may be approximate or exact. MIPS search is used as part of DeepMinds RETRO algorithm. References See also Nearest neighbor search Title Meta-learning (computer science) URL https//en.wikipedia.org/wiki/Meta-learning_(computer_science) Content Meta-learning is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017, the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn. Flexibility is important because each learning algorithm is based on a set of assumptions about the data, its inductive bias. This means that it will only learn well if the bias matches the learning problem. A learning algorithm may perform very well in one domain, but not on the next. This poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood. By using different kinds of metadata, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta-learning approaches bear a strong resemblance to the critique of metaheuristic, a possibly related problem. A good analogy to meta-learning, and the inspiration for J rgen Schmidhubers early work (1987) and Yoshua Bengio et al.s work (1991), considers that genetic evolution learns the learning procedure encoded in genes and executed in each individuals brain. In an open-ended hierarchical meta-learning system using genetic programming, better evolutionary methods can be learned by meta evolution, which itself can be improved by meta meta evolution, etc. Definition A proposed definition for a meta-learning system combines three requirements The system must include a learning subsystem. Experience is gained by exploiting meta knowledge extracted in a previous learning episode on a single dataset, or from different domains. Learning bias must be chosen dynamically. Bias refers to the assumptions that influence the choice of explanatory hypotheses and not the notion of bias represented in the bias-variance dilemma. Meta-learning is concerned with two aspects of learning bias. Declarative bias specifies the representation of the space of hypotheses, and affects the size of the search space (e.g., represent hypotheses using linear functions only). Procedural bias imposes constraints on the ordering of the inductive hypotheses (e.g., preferring smaller hypotheses). Common approaches There are three common approaches using (cyclic) networks with external or internal memory (model-based) learning effective distance metrics (metrics-based) explicitly optimizing model parameters for fast learning (optimization-based). Model-Based Model-based meta-learning models updates its parameters rapidly with a few training steps, which can be achieved by its internal architecture or controlled by another meta-learner model. Memory-Augmented Neural Networks A Memory-Augmented Neural Network, or MANN for short, is claimed to be able to encode new information quickly and thus to adapt to new tasks after only a few examples. Meta Networks Meta Networks (MetaNet) learns a meta-level knowledge across tasks and shifts its inductive biases via fast parameterization for rapid generalization. Metric-Based The core idea in metric-based meta-learning is similar to nearest neighbors algorithms, which weight is generated by a kernel function. It aims to learn a metric or distance function over objects. The notion of a good metric is problem-dependent. It should represent the relationship between inputs in the task space and facilitate problem solving. Convolutional Siamese Neural Network Siamese neural network is composed of two twin networks whose output is jointly trained. There is a function above to learn the relationship between input data sample pairs. The two networks are the same, sharing the same weight and network parameters. Matching Networks Matching Networks learn a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. Relation Network The Relation Network (RN), is trained end-to-end from scratch. During meta-learning, it learns to learn a deep distance metric to compare a small number of images within episodes, each of which is designed to simulate the few-shot setting. Prototypical Networks Prototypical Networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve satisfied results. Optimization-Based What optimization-based meta-learning algorithms intend for is to adjust the optimization algorithm so that the model can be good at learning with a few examples. LSTM Meta-Learner LSTM-based meta-learner is to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training. Temporal Discreteness Model-Agnostic Meta-Learning (MAML) is a fairly general optimization algorithm, compatible with any model that learns through gradient descent. Reptile Reptile is a remarkably simple meta-learning optimization algorithm, given that both of its components rely on meta-optimization through gradient descent and both are model-agnostic. Examples Some approaches which have been viewed as instances of meta-learning Recurrent neural networks (RNNs) are universal computers. In 1993, J rgen Schmidhuber showed how self-referential RNNs can in principle learn by backpropagation to run their own weight change algorithm, which may be quite different from backpropagation. In 2001, Sepp Hochreiter  A.S. Younger  P.R. Conwell built a successful supervised meta-learner based on Long short-term memory RNNs. It learned through backpropagation a learning algorithm for quadratic functions that is much faster than backpropagation. Researchers at Deepmind (Marcin Andrychowicz et al.) extended this approach to optimization in 2017. In the 1990s, Meta Reinforcement Learning or Meta RL was achieved in Schmidhubers research group through self-modifying policies written in a universal programming language that contains special instructions for changing the policy itself. There is a single lifelong trial. The goal of the RL agent is to maximize reward. It learns to accelerate reward intake by continually improving its own learning algorithm which is part of the self-referential policy. An extreme type of Meta Reinforcement Learning is embodied by the G del machine, a theoretical construct which can inspect and modify any part of its own software which also contains a general theorem prover. It can achieve recursive self-improvement in a provably optimal way. Model-Agnostic Meta-Learning (MAML) was introduced in 2017 by Chelsea Finn et al. Given a sequence of tasks, the parameters of a given model are trained such that few iterations of gradient descent with few training data from a new task will lead to good generalization performance on that task. MAML trains the model to be easy to fine-tune. MAML was successfully applied to few-shot image classification benchmarks and to policy-gradient-based reinforcement learning. Variational Bayes-Adaptive Deep RL (VariBAD) was introduced in 2019. While MAML is optimization-based, VariBAD is a model-based method for meta reinforcement learning, and leverages a variational autoencoder to capture the task information in an internal memory, thus conditioning its decision making on the task. When addressing a set of tasks, most meta learning approaches optimize the average score across all tasks. Hence, certain tasks may be sacrificed in favor of the average score, which is often unacceptable in real-world applications. By contrast, Robust Meta Reinforcement Learning (RoML) focuses on improving low-score tasks, increasing robustness to the selection of task. RoML works as a meta-algorithm, as it can be applied on top of other meta learning algorithms (such as MAML and VariBAD) to increase their robustness. It is applicable to both supervised meta learning and meta reinforcement learning. Discovering meta-knowledge works by inducing knowledge (e.g. rules) that expresses how each learning method will perform on different learning problems. The metadata is formed by characteristics of the data (general, statistical, information-theoretic,... ) in the learning problem, and characteristics of the learning algorithm (type, parameter settings, performance measures,...). Another learning algorithm then learns how the data characteristics relate to the algorithm characteristics. Given a new learning problem, the data characteristics are measured, and the performance of different learning algorithms are predicted. Hence, one can predict the algorithms best suited for the new problem. Stacked generalisation works by combining multiple (different) learning algorithms. The metadata is formed by the predictions of those different algorithms. Another learning algorithm learns from this metadata to predict which combinations of algorithms give generally good results. Given a new learning problem, the predictions of the selected set of algorithms are combined (e.g. by (weighted) voting) to provide the final prediction. Since each algorithm is deemed to work on a subset of problems, a combination is hoped to be more flexible and able to make good predictions. Boosting is related to stacked generalisation, but uses the same algorithm multiple times, where the examples in the training data get different weights over each run. This yields different predictions, each focused on rightly predicting a subset of the data, and combining those predictions leads to better (but more expensive) results. Dynamic bias selection works by altering the inductive bias of a learning algorithm to match the given problem. This is done by altering key aspects of the learning algorithm, such as the hypothesis representation, heuristic formulae, or parameters. Many different approaches exist. Inductive transfer studies how the learning process can be improved over time. Metadata consists of knowledge about previous learning episodes and is used to efficiently develop an effective hypothesis for a new task. A related approach is called learning to learn, in which the goal is to use acquired knowledge from one domain to help learning in other domains. Other approaches using metadata to improve automatic learning are learning classifier systems, case-based reasoning and constraint satisfaction. Some initial, theoretical work has been initiated to use Applied Behavioral Analysis as a foundation for agent-mediated meta-learning about the performances of human learners, and adjust the instructional course of an artificial agent. AutoML such as Google Brains AI building AI project, which according to Google briefly exceeded existing ImageNet benchmarks in 2017. References External links Metalearning article in Scholarpedia Vilalta, R. Drissi, Y. (2002). A perspective view and survey of meta-learning (PDF). Artificial Intelligence Review. 18 (2) 77 95. doi10.1023/A1019956318069. Giraud-Carrier, C. Keller, J. (2002). Meta-Learning. In Meij, J. (ed.). Dealing with the data flood. The Hague STT/Beweton. Brazdil, P. Giraud-Carrier, C. Soares, C. Vilalta, R. (2009). Metalearning Concepts and Systems. Metalearning applications to data mining. Springer. ISBN 978-3-540-73262-4. Video courses about Meta-Learning with step-by-step explanation of MAML, Prototypical Networks, and Relation Networks. Title MLOps URL https//en.wikipedia.org/wiki/MLOps Content MLOps or ML Ops is a paradigm that aims to deploy and maintain machine learning models in production reliably and efficiently. The word is a compound of machine learning and the continuous delivery practice (CI/CD) of DevOps in the software field. Machine learning models are tested and developed in isolated experimental systems. When an algorithm is ready to be launched, MLOps is practiced between Data Scientists, DevOps, and Machine Learning engineers to transition the algorithm to production systems. Similar to DevOps or DataOps approaches, MLOps seeks to increase automation and improve the quality of production models, while also focusing on business and regulatory requirements. While MLOps started as a set of best practices, it is slowly evolving into an independent approach to ML lifecycle management. MLOps applies to the entire lifecycle - from integrating with model generation (software development lifecycle, continuous integration/continuous delivery), orchestration, and deployment, to health, diagnostics, governance, and business metrics. Definition MLOps is a paradigm, including aspects like best practices, sets of concepts, as well as a development culture when it comes to the end-to-end conceptualization, implementation, monitoring, deployment, and scalability of machine learning products. Most of all, it is an engineering practice that leverages three contributing disciplines machine learning, software engineering (especially DevOps), and data engineering. MLOps is aimed at productionizing machine learning systems by bridging the gap between development (Dev) and operations (Ops). Essentially, MLOps aims to facilitate the creation of machine learning products by leveraging these principles CI/CD automation, workflow orchestration, reproducibility versioning of data, model, and code collaboration continuous ML training and evaluation ML metadata tracking and logging continuous monitoring and feedback loops. History The challenges of the ongoing use of machine learning in applications were highlighted in a 2015 paper. The predicted growth in machine learning included an estimated doubling of ML pilots and implementations from 2017 to 2018, and again from 2018 to 2020. MLOps rapidly began to gain traction among AI/ML experts, companies, and technology journalists as a solution that can address the complexity and growth of machine learning in businesses. Reports show a majority (up to 88) of corporate machine learning initiatives are struggling to move beyond test stages. However, those organizations that actually put machine learning into production saw a 3-15 profit margin increases. The MLOps market was estimated at 126 billion by 2025 due to rapid adoption. Architecture Machine Learning systems can be categorized in eight different categories data collection, data processing, feature engineering, data labeling, model design, model training and optimization, endpoint deployment, and endpoint monitoring. Each step in the machine learning lifecycle is built in its own system, but requires interconnection. These are the minimum systems that enterprises need to scale machine learning within their organization. Goals There are a number of goals enterprises want to achieve through MLOps systems successfully implementing ML across the enterprise, including Deployment and automation Reproducibility of models and predictions Diagnostics Governance and regulatory compliance Scalability Collaboration Business uses Monitoring and management A standard practice, such as MLOps, takes into account each of the aforementioned areas, which can help enterprises optimize workflows and avoid issues during implementation. A common architecture of an MLOps system would include data science platforms where models are constructed and the analytical engines where computations are performed, with the MLOps tool orchestrating the movement of machine learning models, data and outcomes between the systems. See also ModelOps, according to Gartner, MLOps is a subset of ModelOps. MLOps is focused on the operationalization of ML models, while ModelOps covers the operationalization of all types of AI models. AIOps, a similarly named, but different concept - using AI (ML) in IT and Operations. Title MobileNet URL https//en.wikipedia.org/wiki/MobileNet Content MobileNet is a family of convolutional neural network (CNN) architectures designed for image classification, object detection, and other computer vision tasks. They are designed for small size, low latency, and low power consumption, making them suitable for on-device inference and edge computing on resource-constrained devices like mobile phones and embedded systems. They were originally designed to be run efficiently on mobile devices with TensorFlow Lite. The need for efficient deep learning models on mobile devices led researchers at Google to develop MobileNet. As of October 2024, the family has four versions, each improving upon the previous one in terms of performance and efficiency. Features was published in April 2017. Its main architectural innovation was incorporation of depthwise separable convolutions. It was first developed by Laurent Sifre during an internship at Google Brain in 2013 as an architectural variation on AlexNet to improve convergence speed and model size. The depthwise separable convolution decomposes a single standard convolution into two convolutions a depthwise convolution that filters each input channel independently and a pointwise convolution ( 1 1 displaystyle 1times 1 convolution) that combines the outputs of the depthwise convolution. This factorization significantly reduces computational cost. The has two hyperparameters a width multiplier displaystyle alpha  that controls the number of channels in each layer. Smaller values of displaystyle alpha  lead to smaller and faster models, but at the cost of reduced accuracy, and a resolution multiplier displaystyle rho  , which controls the input resolution of the images. Lower resolutions result in faster processing but potentially lower accuracy. was published in March 2019. It uses inverted residual layers and linear bottlenecks. Inverted residuals modify the traditional residual block structure. Instead of compressing the input channels before the depthwise convolution, they expand them. This expansion is followed by a 1 1 displaystyle 1times 1 depthwise convolution and then a 1 1 displaystyle 1times 1 projection layer that reduces the number of channels back down. This inverted structure helps to maintain representational capacity by allowing the depthwise convolution to operate on a higher-dimensional feature space, thus preserving more information flow during the convolutional process. Linear bottlenecks removes the typical ReLU activation function in the projection layers. This was rationalized by arguing that that nonlinear activation loses information in lower-dimensional spaces, which is problematic when the number of channels is already small. was published in 2019. The publication included -Small, -Large, and MobileNetEdgeTPU (optimized for Pixel 4). They were found by a form of neural architecture search (NAS) that takes mobile latency into account, to achieve good trade-off between accuracy and latency. It used piecewise-linear approximations of swish and sigmoid activation functions (which they called h-swish and h-sigmoid), squeeze-and-excitation modules, and the inverted bottlenecks of . was published in September 2024. The publication included a large number of architectures found by NAS. Compared to the architectural modules used in , the series included the universal inverted bottleneck, which includes both inverted residual and inverted bottleneck as special cases, and attention modules with multi-query attention. See also Convolutional neural network Deep learning TensorFlow Lite External links models/research/slim/nets/mobilenet at master tensorflow/models. GitHub. Retrieved 2024-10-18. Keras documentation MobileNet, , and . Keras. Retrieved October 18, 2024. Title Mode collapse URL https//en.wikipedia.org/wiki/Mode_collapse Content In machine learning, mode collapse is a failure mode observed in generative models, originally noted in Generative Adversarial Networks (GANs). It occurs when the model produces outputs that are less diverse than expected, effectively collapsing to generate only a few modes of the data distribution while ignoring others. This phenomenon undermines the goal of generative models to capture the full diversity of the training data. There are typically two times at which a model can collapse either during training or during post-training finetuning. Mode collapse reduces the utility of generative models in applications, such as in image synthesis (repetitive or near-identical images) data augmentation (limited diversity in synthetic data) scientific simulations (failure to explore all plausible scenarios). Distinctions Mode collapse is distinct from overfitting, where a model learns detailed patterns in the training data that does not generalize to the test data, and underfitting, where it fails to learn patterns. Memorization is where a model learns to reproduce data from the training data. Memorization is often confused with mode collapse. However, a model can memorize the training dataset without mode collapse. Indeed, if a model is severely mode-collapsed, then it have failed to memorize large parts of the training dataset. Model collapse is one particular mechanism for the phenomenon of mode collapse, i.e. when a generative model 2 is pretrained mainly on the outputs of model 1, then another new generative model 3 is pretrained mainly on the outputs of model 2, etc. When models are trained in this way, each model is typically more mode-collapsed than the previous one. However, there are other mechanisms for mode collapse. In GANs Training-time mode collapse was originally noted and studied in GANs, where it arises primarily due to imbalances in the training dynamics between the generator and discriminator in GANs. In the original GAN paper, it was also called the Helvetica scenario. Common causes include If the discriminator learns too slowly, the generator may exploit weaknesses by producing a narrow set of outputs that consistently fool the discriminator. Traditional GAN loss functions (e.g., Jensen-Shannon divergence) may be too lenient on generating same-looking outputs. The adversarial training process can lead to oscillatory behavior, where the generator and discriminator fail to converge to a stable equilibrium, but instead engage in a rock-beats-paper-beats-scissors kind of cycling. The generator would generate just rock until the discriminator learns to classify that as generated, then the generator switch to generating just scissors, and so on. The generator would always be mode-collapsed, though the precise mode in which it collapses to would change during training. Several GAN-specific strategies were developed to mitigate mode collapse Two time-scale update rule. Mini-batch discrimination allows the discriminator to evaluate entire batches of samples, encouraging diversity. Unrolled GANs optimize the generator against future states of the discriminator. Wasserstein GAN uses Earth Movers distance to provide more stable gradients. Use a big and balanced training dataset. Regularization methods such as gradient penalty and spectral normalization. Finetuning The large language models are usually trained in two steps. In the first step (pretraining), the model is trained to simply generate text sampled from a large dataset. In the second step (finetuning), the model is trained to perform specific tasks by training it on a small dataset containing just the task-specific data. For example, to make a chatbot in this method, one first pretrains a large transformer model over a few trillion words of text scraped from the Internet, then finetunes it on a few million words of example chatlogs that the model should imitate. Mode collapse may occur during finetuning, as the model learns to generate text that accomplishes the specific task, but loses ability to generate other forms of text. It may also be able to generate a smaller subset of texts that accomplish the specific task. It is hypothesized that there is a tradeoff between quality and diversity. Given a single pretrained model, one may finetune it to perform a specific task. More finetuning would result in higher average task performance, but less diverse outputs. Less finetuning would result in lower average performance, but more diverse outputs. A similar tradeoff has been observed in image generation models and GAN-based text generators. Similarly, mode collapse may occur during RLHF, via reward hacking the reward model or other mechanisms. See also Variational autoencoder Generative model Generative artificial intelligence Generative pre-trained transformer Overfitting Title Model compression URL https//en.wikipedia.org/wiki/Model_compression Content Model compression is a machine learning technique for reducing the size of trained models. Large models can achieve high accuracy, but often at the cost of significant resource requirements. Compression techniques aim to compress models without significant performance reduction. Smaller models require less storage space, and consume less memory and compute during inference. Compressed models enable deployment on resource-constrained devices such as smartphones, embedded systems, edge computing devices, and consumer electronics computers. Efficient inference is also valuable for large corporations that serve large model inference over an API, allowing them to reduce computational costs and improve response times for users. Model compression is not to be confused with knowledge distillation, in which a separate, smaller student model is trained to imitate the input-output behavior of a larger teacher model. Techniques Several techniques are employed for model compression. Pruning Pruning sparsifies a large model by setting some parameters to exactly zero. This effectively reduces the number of parameters. This allows the use of sparse matrix operations, which are faster than dense matrix operations. Pruning criteria can be based on magnitudes of parameters, the statistical pattern of neural activations, Hessian values, etc. Quantization Quantization reduces the numerical precision of weights and activations. For example, instead of storing weights as 32-bit floating-point numbers, they can be represented using 8-bit integers. Low-precision parameters take up less space, and takes less compute to perform arithmetic with. It is also possible to quantize some parameters more aggressively than others, so for example, a less important parameter can have 8-bit precision while another, more important parameter, can have 16-bit precision. Inference with such models requires mixed-precision arithmetic. Quantized models can also be used during training (rather than after training). PyTorch implements automatic mixed-precision (AMP), which performs autocasting, gradient scaling, and loss scaling. Low-rank factorization Weight matrices can be approximated by low-rank matrices. Let W displaystyle W be a weight matrix of shape m n displaystyle mtimes n . A low-rank approximation is W U V T displaystyle Wapprox UVT , where U displaystyle U and V displaystyle V are matrices of shapes m k , n k displaystyle mtimes k,ntimes k . When k displaystyle k is small, this both reduces the number of parameters needed to represent W displaystyle W approximately, and accelerates matrix multiplication by W displaystyle W . Low-rank approximations can be found by singular value decomposition (SVD). The choice of rank for each weight matrix is a hyperparameter, and jointly optimized as a mixed discrete-continuous optimization problem. The rank of weight matrices may also be pruned after training, taking into account the effect of activation functions like ReLU on the implicit rank of the weight matrices. Training Model compression may be decoupled from training, that is, a model is first trained without regard for how it might be compressed, then it is compressed. However, it may also be combined with training. The train big, then compress method trains a large model for a small number of training steps (less than it would be if it were trained to convergence), then heavily compress the model. It is found that at the same compute budget, this method results in a better model than lightly compressed, small models. In Deep Compression, the compression has three steps. First loop (pruning) prune all weights lower than a threshold, then finetune the network, then prune again, etc. Second loop (quantization) cluster weights, then enforce weight sharing among all weights in each cluster, then finetune the network, then cluster again, etc. Third step Use Huffman coding to losslessly compress the model. The SqueezeNet paper reported that Deep Compression achieved a compression ratio of 35 on AlexNet, and a ratio of 10 on SqueezeNets. References Review papers Li, Zhuo Li, Hengyi Meng, Lin (March 12, 2023). Model Compression for Deep Neural Networks A Survey. Computers. 12 (3). MDPI AG 60. doi10.3390/. ISSN 2073-431X. Deng, By Lei Li, Guoqi Han, Song Shi, Luping Xie, Yuan (March 20, 2020). Model Compression and Hardware Acceleration for Neural Networks A Comprehensive Survey. Proceedings of the IEEE. 108 (4) 485 532. doi10.1109/JPROC.2020.2976475. Retrieved October 18, 2024. Cheng, Yu Wang, Duo Zhou, Pan Zhang, Tao (October 23, 2017). A Survey of Model Compression and Acceleration for Deep Neural Networks. arXiv1710.09282 cs.LG. Choudhary, Tejalal Mishra, Vipul Goswami, Anurag Sarangapani, Jagannathan (February 8, 2020). A comprehensive survey on model compression and acceleration. Artificial Intelligence Review. 53 (7). Springer Science and Business Media LLC 5113 5155. doi10.1007/-020-09816-7. ISSN 0269-2821. Title Mountain car problem URL https//en.wikipedia.org/wiki/Mountain_car_problem Content Mountain Car, a standard testing domain in Reinforcement learning, is a problem in which an under-powered car must drive up a steep hill. Since gravity is stronger than the cars engine, even at full throttle, the car cannot simply accelerate up the steep slope. The car is situated in a valley and must learn to leverage potential energy by driving up the opposite hill before the car is able to make it to the goal at the top of the rightmost hill. The domain has been used as a test bed in various reinforcement learning papers. Introduction The mountain car problem, although fairly simple, is commonly applied because it requires a reinforcement learning agent to learn on two continuous variables position and velocity. For any given state (position and velocity) of the car, the agent is given the possibility of driving left, driving right, or not using the engine at all. In the standard version of the problem, the agent receives a negative reward at every time step when the goal is not reached the agent has no information about the goal until an initial success. History The mountain car problem appeared first in Andrew Moores PhD thesis (1990). It was later more strictly defined in Singh and Suttons reinforcement learning paper with eligibility traces. The problem became more widely studied when Sutton and Barto added it to their book Reinforcement Learning An Introduction (1998). Throughout the years many versions of the problem have been used, such as those which modify the reward function, termination condition, and the start state. Techniques used to solve mountain car Q-learning and similar techniques for mapping discrete states to discrete actions need to be extended to be able to deal with the continuous state space of the problem. Approaches often fall into one of two categories, state space discretization or function approximation. Discretization In this approach, two continuous state variables are pushed into discrete states by bucketing each continuous variable into multiple discrete states. This approach works with properly tuned parameters but a disadvantage is information gathered from one state is not used to evaluate another state. Tile coding can be used to improve discretization and involves continuous variables mapping into sets of buckets offset from one another. Each step of training has a wider impact on the value function approximation because when the offset grids are summed, the information is diffused. Function approximation Function approximation is another way to solve the mountain car. By choosing a set of basis functions beforehand, or by generating them as the car drives, the agent can approximate the value function at each state. Unlike the step-wise version of the value function created with discretization, function approximation can more cleanly estimate the true smooth function of the mountain car domain. Eligibility traces One aspect of the problem involves the delay of actual reward. The agent is not able to learn about the goal until a successful completion. Given a naive approach for each trial the car can only backup the reward of the goal slightly. This is a problem for naive discretization because each discrete state will only be backed up once, taking a larger number of episodes to learn the problem. This problem can be alleviated via the mechanism of eligibility traces, which will automatically backup the reward given to states before, dramatically increasing the speed of learning. Eligibility traces can be viewed as a bridge from temporal difference learning methods to Monte Carlo methods. Technical details The mountain car problem has undergone many iterations. This section focuses on the standard well-defined version from Sutton (2008). State variables Two-dimensional continuous state space. V e l o c i t .07 , 0.07 ) displaystyle .07,0.07) P o s i t i o .2 , 0.6 ) displaystyle .2,0.6) Actions One-dimensional discrete action space. m o t o .001  cos ( 3 P o s i t i o n ) ( 0.0025 ) displaystyle .001cos(3Position)(-0.0025) P o s i t i o . P o s i t i o .5 displaystyle .5 V e l o c i t .0 displaystyle .0 Termination condition End the simulation when P o s i t i o n 0.6 displaystyle Positiongeq 0.6 Variations There are many versions of the mountain car which deviate in different ways from the standard model. Variables that vary include but are not limited to changing the constants (gravity and steepness) of the problem so specific tuning for specific policies become irrelevant and altering the reward function to affect the agents ability to learn in a different manner. An example is changing the reward to be equal to the distance from the goal, or changing the reward to zero everywhere and one at the goal. Additionally, a 3D mountain car can be used, with a 4D continuous state space. References Implementations C Mountain Car Software. Richard s. Sutton. Java Mountain Car with support for RL Glue Python, with good discussion (blog post - down page) Further reading Sutton, Richard S. (1996). Mountain Car with Sparse Coarse Coding. Advances in Neural Information Processing Systems. MIT Press. pp. 1038 1044. CiteSeerx 10.1.1.51.4764. Mountain Car with Replacing Eligibility Traces More discussion on Continuous State Spaces. 2000. pp. 903 910. CiteSeerX 10.1.1.97.9314. Gaussian Processes with Mountain Car Title Multi-armed bandit URL https//en.wikipedia.org/wiki/Multi-armed_bandit Content In probability theory and machine learning, the multi-armed bandit problem (sometimes called the K- or N-armed bandit problem) is a problem in which a decision maker iteratively selects one of multiple fixed choices (i.e., arms or actions) when the properties of each choice are only partially known at the time of allocation, and may become better understood as time passes. A fundamental aspect of bandit problems is that choosing an arm does not affect the properties of the arm or other arms. Instances of the multi-armed bandit problem include the task of iteratively allocating a fixed, limited set of resources between competing (alternative) choices in a way that minimizes the regret. A notable alternative setup for the multi-armed bandit problem include the best arm identification (BAI) problem where the goal is instead to identify the best choice by the end of a finite number of rounds. The multi-armed bandit problem is a classic reinforcement learning problem that exemplifies the exploration exploitation tradeoff dilemma. In contrast to general RL, the selected actions in bandit problems do not affect the reward distribution of the arms. The name comes from imagining a gambler at a row of slot machines (sometimes known as one-armed bandits), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine. The multi-armed bandit problem also falls into the broad category of stochastic scheduling. In the problem, each machine provides a random reward from a probability distribution specific to that machine, that is not known a priori. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. The crucial tradeoff the gambler faces at each trial is between exploitation of the machine that has the highest expected payoff and exploration to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in machine learning. In practice, multi-armed bandits have been used to model problems such as managing research projects in a large organization, like a science foundation or a pharmaceutical company. In early versions of the problem, the gambler begins with no initial knowledge about the machines. Herbert Robbins in 1952, realizing the importance of the problem, constructed convergent population selection strategies in some aspects of the sequential design of experiments. A theorem, the Gittins index, first published by John C. Gittins, gives an optimal policy for maximizing the expected discounted reward. Empirical motivation The multi-armed bandit problem models an agent that simultaneously attempts to acquire new knowledge (called exploration) and optimize their decisions based on existing knowledge (called exploitation). The agent attempts to balance these competing tasks in order to maximize their total value over the period of time considered. There are many practical applications of the bandit model, for example clinical trials investigating the effects of different experimental treatments while minimizing patient losses, adaptive routing efforts for minimizing delays in a network, financial portfolio design In these practical examples, the problem requires balancing reward maximization based on the knowledge already acquired with attempting new actions to further increase knowledge. This is known as the exploitation vs. exploration tradeoff in machine learning. The model has also been used to control dynamic allocation of resources to different projects, answering the question of which project to work on, given uncertainty about the difficulty and payoff of each possibility. Originally considered by Allied scientists in World War II, it proved so intractable that, according to Peter Whittle, the problem was proposed to be dropped over Germany so that German scientists could also waste their time on it. The version of the problem now commonly analyzed was formulated by Herbert Robbins in 1952. The multi-armed bandit model The multi-armed bandit (short bandit or MAB) can be seen as a set of real distributions . Let 1 , , K displaystyle mu _1,dots ,mu _K be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon H displaystyle H is the number of rounds that remain to be played. The bandit problem is formally equivalent to a one-state Markov decision process. The regret displaystyle rho  after T displaystyle T rounds is defined as the expected difference between the reward sum associated with an optimal strategy and the sum of the collected rewards  T . A zero-regret strategy is a strategy whose average regret per round / T displaystyle rho /T tends to zero with probability 1 when the number of played rounds tends to infinity. Intuitively, zero-regret strategies are guaranteed to converge to a (not necessarily unique) optimal strategy if enough rounds are played. Variations A common formulation is the Binary multi-armed bandit or Bernoulli multi-armed bandit, which issues a reward of one with probability p displaystyle p , and otherwise a reward of zero. Another formulation of the multi-armed bandit has each arm representing an independent Markov machine. Each time a particular arm is played, the state of that machine advances to a new one, chosen according to the Markov state evolution probabilities. There is a reward depending on the current state of the machine. In a generalization called the restless bandit problem, the states of non-played arms can also evolve over time. There has also been discussion of systems where the number of choices (about which arm to play) increases over time. Computer science researchers have studied multi-armed bandits under worst-case assumptions, obtaining algorithms to minimize regret in both finite and infinite (asymptotic) time horizons for both stochastic and non-stochastic arm payoffs. Best Arm Identification An important variation of the classical regret minimization problem in multi-armed bandits is the one of Best Arm Identification (BAI), also known as pure exploration. This problem is crucial in various applications, including clinical trials, adaptive routing, recommendation systems, and A/B testing. In BAI, the objective is to identify the arm having the highest expected reward. An algorithm in this setting is characterized by a sampling rule, a decision rule, and a stopping rule, described as follows Sampling rule ( a t ) t 1 displaystyle (a_t)_tgeq 1 is a sequence of actions at each time step Stopping rule displaystyle tau  is a (random) stopping time which suggests when to stop collecting samples Decision rule a  displaystyle hat a_tau  is a guess on the best arm based on the data collected up to time displaystyle tau  There are two predominant settings in BAI Fixed budget setting Given a time horizon T 1 displaystyle Tgeq 1 , the objective is to identify the arm with the highest expected reward a arg max k k displaystyle astar in arg max _kmu _k minimizing probability of error displaystyle delta  . Fixed confidence setting Given a confidence level ( 0 , 1 ) displaystyle delta in (0,1) , the objective is to identify the arm with the highest expected reward a arg max k k displaystyle astar in arg max _kmu _k with the least possible amount of trials and with probability of error P ( a  a ) displaystyle mathbb P (hat a_tau neq astar )leq delta  . For example using a decision rule, we could use m 1 displaystyle m_1 where m displaystyle m is the machine no.1 (you can use a different variable respectively) and 1 displaystyle 1 is the amount for each time an attempt is made at pulling the lever, where m 1 , m 2 , ( . . . )  M displaystyle int sum m_1,m_2,(...)M , identify M displaystyle M as the sum of each attempts m 1  m 2 displaystyle m_1m_2 , (...) as needed, and from there you can get a ratio, sum or mean as quantitative probability and sample your formulation for each slots. You can also do k i N ( n j ) displaystyle int sum _kpropto _iN-(n_j) where m 1  m 2 displaystyle  equal to each a unique machine slot, x , y displaystyle x,y is the amount each time the lever is triggered, N displaystyle N is the sum of ( m 1 x , y )  ( m 2 x , y ) ( . . . ) displaystyle (m1_x,_y)(m2_x,_y)(...) , k displaystyle k would be the total available amount in your possession, k displaystyle k is relative to N displaystyle N where . It is possible to express this construction using a combination of multiple algebraic formulation, as mentioned above where you can limit with T displaystyle T for, or in Time and so on. Bandit strategies A major breakthrough was the construction of optimal population selection strategies, or policies (that possess uniformly maximum convergence rate to the population with highest mean) in the work described below. Optimal solutions In the paper Asymptotically efficient adaptive allocation rules, Lai and Robbins (following papers of Robbins and his co-workers going back to Robbins in the year 1952) constructed convergent population selection policies that possess the fastest rate of convergence (to the population with highest mean) for the case that the population reward distributions are the one-parameter exponential family. Then, in Katehakis and Robbins simplifications of the policy and the main proof were given for the case of normal populations with known variances. The next notable progress was obtained by Burnetas and Katehakis in the paper Optimal adaptive policies for sequential allocation problems, where index based policies with uniformly maximum convergence rate were constructed, under more general conditions that include the case in which the distributions of outcomes from each population depend on a vector of unknown parameters. Burnetas and Katehakis (1996) also provided an explicit solution for the important case in which the distributions of outcomes follow arbitrary (i.e., non-parametric) discrete, univariate distributions. Later in Optimal adaptive policies for Markov decision processes Burnetas and Katehakis studied the much larger model of Markov Decision Processes under partial information, where the transition law and/or the expected one period rewards may depend on unknown parameters. In this work, the authors constructed an explicit form for a class of adaptive policies with uniformly maximum convergence rate properties for the total expected finite horizon reward under sufficient assumptions of finite state-action spaces and irreducibility of the transition law. A main feature of these policies is that the choice of actions, at each state and time period, is based on indices that are inflations of the right-hand side of the estimated average reward optimality equations. These inflations have recently been called the optimistic approach in the work of Tewari and Bartlett, Ortner Filippi, Capp , and Garivier, and Honda and Takemura. For Bernoulli multi-armed bandits, Pilarski et al. studied computation methods of deriving fully optimal solutions (not just asymptotically) using dynamic programming in the paper Optimal Policy for Bernoulli Bandits Computation and Algorithm Gauge. Via indexing schemes, lookup tables, and other techniques, this work provided practically applicable optimal solutions for Bernoulli bandits provided that time horizons and numbers of arms did not become excessively large. Pilarski et al. later extended this work in Delayed Reward Bernoulli Bandits Optimal Policy and Predictive Meta-Algorithm PARDI to create a method of determining the optimal policy for Bernoulli bandits when rewards may not be immediately revealed following a decision and may be delayed. This method relies upon calculating expected values of reward outcomes which have not yet been revealed and updating posterior probabilities when rewards are revealed. When optimal solutions to multi-arm bandit tasks are used to derive the value of animals choices, the activity of neurons in the amygdala and ventral striatum encodes the values derived from these policies, and can be used to decode when the animals make exploratory versus exploitative choices. Moreover, optimal policies better predict animals choice behavior than alternative strategies (described below). This suggests that the optimal solutions to multi-arm bandit problems are biologically plausible, despite being computationally demanding. Approximate solutions Many strategies exist which provide an approximate solution to the bandit problem, and can be put into the four broad categories detailed below. Semi-uniform strategies Semi-uniform strategies were the earliest (and simplest) strategies discovered to approximately solve the bandit problem. All those strategies have in common a greedy behavior where the best lever (based on previous observations) is always pulled except when a (uniformly) random action is taken. Epsilon-greedy strategy The best lever is selected for a proportion 1 displaystyle 1-epsilon  of the trials, and a lever is selected at random (with uniform probability) for a proportion displaystyle epsilon  . A typical parameter value might .1 displaystyle .1 , but this can vary widely depending on circumstances and predilections. Epsilon-first strategy A pure exploration phase is followed by a pure exploitation phase. For N displaystyle N trials in total, the exploration phase occupies N displaystyle epsilon N trials and the exploitation phase ( 1 ) N displaystyle (1-epsilon )N trials. During the exploration phase, a lever is randomly selected (with uniform probability) during the exploitation phase, the best lever is always selected. Epsilon-decreasing strategy Similar to the epsilon-greedy strategy, except that the value of displaystyle epsilon  decreases as the experiment progresses, resulting in highly explorative behaviour at the start and highly exploitative behaviour at the finish. Adaptive epsilon-greedy strategy based on value differences (VDBE) Similar to the epsilon-decreasing strategy, except that epsilon is reduced on basis of the learning progress instead of manual tuning (Tokic, 2010). High fluctuations in the value estimates lead to a high epsilon (high exploration, low exploitation) low fluctuations to a low epsilon (low exploration, high exploitation). Further improvements can be achieved by a softmax-weighted action selection in case of exploratory actions (Tokic  Palm, 2011). Adaptive epsilon-greedy strategy based on Bayesian ensembles (Epsilon-BMC) An adaptive epsilon adaptation strategy for reinforcement learning similar to VBDE, with monotone convergence guarantees. In this framework, the epsilon parameter is viewed as the expectation of a posterior distribution weighting a greedy agent (that fully trusts the learned reward) and uniform learning agent (that distrusts the learned reward). This posterior is approximated using a suitable Beta distribution under the assumption of normality of observed rewards. In order to address the possible risk of decreasing epsilon too quickly, uncertainty in the variance of the learned reward is also modeled and updated using a normal-gamma model. (Gimelfarb et al., 2019). Probability matching strategies Probability matching strategies reflect the idea that the number of pulls for a given lever should match its actual probability of being the optimal lever. Probability matching strategies are also known as Thompson sampling or Bayesian Bandits, and are surprisingly easy to implement if you can sample from the posterior for the mean value of each alternative. Probability matching strategies also admit solutions to so-called contextual bandit problems. Pricing strategies Pricing strategies establish a price for each lever. For example, as illustrated with the POKER algorithm, the price can be the sum of the expected reward plus an estimation of extra future rewards that will gain through the additional knowledge. The lever of highest price is always pulled. Contextual bandit A useful generalization of the multi-armed bandit is the contextual multi-armed bandit. At each iteration an agent still has to choose between arms, but they also see a d-dimensional feature vector, the context vector they can use together with the rewards of the arms played in the past to make the choice of the arm to play. Over time, the learners aim is to collect enough information about how the context vectors and rewards relate to each other, so that it can predict the next best arm to play by looking at the feature vectors. Approximate solutions for contextual bandit Many strategies exist that provide an approximate solution to the contextual bandit problem, and can be put into two broad categories detailed below. Online linear bandits LinUCB (Upper Confidence Bound) algorithm the authors assume a linear dependency between the expected reward of an action and its context and model the representation space using a set of linear predictors. LinRel (Linear Associative Reinforcement Learning) algorithm Similar to LinUCB, but utilizes singular value decomposition rather than ridge regression to obtain an estimate of confidence. Online non-linear bandits UCBogram algorithm The nonlinear reward functions are estimated using a piecewise constant estimator called a regressogram in nonparametric regression. Then, UCB is employed on each constant piece. Successive refinements of the partition of the context space are scheduled or chosen adaptively. Generalized linear algorithms The reward distribution follows a generalized linear model, an extension to linear bandits. KernelUCB algorithm a kernelized non-linear version of LinUCB, with efficient implementation and finite-time analysis. Bandit Forest algorithm a random forest is built and analyzed w.r.t the random forest built knowing the joint distribution of contexts and rewards. Oracle-based algorithm The algorithm reduces the contextual bandit problem into a series of supervised learning problem, and does not rely on typical realizability assumption on the reward function. Constrained contextual bandit In practice, there is usually a cost associated with the resource consumed by each action and the total cost is limited by a budget in many applications such as crowdsourcing and clinical trials. Constrained contextual bandit (CCB) is such a model that considers both the time and budget constraints in a multi-armed bandit setting. A. Badanidiyuru et al. first studied contextual bandits with budget constraints, also referred to as Resourceful Contextual Bandits, and show that a O ( T ) displaystyle O(sqrt T) regret is achievable. However, their work focuses on a finite set of policies, and the algorithm is computationally inefficient. A simple algorithm with logarithmic regret is proposed in UCB-ALP algorithm The framework of UCB-ALP is shown in the right figure. UCB-ALP is a simple algorithm that combines the UCB method with an Adaptive Linear Programming (ALP) algorithm, and can be easily deployed in practical systems. It is the first work that show how to achieve logarithmic regret in constrained contextual bandits. Although is devoted to a special case with single budget constraint and fixed cost, the results shed light on the design and analysis of algorithms for more general CCB problems. Adversarial bandit Another variant of the multi-armed bandit problem is called the adversarial bandit, first introduced by Auer and Cesa-Bianchi (1998). In this variant, at each iteration, an agent chooses an arm and an adversary simultaneously chooses the payoff structure for each arm. This is one of the strongest generalizations of the bandit problem as it removes all assumptions of the distribution and a solution to the adversarial bandit problem is a generalized solution to the more specific bandit problems. Example Iterated prisoners dilemma An example often considered for adversarial bandits is the iterated prisoners dilemma. In this example, each adversary has two arms to pull. They can either Deny or Confess. Standard stochastic bandit algorithms dont work very well with these iterations. For example, if the opponent cooperates in the first 100 rounds, defects for the next 200, then cooperate in the following 300, etc. then algorithms such as UCB wont be able to react very quickly to these changes. This is because after a certain point sub-optimal arms are rarely pulled to limit exploration and focus on exploitation. When the environment changes the algorithm is unable to adapt or may not even detect the change. Approximate solutions Source is a popular algorithm for adversarial multiarmed bandits, suggested and analyzed in this setting by Auer et al. 2002b. Recently there was an increased interest in the performance of this algorithm in the stochastic setting, due to its new applications to stochastic multi-armed bandits with side information Seldin et al., 2011 and to multi-armed bandits in the mixed stochastic-adversarial setting Bubeck and Slivkins, 2012. The paper presented an empirical evaluation and improved analysis of the performance of the algorithm in the stochastic setting, as well as a modification of the algorithm capable of achieving logarithmic regret in stochastic environment. Algorithm Parameters Real ( 0 , 1  displaystyle gamma in (0,1 Initialisation i ( 1 )  1 displaystyle omega _i(1)1 for  , . . . , K displaystyle ,...,K For each , ..., T 1. Set p i ( t )  ( 1 ) i ( t )  , . . . , K displaystyle ,...,K 2. Draw i t displaystyle i_t randomly according to the probabilities p 1 ( t ) , . . . , p K ( t ) displaystyle p_1(t),...,p_K(t) 3. Receive reward x i t ( t )  0 , 1  displaystyle x_i_t(t)in 0,1 4. For  , . . . , K displaystyle ,...,K set x  j ( t )   x j ( t ) / p j ( t ) if . After receiving the rewards the weights are updated. The exponential growth significantly increases the weight of good arms. Regret analysis The (external) regret of the algorithm is at most O ( K T l o g ( K ) ) displaystyle O(sqrt KTlog(K)) Follow the perturbed leader (FPL) algorithm Algorithm Parameters Real displaystyle eta  Initialisation i  R i ( 1 )  0 displaystyle forall iR_i(1)0 For each ,...,T 1. For each arm generate a random noise from an exponential distribution i  Z i ( t ) E x p ( ) displaystyle forall iZ_i(t)sim Exp(eta ) 2. Pull arm I ( t ) displaystyle I(t)  I ( t )  a r g max i  R i ( t )  Z i ( t )  displaystyle I(t)argmax _iR_i(t)Z_i(t) Add noise to each arm and pull the one with the highest value 3. Update value R I ( t ) ( t  1 )  R I ( t ) ( t )  x I ( t ) ( t ) displaystyle R_I(t)(t1)R_I(t)(t)x_I(t)(t) The rest remains the same Explanation We follow the arm that we think has the best performance so far adding exponential noise to it to provide exploration. vs FPL Infinite-armed bandit In the original specification and in the above variants, the bandit problem is specified with a discrete and finite number of arms, often indicated by the variable K displaystyle K . In the infinite armed case, introduced by Agrawal (1995), the arms are a continuous variable in K displaystyle K dimensions. Non-stationary bandit This framework refers to the multi-armed bandit problem in a non-stationary setting (i.e., in presence of concept drift). In the non-stationary setting, it is assumed that the expected reward for an arm k displaystyle k can change at every time step t T displaystyle tin mathcal T  t 1 k t k displaystyle mu _t-1kneq mu _tk . Thus, t k displaystyle mu _tk no longer represents the whole sequence of expected (stationary) rewards for arm k displaystyle k . Instead, k displaystyle mu k denotes the sequence of expected rewards for arm k displaystyle k , defined as  . A dynamic oracle represents the optimal policy to be compared with other policies in the non-stationary setting. The dynamic oracle optimises the expected reward at each step t T displaystyle tin mathcal T by always selecting the best arm, with expected reward of t displaystyle mu _t . Thus, the cumulative expected reward D ( T ) displaystyle mathcal D(T) for the dynamic oracle at final time step T displaystyle T is defined as D ( T )   . displaystyle mathcal D(T)sum _. Hence, the regret ( T ) displaystyle rho pi (T) for policy displaystyle pi  is computed as the difference between D ( T ) displaystyle mathcal D(T) and the cumulative expected reward at step T displaystyle T for policy displaystyle pi   ( T )    . displaystyle rho pi (T)sum _. Garivier and Moulines derive some of the first results with respect to bandit problems where the underlying model can change during play. A number of algorithms were presented to deal with this case, including Discounted UCB and Sliding-Window UCB. A similar approach based on Thompson Sampling algorithm is the f-Discounted-Sliding-Window Thompson Sampling (f-dsw TS) proposed by Cavenaghi et al. The f-dsw TS algorithm exploits a discount factor on the reward history and an arm-related sliding window to contrast concept drift in non-stationary environments. Another work by Burtini et al. introduces a weighted least squares Thompson sampling approach (WLS-TS), which proves beneficial in both the known and unknown non-stationary cases. Other variants Many variants of the problem have been proposed in recent years. Dueling bandit The dueling bandit variant was introduced by Yue et al. (2012) to model the exploration-versus-exploitation tradeoff for relative feedback. In this variant the gambler is allowed to pull two levers at the same time, but they only get a binary feedback telling which lever provided the best reward. The difficulty of this problem stems from the fact that the gambler has no way of directly observing the reward of their actions. The earliest algorithms for this problem were InterleaveFiltering and Beat-The-Mean. The relative feedback of dueling bandits can also lead to voting paradoxes. A solution is to take the Condorcet winner as a reference. More recently, researchers have generalized algorithms from traditional MAB to dueling bandits Relative Upper Confidence Bounds (RUCB), Relative EXponential weighing (), Copeland Confidence Bounds (CCB), Relative Minimum Empirical Divergence (RMED), and Double Thompson Sampling (DTS). Collaborative bandit Approaches using multiple bandits that cooperate sharing knowledge in order to better optimize their performance started in 2013 with A Gang of Bandits, an algorithm relying on a similarity graph between the different bandit problems to share knowledge. The need of a similarity graph was removed in 2014 by the work on the CLUB algorithm. Following this work, several other researchers created algorithms to learn multiple models at the same time under bandit feedback. For example, COFIBA was introduced by Li and Karatzoglou and Gentile (SIGIR 2016), where the classical collaborative filtering, and content-based filtering methods try to learn a static recommendation model given training data. Combinatorial bandit The Combinatorial Multiarmed Bandit (CMAB) problem arises when instead of a single discrete variable to choose from, an agent needs to choose values for a set of variables. Assuming each variable is discrete, the number of possible choices per iteration is exponential in the number of variables. Several CMAB settings have been studied in the literature, from settings where the variables are binary to more general setting where each variable can take an arbitrary set of values. See also Gittins index a powerful, general strategy for analyzing bandit problems. Greedy algorithm Optimal stopping Search theory Stochastic scheduling References Further reading Guha, S. Munagala, K. Shi, P. (2010), Approximation algorithms for restless bandit problems, Journal of the ACM, 58 1 50, arXiv0711.3861, doi10.1145/1870103.1870106, S2CID 1654066 Dayanik, S. Powell, W. Yamazaki, K. (2008), Index policies for discounted bandit problems with availability constraints, Advances in Applied Probability, 40 (2) 377 400, doi10.1239/aap/1214950209. Powell, Warren B. (2007), Chapter 10, Approximate Dynamic Programming Solving the Curses of Dimensionality, New York John Wiley and Sons, ISBN 978-0-470-17155-4. Robbins, H. (1952), Some aspects of the sequential design of experiments, Bulletin of the American Mathematical Society, 58 (5) 527 535, doi10.1090/-9904-1952-09620-8. Sutton, Richard Barto, Andrew (1998), Reinforcement Learning, MIT Press, ISBN 978-0-262-19398-6, archived from the original on 2013-12-11. Allesiardo, Robin (2014), A Neural Networks Committee for the Contextual Bandit Problem, Neural Information Processing 21st International Conference, ICONIP 2014, Malaisia, November 03-06,2014, Proceedings, Lecture Notes in Computer Science, vol. 8834, Springer, pp. 374 381, arXiv1409.8191, doi10.1007/978-3-319-12637-1_47, ISBN 978-3-319-12636-4, S2CID 14155718. Weber, Richard (1992), On the Gittins index for multiarmed bandits, Annals of Applied Probability, 2 (4) 1024 1033, doi10.1214/aoap/1177005588, JSTOR 2959678. Katehakis, M. C. Derman (1986), Computing optimal sequential allocation rules in clinical trials, Adaptive statistical procedures and related topics, Institute of Mathematical Statistics Lecture Notes - Monograph Series, vol. 8, pp. 29 39, doi10.1214/lnms/1215540286, ISBN 978-0-940600-09-6, JSTOR 4355518. Katehakis, Michael N. Veinott, Jr., Arthur F. (1987), The multi-armed bandit problem decomposition and computation, Mathematics of Operations Research, 12 (2) 262 268, doi10.1287/moor.12.2.262, JSTOR 3689689, S2CID 656323 External links MABWiser, open-source Python implementation of bandit strategies that supports context-free, parametric and non-parametric contextual policies with built-in parallelization and simulation capability. PyMaBandits, open-source implementation of bandit strategies in Python and Matlab. Contextual, open-source R package facilitating the simulation and evaluation of both context-free and contextual Multi-Armed Bandit policies. bandit.sourceforge.net Bandit project, open-source implementation of bandit strategies. Banditlib, open-source implementation of bandit strategies in C. Leslie Pack Kaelbling and Michael L. Littman (1996). Exploitation versus Exploration The Single-State Case. Tutorial Introduction to Bandits Algorithms and Theory. . . Feynmans restaurant problem, a classic example (with known answer) of the exploitation vs. exploration tradeoff. Bandit algorithms vs. A-B testing. S. Bubeck and N. Cesa-Bianchi A Survey on Bandits. A Survey on Contextual Multi-armed Bandits, a survey/tutorial for Contextual Bandits. Blog post on multi-armed bandit strategies, with Python code. Animated, interactive plots illustrating Epsilon-greedy, Thompson sampling, and Upper Confidence Bound exploration/exploitation balancing strategies. Title Multi-task learning URL https//en.wikipedia.org/wiki/Multi-task_learning Content Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Inherently, Multi-task learning is a multi-objective optimization problem having trade-offs between different tasks. Early versions of MTL were called hints. In a widely cited 1997 paper, Rich Caruana gave the following characterizationMultitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation what is learned for each task can help other tasks be learned better. In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each users spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification. Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks. Methods The key challenge in multi-task learning, is how to combine learning signals from multiple tasks into a single model. This may strongly depend on how well different task agree with each other, or contradict each other. There are several ways to address this challenge Task grouping and overlap Within the MTL paradigm, information can be shared across some or all of the tasks. Depending on the structure of task relatedness, one may want to share information selectively across the tasks. For example, tasks may be grouped or exist in a hierarchy, or be related according to some general metric. Suppose, as developed more formally below, that the parameter vector modeling each task is a linear combination of some underlying basis. Similarity in terms of this basis can indicate the relatedness of the tasks. For example, with sparsity, overlap of nonzero coefficients across tasks indicates commonality. A task grouping then corresponds to those tasks lying in a subspace generated by some subset of basis elements, where tasks in different groups may be disjoint or overlap arbitrarily in terms of their bases. Task relatedness can be imposed a priori or learned from the data. Hierarchical task relatedness can also be exploited implicitly without assuming a priori knowledge or learning relations explicitly. For example, the explicit learning of sample relevance across tasks can be done to guarantee the effectiveness of joint learning across multiple domains. Exploiting unrelated tasks One can attempt learning a group of principal tasks using a group of auxiliary tasks, unrelated to the principal ones. In many applications, joint learning of unrelated tasks which use the same input data can be beneficial. The reason is that prior knowledge about task relatedness can lead to sparser and more informative representations for each task grouping, essentially by screening out idiosyncrasies of the data distribution. Novel methods which builds on a prior multitask methodology by favoring a shared low-dimensional representation within each task grouping have been proposed. The programmer can impose a penalty on tasks from different groups which encourages the two representations to be orthogonal. Experiments on synthetic and real data have indicated that incorporating unrelated tasks can result in significant improvements over standard multi-task learning methods. Transfer of knowledge Related to multi-task learning is the concept of knowledge transfer. Whereas traditional multi-task learning implies that a shared representation is developed concurrently across tasks, transfer of knowledge implies a sequentially shared representation. Large scale machine learning projects such as the deep convolutional neural network GoogLeNet, an image-based object classifier, can develop robust representations which may be useful to further algorithms learning related tasks. For example, the pre-trained model can be used as a feature extractor to perform pre-processing for another learning algorithm. Or the pre-trained model can be used to initialize a model with similar architecture which is then fine-tuned to learn a different classification task. Multiple non-stationary tasks Traditionally Multi-task learning and transfer of knowledge are applied to stationary learning settings. Their extension to non-stationary environments is termed Group online adaptive learning (GOAL). Sharing information could be particularly useful if learners operate in continuously changing environments, because a learner could benefit from previous experience of another learner to quickly adapt to their new environment. Such group-adaptive learning has numerous applications, from predicting financial time-series, through content recommendation systems, to visual understanding for adaptive autonomous agents. Multi-task optimization Multitask optimization In some cases, the simultaneous training of seemingly related tasks may hinder performance compared to single-task models. Commonly, MTL models employ task-specific modules on top of a joint feature representation obtained using a shared module. Since this joint representation must capture useful features across all tasks, MTL may hinder individual task performance if the different tasks seek conflicting representation, i.e., the gradients of different tasks point to opposing directions or differ significantly in magnitude. This phenomenon is commonly referred to as negative transfer. To mitigate this issue, various MTL optimization methods have been proposed. Commonly, the per-task gradients are combined into a joint update direction through various aggregation algorithms or heuristics. Mathematics Reproducing Hilbert space of vector valued functions (RKHSvv) The MTL problem can be cast within the context of RKHSvv (a complete inner product space of vector-valued functions equipped with a reproducing kernel). In particular, recent focus has been on cases where task structure can be identified via a separable kernel, described below. The presentation here derives from Ciliberto et al., 2015. RKHSvv concepts Suppose the training data set is S  , . . . , T displaystyle tin 1,...,T . Let  . In this setting there is a consistent input and output space and the same loss function L  R R R  displaystyle mathcal Lmathbb R times mathbb R rightarrow mathbb R _ for each task . This results in the regularized machine learning problem where H displaystyle mathcal H is a vector valued reproducing kernel Hilbert space with functions f  X Y T displaystyle fmathcal Xrightarrow mathcal YT having components f t  X Y displaystyle f_tmathcal Xrightarrow mathcal Y . The reproducing kernel for the space H displaystyle mathcal H of functions f  X R T displaystyle fmathcal Xrightarrow mathbb R T is a symmetric matrix-valued function  X X R T T displaystyle Gamma mathcal Xtimes mathcal Xrightarrow mathbb R Ttimes T , such that ( , x ) c H displaystyle Gamma (cdot ,x)cin mathcal H and the following reproducing property holds The reproducing kernel gives rise to a representer theorem showing that any solution to equation 1 has the form Separable kernels The form of the kernel induces both the representation of the feature space and structures the output across tasks. A natural simplification is to choose a separable kernel, which factors into separate kernels on the input space X and on the tasks  1 , . . . , T  displaystyle 1,...,T . In this case the kernel relating scalar components f t displaystyle f_t and f s displaystyle f_s is given by ( ( x i , t ) , ( x j , s ) )  k ( x i , x j ) k T ( s , t )  k ( x i , x j ) A s , t textstyle gamma ((x_i,t),(x_j,s))k(x_i,x_j)k_T(s,t)k(x_i,x_j)A_s,t . For vector valued functions f H displaystyle fin mathcal H we can write ( x i , x j )  k ( x i , x j ) A displaystyle Gamma (x_i,x_j)k(x_i,x_j)A , where k is a scalar reproducing kernel, and A is a symmetric positive semi-definite T T displaystyle Ttimes T matrix. Henceforth denote S   . This factorization property, separability, implies the input feature space representation does not vary by task. That is, there is no interaction between the input kernel and the task kernel. The structure on tasks is represented solely by A. Methods for non-separable kernels is a current field of research. For the separable case, the representation theorem is reduced to f ( x )   . The model output on the training data is then KCA , where K is the n n displaystyle ntimes n empirical kernel matrix with entries K i ,  . With the separable kernel, equation 1 can be rewritten as where V is a (weighted) average of L applied entry-wise to Y and KCA. (The weight is zero if Y i t displaystyle Y_it is a missing observation). Note the second term in P can be derived as follows f H 2  . Task structure examples Via the regularizer formulation, one can represent a variety of task structures easily. Letting  . For example, blood levels of some biomarker may be taken on T patients at n t displaystyle n_t time points during the course of a day and interest may lie in regularizing the variance of the predictions across patients. Letting  . (Here  G r  displaystyle G_r the cardinality of group r, and I displaystyle mathbb I  is the indicator function). For example, people in different political parties (groups) might be regularized together with respect to predicting the favorability rating of a politician. Note that this penalty reduces to the first when all tasks are in the same group. Letting . This is equivalent to giving a larger penalty to the distance separating tasks t and s when they are more similar (according to the weight M t , s displaystyle M_t,s ,) i.e. displaystyle delta  regularizes t , s   f t f s   H k 2 M t , s displaystyle sum _t,sf_t-f_s_mathcal H_k2M_t,s . All of the above choices of A also induce the additional regularization term t   f   H k 2 textstyle lambda sum _tf_mathcal H_k2 which penalizes complexity in f more broadly. Learning tasks together with their structure Learning problem P can be generalized to admit learning task matrix A as follows Choice of F  S  T R  displaystyle FS_Trightarrow mathbb R _ must be designed to learn matrices A of a given type. See Special cases below. Optimization of Q Restricting to the case of convex losses and coercive penalties Ciliberto et al. have shown that although Q is not convex jointly in C and A, a related problem is jointly convex. Specifically on the convex set . And if ( C R , A R ) displaystyle (C_R,A_R) is a minimizer for R then ( C R A R , A R ) displaystyle (C_RA_Rdagger ,A_R) is a minimizer for Q. R may be solved by a barrier method on a closed set by introducing the following perturbation The perturbation via the barrier 2 t r ( A ) displaystyle delta 2tr(Adagger ) forces the objective functions to be equal to  displaystyle infty  on the boundary of R n T S  T displaystyle Rntimes Ttimes S_T . S can be solved with a block coordinate descent method, alternating in C and A. This results in a sequence of minimizers ( C m , A m ) displaystyle (C_m,A_m) in S that converges to the solution in R as m 0 displaystyle delta _mrightarrow 0 , and hence gives the solution to Q. Special cases Spectral penalties - Dinnuzo et al suggested setting F as the Frobenius norm t r ( A A ) displaystyle sqrt tr(Atop A) . They optimized Q directly using block coordinate descent, not accounting for difficulties at the boundary of R n T S  T displaystyle mathbb R ntimes Ttimes S_T . Clustered tasks learning - Jacob et al suggested to learn A in the setting where T tasks are organized in R disjoint clusters. In this case let E  0 , 1  T R displaystyle Ein 0,1Ttimes R be the matrix with E t , ) . Setting . M is not convex, but there is a convex relaxation S  . In this formulation, F ( A )  I ( A ( M )  A  M S C  ) displaystyle F(A)mathbb I (A(M)in AMin mathcal S_C) . Generalizations Non-convex penalties - Penalties can be constructed such that A is constrained to be a graph Laplacian, or that A has low rank factorization. However these penalties are not convex, and the analysis of the barrier method proposed by Ciliberto et al. does not go through in these cases. Non-separable kernels - Separable kernels are limited, in particular they do not account for structures in the interaction space between the input and output domains jointly. Future work is needed to develop models for these kernels. Software package A Matlab package called Multi-Task Learning via StructurAl Regularization (MALSAR) implements the following multi-task learning algorithms Mean-Regularized Multi-Task Learning, Multi-Task Learning with Joint Feature Selection, Robust Multi-Task Feature Learning, Trace-Norm Regularized Multi-Task Learning, Alternating Structural Optimization, Incoherent Low-Rank and Sparse Learning, Robust Low-Rank Multi-Task Learning, Clustered Multi-Task Learning, Multi-Task Learning with Graph Structures. Literature Multi-Target Prediction A Unifying View on Problems and Methods Willem Waegeman, Krzysztof Dembczynski, Eyke Huellermeier https//arxiv.org/abs/1809.02352v1 See also References External links The Biosignals Intelligence Group at UIUC Washington University in St. Louis Department of Computer Science Software The Multi-Task Learning via Structural Regularization Package Online Multi-Task Learning Toolkit (OMT) A general-purpose online multi-task learning toolkit based on conditional random field models and stochastic gradient descent training (C, .NET) Title Multimodal sentiment analysis URL https//en.wikipedia.org/wiki/Multimodal_sentiment_analysis Content Multimodal sentiment analysis is a technology for traditional text-based sentiment analysis, which includes modalities such as audio and visual data. It can be bimodal, which includes different combinations of two modalities, or trimodal, which incorporates three modalities. With the extensive amount of social media data available online in different forms such as videos and images, the conventional text-based sentiment analysis has evolved into more complex models of multimodal sentiment analysis, which can be applied in the development of virtual assistants, analysis of YouTube movie reviews, analysis of news videos, and emotion recognition (sometimes known as emotion detection) such as depression monitoring, among others. Similar to the traditional sentiment analysis, one of the most basic task in multimodal sentiment analysis is sentiment classification, which classifies different sentiments into categories such as positive, negative, or neutral. The complexity of analyzing text, audio, and visual features to perform such a task requires the application of different fusion techniques, such as feature-level, decision-level, and hybrid fusion. The performance of these fusion techniques and the classification algorithms applied, are influenced by the type of textual, audio, and visual features employed in the analysis. Features Feature engineering, which involves the selection of features that are fed into machine learning algorithms, plays a key role in the sentiment classification performance. In multimodal sentiment analysis, a combination of different textual, audio, and visual features are employed. Textual features Similar to the conventional text-based sentiment analysis, some of the most commonly used textual features in multimodal sentiment analysis are unigrams and n-grams, which are basically a sequence of words in a given textual document. These features are applied using bag-of-words or bag-of-concepts feature representations, in which words or concepts are represented as vectors in a suitable space. Audio features Sentiment and emotion characteristics are prominent in different phonetic and prosodic properties contained in audio features. Some of the most important audio features employed in multimodal sentiment analysis are mel-frequency cepstrum (MFCC), spectral centroid, spectral flux, beat histogram, beat sum, strongest beat, pause duration, and pitch. OpenSMILE and Praat are popular open-source toolkits for extracting such audio features. Visual features One of the main advantages of analyzing videos with respect to texts alone, is the presence of rich sentiment cues in visual data. Visual features include facial expressions, which are of paramount importance in capturing sentiments and emotions, as they are a main channel of forming a persons present state of mind. Specifically, smile, is considered to be one of the most predictive visual cues in multimodal sentiment analysis. OpenFace is an open-source facial analysis toolkit available for extracting and understanding such visual features. Fusion techniques Unlike the traditional text-based sentiment analysis, multimodal sentiment analysis undergo a fusion process in which data from different modalities (text, audio, or visual) are fused and analyzed together. The existing approaches in multimodal sentiment analysis data fusion can be grouped into three main categories feature-level, decision-level, and hybrid fusion, and the performance of the sentiment classification depends on which type of fusion technique is employed. Feature-level fusion Feature-level fusion (sometimes known as early fusion) gathers all the features from each modality (text, audio, or visual) and joins them together into a single feature vector, which is eventually fed into a classification algorithm. One of the difficulties in implementing this technique is the integration of the heterogeneous features. Decision-level fusion Decision-level fusion (sometimes known as late fusion), feeds data from each modality (text, audio, or visual) independently into its own classification algorithm, and obtains the final sentiment classification results by fusing each result into a single decision vector. One of the advantages of this fusion technique is that it eliminates the need to fuse heterogeneous data, and each modality can utilize its most appropriate classification algorithm. Hybrid fusion Hybrid fusion is a combination of feature-level and decision-level fusion techniques, which exploits complementary information from both methods during the classification process. It usually involves a two-step procedure wherein feature-level fusion is initially performed between two modalities, and decision-level fusion is then applied as a second step, to fuse the initial results from the feature-level fusion, with the remaining modality. Applications Similar to text-based sentiment analysis, multimodal sentiment analysis can be applied in the development of different forms of recommender systems such as in the analysis of user-generated videos of movie reviews and general product reviews, to predict the sentiments of customers, and subsequently create product or service recommendations. Multimodal sentiment analysis also plays an important role in the advancement of virtual assistants through the application of natural language processing (NLP) and machine learning techniques. In the healthcare domain, multimodal sentiment analysis can be utilized to detect certain medical conditions such as stress, anxiety, or depression. Multimodal sentiment analysis can also be applied in understanding the sentiments contained in video news programs, which is considered as a complicated and challenging domain, as sentiments expressed by reporters tend to be less obvious or neutral. Title Multiple instance learning URL https//en.wikipedia.org/wiki/Multiple_instance_learning Content In machine learning, multiple-instance learning (MIL) is a type of supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept. Babenko (2008) gives a simple example for MIL. Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some arent. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the positive key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesnt. Machine learning Depending on the type and variation in training data, machine learning can be roughly categorized into three frameworks supervised learning, unsupervised learning, and reinforcement learning. Multiple instance learning (MIL) falls under the supervised learning framework, where every training instance has a label, either discrete or real valued. MIL deals with problems with incomplete knowledge of labels in training sets. More precisely, in multiple-instance learning, the training set consists of labeled bags, each of which is a collection of unlabeled instances. A bag is positively labeled if at least one instance in it is positive, and is negatively labeled if all instances in it are negative. The goal of the MIL is to predict the labels of new, unseen bags. History Keeler et al., in his work in the early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction. They tried to create a learning system that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldnt say exactly which of its low-energy shapes are responsible for that. One of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasnt really useful. Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning. Solution to the multiple instance learning problem that Dietterich et al. proposed is the axis-parallel rectangle (APR) algorithm. It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset, which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but APR was designed with Musk data in mind. Problem of multi-instance learning is not unique to drug finding. In 1998, Maron and Ratan found another application of multiple instance learning to scene classification in machine vision, and devised Diverse Density framework. Given an image, an instance is taken to be one or more fixed-size subimages, and the bag of instances is taken to be the entire image. An image is labeled positive if it contains the target scene - a waterfall, for example - and negative otherwise. Multiple instance learning can be used to learn the properties of the subimages which characterize the target scene. From there on, these frameworks have been applied to a wide spectrum of applications, ranging from image concept learning and text categorization, to stock market prediction. Examples Take image classification for example Amores (2013). Given an image, we want to know its target class based on its visual content. For instance, the target class might be beach, where the image contains both sand and water. In MIL terms, the image is described as a bag  , . . , X N  displaystyle ,..,X_N , where each X i displaystyle X_i is the feature vector (called instance) extracted from the corresponding i displaystyle i -th region in the image and N displaystyle N is the total regions (instances) partitioning the image. The bag is labeled positive (beach) if it contains both sand region instances and water region instances. Examples of where MIL is applied are Molecule activity Predicting binding sites of Calmodulin binding proteins Predicting function for alternatively spliced isoforms Li, Menon  et al. (2014),Eksi et al. (2013) Image classification Maron  Ratan (1998) Text or document categorization Kotzias et al. (2015) Predicting functional binding sites of MicroRNA targets Bandyopadhyay, Ghosh  et al. (2015) Medical image classification Zhu et al. (2016), P.J.Sudharshan et al. (2019) Numerous researchers have worked on adapting classical classification techniques, such as support vector machines or boosting, to work within the context of multiple-instance learning. Definitions If the space of instances is X displaystyle mathcal X , then the set of bags is the set of functions N  . For each bag B N X displaystyle Bin mathbb N mathcal X and each instance x X displaystyle xin mathcal X , B ( x ) displaystyle B(x) is viewed as the number of times x displaystyle x occurs in B displaystyle B . Let Y displaystyle mathcal Y be the space of labels, then a multiple instance concept is a map c  N X Y displaystyle cmathbb N mathcal Xrightarrow mathcal Y . The goal of MIL is to learn such a concept. The remainder of the article will focus on binary classification, where  . Assumptions Most of the work on multiple instance learning, including Dietterich et al. (1997) and Maron  Lozano-P rez (1997) early papers, make the assumption regarding the relationship between the instances within a bag and the class label of the bag. Because of its importance, that assumption is often called standard MI assumption. Standard assumption The standard assumption takes each instance x X displaystyle xin mathcal X to have an associated label y  0 , 1  displaystyle yin 0,1 which is hidden to the learner. The pair ( x , y ) displaystyle (x,y) is called an instance-level concept. A bag is now viewed as a multiset of instance-level concepts, and is labeled positive if at least one of its instances has a positive label, and negative if all of its instances have negative labels. Formally, let . The label of B displaystyle B is then c ( B )  1 ) . Standard MI assumption is asymmetric, which means that if the positive and negative labels are reversed, the assumption has a different meaning. Because of that, when we use this assumption, we need to be clear which label should be the positive one. The standard assumption might be viewed as too strict, and therefore in the recent years, researchers tried to relax that position, which gave rise to other more loose assumptions. The reason for this is the belief that standard MIL assumption is appropriate for the Musk dataset, but since MIL can be applied to numerous other problems, some different assumptions could probably be more appropriate. Guided by that idea, Weidmann formulated a hierarchy of generalized instance-based assumptions for MIL. It consists of the standard MI assumption and three types of generalized MI assumptions, each more general than the last, in the sense that the former can be obtained as a specific choice of parameters of the latter, standard displaystyle subset  presence-based displaystyle subset  threshold-based displaystyle subset  count-based, with the count-based assumption being the most general and the standard assumption being the least general. (Note however, that any bag meeting the count-based assumption meets the threshold-based assumption which in turn meets the presence-based assumption which, again in turn, meet the standard assumption. In that sense it is also correct to state that the standard assumption is the weakest, hence most general, and the count-based assumption is the strongest, hence least general.) One would expect an algorithm which performs well under one of these assumptions to perform at least as well under the less general assumptions. Presence-, threshold-, and count-based assumptions The presence-based assumption is a generalization of the standard assumption, wherein a bag must contain all instances that belong to a set of required instance-level concepts in order to be labeled positive. Formally, let C R X Y displaystyle C_Rsubseteq mathcal Xtimes mathcal Y be the set of required instance-level concepts, and let  ( B , c i ) displaystyle (B,c_i) denote the number of times the instance-level concept c i displaystyle c_i occurs in the bag B displaystyle B . Then c ( B )  1  ( B , c i ) 1 displaystyle c(B)1Leftrightarrow (B,c_i)geq 1 for all c i C R displaystyle c_iin C_R . Note that, by taking C R displaystyle C_R to contain only one instance-level concept, the presence-based assumption reduces to the standard assumption. A further generalization comes with the threshold-based assumption, where each required instance-level concept must occur not only once in a bag, but some minimum (threshold) number of times in order for the bag to be labeled positive. With the notation above, to each required instance-level concept c i C R displaystyle c_iin C_R is associated a threshold l i N displaystyle l_iin mathbb N  . For a bag B displaystyle B , c ( B )  1  ( B , c i ) l i displaystyle c(B)1Leftrightarrow (B,c_i)geq l_i for all c i C R displaystyle c_iin C_R . The count-based assumption is a final generalization which enforces both lower and upper bounds for the number of times a required concept can occur in a positively labeled bag. Each required instance-level concept c i C R displaystyle c_iin C_R has a lower threshold l i N displaystyle l_iin mathbb N  and upper threshold u i N displaystyle u_iin mathbb N  with l i u i displaystyle l_ileq u_i . A bag B displaystyle B is labeled according to c ( B )  1 l i  ( B , c i ) u i displaystyle c(B)1Leftrightarrow l_ileq (B,c_i)leq u_i for all c i C R displaystyle c_iin C_R . GMIL assumption Scott, Zhang, and Brown (2005) describe another generalization of the standard model, which they call generalized multiple instance learning (GMIL). The GMIL assumption specifies a set of required instances Q X displaystyle Qsubseteq mathcal X . A bag X displaystyle X is labeled positive if it contains instances which are sufficiently close to at least r displaystyle r of the required instances Q displaystyle Q . Under only this condition, the GMIL assumption is equivalent to the presence-based assumption. However, Scott et al. describe a further generalization in which there is a set of attraction points Q X displaystyle Qsubseteq mathcal X and a set of repulsion points Q X displaystyle overline Qsubseteq mathcal X . A bag is labeled positive if and only if it contains instances which are sufficiently close to at least r displaystyle r of the attraction points and are sufficiently close to at most s displaystyle s of the repulsion points. This condition is strictly more general than the presence-based, though it does not fall within the above hierarchy. Collective assumption In contrast to the previous assumptions where the bags were viewed as fixed, the collective assumption views a bag B displaystyle B as a distribution p ( x  B ) displaystyle p(xB) over instances X displaystyle mathcal X , and similarly view labels as a distribution p ( y  x ) displaystyle p(yx) over instances. The goal of an algorithm operating under the collective assumption is then to model the distribution p ( y  B )  X p ( y  x ) p ( x  B ) d x displaystyle p(yB)int _mathcal Xp(yx)p(xB)dx . Since p ( x  B ) displaystyle p(xB) is typically considered fixed but unknown, algorithms instead focus on computing the empirical version p  ( y  B )  1 n B  . Since p ( y  x ) displaystyle p(yx) is also typically taken to be fixed but unknown, most collective-assumption based methods focus on learning this distribution, as in the single-instance version. While the collective assumption weights every instance with equal importance, Foulds extended the collective assumption to incorporate instance weights. The weighted collective assumption is then that p  ( y  B )  1 w B ) . Algorithms There are two major flavors of algorithms for Multiple Instance Learning instance-based and metadata-based, or embedding-based algorithms. The term instance-based denotes that the algorithm attempts to find a set of representative instances based on an MI assumption and classify future bags from these representatives. By contrast, metadata-based algorithms make no assumptions about the relationship between instances and bag labels, and instead try to extract instance-independent information (or metadata) about the bags in order to learn the concept. For a survey of some of the modern MI algorithms see Foulds and Frank. Instance-based algorithms The earliest proposed MI algorithms were a set of iterated-discrimination algorithms developed by Dietterich et al., and Diverse Density developed by Maron and Lozano-P rez. Both of these algorithms operated under the standard assumption. Iterated-discrimination Broadly, all of the iterated-discrimination algorithms consist of two phases. The first phase is to grow an axis parallel rectangle (APR) which contains at least one instance from each positive bag and no instances from any negative bags. This is done iteratively starting from a random instance x 1 B 1 displaystyle x_1in B_1 in a positive bag, the APR is expanded to the smallest APR covering any instance x 2 displaystyle x_2 in a new positive bag B 2 displaystyle B_2 . This process is repeated until the APR covers at least one instance from each positive bag. Then, each instance x i displaystyle x_i contained in the APR is given a relevance, corresponding to how many negative points it excludes from the APR if removed. The algorithm then selects candidate representative instances in order of decreasing relevance, until no instance contained in a negative bag is also contained in the APR. The algorithm repeats these growth and representative selection steps until convergence, where APR size at each iteration is taken to be only along candidate representatives. After the first phase, the APR is thought to tightly contain only the representative attributes. The second phase expands this tight APR as follows a Gaussian distribution is centered at each attribute and a looser APR is drawn such that positive instances will fall outside the tight APR with fixed probability. Though iterated discrimination techniques work well with the standard assumption, they do not generalize well to other MI assumptions. Diverse Density In its simplest form, Diverse Density (DD) assumes a single representative instance t displaystyle t as the concept. This representative instance must be dense in that it is much closer to instances from positive bags than from negative bags, as well as diverse in that it is close to at least one instance from each positive bag. Let B    B i   1 m displaystyle mathcal BB_i_1m be the set of positively labeled bags and let  . Letting B i j displaystyle B_ij denote the jth instance of bag i, the noisy-or model gives P r ( t  B i  )  1 j ( 1 P r ( t  B i j  ) ) displaystyle Pr(tB_i)1-prod _jleft(1-Prleft(tB_ijright)right) P r ( t  B i )  j ( 1 P r ( t  B i j ) ) displaystyle Pr(tB_i-)prod _jleft(1-Prleft(tB_ij-right)right) P ( t  B i j ) displaystyle P(tB_ij) is taken to be the scaled distance P ( t  B i j ) exp ( k s k 2 ( x k ( B i j ) k ) 2 ) displaystyle P(tB_ij)propto exp left(-sum _ks_k2left(x_k-(B_ij)_kright)2right) where . This way, if every positive bag has an instance close to t displaystyle t , then P r ( t  B i  ) displaystyle Pr(tB_i) will be high for each i displaystyle i , but if any negative bag B i displaystyle B_i- has an instance close to t displaystyle t , P r ( t  B i ) displaystyle Pr(tB_i-) will be low. Hence, D D ( t ) displaystyle DD(t) is high only if every positive bag has an instance close to t displaystyle t and no negative bags have an instance close to t displaystyle t . The candidate concept t  displaystyle hat t can be obtained through gradient methods. Classification of new bags can then be done by evaluating proximity to t  displaystyle hat t . Though Diverse Density was originally proposed by Maron et al. in 1998, more recent MIL algorithms use the DD framework, such as EM-DD in 2001 and DD-SVM in 2004, and MILES in 2006 A number of single-instance algorithms have also been adapted to a multiple-instance context under the standard assumption, including Support vector machines Artificial neural networks Decision trees Boosting Post 2000, there was a movement away from the standard assumption and the development of algorithms designed to tackle the more general assumptions listed above. Weidmann proposes a Two-Level Classification (TLC) algorithm to learn concepts under the count-based assumption. The first step tries to learn instance-level concepts by building a decision tree from each instance in each bag of the training set. Each bag is then mapped to a feature vector based on the counts in the decision tree. In the second step, a single-instance algorithm is run on the feature vectors to learn the concept Scott et al. proposed an algorithm, GMIL-1, to learn concepts under the GMIL assumption in 2005. GMIL-1 enumerates all axis-parallel rectangles  R i  i I displaystyle R_i_iin I in the original space of instances, and defines a new feature space of Boolean vectors. A bag B displaystyle B is mapped to a vector . A single-instance algorithm can then be applied to learn the concept in this new feature space. Because of the high dimensionality of the new feature space and the cost of explicitly enumerating all APRs of the original instance space, GMIL-1 is inefficient both in terms of computation and memory. GMIL-2 was developed as a refinement of GMIL-1 in an effort to improve efficiency. GMIL-2 pre-processes the instances to find a set of candidate representative instances. GMIL-2 then maps each bag to a Boolean vector, as in GMIL-1, but only considers APRs corresponding to unique subsets of the candidate representative instances. This significantly reduces the memory and computational requirements. Xu (2003) proposed several algorithms based on logistic regression and boosting methods to learn concepts under the collective assumption. Metadata-based (or embedding-based) algorithms By mapping each bag to a feature vector of metadata, metadata-based algorithms allow the flexibility of using an arbitrary single-instance algorithm to perform the actual classification task. Future bags are simply mapped (embedded) into the feature space of metadata and labeled by the chosen classifier. Therefore, much of the focus for metadata-based algorithms is on what features or what type of embedding leads to effective classification. Note that some of the previously mentioned algorithms, such as TLC and GMIL could be considered metadata-based. One approach is to let the metadata for each bag be some set of statistics over the instances in the bag. The SimpleMI algorithm takes this approach, where the metadata of a bag is taken to be a simple summary statistic, such as the average or minimum and maximum of each instance variable taken over all instances in the bag. There are other algorithms which use more complex statistics, but SimpleMI was shown to be surprisingly competitive for a number of datasets, despite its apparent lack of complexity. Another common approach is to consider the geometry of the bags themselves as metadata. This is the approach taken by the MIGraph and miGraph algorithms, which represent each bag as a graph whose nodes are the instances in the bag. There is an edge between two nodes if the distance (up to some metric on the instance space) between the corresponding instances is less than some threshold. Classification is done via an SVM with a graph kernel (MIGraph and miGraph only differ in their choice of kernel). Similar approaches are taken by MILES and MInD. MILES represents a bag by its similarities to instances in the training set, while MInD represents a bag by its distances to other bags. A modification of k-nearest neighbors (kNN) can also be considered a metadata-based algorithm with geometric metadata, though the mapping between bags and metadata features is not explicit. However, it is necessary to specify the metric used to compute the distance between bags. Wang and Zucker (2000) suggest the (maximum and minimum, respectively) Hausdorff metrics for bags A displaystyle A and B displaystyle B  H ( A , B )  max  max A min B a b , max B min A a b  displaystyle H(A,B)max leftmax _Amin _Ba-b,max _Bmin _Aa-bright h 1 ( A , B )  min A min B a b displaystyle h_1(A,B)min _Amin _Ba-b They define two variations of kNN, Bayesian-kNN and citation-kNN, as adaptations of the traditional nearest-neighbor problem to the multiple-instance setting. Generalizations So far this article has considered multiple instance learning exclusively in the context of binary classifiers. However, the generalizations of single-instance binary classifiers can carry over to the multiple-instance case. One such generalization is the multiple-instance multiple-label problem (MIML), where each bag can now be associated with any subset of the space of labels. Formally, if X displaystyle mathcal X is the space of features and Y displaystyle mathcal Y is the space of labels, an MIML concept is a map c  N X 2 Y displaystyle cmathbb N mathcal Xrightarrow 2mathcal Y . Zhou and Zhang (2006) propose a solution to the MIML problem via a reduction to either a multiple-instance or multiple-concept problem. Another obvious generalization is to multiple-instance regression. Here, each bag is associated with a single real number as in standard regression. Much like the standard assumption, MI regression assumes there is one instance in each bag, called the prime instance, which determines the label for the bag (up to noise). The ideal goal of MI regression would be to find a hyperplane which minimizes the square loss of the prime instances in each bag, but the prime instances are hidden. In fact, Ray and Page (2001) show that finding a best fit hyperplane which fits one instance from each bag is intractable if there are fewer than three instances per bag, and instead develop an algorithm for approximation. Many of the algorithms developed for MI classification may also provide good approximations to the MI regression problem. See also Supervised learning Multi-label classification References Further reading Recent reviews of the MIL literature include Amores (2013), which provides an extensive review and comparative study of the different paradigms, Foulds  Frank (2010), which provides a thorough review of the different assumptions used by different paradigms in the literature. Dietterich, Thomas G Lathrop, Richard H Lozano-P rez, Tom s (1997). Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence. 89 (1 2) 31 71. doi10.1016/-3702(96)00034-3. Herrera, Francisco Ventura, Sebasti n Bello, Rafael Cornelis, Chris Zafra, Amelia S nchez-Tarrag , D nel Vluymans, Sarah (2016). Multiple Instance Learning. doi10.1007/978-3-319-47759-6. ISBN 978-3-319-47758-9. S2CID 24047205. Amores, Jaume (2013). Multiple instance classification Review, taxonomy and comparative study. Artificial Intelligence. 201 81 105. doi10.1016/j.artint.2013.06.003. Foulds, James Frank, Eibe (2010). A review of multi-instance learning assumptions. The Knowledge Engineering Review. 25 1 25. CiteSeerX 10.1.1.148.2333. doi10.1017/S026988890999035X. S2CID 8601873. Keeler, James D. Rumelhart, David E. Leow, Wee-Kheng (1990). Integrated segmentation and recognition of hand-printed numerals. Proceedings of the 1990 Conference on Advances in Neural Information Processing Systems (NIPS 3). Morgan Kaufmann Publishers. pp. 557 563. ISBN 978-1-55860-184-0. Li, Hong-Dong Menon, Rajasree Omenn, Gilbert S Guan, Yuanfang (2014). The emerging era of genomic data integration for analyzing splice isoform function. Trends in Genetics. 30 (8) 340 7. doi10.1016/j.tig.2014.05.005. PMC 4112133. PMID 24951248. Eksi, Ridvan Li, Hong-Dong Menon, Rajasree Wen, Yuchen Omenn, Gilbert S Kretzler, Matthias Guan, Yuanfang (2013). Systematically Differentiating Functions for Alternatively Spliced Isoforms through Integrating RNA-seq Data. PLOS Computational Biology. 9 (11) . Bibcode2013PLSCB...9E3314E. doi10.1371/journal.pcbi.1003314. PMC 3820534. PMID 24244129. Maron, O. Ratan, A.L. (1998). Multiple-instance learning for natural scene classification. Proceedings of the Fifteenth International Conference on Machine Learning. Morgan Kaufmann Publishers. pp. 341 349. ISBN 978-1-55860-556-5. Kotzias, Dimitrios Denil, Misha De Freitas, Nando Smyth, Padhraic (2015). From Group to Individual Labels Using Deep Features. Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD 15. pp. 597 606. doi10.1145/2783258.2783380. ISBN 9781450336642. S2CID 7729996. Ray, Soumya Page, David (2001). Multiple instance regression (PDF). ICML. Bandyopadhyay, Sanghamitra Ghosh, Dip Mitra, Ramkrishna Zhao, Zhongming (2015). MBSTAR Multiple instance learning for predicting specific functional binding sites in microRNA targets. Scientific Reports. 5 8004. Bibcode2015NatSR...5E8004B. doi10.1038/. PMC 4648438. PMID 25614300. Zhu, Wentao Lou, Qi Vang, Yeeleng Scott Xie, Xiaohui (2017). Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification. Medical Image Computing and Computer-Assisted Intervention MICCAI 2017. Lecture Notes in Computer Science. Vol. 10435. pp. 603 11. arXiv1612.05968. doi10.1007/978-3-319-66179-7_69. ISBN 978-3-319-66178-0. S2CID 9623929. Title Multiple-instance learning URL https//en.wikipedia.org/wiki/Multiple_instance_learning Content In machine learning, multiple-instance learning (MIL) is a type of supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled bags, each containing many instances. In the simple case of multiple-instance binary classification, a bag may be labeled negative if all the instances in it are negative. On the other hand, a bag is labeled positive if there is at least one instance in it which is positive. From a collection of labeled bags, the learner tries to either (i) induce a concept that will label individual instances correctly or (ii) learn how to label bags without inducing the concept. Babenko (2008) gives a simple example for MIL. Imagine several people, and each of them has a key chain that contains few keys. Some of these people are able to enter a certain room, and some arent. The task is then to predict whether a certain key or a certain key chain can get you into that room. To solve this problem we need to find the exact key that is common for all the positive key chains. If we can correctly identify this key, we can also correctly classify an entire key chain - positive if it contains the required key, or negative if it doesnt. Machine learning Depending on the type and variation in training data, machine learning can be roughly categorized into three frameworks supervised learning, unsupervised learning, and reinforcement learning. Multiple instance learning (MIL) falls under the supervised learning framework, where every training instance has a label, either discrete or real valued. MIL deals with problems with incomplete knowledge of labels in training sets. More precisely, in multiple-instance learning, the training set consists of labeled bags, each of which is a collection of unlabeled instances. A bag is positively labeled if at least one instance in it is positive, and is negatively labeled if all instances in it are negative. The goal of the MIL is to predict the labels of new, unseen bags. History Keeler et al., in his work in the early 1990s was the first one to explore the area of MIL. The actual term multi-instance learning was introduced in the middle of the 1990s, by Dietterich et al. while they were investigating the problem of drug activity prediction. They tried to create a learning system that could predict whether new molecule was qualified to make some drug, or not, through analyzing a collection of known molecules. Molecules can have many alternative low-energy states, but only one, or some of them, are qualified to make a drug. The problem arose because scientists could only determine if molecule is qualified, or not, but they couldnt say exactly which of its low-energy shapes are responsible for that. One of the proposed ways to solve this problem was to use supervised learning, and regard all the low-energy shapes of the qualified molecule as positive training instances, while all of the low-energy shapes of unqualified molecules as negative instances. Dietterich et al. showed that such method would have a high false positive noise, from all low-energy shapes that are mislabeled as positive, and thus wasnt really useful. Their approach was to regard each molecule as a labeled bag, and all the alternative low-energy shapes of that molecule as instances in the bag, without individual labels. Thus formulating multiple-instance learning. Solution to the multiple instance learning problem that Dietterich et al. proposed is the axis-parallel rectangle (APR) algorithm. It attempts to search for appropriate axis-parallel rectangles constructed by the conjunction of the features. They tested the algorithm on Musk dataset, which is a concrete test data of drug activity prediction and the most popularly used benchmark in multiple-instance learning. APR algorithm achieved the best result, but APR was designed with Musk data in mind. Problem of multi-instance learning is not unique to drug finding. In 1998, Maron and Ratan found another application of multiple instance learning to scene classification in machine vision, and devised Diverse Density framework. Given an image, an instance is taken to be one or more fixed-size subimages, and the bag of instances is taken to be the entire image. An image is labeled positive if it contains the target scene - a waterfall, for example - and negative otherwise. Multiple instance learning can be used to learn the properties of the subimages which characterize the target scene. From there on, these frameworks have been applied to a wide spectrum of applications, ranging from image concept learning and text categorization, to stock market prediction. Examples Take image classification for example Amores (2013). Given an image, we want to know its target class based on its visual content. For instance, the target class might be beach, where the image contains both sand and water. In MIL terms, the image is described as a bag  , . . , X N  displaystyle ,..,X_N , where each X i displaystyle X_i is the feature vector (called instance) extracted from the corresponding i displaystyle i -th region in the image and N displaystyle N is the total regions (instances) partitioning the image. The bag is labeled positive (beach) if it contains both sand region instances and water region instances. Examples of where MIL is applied are Molecule activity Predicting binding sites of Calmodulin binding proteins Predicting function for alternatively spliced isoforms Li, Menon  et al. (2014),Eksi et al. (2013) Image classification Maron  Ratan (1998) Text or document categorization Kotzias et al. (2015) Predicting functional binding sites of MicroRNA targets Bandyopadhyay, Ghosh  et al. (2015) Medical image classification Zhu et al. (2016), P.J.Sudharshan et al. (2019) Numerous researchers have worked on adapting classical classification techniques, such as support vector machines or boosting, to work within the context of multiple-instance learning. Definitions If the space of instances is X displaystyle mathcal X , then the set of bags is the set of functions N  . For each bag B N X displaystyle Bin mathbb N mathcal X and each instance x X displaystyle xin mathcal X , B ( x ) displaystyle B(x) is viewed as the number of times x displaystyle x occurs in B displaystyle B . Let Y displaystyle mathcal Y be the space of labels, then a multiple instance concept is a map c  N X Y displaystyle cmathbb N mathcal Xrightarrow mathcal Y . The goal of MIL is to learn such a concept. The remainder of the article will focus on binary classification, where  . Assumptions Most of the work on multiple instance learning, including Dietterich et al. (1997) and Maron  Lozano-P rez (1997) early papers, make the assumption regarding the relationship between the instances within a bag and the class label of the bag. Because of its importance, that assumption is often called standard MI assumption. Standard assumption The standard assumption takes each instance x X displaystyle xin mathcal X to have an associated label y  0 , 1  displaystyle yin 0,1 which is hidden to the learner. The pair ( x , y ) displaystyle (x,y) is called an instance-level concept. A bag is now viewed as a multiset of instance-level concepts, and is labeled positive if at least one of its instances has a positive label, and negative if all of its instances have negative labels. Formally, let . The label of B displaystyle B is then c ( B )  1 ) . Standard MI assumption is asymmetric, which means that if the positive and negative labels are reversed, the assumption has a different meaning. Because of that, when we use this assumption, we need to be clear which label should be the positive one. The standard assumption might be viewed as too strict, and therefore in the recent years, researchers tried to relax that position, which gave rise to other more loose assumptions. The reason for this is the belief that standard MIL assumption is appropriate for the Musk dataset, but since MIL can be applied to numerous other problems, some different assumptions could probably be more appropriate. Guided by that idea, Weidmann formulated a hierarchy of generalized instance-based assumptions for MIL. It consists of the standard MI assumption and three types of generalized MI assumptions, each more general than the last, in the sense that the former can be obtained as a specific choice of parameters of the latter, standard displaystyle subset  presence-based displaystyle subset  threshold-based displaystyle subset  count-based, with the count-based assumption being the most general and the standard assumption being the least general. (Note however, that any bag meeting the count-based assumption meets the threshold-based assumption which in turn meets the presence-based assumption which, again in turn, meet the standard assumption. In that sense it is also correct to state that the standard assumption is the weakest, hence most general, and the count-based assumption is the strongest, hence least general.) One would expect an algorithm which performs well under one of these assumptions to perform at least as well under the less general assumptions. Presence-, threshold-, and count-based assumptions The presence-based assumption is a generalization of the standard assumption, wherein a bag must contain all instances that belong to a set of required instance-level concepts in order to be labeled positive. Formally, let C R X Y displaystyle C_Rsubseteq mathcal Xtimes mathcal Y be the set of required instance-level concepts, and let  ( B , c i ) displaystyle (B,c_i) denote the number of times the instance-level concept c i displaystyle c_i occurs in the bag B displaystyle B . Then c ( B )  1  ( B , c i ) 1 displaystyle c(B)1Leftrightarrow (B,c_i)geq 1 for all c i C R displaystyle c_iin C_R . Note that, by taking C R displaystyle C_R to contain only one instance-level concept, the presence-based assumption reduces to the standard assumption. A further generalization comes with the threshold-based assumption, where each required instance-level concept must occur not only once in a bag, but some minimum (threshold) number of times in order for the bag to be labeled positive. With the notation above, to each required instance-level concept c i C R displaystyle c_iin C_R is associated a threshold l i N displaystyle l_iin mathbb N  . For a bag B displaystyle B , c ( B )  1  ( B , c i ) l i displaystyle c(B)1Leftrightarrow (B,c_i)geq l_i for all c i C R displaystyle c_iin C_R . The count-based assumption is a final generalization which enforces both lower and upper bounds for the number of times a required concept can occur in a positively labeled bag. Each required instance-level concept c i C R displaystyle c_iin C_R has a lower threshold l i N displaystyle l_iin mathbb N  and upper threshold u i N displaystyle u_iin mathbb N  with l i u i displaystyle l_ileq u_i . A bag B displaystyle B is labeled according to c ( B )  1 l i  ( B , c i ) u i displaystyle c(B)1Leftrightarrow l_ileq (B,c_i)leq u_i for all c i C R displaystyle c_iin C_R . GMIL assumption Scott, Zhang, and Brown (2005) describe another generalization of the standard model, which they call generalized multiple instance learning (GMIL). The GMIL assumption specifies a set of required instances Q X displaystyle Qsubseteq mathcal X . A bag X displaystyle X is labeled positive if it contains instances which are sufficiently close to at least r displaystyle r of the required instances Q displaystyle Q . Under only this condition, the GMIL assumption is equivalent to the presence-based assumption. However, Scott et al. describe a further generalization in which there is a set of attraction points Q X displaystyle Qsubseteq mathcal X and a set of repulsion points Q X displaystyle overline Qsubseteq mathcal X . A bag is labeled positive if and only if it contains instances which are sufficiently close to at least r displaystyle r of the attraction points and are sufficiently close to at most s displaystyle s of the repulsion points. This condition is strictly more general than the presence-based, though it does not fall within the above hierarchy. Collective assumption In contrast to the previous assumptions where the bags were viewed as fixed, the collective assumption views a bag B displaystyle B as a distribution p ( x  B ) displaystyle p(xB) over instances X displaystyle mathcal X , and similarly view labels as a distribution p ( y  x ) displaystyle p(yx) over instances. The goal of an algorithm operating under the collective assumption is then to model the distribution p ( y  B )  X p ( y  x ) p ( x  B ) d x displaystyle p(yB)int _mathcal Xp(yx)p(xB)dx . Since p ( x  B ) displaystyle p(xB) is typically considered fixed but unknown, algorithms instead focus on computing the empirical version p  ( y  B )  1 n B  . Since p ( y  x ) displaystyle p(yx) is also typically taken to be fixed but unknown, most collective-assumption based methods focus on learning this distribution, as in the single-instance version. While the collective assumption weights every instance with equal importance, Foulds extended the collective assumption to incorporate instance weights. The weighted collective assumption is then that p  ( y  B )  1 w B ) . Algorithms There are two major flavors of algorithms for Multiple Instance Learning instance-based and metadata-based, or embedding-based algorithms. The term instance-based denotes that the algorithm attempts to find a set of representative instances based on an MI assumption and classify future bags from these representatives. By contrast, metadata-based algorithms make no assumptions about the relationship between instances and bag labels, and instead try to extract instance-independent information (or metadata) about the bags in order to learn the concept. For a survey of some of the modern MI algorithms see Foulds and Frank. Instance-based algorithms The earliest proposed MI algorithms were a set of iterated-discrimination algorithms developed by Dietterich et al., and Diverse Density developed by Maron and Lozano-P rez. Both of these algorithms operated under the standard assumption. Iterated-discrimination Broadly, all of the iterated-discrimination algorithms consist of two phases. The first phase is to grow an axis parallel rectangle (APR) which contains at least one instance from each positive bag and no instances from any negative bags. This is done iteratively starting from a random instance x 1 B 1 displaystyle x_1in B_1 in a positive bag, the APR is expanded to the smallest APR covering any instance x 2 displaystyle x_2 in a new positive bag B 2 displaystyle B_2 . This process is repeated until the APR covers at least one instance from each positive bag. Then, each instance x i displaystyle x_i contained in the APR is given a relevance, corresponding to how many negative points it excludes from the APR if removed. The algorithm then selects candidate representative instances in order of decreasing relevance, until no instance contained in a negative bag is also contained in the APR. The algorithm repeats these growth and representative selection steps until convergence, where APR size at each iteration is taken to be only along candidate representatives. After the first phase, the APR is thought to tightly contain only the representative attributes. The second phase expands this tight APR as follows a Gaussian distribution is centered at each attribute and a looser APR is drawn such that positive instances will fall outside the tight APR with fixed probability. Though iterated discrimination techniques work well with the standard assumption, they do not generalize well to other MI assumptions. Diverse Density In its simplest form, Diverse Density (DD) assumes a single representative instance t displaystyle t as the concept. This representative instance must be dense in that it is much closer to instances from positive bags than from negative bags, as well as diverse in that it is close to at least one instance from each positive bag. Let B    B i   1 m displaystyle mathcal BB_i_1m be the set of positively labeled bags and let  . Letting B i j displaystyle B_ij denote the jth instance of bag i, the noisy-or model gives P r ( t  B i  )  1 j ( 1 P r ( t  B i j  ) ) displaystyle Pr(tB_i)1-prod _jleft(1-Prleft(tB_ijright)right) P r ( t  B i )  j ( 1 P r ( t  B i j ) ) displaystyle Pr(tB_i-)prod _jleft(1-Prleft(tB_ij-right)right) P ( t  B i j ) displaystyle P(tB_ij) is taken to be the scaled distance P ( t  B i j ) exp ( k s k 2 ( x k ( B i j ) k ) 2 ) displaystyle P(tB_ij)propto exp left(-sum _ks_k2left(x_k-(B_ij)_kright)2right) where . This way, if every positive bag has an instance close to t displaystyle t , then P r ( t  B i  ) displaystyle Pr(tB_i) will be high for each i displaystyle i , but if any negative bag B i displaystyle B_i- has an instance close to t displaystyle t , P r ( t  B i ) displaystyle Pr(tB_i-) will be low. Hence, D D ( t ) displaystyle DD(t) is high only if every positive bag has an instance close to t displaystyle t and no negative bags have an instance close to t displaystyle t . The candidate concept t  displaystyle hat t can be obtained through gradient methods. Classification of new bags can then be done by evaluating proximity to t  displaystyle hat t . Though Diverse Density was originally proposed by Maron et al. in 1998, more recent MIL algorithms use the DD framework, such as EM-DD in 2001 and DD-SVM in 2004, and MILES in 2006 A number of single-instance algorithms have also been adapted to a multiple-instance context under the standard assumption, including Support vector machines Artificial neural networks Decision trees Boosting Post 2000, there was a movement away from the standard assumption and the development of algorithms designed to tackle the more general assumptions listed above. Weidmann proposes a Two-Level Classification (TLC) algorithm to learn concepts under the count-based assumption. The first step tries to learn instance-level concepts by building a decision tree from each instance in each bag of the training set. Each bag is then mapped to a feature vector based on the counts in the decision tree. In the second step, a single-instance algorithm is run on the feature vectors to learn the concept Scott et al. proposed an algorithm, GMIL-1, to learn concepts under the GMIL assumption in 2005. GMIL-1 enumerates all axis-parallel rectangles  R i  i I displaystyle R_i_iin I in the original space of instances, and defines a new feature space of Boolean vectors. A bag B displaystyle B is mapped to a vector . A single-instance algorithm can then be applied to learn the concept in this new feature space. Because of the high dimensionality of the new feature space and the cost of explicitly enumerating all APRs of the original instance space, GMIL-1 is inefficient both in terms of computation and memory. GMIL-2 was developed as a refinement of GMIL-1 in an effort to improve efficiency. GMIL-2 pre-processes the instances to find a set of candidate representative instances. GMIL-2 then maps each bag to a Boolean vector, as in GMIL-1, but only considers APRs corresponding to unique subsets of the candidate representative instances. This significantly reduces the memory and computational requirements. Xu (2003) proposed several algorithms based on logistic regression and boosting methods to learn concepts under the collective assumption. Metadata-based (or embedding-based) algorithms By mapping each bag to a feature vector of metadata, metadata-based algorithms allow the flexibility of using an arbitrary single-instance algorithm to perform the actual classification task. Future bags are simply mapped (embedded) into the feature space of metadata and labeled by the chosen classifier. Therefore, much of the focus for metadata-based algorithms is on what features or what type of embedding leads to effective classification. Note that some of the previously mentioned algorithms, such as TLC and GMIL could be considered metadata-based. One approach is to let the metadata for each bag be some set of statistics over the instances in the bag. The SimpleMI algorithm takes this approach, where the metadata of a bag is taken to be a simple summary statistic, such as the average or minimum and maximum of each instance variable taken over all instances in the bag. There are other algorithms which use more complex statistics, but SimpleMI was shown to be surprisingly competitive for a number of datasets, despite its apparent lack of complexity. Another common approach is to consider the geometry of the bags themselves as metadata. This is the approach taken by the MIGraph and miGraph algorithms, which represent each bag as a graph whose nodes are the instances in the bag. There is an edge between two nodes if the distance (up to some metric on the instance space) between the corresponding instances is less than some threshold. Classification is done via an SVM with a graph kernel (MIGraph and miGraph only differ in their choice of kernel). Similar approaches are taken by MILES and MInD. MILES represents a bag by its similarities to instances in the training set, while MInD represents a bag by its distances to other bags. A modification of k-nearest neighbors (kNN) can also be considered a metadata-based algorithm with geometric metadata, though the mapping between bags and metadata features is not explicit. However, it is necessary to specify the metric used to compute the distance between bags. Wang and Zucker (2000) suggest the (maximum and minimum, respectively) Hausdorff metrics for bags A displaystyle A and B displaystyle B  H ( A , B )  max  max A min B a b , max B min A a b  displaystyle H(A,B)max leftmax _Amin _Ba-b,max _Bmin _Aa-bright h 1 ( A , B )  min A min B a b displaystyle h_1(A,B)min _Amin _Ba-b They define two variations of kNN, Bayesian-kNN and citation-kNN, as adaptations of the traditional nearest-neighbor problem to the multiple-instance setting. Generalizations So far this article has considered multiple instance learning exclusively in the context of binary classifiers. However, the generalizations of single-instance binary classifiers can carry over to the multiple-instance case. One such generalization is the multiple-instance multiple-label problem (MIML), where each bag can now be associated with any subset of the space of labels. Formally, if X displaystyle mathcal X is the space of features and Y displaystyle mathcal Y is the space of labels, an MIML concept is a map c  N X 2 Y displaystyle cmathbb N mathcal Xrightarrow 2mathcal Y . Zhou and Zhang (2006) propose a solution to the MIML problem via a reduction to either a multiple-instance or multiple-concept problem. Another obvious generalization is to multiple-instance regression. Here, each bag is associated with a single real number as in standard regression. Much like the standard assumption, MI regression assumes there is one instance in each bag, called the prime instance, which determines the label for the bag (up to noise). The ideal goal of MI regression would be to find a hyperplane which minimizes the square loss of the prime instances in each bag, but the prime instances are hidden. In fact, Ray and Page (2001) show that finding a best fit hyperplane which fits one instance from each bag is intractable if there are fewer than three instances per bag, and instead develop an algorithm for approximation. Many of the algorithms developed for MI classification may also provide good approximations to the MI regression problem. See also Supervised learning Multi-label classification References Further reading Recent reviews of the MIL literature include Amores (2013), which provides an extensive review and comparative study of the different paradigms, Foulds  Frank (2010), which provides a thorough review of the different assumptions used by different paradigms in the literature. Dietterich, Thomas G Lathrop, Richard H Lozano-P rez, Tom s (1997). Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence. 89 (1 2) 31 71. doi10.1016/-3702(96)00034-3. Herrera, Francisco Ventura, Sebasti n Bello, Rafael Cornelis, Chris Zafra, Amelia S nchez-Tarrag , D nel Vluymans, Sarah (2016). Multiple Instance Learning. doi10.1007/978-3-319-47759-6. ISBN 978-3-319-47758-9. S2CID 24047205. Amores, Jaume (2013). Multiple instance classification Review, taxonomy and comparative study. Artificial Intelligence. 201 81 105. doi10.1016/j.artint.2013.06.003. Foulds, James Frank, Eibe (2010). A review of multi-instance learning assumptions. The Knowledge Engineering Review. 25 1 25. CiteSeerX 10.1.1.148.2333. doi10.1017/S026988890999035X. S2CID 8601873. Keeler, James D. Rumelhart, David E. Leow, Wee-Kheng (1990). Integrated segmentation and recognition of hand-printed numerals. Proceedings of the 1990 Conference on Advances in Neural Information Processing Systems (NIPS 3). Morgan Kaufmann Publishers. pp. 557 563. ISBN 978-1-55860-184-0. Li, Hong-Dong Menon, Rajasree Omenn, Gilbert S Guan, Yuanfang (2014). The emerging era of genomic data integration for analyzing splice isoform function. Trends in Genetics. 30 (8) 340 7. doi10.1016/j.tig.2014.05.005. PMC 4112133. PMID 24951248. Eksi, Ridvan Li, Hong-Dong Menon, Rajasree Wen, Yuchen Omenn, Gilbert S Kretzler, Matthias Guan, Yuanfang (2013). Systematically Differentiating Functions for Alternatively Spliced Isoforms through Integrating RNA-seq Data. PLOS Computational Biology. 9 (11) . Bibcode2013PLSCB...9E3314E. doi10.1371/journal.pcbi.1003314. PMC 3820534. PMID 24244129. Maron, O. Ratan, A.L. (1998). Multiple-instance learning for natural scene classification. Proceedings of the Fifteenth International Conference on Machine Learning. Morgan Kaufmann Publishers. pp. 341 349. ISBN 978-1-55860-556-5. Kotzias, Dimitrios Denil, Misha De Freitas, Nando Smyth, Padhraic (2015). From Group to Individual Labels Using Deep Features. Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD 15. pp. 597 606. doi10.1145/2783258.2783380. ISBN 9781450336642. S2CID 7729996. Ray, Soumya Page, David (2001). Multiple instance regression (PDF). ICML. Bandyopadhyay, Sanghamitra Ghosh, Dip Mitra, Ramkrishna Zhao, Zhongming (2015). MBSTAR Multiple instance learning for predicting specific functional binding sites in microRNA targets. Scientific Reports. 5 8004. Bibcode2015NatSR...5E8004B. doi10.1038/. PMC 4648438. PMID 25614300. Zhu, Wentao Lou, Qi Vang, Yeeleng Scott Xie, Xiaohui (2017). Deep Multi-instance Networks with Sparse Label Assignment for Whole Mammogram Classification. Medical Image Computing and Computer-Assisted Intervention MICCAI 2017. Lecture Notes in Computer Science. Vol. 10435. pp. 603 11. arXiv1612.05968. doi10.1007/978-3-319-66179-7_69. ISBN 978-3-319-66178-0. S2CID 9623929. Title Multiplicative weight update method URL https//en.wikipedia.org/wiki/Multiplicative_weight_update_method Content The multiplicative weights update method is an algorithmic technique most commonly used for decision making and prediction, and also widely deployed in game theory and algorithm design. The simplest use case is the problem of prediction from expert advice, in which a decision maker needs to iteratively decide on an expert whose advice to follow. The method assigns initial weights to the experts (usually identical initial weights), and updates these weights multiplicatively and iteratively according to the feedback of how well an expert performed reducing it in case of poor performance, and increasing it otherwise. It was discovered repeatedly in very diverse fields such as machine learning (AdaBoost, Winnow, Hedge), optimization (solving linear programs), theoretical computer science (devising fast algorithm for LPs and SDPs), and game theory. Name Multiplicative weights implies the iterative rule used in algorithms derived from the multiplicative weight update method. It is given with different names in the different fields where it was discovered or rediscovered. History and background The earliest known version of this technique was in an algorithm named fictitious play which was proposed in game theory in the early 1950s. Grigoriadis and Khachiyan applied a randomized variant of fictitious play to solve two-player zero-sum games efficiently using the multiplicative weights algorithm. In this case, player allocates higher weight to the actions that had a better outcome and choose his strategy relying on these weights. In machine learning, Littlestone applied the earliest form of the multiplicative weights update rule in his famous winnow algorithm, which is similar to Minsky and Paperts earlier perceptron learning algorithm. Later, he generalized the winnow algorithm to weighted majority algorithm. Freund and Schapire followed his steps and generalized the winnow algorithm in the form of hedge algorithm. The multiplicative weights algorithm is also widely applied in computational geometry such as Kenneth Clarksons algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find set covers for hypergraphs with small VC dimension. In operations research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently. In computer science field, some researchers have previously observed the close relationships between multiplicative update algorithms used in different contexts. Young discovered the similarities between fast LP algorithms and Raghavans method of pessimistic estimators for derandomization of randomized rounding algorithms Klivans and Servedio linked boosting algorithms in learning theory to proofs of Yaos XOR Lemma Garg and Khandekar defined a common framework for convex optimization problems that contains Garg-Konemann and Plotkin-Shmoys-Tardos as subcases. The Hedge algorithm is a special case of mirror descent. General setup A binary decision needs to be made based on n experts opinions to attain an associated payoff. In the first round, all experts opinions have the same weight. The decision maker will make the first decision based on the majority of the experts prediction. Then, in each successive round, the decision maker will repeatedly update the weight of each experts opinion depending on the correctness of his prior predictions. Real life examples includes predicting if it is rainy tomorrow or if the stock market will go up or go down. Algorithm analysis Halving algorithm Given a sequential game played between an adversary and an aggregator who is advised by N experts, the goal is for the aggregator to make as few mistakes as possible. Assume there is an expert among the N experts who always gives the correct prediction. In the halving algorithm, only the consistent experts are retained. Experts who make mistakes will be dismissed. For every decision, the aggregator decides by taking a majority vote among the remaining experts. Therefore, every time the aggregator makes a mistake, at least half of the remaining experts are dismissed. The aggregator makes at most (N) mistakes. Weighted majority algorithm Source Unlike halving algorithm which dismisses experts who have made mistakes, weighted majority algorithm discounts their advice. Given the same expert advice setup, suppose we have n decisions, and we need to select one decision for each loop. In each loop, every decision incurs a cost. All costs will be revealed after making the choice. The cost is 0 if the expert is correct, and 1 otherwise. this algorithms goal is to limit its cumulative losses to roughly the same as the best of experts. The very first algorithm that makes choice based on majority vote every iteration does not work since the majority of the experts can be wrong consistently every time. The weighted majority algorithm corrects above trivial algorithm by keeping a weight of experts instead of fixing the cost at either 1 or 0. This would make fewer mistakes compared to halving algorithm. Initialization Fix an 1 / 2 displaystyle eta leq 1/2 . For each expert, associate the weight w i 1 displaystyle w_i1 1. For t displaystyle t  1 displaystyle mathit 1 , 2 displaystyle mathit 2 ,..., T displaystyle T 1. Make the prediction given by the weighted majority of the experts predictions based on their weights w 1 t , . . . , w n t displaystyle mathbb w_1 t,...,mathbb w_n t . That is, choose 0 or 1 depending on which prediction has a higher total weight of experts advising it (breaking ties arbitrarily). 2. For every expert i that predicted wrongly, decrease his weight for the next round by multiplying it by a factor of (1- ) w i t  1 displaystyle w_it1  ( 1 ) w i t displaystyle (1-eta )w_it (update rule) . When displaystyle eta  increases, the weight of the experts advice will decrease. Note that some researchers . After T displaystyle T steps, let m i T displaystyle m_iT be the number of mistakes of expert i and M T displaystyle MT be the number of mistakes our algorithm has made. Then we have the following bound for every i displaystyle i  M T 2 ( 1  ) m i T  2 ln ( n ) displaystyle MTleq 2(1eta )m_iTfrac 2ln(n)eta  . In particular, this holds for i which is the best expert. Since the best expert will have the least m i T displaystyle m_iT , it will give the best bound on the number of mistakes made by the algorithm as a whole. Randomized weighted majority algorithm This algorithm can be understood as follows Given the same setup with N experts. Consider the special situation where the proportions of experts predicting positive and negative, counting the weights, are both close to 50. Then, there might be a tie. Following the weight update rule in weighted majority algorithm, the predictions made by the algorithm would be randomized. The algorithm calculates the probabilities of experts predicting positive or negatives, and then makes a random decision based on the computed fraction predict f ( x )   1 with probability q 1 W 0 otherwise displaystyle f(x)begincases1textwith probabilityfrac q_1W0textotherwiseendcases where  . The number of mistakes made by the randomized weighted majority algorithm is bounded as E   mistakes of the learner  (  mistakes of the best expert )  c ln ( N ) displaystyle Elefttextmistakes of the learnerrightleq alpha _beta left(text mistakes of the best expertright)c_beta ln(N)   . Note that only the learning algorithm is randomized. The underlying assumption is that the examples and experts predictions are not random. The only randomness is the randomness where the learner makes his own prediction. In this randomized algorithm, 1 displaystyle alpha _beta rightarrow 1 if 1 displaystyle beta rightarrow 1 . Compared to weighted algorithm, this randomness halved the number of mistakes the algorithm is going to make. However, it is important to note that in some research, people . Applications The multiplicative weights method is usually used to solve a constrained optimization problem. Let each expert be the constraint in the problem, and the events represent the points in the area of interest. The punishment of the expert corresponds to how well its corresponding constraint is satisfied on the point represented by an event. Solving zero-sum games approximately (Oracle algorithm) Source Suppose we were given the distribution P displaystyle P on experts. Let A displaystyle A  payoff matrix of a finite two-player zero-sum game, with n displaystyle n rows. When the row player p r displaystyle p_r uses plan i displaystyle i and the column player p c displaystyle p_c uses plan j displaystyle j , the payoff of player p c displaystyle p_c is A ( i , j ) displaystyle Aleft(i,jright) A i j displaystyle A_ij , assuming A ( i , j )  0 , 1  displaystyle Aleft(i,jright)in left0,1right . If player p r displaystyle p_r chooses action i displaystyle i from a distribution P displaystyle P over the rows, then the expected result for player p c displaystyle p_c selecting action j displaystyle j is A ( P , j )  E i P  A ( i , j )  displaystyle Aleft(P,jright)E_iin PleftAleft(i,jright)right . To maximize A ( P , j ) displaystyle Aleft(P,jright) , player p c displaystyle p_c should choose plan j displaystyle j . Similarly, the expected payoff for player p l displaystyle p_l is A ( i , P )  E j P  A ( i , j )  displaystyle Aleft(i,Pright)E_jin PleftAleft(i,jright)right . Choosing plan i displaystyle i would minimize this payoff. By John Von Neumanns Min-Max Theorem, we obtain min P max j A ( P , j )  max Q min i A ( i , Q ) displaystyle min _Pmax _jAleft(P,jright)max _Qmin _iAleft(i,Qright) where P and i changes over the distributions over rows, Q and j changes over the columns. Then, let displaystyle lambda  denote the common value of above quantities, also named as the value of the game. Let  0 displaystyle delta 0 be an error parameter. To solve the zero-sum game bounded by additive error of displaystyle delta  , min i A ( i , q ) displaystyle lambda -delta leq min _iAleft(i,qright) max j A ( p , j )  displaystyle max _jAleft(p,jright)leq lambda delta  So there is an algorithm solving zero-sum game up to an additive factor of using O((n)/ 2 displaystyle delta 2 ) calls to ORACLE, with an additional processing time of O(n) per call Bailey and Piliouras showed that although the time average behavior of multiplicative weights update converges to Nash equilibria in zero-sum games the day-to-day (last iterate) behavior diverges away from it. Machine learning In machine learning, Littlestone and Warmuth generalized the winnow algorithm to the weighted majority algorithm. Later, Freund and Schapire generalized it in the form of hedge algorithm. AdaBoost Algorithm formulated by Yoav Freund and Robert Schapire also employed the Multiplicative Weight Update Method. Winnow algorithm Based on current knowledge in algorithms, the multiplicative weight update method was first used in Littlestones winnow algorithm. It is used in machine learning to solve a linear program. Given m displaystyle m labeled examples ( a 1 , l 1 ) , , ( a m , l m ) displaystyle left(a_1,l_1right),text ,left(a_m,l_mright) where a j R n displaystyle a_jin mathbb R n are feature vectors, and l j  1 , 1  displaystyle l_jin left-1,1rightquad  are their labels. The aim is to find non-negative weights such that for all examples, the sign of the weighted combination of the features matches its labels. That is, require that l j a j x 0 displaystyle l_ja_jxgeq 0 for all j displaystyle j . Without loss of generality, assume the total weight is 1 so that they form a distribution. Thus, for notational convenience, redefine a j displaystyle a_j to be l j a j displaystyle l_ja_j , the problem reduces to finding a solution to the following LP  . This is general form of LP. Hedge algorithm Source The hedge algorithm is similar to the weighted majority algorithm. However, their exponential update rules are different. It is generally used to solve the problem of binary allocation in which we need to allocate different portion of resources into N different options. The loss with every option is available at the end of every iteration. The goal is to reduce the total loss suffered for a particular allocation. The allocation for the following iteration is then revised, based on the total loss suffered in the current iteration using multiplicative update. Analysis Assume the learning rate  0 displaystyle eta 0 and for t  T  displaystyle tin T , p t displaystyle pt is picked by Hedge. Then for all experts i displaystyle i , t T p t m t t T m i t  ln ( N )  T displaystyle sum _tleq Tptmtleq sum _tleq Tm_itfrac ln(N)eta eta T Initialization Fix an  0 displaystyle eta 0 . For each expert, associate the weight w i 1 displaystyle w_i1 1 For ,...,T 1. Pick the distribution p i  . 2. Observe the cost of the decision m t displaystyle mt . 3. Set w i t  1  w i t exp ( m i t displaystyle w_it1w_itexp(-eta m_it ). AdaBoost algorithm This algorithm maintains a set of weights w t displaystyle wt over the training examples. On every iteration t displaystyle t , a distribution p t displaystyle pt is computed by normalizing these weights. This distribution is fed to the weak learner WeakLearn which generates a hypothesis h t displaystyle h_t that (hopefully) has small error with respect to the distribution. Using the new hypothesis h t displaystyle h_t , AdaBoost generates the next weight vector w t  1 displaystyle wt1 . The process repeats. After T such iterations, the final hypothesis h f displaystyle h_f is the output. The hypothesis h f displaystyle h_f combines the outputs of the T weak hypotheses using a weighted majority vote. Input Sequence of N displaystyle N labeled examples ( x 1 displaystyle x_1 , y 1 displaystyle y_1 ),...,( x N displaystyle x_N , y N displaystyle y_N ) Distribution D displaystyle D over the N displaystyle N examples Weak learning algorithm WeakLearn Integer T displaystyle T specifying number of iterations Initialize the weight vector w i 1  D ( i ) displaystyle w_i1D(i) for  , . . . , N displaystyle ,...,N . Do for  , . . . , T displaystyle ,...,T 1. Set p  . 2. Call WeakLearn, providing it with the distribution p t displaystyle pt  get back a hypothesis h t  X displaystyle h_tXrightarrow  0,1. 3. Calculate the error of h t   . 4. Set  . 5. Set the new weight vector to be w i t  1  w i t t 1  h t ( x i ) y i  displaystyle w_it1w_itbeta _t1-h_t(x_i)-y_i . Output the hypothesis f ( x )  h f ( x )   1 if .e., there is no solution to this linear system of inequalities. Solution Given vector p n displaystyle pin Delta _n , solves the following relaxed problem ? x  p T A x p T b displaystyle exists ?xptextsf T!!Axgeq ptextsf T!b (2) If there exists a x satisfying (1), then x satisfies (2) for all p n displaystyle pin Delta _n . The contrapositive of this statement is also true. Suppose if oracle returns a feasible solution for a p displaystyle p , the solution x displaystyle x it returns has bounded width max i  ( A x ) i b i  1 displaystyle max _i(Ax)_i-b_ileq 1 . So if there is a solution to (1), then there is an algorithm that its output x satisfies the system (2) up to an additive error of 2 displaystyle 2epsilon  . The algorithm makes at most ln ( m ) 2 displaystyle frac ln(m)epsilon 2 calls to a width-bounded oracle for the problem (2). The contrapositive stands true as well. The multiplicative updates is applied in the algorithm in this case. Other applications Evolutionary game theory Multiplicative weights update is the discrete-time variant of the replicator equation (replicator dynamics), which is a commonly used model in evolutionary game theory. It converges to Nash equilibrium when applied to a congestion game. Operations research and online statistical decision-making In operations research and on-line statistical decision making problem field, the weighted majority algorithm and its more complicated versions have been found independently. Computational geometry The multiplicative weights algorithm is also widely applied in computational geometry, such as Clarksons algorithm for linear programming (LP) with a bounded number of variables in linear time. Later, Bronnimann and Goodrich employed analogous methods to find Set Covers for hypergraphs with small VC dimension. Gradient descent method Matrix multiplicative weights update Plotkin, Shmoys, Tardos framework for packing/covering LPs Approximating multi-commodity flow problems O (logn)- approximation for many NP-hard problems Learning theory and boosting Hard-core sets and the XOR lemma Hannans algorithm and multiplicative weights Online convex optimization References External links The Game Theory of Life a Quanta Magazine article describing the use of the method to evolutionary biology in a paper by Erick Chastain, Adi Livnat, Christos Papadimitriou, and Umesh Vazirani Title Multitask optimization URL https//en.wikipedia.org/wiki/Multitask_optimization Content Multi-task optimization is a paradigm in the optimization literature that focuses on solving multiple self-contained tasks simultaneously. The paradigm has been inspired by the well-established concepts of transfer learning and multi-task learning in predictive analytics. The key motivation behind multi-task optimization is that if optimization tasks are related to each other in terms of their optimal solutions or the general characteristics of their function landscapes, the search progress can be transferred to substantially accelerate the search on the other. The success of the paradigm is not necessarily limited to one-way knowledge transfers from simpler to more complex tasks. In practice an attempt is to intentionally solve a more difficult task that may unintentionally solve several smaller problems. There is a direct relationship between multitask optimization and multi-objective optimization. Methods There are several common approaches for multi-task optimization Bayesian optimization, evolutionary computation, and approaches based on Game theory. Multi-task Bayesian optimization Multi-task Bayesian optimization is a modern model-based approach that leverages the concept of knowledge transfer to speed up the automatic hyperparameter optimization process of machine learning algorithms. The method builds a multi-task Gaussian process model on the data originating from different searches progressing in tandem. The captured inter-task dependencies are thereafter utilized to better inform the subsequent sampling of candidate solutions in respective search spaces. Evolutionary multi-tasking Evolutionary multi-tasking has been explored as a means of exploiting the implicit parallelism of population-based search algorithms to simultaneously progress multiple distinct optimization tasks. By mapping all tasks to a unified search space, the evolving population of candidate solutions can harness the hidden relationships between them through continuous genetic transfer. This is induced when solutions associated with different tasks crossover. Recently, modes of knowledge transfer that are different from direct solution crossover have been explored. Game-theoretic optimization Game-theoretic approaches to multi-task optimization propose to view the optimization problem as a game, where each task is a player. All players compete through the reward matrix of the game, and try to reach a solution that satisfies all players (all tasks). This view provide insight about how to build efficient algorithms based on gradient descent optimization (GD), which is particularly important for training deep neural networks. In GD for MTL, the problem is that each task provides its own loss, and it is not clear how to combine all losses and create a single unified gradient, leading to several different aggregation strategies. This aggregation problem can be solved by defining a game matrix where the reward of each player is the agreement of its own gradient with the common gradient, and then setting the common gradient to be the Nash Cooperative bargaining of that system. Applications Algorithms for multi-task optimization span a wide array of real-world applications. Recent studies highlight the potential for speed-ups in the optimization of engineering design parameters by conducting related designs jointly in a multi-task manner. In machine learning, the transfer of optimized features across related data sets can enhance the efficiency of the training process as well as improve the generalization capability of learned models. In addition, the concept of multi-tasking has led to advances in automatic hyperparameter optimization of machine learning models and ensemble learning. Applications have also been reported in cloud computing, with future developments geared towards cloud-based on-demand optimization services that can cater to multiple customers simultaneously. Recent work has additionally shown applications in chemistry. In addition, some recent works have applied multi-task optimization algorithms in industrial manufacturing. See also Multi-objective optimization Multi-task learning Multicriteria classification Multiple-criteria decision analysis Title Multivariate adaptive regression spline URL https//en.wikipedia.org/wiki/Multivariate_adaptive_regression_spline Content In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991. It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables. The term MARS is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open-source implementations of MARS are called Earth. The basics This section introduces MARS using a few examples. We start with a set of data a matrix of input variables x, and a vector of the observed responses y, with a response for each row in x. For example, the data could be Here there is only one independent variable, so the x matrix is just a single column. Given these measurements, we would like to build a model which predicts the expected y for a given x. A linear model for the above data is y   37  5.1 x displaystyle widehat y-375.1x The hat on the y  displaystyle widehat y indicates that y  displaystyle widehat y is estimated from the data. The figure on the right shows a plot of this function a line giving the predicted y  displaystyle widehat y versus x, with the original values of y shown as red dots. The data at the extremes of x indicates that the relationship between y and x may be non-linear (look at the red dots relative to the regression line at low and high values of x). We thus turn to MARS to automatically build a model taking into account non-linearities. MARS software constructs a model from the given x and y as follows y   25  6.1 max ( 0 , x 13 ) 3.1 max ( 0 , 13 x ) displaystyle beginalignedwidehat y 256.1max(0,x-13)-3.1max(0,13-x)endaligned The figure on the right shows a plot of this function the predicted y  displaystyle widehat y versus x, with the original values of y once again shown as red dots. The predicted response is now a better fit to the original y values. MARS has automatically produced a kink in the predicted y to take into account non-linearity. The kink is produced by hinge functions. The hinge functions are the expressions starting with max displaystyle max  (where max ( a , b ) displaystyle max(a,b) is a displaystyle a if a  b displaystyle ab , else b displaystyle b ). Hinge functions are described in more detail below. In this simple example, we can easily see from the plot that y has a non-linear relationship with x (and might perhaps guess that y varies with the square of x). However, in general there will be multiple independent variables, and the relationship between y and these variables will be unclear and not easily visible by plotting. We can use MARS to discover that non-linear relationship. An example MARS expression with multiple variables is o z o n .2  0.93 max ( 0 , t e m p 58 ) 0.64 max ( 0 , t e m p 68 ) 0.046 max ( 0 , 234 i b t ) 0.016 max ( 0 , w i n d 7 ) max ( 0 , 200 v i s ) displaystyle beginalignedmathrm ozone  5.20.93max(0,mathrm temp -58)-0.64max(0,mathrm temp -68)-0.046max(0,234-mathrm ibt )-0.016max(0,mathrm wind -7)max(0,200-mathrm vis )endaligned This expression models air pollution (the ozone level) as a function of the temperature and a few other variables. Note that the last term in the formula (on the last line) incorporates an interaction between w i n d displaystyle mathrm wind  and v i s displaystyle mathrm vis  . The figure on the right plots the predicted o z o n e displaystyle mathrm ozone  as w i n d displaystyle mathrm wind  and v i s displaystyle mathrm vis  vary, with the other variables fixed at their median values. The figure shows that wind does not affect the ozone level unless visibility is low. We see that MARS can build quite flexible regression surfaces by combining hinge functions. To obtain the above expression, the MARS model building procedure automatically selects which variables to use (some variables are important, others not), the positions of the kinks in the hinge functions, and how the hinge functions are combined. The MARS model MARS builds models of the form f  ( x )   ) . displaystyle widehat f(x)sum _). The model is a weighted sum of basis functions B i ( x ) displaystyle B_i(x) . Each c i displaystyle c_i is a constant coefficient. For example, each line in the formula for ozone above is one basis function multiplied by its coefficient. Each basis function B i ( x ) displaystyle B_i(x) takes one of the following three forms 1) a constant 1. There is just one such term, the intercept. In the ozone formula above, the intercept term is 5.2. 2) a hinge function. A hinge function has the form max ( 0 , x constant ) displaystyle max(0,x-textconstant) or max ( 0 , constant x ) displaystyle max(0,textconstant-x) . MARS automatically selects variables and values of those variables for knots of the hinge functions. Examples of such basis functions can be seen in the middle three lines of the ozone formula. 3) a product of two or more hinge functions. These basis functions can model interaction between two or more variables. An example is the last line of the ozone formula. Hinge functions A key part of MARS models are hinge functions taking the form max ( 0 , x c ) displaystyle max(0,x-c) or max ( 0 , c x ) displaystyle max(0,c-x) where c displaystyle c is a constant, called the knot. The figure on the right shows a mirrored pair of hinge functions with a knot at 3.1. A hinge function is zero for part of its range, so can be used to partition the data into disjoint regions, each of which can be treated independently. Thus for example a mirrored pair of hinge functions in the expression 6.1 max ( 0 , x 13 ) 3.1 max ( 0 , 13 x ) displaystyle 6.1max(0,x-13)-3.1max(0,13-x) creates the piecewise linear graph shown for the simple MARS model in the previous section. One might assume that only piecewise linear functions can be formed from hinge functions, but hinge functions can be multiplied together to form non-linear functions. Hinge functions are also called ramp, hockey stick, or rectifier functions. Instead of the max displaystyle max  notation used in this article, hinge functions are often represented by  ( x i c )   displaystyle pm (x_i-c)_ where    displaystyle cdot _ means take the positive part. The model building process MARS builds a model in two phases the forward and the backward pass. This two-stage approach is the same as that used by recursive partitioning trees. The forward pass MARS starts with a model which consists of just the intercept term (which is the mean of the response values). MARS then repeatedly adds basis function in pairs to the model. At each step it finds the pair of basis functions that gives the maximum reduction in sum-of-squares residual error (it is a greedy algorithm). The two basis functions in the pair are identical except that a different side of a mirrored hinge function is used for each function. Each new basis function consists of a term already in the model (which could perhaps be the intercept term) multiplied by a new hinge function. A hinge function is defined by a variable and a knot, so to add a new basis function, MARS must search over all combinations of the following 1) existing terms (called parent terms in this context) 2) all variables (to select one for the new basis function) 3) all values of each variable (for the knot of the new hinge function). To calculate the coefficient of each term, MARS applies a linear regression over the terms. This process of adding terms continues until the change in residual error is too small to continue or until the maximum number of terms is reached. The maximum number of terms is specified by the user before model building starts. The search at each step is usually done in a brute-force fashion, but a key aspect of MARS is that because of the nature of hinge functions, the search can be done quickly using a fast least-squares update technique. Brute-force search can be sped up by using a heuristic that reduces the number of parent terms considered at each step (Fast MARS). The backward pass The forward pass usually overfits the model. To build a model with better generalization ability, the backward pass prunes the model, deleting the least effective term at each step until it finds the best submodel. Model subsets are compared using the Generalized cross validation (GCV) criterion described below. The backward pass has an advantage over the forward pass at any step it can choose any term to delete, whereas the forward pass at each step can only see the next pair of terms. The forward pass adds terms in pairs, but the backward pass typically discards one side of the pair and so terms are often not seen in pairs in the final model. A paired hinge can be seen in the equation for y  displaystyle widehat y in the first MARS example above there are no complete pairs retained in the ozone example. Generalized cross validation The backward pass compares the performance of different models using Generalized Cross-Validation (GCV), a minor variant on the Akaike information criterion that approximates the leave-one-out cross-validation score in the special case where errors are Gaussian, or where the squared error loss function is used. GCV was introduced by Craven and Wahba and extended by Friedman for MARS lower values of GCV indicate better models. The formula for the GCV is ). The effective number of parameters is defined as (effective number of parameters)  (number of mars terms)  (penalty) ((number of Mars terms) 1 ) / 2 where penalty is typically 2 (giving results equivalent to the Akaike information criterion) but can be increased by the user if they so desire. Note that (number of Mars terms 1 ) / 2 is the number of hinge-function knots, so the formula penalizes the addition of knots. Thus the GCV formula adjusts (i.e. increases) the training RSS to penalize more complex models. We penalize flexibility because models that are too flexible will model the specific realization of noise in the data instead of just the systematic structure of the data. Constraints One constraint has already been mentioned the user can specify the maximum number of terms in the forward pass. A further constraint can be placed on the forward pass by specifying a maximum allowable degree of interaction. Typically only one or two degrees of interaction are allowed, but higher degrees can be used when the data warrants it. The maximum degree of interaction in the first MARS example above is one (i.e. no interactions or an additive model) in the ozone example it is two. Other constraints on the forward pass are possible. For example, the user can specify that interactions are allowed only for certain input variables. Such constraints could make sense because of knowledge of the process that generated the data. Pros and cons No regression modeling technique is best for all situations. The guidelines below are intended to give an idea of the pros and cons of MARS, but there will be exceptions to the guidelines. It is useful to compare MARS to recursive partitioning and this is done below. (Recursive partitioning is also commonly called regression trees, decision trees, or CART see the recursive partitioning article for details). MARS models are more flexible than linear regression models. MARS models are simple to understand and interpret. Compare the equation for ozone concentration above to, say, the innards of a trained neural network or a random forest. MARS can handle both continuous and categorical data. MARS tends to be better than recursive partitioning for numeric data because hinges are more appropriate for numeric variables than the piecewise constant segmentation used by recursive partitioning. Building MARS models often requires little or no data preparation. The hinge functions automatically partition the input data, so the effect of outliers is contained. In this respect MARS is similar to recursive partitioning which also partitions the data into disjoint regions, although using a different method. MARS (like recursive partitioning) does automatic variable selection (meaning it includes important variables in the model and excludes unimportant ones). However, there can be some arbitrariness in the selection, especially when there are correlated predictors, and this can affect interpretability. MARS models tend to have a good bias-variance trade-off. The models are flexible enough to model non-linearity and variable interactions (thus MARS models have fairly low bias), yet the constrained form of MARS basis functions prevents too much flexibility (thus MARS models have fairly low variance). MARS is suitable for handling large datasets, and implementations run very quickly. However, recursive partitioning can be faster than MARS. With MARS models, as with any non-parametric regression, parameter confidence intervals and other checks on the model cannot be calculated directly (unlike linear regression models). Cross-validation and related techniques must be used for validating the model instead. The earth, mda, and polspline implementations do not allow missing values in predictors, but free implementations of regression trees (such as rpart and party) do allow missing values using a technique called surrogate splits. MARS models can make predictions very quickly, as they only require evaluating a linear function of the predictors. The resulting fitted function is continuous, unlike recursive partitioning, which can give a more realistic model in some situations. (However, the model is not smooth or differentiable). Extensions and related concepts Generalized linear models (GLMs) can be incorporated into MARS models by applying a link function after the MARS model is built. Thus, for example, MARS models can incorporate logistic regression to predict probabilities. Non-linear regression is used when the underlying form of the function is known and regression is used only to estimate the parameters of that function. MARS, on the other hand, estimates the functions themselves, albeit with severe constraints on the nature of the functions. (These constraints are necessary because discovering a model from the data is an inverse problem that is not well-posed without constraints on the model.) Recursive partitioning (commonly called CART). MARS can be seen as a generalization of recursive partitioning that allows for continuous models, which can provide a better fit for numerical data. Generalized additive models. Unlike MARS, GAMs fit smooth loess or polynomial splines rather than hinge functions, and they do not automatically model variable interactions. The smoother fit and lack of regression terms reduces variance when compared to MARS, but ignoring variable interactions can worsen the bias. TSMARS. Time Series Mars is the term used when MARS models are applied in a time series context. Typically in this set up the predictors are the lagged time series values resulting in autoregressive spline models. These models and extensions to include moving average spline models are described in Univariate Time Series Modelling and Forecasting using TSMARS A study of threshold time series autoregressive, seasonal and moving average models using TSMARS. Bayesian MARS (BMARS) uses the same model form, but builds the model using a Bayesian approach. It may arrive at different optimal MARS models because the model building approach is different. The result of BMARS is typically an ensemble of posterior samples of MARS models, which allows for probabilistic prediction. See also Linear regression Local regression Rational function modeling Segmented regression Spline interpolation Spline regression References Further reading Hastie T., Tibshirani R., and Friedman J.H. (2009) The Elements of Statistical Learning, 2nd edition. Springer, ISBN 978-0-387-84857-0 (has a section on MARS) Faraway J. (2005) Extending the Linear Model with R, CRC, ISBN 978-1-58488-424-8 (has an example using MARS with R) Heping Zhang and Burton H. Singer (2010) Recursive Partitioning and Applications, 2nd edition. Springer, ISBN 978-1-4419-6823-4 (has a chapter on MARS and discusses some tweaks to the algorithm) Denison D.G.T., Holmes C.C., Mallick B.K., and Smith A.F.M. (2004) Bayesian Methods for Nonlinear Classification and Regression, Wiley, ISBN 978-0-471-49036-4 Berk R.A. (2008) Statistical learning from a regression perspective, Springer, ISBN 978-0-387-77500-5 External links Several free and commercial software packages are available for fitting MARS-type models. Free software R packages earth function in the earth package mars function in the mda package polymars function in the polspline package. Not Friedmans MARS. bass function in the BASS package for Bayesian MARS. Matlab code ARESLab Adaptive Regression Splines toolbox for Matlab Code from the book Bayesian Methods for Nonlinear Classification and Regression for Bayesian MARS. Python Earth Multivariate adaptive regression splines py-earth pyBASS for Bayesian MARS. Commercial software MARS from Salford Systems. Based on Friedmans implementation. STATISTICA Data Miner from StatSoft ADAPTIVEREG from SAS. Title Native-language identification URL https//en.wikipedia.org/wiki/Native-language_identification Content Native-language identification (NLI) is the task of determining an authors native language () based only on their writings in a second language (). NLI works through identifying language-usage patterns that are common to specific groups and then applying this knowledge to predict the native language of previously unseen texts. This is motivated in part by applications in second-language acquisition, language teaching and forensic linguistics, amongst others. Overview NLI works under the assumption that an authors will dispose them towards particular language production patterns in their , as influenced by their native language. This relates to cross-linguistic influence (CLI), a key topic in the field of second-language acquisition (SLA) that analyzes transfer effects from the on later learned languages. Using large-scale English data, NLI methods achieve over 80 accuracy in predicting the native language of texts written by authors from 11 different backgrounds. This can be compared to a baseline of 9 for choosing randomly. Applications Pedagogy and language transfer This identification of -specific features has been used to study language transfer effects in second-language acquisition. This is useful for developing pedagogical material, teaching methods, -specific instructions and generating learner feedback that is tailored to their native language. Forensic linguistics NLI methods can also be applied in forensic linguistics as a method of performing authorship profiling in order to infer the attributes of an author, including their linguistic background. This is particularly useful in situations where a text, e.g. an anonymous letter, is the key piece of evidence in an investigation and clues about the native language of a writer can help investigators in identifying the source. This has already attracted interest and funding from intelligence agencies. Methodology Natural language processing methods are used to extract and identify language usage patterns common to speakers of an -group. This is done using language learner data, usually from a learner corpus. Next, machine learning is applied to train classifiers, like support vector machines, for predicting the of unseen texts. A range of ensemble based systems have also been applied to the task and shown to improve performance over single classifier systems. Various linguistic feature types have been applied for this task. These include syntactic features such as constituent parses, grammatical dependencies and part-of-speech tags. Surface level lexical features such as character, word and lemma n-grams have also been found to be quite useful for this task. However, it seems that character n-grams are the single best feature for the task. 2013 shared task The Building Educational Applications (BEA) workshop at NAACL 2013 hosted the inaugural NLI shared task. The competition resulted in 29 entries from teams across the globe, 24 of which also published a paper describing their systems and approaches. See also Title Nature Machine Intelligence URL https//en.wikipedia.org/wiki/Nature_Machine_Intelligence Content Nature Machine Intelligence is a monthly peer-reviewed scientific journal published by Nature Portfolio covering machine learning and artificial intelligence. The editor-in-chief is Liesbeth Venema. History The journal was created in response to the machine learning explosion of the 2010s. It launched in January 2019, and its opening was met with controversy and boycotts within the machine learning research community due to opposition to Nature publishing the journal as closed access. To address this issue, now Nature Machine Intelligence gives authors an option to publish open access papers for an additional fee, and authors remain owners of the research reported, and the code and data supporting the main findings of an article should be openly available. Moreover, preprints are allowed, in fact encouraged, and a link to the preprint can be added below the abstract, visible to all readers. Abstracting and indexing According to the Journal Citation Reports, the journal has a 2021 impact factor of 25.898, ranking it 1st out of 144 journals in the category Computer Science, Artificial intelligence and first out of 113 journals in the category Computer Science, Interdisciplinary Applications. References External links Official website Title Neural modeling fields URL https//en.wikipedia.org/wiki/Neural_modeling_fields Content Neural modeling field (NMF) is a mathematical framework for machine learning which combines ideas from neural networks, fuzzy logic, and model based recognition. It has also been referred to as modeling fields, modeling fields theory (MFT), Maximum likelihood artificial neural networks (MLANS). This framework has been developed by Leonid Perlovsky at the AFRL. NMF is interpreted as a mathematical description of the minds mechanisms, including concepts, emotions, instincts, imagination, thinking, and understanding. NMF is a multi-level, hetero-hierarchical system. At each level in NMF there are concept-models encapsulating the knowledge they generate so-called top-down signals, interacting with input, bottom-up signals. These interactions are governed by dynamic equations, which drive concept-model learning, adaptation, and formation of new concept-models for better correspondence to the input, bottom-up signals. Concept models and similarity measures In the general case, NMF system consists of multiple processing levels. At each level, output signals are the concepts recognized in (or formed from) input, bottom-up signals. Input signals are associated with (or recognized, or grouped into) concepts according to the models and at this level. In the process of learning the concept-models are adapted for better representation of the input signals so that similarity between the concept-models and signals increases. This increase in similarity can be interpreted as satisfaction of an instinct for knowledge, and is felt as aesthetic emotions. Each hierarchical level consists of N neurons enumerated by index ..N. These neurons receive input, bottom-up signals, X(n), from lower levels in the processing hierarchy. X(n) is a field of bottom-up neuronal synaptic activations, coming from neurons at a lower level. Each neuron has a number of synapses for generality, each neuron activation is described as a set of numbers, X ( n )   X d ( n )  , .. D . displaystyle vec X(n)X_d(n),..D. , where D is the number or dimensions necessary to describe individual neurons activation. Top-down, or priming signals to these neurons are sent by concept-models, Mm(Sm,n) M m ( S m , n ) , .. M . displaystyle vec M_m(vec S_m,n),..M. , where M is the number of models. Each model is characterized by its parameters, Sm in the neuron structure of the brain they are encoded by strength of synaptic connections, mathematically, they are given by a set of numbers, S .. A . displaystyle vec S_mS_ma,..A. , where A is the number of dimensions necessary to describe individual model. Models represent signals in the following way. Suppose that signal X(n) is coming from sensory neurons n activated by object m, which is characterized by parameters Sm. These parameters may include position, orientation, or lighting of an object m. Model Mm(Sm,n) predicts a value X(n) of a signal at neuron n. For example, during visual perception, a neuron n in the visual cortex receives a signal X(n) from retina and a priming signal Mm(Sm,n) from an object-concept-model m. Neuron n is activated if both the bottom-up signal from lower-level-input and the top-down priming signal are strong. Various models compete for evidence in the bottom-up signals, while adapting their parameters for better match as described below. This is a simplified description of perception. The most benign everyday visual perception uses many levels from retina to object perception. The NMF premise is that the same laws describe the basic interaction dynamics at each level. Perception of minute features, or everyday objects, or cognition of complex abstract concepts is due to the same mechanism described below. Perception and cognition involve concept-models and learning. In perception, concept-models correspond to objects in cognition models correspond to relationships and situations. Learning is an essential part of perception and cognition, and in NMF theory it is driven by the dynamics that increase a similarity measure between the sets of models and signals, L(X,M). The similarity measure is a function of model parameters and associations between the input bottom-up signals and top-down, concept-model signals. In constructing a mathematical description of the similarity measure, it is important to acknowledge two principles First, the visual field content is unknown before perception occurred Second, it may contain any of a number of objects. Important information could be contained in any bottom-up signal Therefore, the similarity measure is constructed so that it accounts for all bottom-up signals, X(n), L (  X ( n )  ,  M m ( S m , n )  )   ) ) . displaystyle L(vec X(n),vec M_m(vec S_m,n))prod _)). (1) This expression contains a product of partial similarities, l(X(n)), over all bottom-up signals therefore it forces the NMF system to account for every signal (even if one term in the product is zero, the product is zero, the similarity is low and the knowledge instinct is not satisfied) this is a reflection of the first principle. Second, before perception occurs, the mind does not know which object gave rise to a signal from a particular retinal neuron. Therefore, a partial similarity measure is constructed so that it treats each model as an alternative (a sum over concept-models) for each input neuron signal. Its constituent elements are conditional partial similarities between signal X(n) and model Mm, l(X(n)m). This measure is conditional on object m being present, therefore, when combining these quantities into the overall similarity measure, L, they are multiplied by r(m), which represent a probabilistic measure of object m actually being present. Combining these elements with the two principles noted above, a similarity measure is constructed as follows L (  X ( n )  ,  M m ( S m , n )  )   ) . displaystyle L(vec X(n),vec M_m(vec S_m,n))prod _). (2) The structure of the expression above follows standard principles of the probability theory a summation is taken over alternatives, m, and various pieces of evidence, n, are multiplied. This expression is not necessarily a probability, but it has a probabilistic structure. If learning is successful, it approximates probabilistic description and leads to near-optimal Bayesian decisions. The name conditional partial similarity for l(X(n)m) (or simply l(nm)) follows the probabilistic terminology. If learning is successful, l(nm) becomes a conditional probability density function, a probabilistic measure that signal in neuron n originated from object m. Then L is a total likelihood of observing signals X(n) coming from objects described by concept-model Mm. Coefficients r(m), called priors in probability theory, contain preliminary biases or expectations, expected objects m have relatively high r(m) values their true values are usually unknown and should be learned, like other parameters Sm. Note that in probability theory, a product of probabilities usually assumes that evidence is independent. Expression for L contains a product over n, but it does not assume independence among various signals X(n). There is a dependence among signals due to concept-models each model Mm(Sm,n) predicts expected signal values in many neurons n. During the learning process, concept-models are constantly modified. Usually, the functional forms of models, Mm(Sm,n), are all fixed and learning-adaptation involves only model parameters, Sm. From time to time a system forms a new concept, while retaining an old one as well alternatively, old concepts are sometimes merged or eliminated. This requires a modification of the similarity measure L The reason is that more models always result in a better fit between the models and data. This is a well known problem, it is addressed by reducing similarity L using a skeptic penalty function, (Penalty method) p(N,M) that grows with the number of models M, and this growth is steeper for a smaller amount of data N. For example, an asymptotically unbiased maximum likelihood estimation leads to multiplicative p(N,M)  exp(-Npar/2), where Npar is a total number of adaptive parameters in all models (this penalty function is known as Akaike information criterion, see (Perlovsky 2001) for further discussion and references). Learning in NMF using dynamic logic algorithm The learning process consists of estimating model parameters S and associating signals with concepts by maximizing the similarity L. Note that all possible combinations of signals and models are accounted for in expression (2) for L. This can be seen by expanding a sum and multiplying all the terms resulting in MN items, a huge number. This is the number of combinations between all signals (N) and all models (M). This is the source of Combinatorial Complexity, which is solved in NMF by utilizing the idea of dynamic logic. An important aspect of dynamic logic is matching vagueness or fuzziness of similarity measures to the uncertainty of models. Initially, parameter values are not known, and uncertainty of models is high so is the fuzziness of the similarity measures. In the process of learning, models become more accurate, and the similarity measure more crisp, the value of the similarity increases. The maximization of similarity L is done as follows. First, the unknown parameters Sm are randomly initialized. Then the association variables f(mn) are computed, f ( m  n )  r ( m ) l ( X ( n  m ) ) ). Equation for f(mn) looks like the Bayes formula for a posteriori probabilities if l(nm) in the result of learning become conditional likelihoods, f(mn) become Bayesian probabilities for signal n originating from object m. The dynamic logic of the NMF is defined as follows d S m d ). d f ( m  n ) d . Equations (3), (4), and (5) define a convergent dynamic NMF system with stationary states defined by maxSmL. It follows that the stationary states of an MF system are the maximum similarity states. When partial similarities are specified as probability density functions (pdf), or likelihoods, the stationary values of parameters Sm are asymptotically unbiased and efficient estimates of these parameters. The computational complexity of dynamic logic is linear in N. Practically, when solving the equations through successive iterations, f(mn) can be recomputed at every iteration using (3), as opposed to incremental formula (5). The proof of the above theorem contains a proof that similarity L increases at each iteration. This has a psychological interpretation that the instinct for increasing knowledge is satisfied at each step, resulting in the positive emotions NMF-dynamic logic system emotionally enjoys learning. Example of dynamic logic operations Finding patterns below noise can be an exceedingly complex problem. If an exact pattern shape is not known and depends on unknown parameters, these parameters should be found by fitting the pattern model to the data. However, when the locations and orientations of patterns are not known, it is not clear which subset of the data points should be selected for fitting. A standard approach for solving this kind of problem is multiple hypothesis testing (Singer et al. 1974). Since all combinations of subsets and models are exhaustively searched, this method faces the problem of combinatorial complexity. In the current example, noisy smile and frown patterns are sought. They are shown in Fig.1a without noise, and in Fig.1b with the noise, as actually measured. The true number of patterns is 3, which is not known. Therefore, at least 4 patterns should be fit to the data, to decide that 3 patterns fit best. The image size in this example is 100x100  10,000 points. If one attempts to fit 4 models to all subsets of 10,000 data points, computation of complexity, MN  106000. An alternative computation by searching through the parameter space, yields lower complexity each pattern is characterized by a 3-parameter parabolic shape. Fitting 4x312 parameters to 100x100 grid by a brute-force testing would take about 1032 to 1040 operations, still a prohibitive computational complexity. To apply NMF and dynamic logic to this problem one needs to develop parametric adaptive models of expected patterns. The models and conditional partial similarities for this case are described in details in a uniform model for noise, Gaussian blobs for highly-fuzzy, poorly resolved patterns, and parabolic models for smiles and frowns. The number of computer operations in this example was about 1010. Thus, a problem that was not solvable due to combinatorial complexity becomes solvable using dynamic logic. During an adaptation process, initially fuzzy and uncertain models are associated with structures in the input signals, and fuzzy models become more definite and crisp with successive iterations. The type, shape, and number, of models are selected so that the internal representation within the system is similar to input signals the NMF concept-models represent structure-objects in the signals. The figure below illustrates operations of dynamic logic. In Fig. 1(a) true smile and frown patterns are shown without noise (b) actual image available for recognition (signal is below noise, signal-to-noise ratio is between 2 dB and 0.7 dB) (c) an initial fuzzy model, a large fuzziness corresponds to uncertainty of knowledge (d) through (m) show improved models at various iteration stages (total of 22 iterations). Every five iterations the algorithm tried to increase or decrease the number of models. Between iterations (d) and (e) the algorithm decided, that it needs three Gaussian models for the best fit. There are several types of models one uniform model describing noise (it is not shown) and a variable number of blob models and parabolic models their number, location, and curvature are estimated from the data. Until about stage (g) the algorithm used simple blob models, at (g) and beyond, the algorithm decided that it needs more complex parabolic models to describe the data. Iterations stopped at (h), when similarity stopped increasing. Neural modeling fields hierarchical organization Above, a single processing level in a hierarchical NMF system was described. At each level of hierarchy there are input signals from lower levels, models, similarity measures (L), emotions, which are defined as changes in similarity, and actions actions include adaptation, behavior satisfying the knowledge instinct maximization of similarity. An input to each level is a set of signals X(n), or in neural terminology, an input field of neuronal activations. The result of signal processing at a given level are activated models, or concepts m recognized in the input signals n these models along with the corresponding instinctual signals and emotions may activate behavioral models and generate behavior at this level. The activated models initiate other actions. They serve as input signals to the next processing level, where more general concept-models are recognized or created. Output signals from a given level, serving as input to the next level, are the model activation signals, am, defined as ..N f(mn). The hierarchical NMF system is illustrated in Fig. 2. Within the hierarchy of the mind, each concept-model finds its mental meaning and purpose at a higher level (in addition to other purposes). For example, consider a concept-model chair. It has a behavioral purpose of initiating sitting behavior (if sitting is required by the body), this is the bodily purpose at the same hierarchical level. In addition, it has a purely mental purpose at a higher level in the hierarchy, a purpose of helping to recognize a more general concept, say of a concert hall, a model of which contains rows of chairs. From time to time a system forms a new concept or eliminates an old one. At every level, the NMF system always keeps a reserve of vague (fuzzy) inactive concept-models. They are inactive in that their parameters are not adapted to the data therefore their similarities to signals are low. Yet, because of a large vagueness (covariance) the similarities are not exactly zero. When a new signal does not fit well into any of the active models, its similarities to inactive models automatically increase (because first, every piece of data is accounted for, and second, inactive models are vague-fuzzy and potentially can grab every signal that does not fit into more specific, less fuzzy, active models. When the activation signal am for an inactive model, m, exceeds a certain threshold, the model is activated. Similarly, when an activation signal for a particular model falls below a threshold, the model is deactivated. Thresholds for activation and deactivation are set usually based on information existing at a higher hierarchical level (prior information, system resources, numbers of activated models of various types, etc.). Activation signals for active models at a particular level  am  form a neuronal field, which serve as input signals to the next level, where more abstract and more general concepts are formed. References Related Leonid Perlovsky Title Neural network quantum states URL https//en.wikipedia.org/wiki/Neural_network_quantum_states Content Neural Network Quantum States (NQS or NNQS) is a general class of variational quantum states parameterized in terms of an artificial neural network. It was first introduced in 2017 by the physicists Giuseppe Carleo and Matthias Troyer to approximate wave functions of many-body quantum systems. Given a many-body quantum state  displaystyle Psi rangle  comprising N displaystyle N degrees of freedom and a choice of associated quantum numbers s 1 s N displaystyle s_1ldots s_N , then an NQS parameterizes the wave-function amplitudes s 1 s N   . This variational form is used in conjunction with specific stochastic learning approaches to approximate quantum states of interest. Learning the Ground-State Wave Function One common application of NQS is to find an approximate representation of the ground state wave function of a given Hamiltonian H  displaystyle hat H . The learning procedure in this case consists in finding the best neural-network weights that minimize the variational energy E ( W )   W  H    W . displaystyle E(W)langle Psi What HPsi Wrangle . Since, for a general artificial neural network, computing the expectation value is an exponentially costly operation in N displaystyle N , stochastic techniques based, for example, on the Monte Carlo method are used to estimate E ( W ) displaystyle E(W) , analogously to what is done in Variational Monte Carlo, see for example for a review. More specifically, a set of M displaystyle M samples S ( 1 ) , S ( 2 ) S ( M ) displaystyle S(1),S(2)ldots S(M) , with S ( i )  s 1 ( i ) s N ( i ) displaystyle S(i)s_1(i)ldots s_N(i) , is generated such that they are uniformly distributed according to the Born probability density P ( S )  F ( s 1 s N  W )  2 displaystyle P(S)propto F(s_1ldots s_NW)2 . Then it can be shown that the sample mean of the so-called local energy E l o c ( S )  S  H   / S  displaystyle E_mathrm loc (S)langle Shat HPsi rangle /langle SPsi rangle  is a statistical estimate of the quantum expectation value E ( W ) displaystyle E(W) , i.e. E ( W ) 1 M i M E l o c ( S ( i ) ) . displaystyle E(W)simeq frac 1Msum _iME_mathrm loc (S(i)). Similarly, it can be shown that the gradient of the energy with respect to the network weights W displaystyle W is also approximated by a sample mean E ( W ) W k 1 M i M ( E l o c ( S ( i ) ) E ( W ) ) O k ( S ( i ) ) , displaystyle frac partial E(W)partial W_ksimeq frac 1Msum _iM(E_mathrm loc (S(i))-E(W))O_kstar (S(i)), where O ( S ( i ) )  log F ( S ( i )  W ) W k displaystyle O(S(i))frac partial log F(S(i)W)partial W_k and can be efficiently computed, in deep networks through backpropagation. The stochastic approximation of the gradients is then used to minimize the energy E ( W ) displaystyle E(W) typically using a stochastic gradient descent approach. When the neural-network parameters are updated at each step of the learning procedure, a new set of samples S ( i ) displaystyle S(i) is generated, in an iterative procedure similar to what done in unsupervised learning. Connection with Tensor Networks Neural-Network representations of quantum wave functions share some similarities with variational quantum states based on tensor networks. For example, connections with matrix product states have been established. These studies have shown that NQS support volume law scaling for the entropy of entanglement. In general, given a NQS with fully-connected weights, it corresponds, in the worse case, to a matrix product state of exponentially large bond dimension in N displaystyle N . See also Differentiable programming Title Normalization (machine learning) URL https//en.wikipedia.org/wiki/Normalization_(machine_learning) Content In machine learning, normalization is a statistical technique with various applications. There are two main forms of normalization, namely data normalization and activation normalization. Data normalization (or feature scaling) includes methods that rescale input data so that the features have the same range, mean, variance, or other statistical properties. For instance, a popular choice of feature scaling method is min-max normalization, where each feature is transformed to have the same range (typically  0 , 1  displaystyle 0,1 or  1 , 1  displaystyle -1,1 ). This solves the problem of different features having vastly different scales, for example if one feature is measured in kilometers and another in nanometers. Activation normalization, on the other hand, is specific to deep learning, and includes methods that rescale the activation of hidden neurons inside neural networks. Normalization is often used to increase the speed of training convergence, reduce sensitivity to variations and feature scales in input data, reduce overfitting, and produce better model generalization to unseen data. Normalization techniques are often theoretically justified as reducing covariance shift, smoothing optimization landscapes, and increasing regularization, though they are mainly justified by empirical success. Batch normalization Batch normalization (BatchNorm) operates on the activations of a layer for each mini-batch. Consider a simple feedforward network, defined by chaining together modules x ( 0 ) x ( 1 ) x ( 2 ) displaystyle x(0)mapsto x(1)mapsto x(2)mapsto cdots  where each network module can be a linear transform, a nonlinear activation function, a convolution, etc. x ( 0 ) displaystyle x(0) is the input vector, x ( 1 ) displaystyle x(1) is the output vector from the first module, etc. BatchNorm is a module that can be inserted at any point in the feedforward network. For example, suppose it is inserted just after x ( l ) displaystyle x(l) , then the network would operate accordingly x ( l ) B N ( x ( l ) ) x ( l  1 ) displaystyle cdots mapsto x(l)mapsto mathrm BN (x(l))mapsto x(l1)mapsto cdots  The BatchNorm module does not operate over individual inputs. Instead, it must operate over one batch of inputs at a time. Concretely, suppose we have a batch of inputs x ( 1 ) ( 0 ) , x ( 2 ) ( 0 ) , , x ( B ) ( 0 ) displaystyle x_(1)(0),x_(2)(0),dots ,x_(B)(0) , fed all at once into the network. We would obtain in the middle of the network some vectors x ( 1 ) ( l ) , x ( 2 ) ( l ) , , x ( B ) ( l ) displaystyle x_(1)(l),x_(2)(l),dots ,x_(B)(l) The BatchNorm module computes the coordinate-wise mean and variance of these vectors i ( l )  1 B . In other words, we are considering the i displaystyle i -th coordinate of each vector in the batch, and computing the mean and variance of these numbers. It then normalizes each coordinate to have zero mean and unit variance x  ( b ) , i ( l )  x ( b ) , i ( l ) i ( l ) ( i ( l ) ) 2  displaystyle hat x_(b),i(l)frac x_(b),i(l)-mu _i(l)sqrt (sigma _i(l))2epsilon  The displaystyle epsilon  is a small positive constant such as 10 9 displaystyle 10-9 added to the variance for numerical stability, to avoid division by zero. Finally, it applies a linear transformation y ( b ) , i ( l )  i x  ( b ) , i ( l )  i displaystyle y_(b),i(l)gamma _ihat x_(b),i(l)beta _i Here, displaystyle gamma  and displaystyle beta  are parameters inside the BatchNorm module. They are learnable parameters, typically trained by gradient descent. The following is a Python implementation of BatchNorm Interpretation displaystyle gamma  and displaystyle beta  allow the network to learn to undo the normalization, if this is beneficial. BatchNorm can be interpreted as removing the purely linear transformations, so that its layers focus solely on modelling the nonlinear aspects of data, which may be beneficial, as a neural network can always be augmented with a linear transformation layer on top. It is claimed in the original publication that BatchNorm works by reducing internal covariance shift, though the claim has both supporters and detractors. Special cases The original paper recommended to only use BatchNorms after a linear transform, not after a nonlinear activation. That is, ( B N ( W x  b ) ) displaystyle phi (mathrm BN (Wxb)) , not B N ( ( W x  b ) ) displaystyle mathrm BN (phi (Wxb)) . Also, the bias b displaystyle b does not matter, since it would be canceled by the subsequent mean subtraction, so it is of the form B N ( W x ) displaystyle mathrm BN (Wx) . That is, if a BatchNorm is preceded by a linear transform, then that linear transforms bias term is set to zero. For convolutional neural networks (CNNs), BatchNorm must preserve the translation-invariance of these models, meaning that it must treat all outputs of the same kernel as if they are different data points within a batch. This is sometimes called Spatial BatchNorm, or BatchNorm2D, or per-channel BatchNorm. Concretely, suppose we have a 2-dimensional convolutional layer defined by x h , w , c ( l )  h , w , c K h h , w w , c , c ( l ) x h , w , c ( l 1 )  b c ( l ) displaystyle x_h,w,c(l)sum _h,w,cK_h-h,w-w,c,c(l)x_h,w,c(l-1)b_c(l) where x h , w , c ( l ) displaystyle x_h,w,c(l) is the activation of the neuron at position ( h , w ) displaystyle (h,w) in the c displaystyle c -th channel of the l displaystyle l -th layer. K h , w , c , c ( l ) displaystyle K_Delta h,Delta w,c,c(l) is a kernel tensor. Each channel c displaystyle c corresponds to a kernel K h h , w w , c , c ( l ) displaystyle K_h-h,w-w,c,c(l) , with indices h , w , c displaystyle Delta h,Delta w,c . b c ( l ) displaystyle b_c(l) is the bias term for the c displaystyle c -th channel of the l displaystyle l -th layer. In order to preserve the translational invariance, BatchNorm treats all outputs from the same kernel in the same batch as more data in a batch. That is, it is applied once per kernel c displaystyle c (equivalently, once per channel c displaystyle c ), not per activation x h , w , c ( l  1 ) displaystyle x_h,w,c(l1)  c ( l )  1 B H W . That is, even though there are only B displaystyle B data points in a batch, all B H W displaystyle BHW outputs from the kernel in this batch are treated equally. Subsequently, normalization and the linear transform is also done per kernel x  ( b ) , h , w , c ( l )  x ( b ) , h , w , c ( l ) c ( l ) ( c ( l ) ) 2  y ( b ) , h , w , c ( l )  c x  ( b ) , h , w , c ( l )  c displaystyle beginalignedhat x_(b),h,w,c(l)frac x_(b),h,w,c(l)-mu _c(l)sqrt (sigma _c(l))2epsilon y_(b),h,w,c(l)gamma _chat x_(b),h,w,c(l)beta _cendaligned Similar considerations apply for BatchNorm for n-dimensional convolutions. The following is a Python implementation of BatchNorm for 2D convolutions Improvements BatchNorm has been very popular and there were many attempted improvements. Some examples include ghost batching randomly partition a batch into sub-batches and perform BatchNorm separately on each weight decay on displaystyle gamma  and displaystyle beta   and combining BatchNorm with GroupNorm. A particular problem with BatchNorm is that during training, the mean and variance are calculated on the fly for each batch (usually as an exponential moving average), but during inference, the mean and variance were frozen from those calculated during training. This train-test disparity degrades performance. The disparity can be decreased by simulating the moving average during inference Eq. 3  E  x   ( 1 ) x , train 2  ( E  x  2  ( 1 ) x 2 , train ) 2 displaystyle beginalignedmu alpha Ex(1-alpha )mu _x,text trainsigma 2(alpha Ex2(1-alpha )mu _x2,text train)-mu 2endaligned where displaystyle alpha  is a hyperparameter to be optimized on a validation set. Other works attempt to eliminate BatchNorm, such as the Normalizer-Free ResNet. Layer normalization Layer normalization (LayerNorm) is a popular alternative to BatchNorm. Unlike BatchNorm, which normalizes activations across the batch dimension for a given feature, LayerNorm normalizes across all the features within a single data sample. Compared to BatchNorm, LayerNorms performance is not affected by batch size. It is a key component of transformer models. For a given data input and layer, LayerNorm computes the mean displaystyle mu  and variance 2 displaystyle sigma 2 over all the neurons in the layer. Similar to BatchNorm, learnable parameters displaystyle gamma  (scale) and displaystyle beta  (shift) are applied. It is defined by x i   x i 2  , y . Examples For example, in CNN, a LayerNorm applies to all activations in a layer. In the previous notation, we have ( l )  1 H W C . In recurrent neural networks and transformers, LayerNorm is applied individually to each timestep. For example, if the hidden vector in an RNN at timestep t displaystyle t is x ( t ) R D displaystyle x(t)in mathbb R D , where D displaystyle D is the dimension of the hidden vector, then LayerNorm will be applied with x i  ( t )  x i ( t ) ( t ) ( ( t ) ) 2  , y i ( t )  i x i  ( t )  i displaystyle hat x_i(t)frac x_i(t)-mu (t)sqrt (sigma (t))2epsilon ,quad y_i(t)gamma _ihat x_i(t)beta _i where ( t )  1 D  . Adaptive Adaptive layer norm (adaLN) computes the , displaystyle gamma ,beta  in a LayerNorm not from the layer activation itself, but from other data. It was first proposed for CNNs, and has been used effectively in diffusion transformers (DiTs). For example, in a DiT, the conditioning information (such as a text encoding vector) is processed by a multilayer perceptron into , displaystyle gamma ,beta  , which is then applied in the LayerNorm module of a transformer. Weight normalization Weight normalization (WeightNorm) is a technique inspired by BatchNorm that normalizes weight matrices in a neural network, rather than its activations. One example is spectral normalization, which divides weight matrices by their spectral norm. The spectral normalization is used in generative adversarial networks (GANs) such as the Wasserstein GAN. The spectral radius can be efficiently computed by the following algorithm INPUT matrix W displaystyle W and initial guess x displaystyle x Iterate x 1 W x 2 W x displaystyle xmapsto frac 1Wx_2Wx to convergence x displaystyle x . This is the eigenvector of W displaystyle W with eigenvalue W s displaystyle W_s . RETURN x , W x 2 displaystyle x,Wx_2 By reassigning W i W i W i s displaystyle W_ileftarrow frac W_iW_i_s after each update of the discriminator, we can upper-bound W i s 1 displaystyle W_i_sleq 1 , and thus upper-bound D L displaystyle D_L . The algorithm can be further accelerated by memoization at step t displaystyle t , store x i ( t ) displaystyle x_i(t) . Then, at step t  1 displaystyle t1 , use x i ( t ) displaystyle x_i(t) as the initial guess for the algorithm. Since W i ( t  1 ) displaystyle W_i(t1) is very close to W i ( t ) displaystyle W_i(t) , so is x i ( t ) displaystyle x_i(t) to x i ( t  1 ) displaystyle x_i(t1) , thus allowing rapid convergence. CNN-specific normalization There are some activation normalization techniques that are only used for CNNs. Response normalization Local response normalization was used in AlexNet. It was applied in a convolutional layer, just after a nonlinear activation function. It was defined by b x , y  . I.e., each pixel in a channel is suppressed by the activations of the same pixel in its adjacent channels. k , n , , displaystyle k,n,alpha ,beta  are hyperparameters picked by using a validation set. It was a variant of the earlier local contrast normalization. b x , y  . The hyperparameters k , n , , displaystyle k,n,alpha ,beta  , and the size of the small window, are picked by using a validation set. Similar methods were called divisive normalization, as they divide activations by a number depending on the activations. They were originally inspired by biology, where it was used to explain nonlinear responses of cortical neurons and nonlinear masking in visual perception. Both kinds of local normalization were obviated by batch normalization, which is a more global form of normalization. Response normalization reappeared in ConvNeXT-2 as global response normalization. Group normalization Group normalization (GroupNorm) is a technique also solely used for CNNs. It can be understood as the LayerNorm for CNN applied once per channel group. Suppose at a layer l displaystyle l , there are channels 1 , 2 , , C displaystyle 1,2,dots ,C , then it is partitioned into groups g 1 , g 2 , , g G displaystyle g_1,g_2,dots ,g_G . Then, LayerNorm is applied to each group. Instance normalization Instance normalization (InstanceNorm), or contrast normalization, is a technique first developed for neural style transfer, and is also only used for CNNs. It can be understood as the LayerNorm for CNN applied once per channel, or equivalently, as group normalization where each group consists of a single channel c ( l )  1 H W . In the AdaIN method of style transfer, we take a CNN and two input images, one for content and one for style. Each image is processed through the same CNN, and at a certain layer l displaystyle l , AdaIn is applied. Let x ( l ) , content displaystyle x(l),text content be the activation in the content image, and x ( l ) , style displaystyle x(l),text style be the activation in the style image. Then, AdaIn first computes the mean and variance of the activations of the content image x ( l ) displaystyle x(l) , then uses those as the , displaystyle gamma ,beta  for InstanceNorm on x ( l ) , content displaystyle x(l),text content . Note that x ( l ) , style displaystyle x(l),text style itself remains unchanged. Explicitly, we have y h , w , c ( l ) , . The original 2017 transformer used the post-LN configuration for its LayerNorms. It was difficult to train, and required careful hyperparameter tuning and a warm-up in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018, was found to be easier to train, requiring no warm-up, leading to faster convergence. FixNorm and ScaleNorm both normalize activation vectors in a transformer. The FixNorm method divides the output vectors from a transformer by their norms, then multiplies by a learned parameter g displaystyle g . The ScaleNorm replaces all LayerNorms inside a transformer by division with norm, then multiplying by a learned parameter g displaystyle g (shared by all ScaleNorm modules of a transformer). Query-Key normalization (QKNorm) normalizes query and key vectors to have unit norm. In nGPT, many vectors are normalized to have unit norm hidden state vectors, input and output embedding vectors, weight matrix columns, and query and key vectors. Miscellaneous Gradient normalization (GradNorm) normalizes gradient vectors during backpropagation. See also Data preprocessing Feature scaling References Further reading Normalization Layers. labml.ai Deep Learning Paper Implementations. Retrieved 2024-08-07. Title Novelty detection URL https//en.wikipedia.org/wiki/Novelty_detection Content Novelty detection is the mechanism by which an intelligent organism is able to identify an incoming sensory pattern as being hitherto unknown. If the pattern is sufficiently salient or associated with a high positive or strong negative utility, it will be given computational resources for effective future processing. The principle is long known in neurophysiology, with roots in the orienting response research by E. N. Sokolov in the 1950s. The reverse phenomenon is habituation, i.e., the phenomenon that known patterns yield a less marked response. Early neural modeling attempts were by Yehuda Salu. An increasing body of knowledge has been collected concerning the corresponding mechanisms in the brain. In technology, the principle became important for radar detection methods during the Cold War, where unusual aircraft-reflection patterns could indicate an attack by a new type of aircraft. Today, the phenomenon plays an important role in machine learning and data science, where the corresponding methods are known as anomaly detection or outlier detection. An extensive methodological overview is given by Markou and Singh. See also Change detection Outlier Reward system Title Offline learning URL https//en.wikipedia.org/wiki/Offline_learning Content Offline learning is a machine learning training approach in which a model is trained on a fixed dataset that is not updated during the learning process. This dataset is collected beforehand, and the learning typically occurs in a batch mode (i.e., the model is updated using batches of data, rather than a single input-output pair at a time). Once the model is trained, it can make predictions on new, unseen data. In online learning, only the set of possible elements is known, whereas in offline learning, the learner also knows the order in which they are presented. See also Online machine learning Incremental learning Title Overfitting URL https//en.wikipedia.org/wiki/Overfitting Content In mathematical modeling, overfitting is the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably. An overfitted model is a mathematical model that contains more parameters than can be justified by the data. In a mathematical sense, these parameters represent the degree of a polynomial. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure. 45 Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Underfitting would occur, for example, when fitting a linear model to nonlinear data. Such a model will tend to have poor predictive performance. The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data overfitting occurs when a model begins to memorize training data rather than learning to generalize from a trend. As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. Overfitting is directly related to approximation error of the selected function class and the optimization error of the optimization procedure. A function class that is too large, in a suitable sense, relative to the dataset size is likely to overfit. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new dataset than on the dataset used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data. To lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is to either (1) explicitly penalize overly complex models or (2) test the models ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter. Statistical inference In statistics, an inference is drawn from a statistical model, which has been selected via some procedure. Burnham  Anderson, in their much-cited text on model selection, argue that to avoid overfitting, we should adhere to the Principle of Parsimony. The authors also state the following. 32 33 Overfitted models ... are often free of bias in the parameter estimators, but have estimated (and actual) sampling variances that are needlessly large (the precision of the estimators is poor, relative to what could have been accomplished with a more parsimonious model). False treatment effects tend to be identified, and false variables are included with overfitted models. ... A best approximating model is achieved by properly balancing the errors of underfitting and overfitting. Overfitting is more likely to be a serious concern when there is little theory available to guide the analysis, in part because then there tend to be a large number of models to select from. The book Model Selection and Model Averaging (2008) puts it this way. Given a data set, you can fit thousands of models at the push of a button, but how do you choose the best? With so many candidate models, overfitting is a real danger. Is the monkey who typed Hamlet actually a good writer? Regression In regression analysis, overfitting occurs frequently. As an extreme example, if there are p variables in a linear regression with p data points, the fitted line can go exactly through every point. For logistic regression or Cox proportional hazards models, there are a variety of rules of thumb (e.g. 5 9, 10 and 10 15 the guideline of 10 observations per independent variable is known as the one in ten rule). In the process of regression model selection, the mean squared error of the random regression function can be split into random noise, approximation bias, and variance in the estimate of the regression function. The bias variance tradeoff is often used to overcome overfit models. With a large set of explanatory variables that actually have no relation to the dependent variable being predicted, some variables will in general be falsely found to be statistically significant and the researcher may thus retain them in the model, thereby overfitting the model. This is known as Freedmans paradox. Machine learning Usually, a learning algorithm is trained using some set of training data exemplary situations for which the desired output is known. The goal is that the algorithm will also perform well on predicting the output when fed validation data that was not encountered during its training. Overfitting is the use of models or procedures that violate Occams razor, for example by including more adjustable parameters than are ultimately optimal, or by using a more complicated approach than is ultimately optimal. For an example where there are too many adjustable parameters, consider a dataset where training data for y can be adequately predicted by a linear function of two independent variables. Such a function requires only three parameters (the intercept and two slopes). Replacing this simple function with a new, more complex quadratic function, or with a new, more complex linear function on more than two independent variables, carries a risk Occams razor implies that any given complex function is a priori less probable than any given simple function. If the new, more complicated function is selected instead of the simple function, and if there was not a large enough gain in training data fit to offset the complexity increase, then the new complex function overfits the data and the complex overfitted function will likely perform worse than the simpler function on validation data outside the training dataset, even though the complex function performed as well, or perhaps even better, on the training dataset. When comparing different types of models, complexity cannot be measured solely by counting how many parameters exist in each model the expressivity of each parameter must be considered as well. For example, it is nontrivial to directly compare the complexity of a neural net (which can track curvilinear relationships) with m parameters to a regression model with n parameters. Overfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data that have no causal relation to the target function. In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse. As a simple example, consider a database of retail purchases that includes the item bought, the purchaser, and the date and time of purchase. Its easy to construct a model that will fit the training set perfectly by using the date and time of purchase to predict the other attributes, but this model will not generalize at all to new data because those past times will never occur again. Generally, a learning algorithm is said to overfit relative to a simpler one if it is more accurate in fitting known data (hindsight) but less accurate in predicting new data (foresight). One can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups information that is relevant for the future, and irrelevant information (noise). Everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise exists in past information that needs to be ignored. The problem is determining which part to ignore. A learning algorithm that can reduce the risk of fitting noise is called robust. Consequences The most obvious consequence of overfitting is poor performance on the validation dataset. Other negative consequences include A function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data entry. A more complex, overfitted function is likely to be less portable than a simple one. At one extreme, a one-variable linear regression is so portable that, if necessary, it could even be done by hand. At the other extreme are models that can be reproduced only by exactly duplicating the original modelers entire setup, making reuse or scientific reproduction difficult. It may be possible to reconstruct details of individual training instances from an overfitted machine learning models training set. This may be undesirable if, for example, the training data includes sensitive personally identifiable information (PII). This phenomenon also presents problems in the area of artificial intelligence and copyright, with the developers of some generative deep learning models such as Stable Diffusion and GitHub Copilot being sued for copyright infringement because these models have been found to be capable of reproducing certain copyrighted items from their training data. Remedy The optimal function usually needs verification on bigger or completely new datasets. There are, however, methods like minimum spanning tree or life-time of correlation that applies the dependence between correlation coefficients and time-series (window width). Whenever the window width is big enough, the correlation coefficients are stable and dont depend on the window width size anymore. Therefore, a correlation matrix can be created by calculating a coefficient of correlation between investigated variables. This matrix can be represented topologically as a complex network where direct and indirect influences between variables are visualized. Dropout regularisation (random removal of training set data) can also improve robustness and therefore reduce over-fitting by probabilistically removing inputs to a layer. Underfitting Underfitting is the inverse of overfitting, meaning that the statistical model or machine learning algorithm is too simplistic to accurately capture the patterns in the data. A sign of underfitting is that there is a high bias and low variance detected in the current model or algorithm used (the inverse of overfitting low bias and high variance). This can be gathered from the Bias-variance tradeoff, which is the method of analyzing a model or algorithm for bias error, variance error, and irreducible error. With a high bias and low variance, the result of the model is that it will inaccurately represent the data points and thus insufficiently be able to predict future data results (see Generalization error). As shown in Figure 5, the linear line could not represent all the given data points due to the line not resembling the curvature of the points. We would expect to see a parabola-shaped line as shown in Figure 6 and Figure 1. If we were to use Figure 5 for analysis, we would get false predictive results contrary to the results if we analyzed Figure 6. Burnham  Anderson state the following. 32 ... an underfitted model would ignore some important replicable (i.e., conceptually replicable in most other samples) structure in the data and thus fail to identify effects that were actually supported by the data. In this case, bias in the parameter estimators is often substantial, and the sampling variance is underestimated, both factors resulting in poor confidence interval coverage. Underfitted models tend to miss important treatment effects in experimental settings. Resolving underfitting There are multiple ways to deal with underfitting Increase the complexity of the model If the model is too simple, it may be necessary to increase its complexity by adding more features, increasing the number of parameters, or using a more flexible model. However, this should be done carefully to avoid overfitting. Use a different algorithm If the current algorithm is not able to capture the patterns in the data, it may be necessary to try a different one. For example, a neural network may be more effective than a linear regression model for some types of data. Increase the amount of training data If the model is underfitting due to a lack of data, increasing the amount of training data may help. This will allow the model to better capture the underlying patterns in the data. Regularization Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. It can also be used to prevent underfitting by controlling the complexity of the model. Ensemble Methods Ensemble methods combine multiple models to create a more accurate prediction. This can help reduce underfitting by allowing multiple models to work together to capture the underlying patterns in the data. Feature engineering Feature engineering involves creating new model features from the existing ones that may be more relevant to the problem at hand. This can help improve the accuracy of the model and prevent underfitting. Benign overfitting Benign overfitting describes the phenomenon of a statistical model that seems to generalize well to unseen data, even when it has been fit perfectly on noisy training data (i.e., obtains perfect predictive accuracy on the training set). The phenomenon is of particular interest in deep neural networks, but is studied from a theoretical perspective in the context of much simpler models, such as linear regression. In particular, it has been shown that overparameterization is essential for benign overfitting in this setting. In other words, the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. See also Notes References Leinweber, D. J. (2007). Stupid data miner tricks. The Journal of Investing. 16 15 22. doi10.3905/joi.2007.681820. S2CID 108627390. Tetko, I. V. Livingstone, D. J. Luik, A. I. (1995). Neural network studies. 1. Comparison of Overfitting and Overtraining (PDF). Journal of Chemical Information and Modeling. 35 (5) 826 833. doi10.1021/ci00027a006. Tip 7 Minimize overfitting. Chicco, D. (December 2017). Ten quick tips for machine learning in computational biology. BioData Mining. 10 (35) 35. doi10.1186/-017-0155-3. PMC 5721660. PMID 29234465. Further reading Christian, Brian Griffiths, Tom (April 2017), Chapter 7 Overfitting, Algorithms To Live By The computer science of human decisions, William Collins, pp. 149 168, ISBN 978-0-00-754799-9 External links The Problem of Overfitting Data Stony Brook University What is overfitting, exactly? Andrew Gelman blog  Linear Regression Bias / Variance Tradeoff University of Washington What is Underfitting IBM Title Paraphrasing (computational linguistics) URL https//en.wikipedia.org/wiki/Paraphrasing_(computational_linguistics) Content Paraphrase or paraphrasing in computational linguistics is the natural language processing task of detecting and generating paraphrases. Applications of paraphrasing are varied including information retrieval, question answering, text summarization, and plagiarism detection. Paraphrasing is also useful in the evaluation of machine translation, as well as semantic parsing and generation of new samples to expand existing corpora. Paraphrase generation Multiple sequence alignment Barzilay and Lee proposed a method to generate paraphrases through the usage of monolingual parallel corpora, namely news articles covering the same event on the same day. Training consists of using multi-sequence alignment to generate sentence-level paraphrases from an unannotated corpus. This is done by finding recurring patterns in each individual corpus, i.e. X (injured/wounded) Y people, Z seriously where X, Y, Z are variables finding pairings between such patterns the represent paraphrases, i.e. X (injured/wounded) Y people, Z seriously and Y were (wounded/hurt) by X, among them Z were in serious condition This is achieved by first clustering similar sentences together using n-gram overlap. Recurring patterns are found within clusters by using multi-sequence alignment. Then the position of argument words is determined by finding areas of high variability within each cluster, aka between words shared by more than 50 of a clusters sentences. Pairings between patterns are then found by comparing similar variable words between different corpora. Finally, new paraphrases can be generated by choosing a matching cluster for a source sentence, then substituting the source sentences argument into any number of patterns in the cluster. Phrase-based machine translation Paraphrase can also be generated through the use of phrase-based translation as proposed by Bannard and Callison-Burch. The chief concept consists of aligning phrases in a pivot language to produce potential paraphrases in the original language. For example, the phrase under control in an English sentence is aligned with the phrase unter kontrolle in its German counterpart. The phrase unter kontrolle is then found in another German sentence with the aligned English phrase being in check, a paraphrase of under control. The probability distribution can be modeled as Pr ( e 2  e 1 ) displaystyle Pr(e_2e_1) , the probability phrase e 2 displaystyle e_2 is a paraphrase of e 1 displaystyle e_1 , which is equivalent to Pr ( e 2  f ) Pr ( f  e 1 ) displaystyle Pr(e_2f)Pr(fe_1) summed over all f displaystyle f , a potential phrase translation in the pivot language. Additionally, the sentence e 1 displaystyle e_1 is added as a prior to add context to the paraphrase. Thus the optimal paraphrase, e 2  displaystyle hat e_2 can be modeled as e 2   arg max e 2 e 1 Pr ( e 2  e 1 , S )  arg max e 2 e 1 f Pr ( e 2  f , S ) Pr ( f  e 1 , S ) displaystyle hat e_2textargmax _e_2neq e_1Pr(e_2e_1,S)textargmax _e_2neq e_1sum _fPr(e_2f,S)Pr(fe_1,S) Pr ( e 2  f ) displaystyle Pr(e_2f) and Pr ( f  e 1 ) displaystyle Pr(fe_1) can be approximated by simply taking their frequencies. Adding S displaystyle S as a prior is modeled by calculating the probability of forming the S displaystyle S when e 1 displaystyle e_1 is substituted with e 2 displaystyle e_2 . Long short-term memory There has been success in using long short-term memory (LSTM) models to generate paraphrases. In short, the model consists of an encoder and decoder component, both implemented using variations of a stacked residual LSTM. First, the encoding LSTM takes a one-hot encoding of all the words in a sentence as input and produces a final hidden vector, which can represent the input sentence. The decoding LSTM takes the hidden vector as input and generates a new sentence, terminating in an end-of-sentence token. The encoder and decoder are trained to take a phrase and reproduce the one-hot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent. New paraphrases are generated by inputting a new phrase to the encoder and passing the output to the decoder. Transformers With the introduction of Transformer models, paraphrase generation approaches improved their ability to generate text by scaling neural network parameters and heavily parallelizing training through feed-forward layers. These models are so fluent in generating text that human experts cannot identify if an example was human-authored or machine-generated. Transformer-based paraphrase generation relies on autoencoding, autoregressive, or sequence-to-sequence methods. Autoencoder models predict word replacement candidates with a one-hot distribution over the vocabulary, while autoregressive and seq2seq models generate new text based on the source predicting one word at a time. More advanced efforts also exist to make paraphrasing controllable according to predefined quality dimensions, such as semantic preservation or lexical diversity. Many Transformer-based paraphrase generation methods rely on unsupervised learning to leverage large amounts of training data and scale their methods. Paraphrase recognition Recursive autoencoders Paraphrase recognition has been attempted by Socher et al through the use of recursive autoencoders. The main concept is to produce a vector representation of a sentence and its components by recursively using an autoencoder. The vector representations of paraphrases should have similar vector representations they are processed, then fed as input into a neural network for classification. Given a sentence W displaystyle W with m displaystyle m words, the autoencoder is designed to take 2 n displaystyle n -dimensional word embeddings as input and produce an n displaystyle n -dimensional vector as output. The same autoencoder is applied to every pair of words in S displaystyle S to produce m / 2 displaystyle lfloor m/2rfloor  vectors. The autoencoder is then applied recursively with the new vectors as inputs until a single vector is produced. Given an odd number of inputs, the first vector is forwarded as-is to the next level of recursion. The autoencoder is trained to reproduce every vector in the full recursion tree, including the initial word embeddings. Given two sentences W 1 displaystyle W_1 and W 2 displaystyle W_2 of length 4 and 3 respectively, the autoencoders would produce 7 and 5 vector representations including the initial word embeddings. The euclidean distance is then taken between every combination of vectors in W 1 displaystyle W_1 and W 2 displaystyle W_2 to produce a similarity matrix S R 7 5 displaystyle Sin mathbb R 7times 5 . S displaystyle S is then subject to a dynamic min-pooling layer to produce a fixed size n p n p displaystyle n_ptimes n_p matrix. Since S displaystyle S are not uniform in size among all potential sentences, S displaystyle S is split into n p displaystyle n_p roughly even sections. The output is then normalized to have mean 0 and standard deviation 1 and is fed into a fully connected layer with a softmax output. The dynamic pooling to softmax model is trained using pairs of known paraphrases. Skip-thought vectors Skip-thought vectors are an attempt to create a vector representation of the semantic meaning of a sentence, similarly to the skip gram model. Skip-thought vectors are produced through the use of a skip-thought model which consists of three key components, an encoder and two decoders. Given a corpus of documents, the skip-thought model is trained to take a sentence as input and encode it into a skip-thought vector. The skip-thought vector is used as input for both decoders one attempts to reproduce the previous sentence and the other the following sentence in its entirety. The encoder and decoder can be implemented through the use of a recursive neural network (RNN) or an LSTM. Since paraphrases carry the same semantic meaning between one another, they should have similar skip-thought vectors. Thus a simple logistic regression can be trained to good performance with the absolute difference and component-wise product of two skip-thought vectors as input. Transformers Similar to how Transformer models influenced paraphrase generation, their application in identifying paraphrases showed great success. Models such as BERT can be adapted with a binary classification layer and trained end-to-end on identification tasks. Transformers achieve strong results when transferring between domains and paraphrasing techniques compared to more traditional machine learning methods such as logistic regression. Other successful methods based on the Transformer architecture include using adversarial learning and meta-learning. Evaluation Multiple methods can be used to evaluate paraphrases. Since paraphrase recognition can be posed as a classification problem, most standard evaluations metrics such as accuracy, score, or an ROC curve do relatively well. However, there is difficulty calculating -scores due to trouble producing a complete list of paraphrases for a given phrase and the fact that good paraphrases are dependent upon context. A metric designed to counter these problems is ParaMetric. ParaMetric aims to calculate the precision and recall of an automatic paraphrase system by comparing the automatic alignment of paraphrases to a manual alignment of similar phrases. Since ParaMetric is simply rating the quality of phrase alignment, it can be used to rate paraphrase generation systems, assuming it uses phrase alignment as part of its generation process. A notable drawback to ParaMetric is the large and exhaustive set of manual alignments that must be initially created before a rating can be produced. The evaluation of paraphrase generation has similar difficulties as the evaluation of machine translation. The quality of a paraphrase depends on its context, whether it is being used as a summary, and how it is generated, among other factors. Additionally, a good paraphrase usually is lexically dissimilar from its source phrase. The simplest method used to evaluate paraphrase generation would be through the use of human judges. Unfortunately, evaluation through human judges tends to be time-consuming. Automated approaches to evaluation prove to be challenging as it is essentially a problem as difficult as paraphrase recognition. While originally used to evaluate machine translations, bilingual evaluation understudy (BLEU) has been used successfully to evaluate paraphrase generation models as well. However, paraphrases often have several lexically different but equally valid solutions, hurting BLEU and other similar evaluation metrics. Metrics specifically designed to evaluate paraphrase generation include paraphrase in n-gram change (PINC) and paraphrase evaluation metric (PEM) along with the aforementioned ParaMetric. PINC is designed to be used with BLEU and help cover its inadequacies. Since BLEU has difficulty measuring lexical dissimilarity, PINC is a measurement of the lack of n-gram overlap between a source sentence and a candidate paraphrase. It is essentially the Jaccard distance between the sentence, excluding n-grams that appear in the source sentence to maintain some semantic equivalence. PEM, on the other hand, attempts to evaluate the adequacy, fluency, and lexical dissimilarity of paraphrases by returning a single value heuristic calculated using N-grams overlap in a pivot language. However, a large drawback to PEM is that it must be trained using large, in-domain parallel corpora and human judges. It is equivalent to training a paraphrase recognition to evaluate a paraphrase generation system. The Quora Question Pairs Dataset, which contains hundreds of thousands of duplicate questions, has become a common dataset for the evaluation of paraphrase detectors. Consistently reliable paraphrase detection have all used the Transformer architecture and all have relied on large amounts of pre-training with more general data before fine-tuning with the question pairs. See also Round-trip translation Text simplification automated processPages displaying wikidata descriptions as a fallback Text normalization Process of transforming text into a single canonical form References External links Microsoft Research Paraphrase Corpus - a dataset consisting of 5800 pairs of sentences extracted from news articles annotated to note whether a pair captures semantic equivalence Paraphrase Database (PPDB) - A searchable database containing millions of paraphrases in 16 different languages Title Parity learning URL https//en.wikipedia.org/wiki/Parity_learning Content Parity learning is a problem in machine learning. An algorithm that solves this problem must find a function , given some samples (x, (x)) and the assurance that computes the parity of bits at some fixed locations. The samples are generated using some distribution over the input. The problem is easy to solve using Gaussian elimination provided that a sufficient number of samples (from a distribution which is not too skewed) are provided to the algorithm. Noisy version (Learning Parity with Noise) In Learning Parity with Noise (LPN), the samples may contain some error. Instead of samples (x, (x)), the algorithm is provided with (x, y), where for random boolean b  0 , 1  displaystyle bin 0,1 . See also Learning with errors References Avrim Blum, Adam Kalai, and Hal Wasserman, Noise-tolerant learning, the parity problem, and the statistical query model, J. ACM 50, no. 4 (2003) 506 519. Adam Tauman Kalai, Yishay Mansour, and Elad Verbin, On agnostic boosting and parity learning, in Proceedings of the 40th annual ACM symposium on Theory of computing (Victoria, British Columbia, Canada ACM, 2008), 629 638, http//portal.acm.org/citation.cfm?. Oded Regev, On lattices, learning with errors, random linear codes, and cryptography, in Proceedings of the thirty-seventh annual ACM symposium on Theory of computing (Baltimore, MD, USA ACM, 2005), 84 93, http//portal.acm.org/citation.cfm?.1060603. Title Pattern language (formal languages) URL https//en.wikipedia.org/wiki/Pattern_language_(formal_languages) Content In theoretical computer science, a pattern language is a formal language that can be defined as the set of all particular instances of a string of constants and variables. Pattern Languages were introduced by Dana Angluin in the context of machine learning. Definition Given a finite set of constant symbols and a countable set X of variable symbols disjoint from , a pattern is a finite non-empty string of symbols from X. The length of a pattern p, denoted by p, is just the number of its symbols. The set of all patterns containing exactly n distinct variables (each of which may occur several times) is denoted by Pn, the set of all patterns at all by P. A substitution is a mapping f P P such that f is a homomorphism with respect to string concatenation ( ), formally p,q P. f(p q)  f(p) f(q) f is non-erasing, formally p P. f(p) , where denotes the empty string and f respects constants, formally s . f(s)  s. If . For a pattern p, its language is defined as the set of all less general patterns that are built from constants only, formally L(p)   s   s p , where  denotes the set of all finite non-empty strings of symbols from . For example, using the , ... , the pattern 0x10xx1 and xxy has length 7 and 3, respectively. An instance of the former pattern is 00z100z0z1 and 01z101z1z1, it is obtained by the substitution that maps x to 0z and to 1z, respectively, and each other symbol to itself. Both 00z100z0z1 and 01z101z1z1 are also instances of xxy. In fact, L(0x10xx1) is a subset of L(xxy). The language of the pattern and is the set of all bit strings which denote an even and odd binary number, respectively. The language of xx is the set of all strings obtainable by concatenating a bit string with itself, e.g. 00, 11, 0101, 1010, 11101110 L(xx). Properties The problem of deciding whether s L(p) for an arbitrary string s  and pattern p is NP-complete (see picture), and so is hence the problem of deciding p q for arbitrary patterns p, q. The class of pattern languages is not closed under ... union e.g. . The class of pattern languages is closed under ... concatenation L(p) L(q)  L(p q) reversal L(p)). If p, q are patterns containing exactly one variable, then p q if and only if L(p) L(q) the same equivalence holds for patterns of equal length. For patterns of different length, the above example . However, any two patterns p and q, of arbitrary lengths, generate the same language if and only if they are equal up to consistent variable renaming. Each pattern p is a common generalization of all strings in its generated language L(p), modulo associativity of ( ). Location in the Chomsky hierarchy In a refined Chomsky hierarchy, the class of pattern languages is a proper superclass and subclass of the singleton and the indexed languages, respectively, but incomparable to the language classes in between due to the latter, the pattern language class is not explicitly shown in the table below. The class of pattern languages is incomparable with the class of finite languages, with the class of regular languages, and with the class of context-free languages the pattern language L(xx) is not context-free (hence neither regular nor finite) due to the pumping lemma the finite (hence also regular and context-free) language  01, 10  is not a pattern language. Each singleton language is trivially a pattern language, generated by a pattern without variables. Each pattern language can be produced by an indexed grammar For example,  ... a ab b c y c xcy ax bx a ycy ax bx b a ab b c c xcy ax bx a ycy ax bx b ... a ab b c c ab x a ycy ax bx b a ab b c c ab a ycy ax bx b ... a ab b c c ab a c y b a ab b c c ab a c b In a similar way, an index grammar can be constructed from any pattern. Learning patterns Given a sample set S of strings, a pattern p is called descriptive of S if S L(p), but not S L(q) L(p) for any other pattern q. Given any sample set S, a descriptive pattern for S can be computed by enumerating all patterns (up to variable renaming) not longer than the shortest string in S, selecting from them the patterns that generate a superset of S, selecting from them the patterns of maximal length, and selecting from them a pattern that is minimal with respect to . Based on this algorithm, the class of pattern languages can be identified in the limit from positive examples. Notes Title Pattern recognition URL https//en.wikipedia.org/wiki/Pattern_recognition Content Pattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent patterns. PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. Pattern recognition systems are commonly trained from labeled training data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and signal processing into consideration. It originated in engineering, and the term is popular in the context of computer vision a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition. In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is spam). Pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence) and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence. Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform most likely matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors. Overview A modern definition of pattern recognition is The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories. Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of simple, in accordance with Occams Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances. A combination of the two that has been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). In cases of unsupervised learning, there may be no training data at all. Sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. The unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. In some fields, the terminology is different. In community ecology, the term classification is used to refer to what is commonly known as clustering. The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors. Features typically are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of male or female, or a blood type of A, B, AB or O), ordinal (consisting of one of a set of ordered items, e.g., large, medium or small), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together, and this is also the case for integer-valued and real-valued data. Many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10). Probabilistic classifiers Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a best label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms They output a confidence value associated with their choice. (Note that some other algorithms may also output confidence values, but in general, only for probabilistic algorithms is this value mathematically grounded in probability theory. Non-probabilistic confidence values can in general not be given any specific meaning, and only used to compare against other confidence values output by the same algorithm.) Correspondingly, they can abstain when the confidence of choosing any particular output is too low. Because of the probabilities output, probabilistic pattern-recognition algorithms can be more effectively incorporated into larger machine-learning tasks, in a way that partially or completely avoids the problem of error propagation. Number of important feature variables Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given. The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of n displaystyle n features the powerset consisting of all 2 n 1 displaystyle 2n-1 subsets of features need to be explored. The Branch-and-Bound algorithm does reduce this complexity but is intractable for medium to large values of the number of available features n displaystyle n Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. Feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features. Problem statement The problem of pattern recognition can be stated as follows Given an unknown function g  X Y displaystyle gmathcal Xrightarrow mathcal Y (the ground truth) that maps input instances x X displaystyle boldsymbol xin mathcal X to output labels y Y displaystyle yin mathcal Y , along with training data  . (For example, if the problem is filtering spam, then x i displaystyle boldsymbol x_i is some representation of an email and y displaystyle y is either spam or non-spam). In order for this to be a well-defined problem, approximates as closely as possible needs to be defined rigorously. In decision theory, this is defined by specifying a loss function or cost function that assigns a specific value to loss resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of X displaystyle mathcal X . In practice, neither the distribution of X displaystyle mathcal X nor the ground truth function g  X Y displaystyle gmathcal Xrightarrow mathcal Y are known exactly, but can be computed only empirically by collecting a large number of samples of X displaystyle mathcal X and hand-labeling them using the correct value of Y displaystyle mathcal Y (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification, the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function h  X Y displaystyle hmathcal Xrightarrow mathcal Y labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness) on a typical test set. For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form p ( l a b e l  x , )  f ( x  ) displaystyle p(rm labelboldsymbol x,boldsymbol theta )fleft(boldsymbol xboldsymbol theta right) where the feature vector input is x displaystyle boldsymbol x , and the function f is typically parameterized by some parameters displaystyle boldsymbol theta  . In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability p ( x  l a b e l ) displaystyle p(boldsymbol xrm label) is instead estimated and combined with the prior probability p ( l a b e l  ) displaystyle p(rm labelboldsymbol theta ) using Bayes rule, as follows p ( l a b e l  x , )  p ( x  l a b e l , ) p ( l a b e l  ) L all labels p ( x  L ) p ( L  ) . displaystyle p(rm labelboldsymbol x,boldsymbol theta )frac p(boldsymbol xrm label,boldsymbol theta )p(rm labelboldsymbol theta )sum _Lin textall labelsp(boldsymbol xL)p(Lboldsymbol theta ). When the labels are continuously distributed (e.g., in regression analysis), the denominator involves integration rather than summation p ( l a b e l  x , )  p ( x  l a b e l , ) p ( l a b e l  ) L all labels p ( x  L ) p ( L  ) d L . displaystyle p(rm labelboldsymbol x,boldsymbol theta )frac p(boldsymbol xrm label,boldsymbol theta )p(rm labelboldsymbol theta )int _Lin textall labelsp(boldsymbol xL)p(Lboldsymbol theta )operatorname d L. The value of displaystyle boldsymbol theta  is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability p ( ) displaystyle p(boldsymbol theta ) on different values of displaystyle boldsymbol theta  . Mathematically  arg max p (  D ) displaystyle boldsymbol theta arg max _boldsymbol theta p(boldsymbol theta mathbf D ) where displaystyle boldsymbol theta  is the value used for displaystyle boldsymbol theta  in the subsequent evaluation procedure, and p (  D ) displaystyle p(boldsymbol theta mathbf D ) , the posterior probability of displaystyle boldsymbol theta  , is given by p (  D )    ( ) . displaystyle p(boldsymbol theta mathbf D )leftprod _ ). In the Bayesian approach to this problem, instead of choosing a single parameter vector displaystyle boldsymbol theta  , the probability of a given label for a new instance x displaystyle boldsymbol x is computed by integrating over all possible values of displaystyle boldsymbol theta  , weighted according to the posterior probability p ( l a b e l  x )  p ( l a b e l  x , ) p (  D ) d . displaystyle p(rm labelboldsymbol x)int p(rm labelboldsymbol x,boldsymbol theta )p(boldsymbol theta mathbf D )operatorname d boldsymbol theta . Frequentist or Bayesian approach to pattern recognition The first pattern classifier the linear discriminant presented by Fisher was developed in the frequentist tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the covariance matrix. Also the probability of each class p ( l a b e l  ) displaystyle p(rm labelboldsymbol theta ) is estimated from the collected dataset. Note that the usage of Bayes rule in a pattern classifier does not make the classification approach Bayesian. Bayesian statistics has its origin in Greek philosophy where a distinction was already made between the a priori and the a posteriori knowledge. Later Kant defined his distinction between what is a priori known before observation and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities p ( l a b e l  ) displaystyle p(rm labelboldsymbol theta ) can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations using e.g., the Beta- (conjugate prior) and Dirichlet-distributions. The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations. Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach. Uses Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctors interpretations and findings. Other typical applications of pattern recognition techniques are automatic speech recognition, speaker identification, classification of text into several categories (e.g., spam or non-spam email messages), the automatic recognition of handwriting on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms. The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems. Optical character recognition is an example of the application of a pattern classifier. The method of signing ones name was captured with stylus and overlay starting in 1990. The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers. Pattern recognition has many real-world applications in image processing. Some examples include identification and authentication e.g., license plate recognition, fingerprint analysis, face detection/verification, and voice-based authentication. medical diagnosis e.g., screening for cervical cancer (Papnet), breast tumors or heart sounds defense various navigation and guidance systems, target recognition systems, shape recognition technology etc. mobility advanced driver assistance systems, autonomous vehicle technology, etc. In psychology, pattern recognition is used to make sense of and identify objects, and is closely related to perception. This explains how the sensory inputs humans receive are made meaningful. Pattern recognition can be thought of in two different ways. The first concerns template matching and the second concerns feature detection. A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long-term memory. If there is a match, the stimulus is identified. Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. One observation is a capital E having three horizontal lines and one vertical line. Algorithms Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative. Classification methods (methods predicting categorical labels) Parametric Linear discriminant analysis Quadratic discriminant analysis Maximum entropy classifier (aka logistic regression, multinomial logistic regression) Note that logistic regression is an algorithm for classification, despite its name. (The name comes from the fact that logistic regression uses an extension of a linear regression model to model the probability of an input being in a particular class.) Nonparametric Decision trees, decision lists Kernel estimation and K-nearest-neighbor algorithms Naive Bayes classifier Neural networks (multi-layer perceptrons) Perceptrons Support vector machines Gene expression programming Clustering methods (methods for classifying and predicting categorical labels) Categorical mixture models Hierarchical clustering (agglomerative or divisive) K-means clustering Correlation clustering Kernel principal component analysis (Kernel PCA) Ensemble learning algorithms (supervised meta-algorithms for combining multiple learning algorithms together) Boosting (meta-algorithm) Bootstrap aggregating (bagging) Ensemble averaging Mixture of experts, hierarchical mixture of experts General methods for predicting arbitrarily-structured (sets of) labels Bayesian networks Markov random fields Multilinear subspace learning algorithms (predicting labels of multidimensional data using tensor representations) Unsupervised Multilinear principal component analysis (MPCA) Real-valued sequence labeling methods (predicting sequences of real-valued labels) Kalman filters Particle filters Regression methods (predicting real-valued labels) Gaussian process regression (kriging) Linear regression and extensions Independent component analysis (ICA) Principal components analysis (PCA) Sequence labeling methods (predicting sequences of categorical labels) Conditional random fields (CRFs) Hidden Markov models (HMMs) Maximum entropy Markov models (MEMMs) Recurrent neural networks (RNNs) Dynamic time warping (DTW) See also References Further reading Fukunaga, Keinosuke (1990). Introduction to Statistical Pattern Recognition (2nd ed.). Boston Academic Press. ISBN 978-0-12-269851-4. Hornegger, Joachim Paulus, Dietrich W. R. (1999). Applied Pattern Recognition A Practical Introduction to Image and Speech Processing in C (2nd ed.). San Francisco Morgan Kaufmann Publishers. ISBN 978-3-528-15558-2. Schuermann, Juergen (1996). Pattern Classification A Unified View of Statistical and Neural Approaches. New York Wiley. ISBN 978-0-471-13534-0. Godfried T. Toussaint, ed. (1988). Computational Morphology. Amsterdam North-Holland Publishing Company. ISBN 9781483296722. Kulikowski, Casimir A. Weiss, Sholom M. (1991). Computer Systems That Learn Classification and Prediction Methods from Statistics, Neural Nets, Machine Learning, and Expert Systems. San Francisco Morgan Kaufmann Publishers. ISBN 978-1-55860-065-2. Duda, Richard O. Hart, Peter E. Stork, David G. (2000). Pattern Classification (2nd ed.). Wiley-Interscience. ISBN 978-0471056690. Jain, Anil.K. Duin, Robert.P.W. Mao, Jianchang (2000). Statistical pattern recognition a review. IEEE Transactions on Pattern Analysis and Machine Intelligence. 22 (1) 4 37. CiteSeerX 10.1.1.123.8151. doi10.1109/34.824819. S2CID 192934. An introductory tutorial to classifiers (introducing the basic terms, with numeric example) Kovalevsky, V. A. (1980). Image Pattern Recognition. New York, NY Springer New York. ISBN 978-1-4612-6033-2. OCLC 852790446. External links The International Association for Pattern Recognition List of Pattern Recognition web sites Journal of Pattern Recognition Research Archived 2008-09-08 at the Wayback Machine Pattern Recognition Info Pattern Recognition (Journal of the Pattern Recognition Society) International Journal of Pattern Recognition and Artificial Intelligence Archived 2004-12-11 at the Wayback Machine International Journal of Applied Pattern Recognition Open Pattern Recognition Project, intended to be an open source platform for sharing algorithms of pattern recognition Improved Fast Pattern Matching Improved Fast Pattern Matching Title Perceiver URL https//en.wikipedia.org/wiki/Perceiver Content Perceiver is a variant of the Transformer architecture, adapted for processing arbitrary forms of data, such as images, sounds and video, and spatial data. Unlike previous notable Transformer systems such as BERT and GPT-3, which were designed for text processing, the Perceiver is designed as a general architecture that can learn from large amounts of heterogeneous data. It accomplishes this with an asymmetric attention mechanism to distill inputs into a latent bottleneck. Perceiver matches or outperforms specialized models on classification tasks. Perceiver was introduced in June 2021 by DeepMind. It was followed by Perceiver IO in August 2021. Design Perceiver is designed without modality-specific elements. For example, it does not have elements specialized to handle images, or text, or audio. Further it can handle multiple correlated input streams of heterogeneous types. It uses a small set of latent units that forms an attention bottleneck through which the inputs must pass. One benefit is to eliminate the quadratic scaling problem found in early transformers. Earlier work used custom feature extractors for each modality. It associates position and modality-specific features with every input element (e.g. every pixel, or audio sample). These features can be learned or constructed using high-fidelity Fourier features. Perceiver uses cross-attention to produce linear complexity layers and to detach network depth from input size. This decoupling allows deeper architectures. Components A cross-attention module maps a (larger) byte array (e.g., a pixel array) and a latent array (smaller) to another latent array, reducing dimensionality. A transformer tower maps one latent array to another latent array, which is used to query the input again. The two components alternate. Both components use query-key-value (QKV) attention. QKV attention applies query, key, and value networks, which are typically multilayer perceptrons to each element of an input array, producing three arrays that preserve the index dimensionality (or sequence length) of their inputs. Perceiver IO Perceiver IO can flexibly query the models latent space to produce outputs of arbitrary size and semantics. It achieves results on tasks with structured output spaces, such as natural language and visual understanding, StarCraft II, and multi-tasking. Perceiver IO matches a Transformer-based BERT baseline on the GLUE language benchmark without the need for input tokenization and achieves state-of-the-art performance on Sintel optical flow estimation. Outputs are produced by attending to the latent array using a specific output query associated with that particular output. For example to predict optical flow on one pixel a query would attend using the pixel s xy coordinates plus an optical flow task embedding to produce a single flow vector. It is a variation on the encoder/decoder architecture used in other designs. Performance Perceivers performance is comparable to ResNet-50 and ViT on ImageNet without 2D convolutions. It attends to 50,000 pixels. It is competitive in all modalities in AudioSet. See also Convolutional neural network Transformer (machine learning model) References External links DeepMind Perceiver and Perceiver IO  Paper Explained on YouTube Perceiver General Perception with Iterative Attention (Google DeepMind Research Paper Explained) on YouTube, with the Fourier features explained in more detail Title PHerc. Paris. 4 URL https//en.wikipedia.org/wiki/PHerc._Paris._4 Content PHerc. Paris. 4 is a carbonized scroll of papyrus, dating to the 1st century BC to the 1st century AD. Part of a corpus known as the Herculaneum papyri, it was buried by hot-ash in the Roman city of Herculaneum during the eruption of Mount Vesuvius in 79 AD. It was subsequently discovered in excavations of the Villa of the Papyri from 1752 1754. Held by the Institut de France in its rolled state, it is now known to be a cornerstone example of non-invasive reading, where in February 2024, an announcement was made that the scrolls contents can be unveiled with the use of non-invasive imaging and machine learning artificial intelligence, paving the way towards the decipherment and scanning of other Herculaneum papyri and otherwise heavily damaged texts. Background and provenance The Villa of the Papyri was buried during the eruption of Vesuvius in 79 AD, subjecting the scrolls to temperatures of 310 320 C, compacting them and converting them to charcoal. The first scrolls were uncovered in 1752, with subsequent excavations uncovering more scrolls. There were attempts to unroll the scrolls, as the contents were realized to contain writings by classical philosophers from schools such as Epicureanism. PHerc. Paris. 4 was amongst a set of six scrolls that entered its present day location at the Institut de France. They were a diplomatic gift, made to commemorate peace between the Kingdom of Naples and Sicily, under the reign of Ferdinand IV and Napoleon, with the negotiations mediated by Charles Alquier. In 1803, a tribute of vases and the scrolls arrived in France under the supervision of Francesco Carelli and was personally exhibited to Napoleon and Jos phine whereupon they entered the collection of the Institut. Of the scrolls that entered the collection, PHerc. Paris. 3 and Paris. 4 remain intact. Paris. 1 is in fragments and bits, Paris. 2 is better preserved, Paris. 5 exploded upon unpeeling, and Paris. 6 crumbled. Unscrolling and reading The 20th century yielded progress in the readings of Herculaneum texts utilizing microscopes, digital photography and multispectral filters approaching the usage infrared spectroscopy to gain better clarity of the texts. In 2015, PHerc. Paris. 1 and PHerc. Paris. 4 were studied side by side, with Paris. 1 having a history of successful limited readings in 1986 1987, with sequences of letters such as  I TOIE and words such as EI OI (Greek would say) proving decipherable. Utilizing a pre-filtered X-ray beam with a double Laue monochromator to convert to a mono-chromatic X-ray beam, the first letters of the unrolled scroll were identified. After the virtual unrolling of the En-Gedi Scroll in 2015, Brent Seales, a computer scientist at the University of Kentucky, spearheaded the effort to uncover the Herculaneum corpus through non-invasive means. On 15 March 2023, Nat Friedman, former CEO of GitHub, and Daniel Gross of Cue, upon hearing a lecture by Seales, launched the Vesuvius Challenge to utilize machine learning and new imaging techniques of the papyri using the Diamond Light Source particle accelerator to create an improved scan of the PHerc. Paris. 4, which was completed in 2019 and subsequently released to the public. The scans were completed at a resolution of 4-8 m per voxel. The Vesuvius Challenge raised US1 million, with an objective of clear readings of the scroll and the future aim of reading other carbonized, sealed fragments of the Herculaneum corpus, with a distant idea towards excavating more portions of the Villa of the Papyri in order to recover more scrolls. In October 2023, 21 year old college student and SpaceX intern Luke Farritor and physicist Casey Handmer identified the word porphyras ( ) or purple on the scroll utilizing neural networking to differentiate the paper and the ink Farritor subsequently won US40,000 for his find. On 5 February 2024, the Grand Prize, for reading PHerc. Paris. 4, was awarded to Farritor, ETH Zurich robotics student Julian Schilliger, and Free University of Berlin Egyptian Ph.D student Youssef Nader for recovering 11 columns of text, or 2000 characters total, which is about 5 of the contents of the scroll. The uncovered text is believed to be written by Epicurean philosopher Philodemus, and is an unrecorded text about pleasure and how it is affected by the abundance or scarcity of items, to which Philodemus disagreed writing As too in the case of food, we do not right away believe things that are scarce to be absolutely more pleasant than those which are abundant. The text revealed from the scroll was published in a paper for Zeitschrift f r Papyrologie und Epigraphik. Reactions Universit degli Studi di Napoli Federico II papyrology professor Federica Nicolardi praised the discovery declaring Its extremely exciting to be reading entire words, not just sequences of letters, from within a scroll, and expressing confidence in further future decipherment of the scroll. Seales described of the decades of work in non-invasive decipherment that with humility, we acknowledge the non-linear and often unpredictable outcomes of research, which is rarely expected, and not ever guaranteed, to lead directly to success. In an interview with Time, Nat Friedman described the contents of the scroll as a 2000 year old blog post, arguing with another poster, and its ancient Substack, and people are beefing with each other, and I think thats just amazing. The goal of the Vesuvius Challenge for 2024 is towards deciphering 90 of the scrolls contents in addition to other fragments held by the Institut de France. Title Phi coefficient URL https//en.wikipedia.org/wiki/Phi_coefficient Content In statistics, the phi coefficient (or mean square contingency coefficient and denoted by or r ) is a measure of association for two binary variables. In machine learning, it is known as the Matthews correlation coefficient (MCC) and used as a measure of the quality of binary (two-class) classifications, introduced by biochemist Brian W. Matthews in 1975. Introduced by Karl Pearson, and also known as the Yule phi coefficient from its introduction by Udny Yule in 1912 this measure is similar to the Pearson correlation coefficient in its interpretation. In meteorology, the phi coefficient, or its square (the latter aligning with M. H. Doolittles original proposition from 1885), is referred to as the Doolittle Skill Score or the Doolittle Measure of Association. Definition A Pearson correlation coefficient estimated for two binary variables will return the phi coefficient. Two binary variables are considered positively associated if most of the data falls along the diagonal cells. In contrast, two binary variables are considered negatively associated if most of the data falls off the diagonal. If we have a 2 2 table for two random variables x and y where , , , , are non-negative counts of numbers of observations that sum to n, the total number of observations. The phi coefficient that describes the association of x and y  . displaystyle . Phi is related to the point-biserial correlation coefficient and Cohens d and estimates the extent of the relationship between two variables (2 2). The phi coefficient can also be expressed using only n displaystyle n , n 11 displaystyle n_11 , n 1 displaystyle n_1bullet  , and n 1 displaystyle n_bullet 1 ,  ) . displaystyle ). Maximum values Although computationally the Pearson correlation coefficient reduces to the phi coefficient in the 2 2 case, they are not in general the same. The Pearson correlation coefficient ranges from 1 to 1, where 1 indicates perfect agreement or disagreement, and 0 indicates no relationship. The phi coefficient has a maximum value that is determined by the distribution of the two variables if one or both variables can take on more than two values. See Davenport and El-Sanhury (1991) for a thorough discussion. Machine learning The MCC is defined identically to phi coefficient, introduced by Karl Pearson, also known as the Yule phi coefficient from its introduction by Udny Yule in 1912. Despite these antecedents which predate Matthewss use by several decades, the term MCC is widely used in the field of bioinformatics and machine learning. The coefficient takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications it returns a value between 1 and 1. A coefficient of 1 represents a perfect prediction, 0 no better than random prediction and 1 indicates total disagreement between prediction and observation. However, if MCC equals neither 1, 0, or 1, it is not a reliable indicator of how similar a predictor is to random guessing because MCC is dependent on the dataset. MCC is closely related to the chi-square statistic for a 2 2 contingency table  MCC   2 n displaystyle textMCCsqrt frac chi 2n where n is the total number of observations. While there is no perfect way of describing the confusion matrix of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures. Other measures, such as the proportion of correct predictions (also termed accuracy), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification. The MCC can be calculated directly from the confusion matrix using the formula . If exactly one of the four sums in the denominator is zero, the denominator can be arbitrarily set to one this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value. In case two or more sums are zero (e.g. both labels and model predictions are all positive or negative), the limit does not exist. The MCC can be calculated with the formula . The original formula as given by Matthews was . As a correlation coefficient, the Matthews correlation coefficient is the geometric mean of the regression coefficients of the problem and its dual. The component regression coefficients of the Matthews correlation coefficient are Markedness ( p) and Youdens J statistic (Informedness or p). Markedness and Informedness correspond to different directions of information flow and generalize Youdens J statistic, the displaystyle delta  p statistics, while their geometric mean generalizes the Matthews Correlation Coefficient to more than two classes. Some scientists claim the Matthews correlation coefficient to be the most informative single score to establish the quality of a binary classifier prediction in a confusion matrix context. Example Given a sample of 12 pictures, 8 of cats and 4 of dogs, where cats belong to class 1 and dogs belong to class 0, ). . All correct predictions are located in the diagonal of the table (highlighted in bold), so it is easy to visually inspect the table for prediction errors, as they will be represented by values outside the diagonal. In abstract terms, the confusion matrix is as follows where . Plugging the numbers from the formula .478 displaystyle textMCCfrac 6times 3-1times 2sqrt (61)times (62)times (31)times (32)frac 16sqrt 1120approx 0.478 Confusion matrix Let us define an experiment from P positive instances and N negative instances for some condition. The four outcomes can be formulated in a 2 2 contingency table or confusion matrix, as follows Multiclass case The Matthews correlation coefficient has been generalized to the multiclass case. The generalization called the R K displaystyle R_K statistic (for K different classes) was defined in terms of a K K displaystyle Ktimes K confusion matrix C displaystyle C . . Instead the minimum value will be between 1 and 0 depending on the true distribution. The maximum value is always 1. This formula can be more easily understood by defining intermediate variables t . This allows the formula to be expressed as .478 displaystyle textMCCfrac (63)times color green12-color blue5times color brown4-color purple7times color maroon8sqrt color green122-color blue52-color purple72sqrt color green122-color brown42-color maroon82frac 32sqrt 4480approx 0.478 An alternative generalization of the Matthews Correlation Coefficient to more than two classes was given by Powers by the definition of Correlation as the geometric mean of Informedness and Markedness. Several generalizations of the Matthews Correlation Coefficient to more than two classes along with new Multivariate Correlation Metrics for multinary classification have been presented by P Stoica and P Babu. Advantages over accuracy and score As explained by Davide Chicco in his paper Ten quick tips for machine learning in computational biology (BioData Mining, 2017) and The advantages of the Matthews correlation coefficient (MCC) over score and accuracy in binary classification evaluation (BMC Genomics, 2020), the Matthews correlation coefficient is more informative than score and accuracy in evaluating binary classification problems, because it takes into account the balance ratios of the four confusion matrix categories (true positives, true negatives, false positives, false negatives). The former article explains, for Tip 8 In order to have an overall understanding of your prediction, you decide to take advantage of common statistical scores, such as accuracy, and score. . Suppose, for example, you have a very imbalanced validation set made of 100 elements, 95 of which are positive elements, and only 5 are negative elements (as explained in Tip 5). And suppose also you made some mistakes in designing and training your machine learning classifier, and now you have an algorithm which always predicts positive. Imagine that you are not aware of this issue. By applying your only-positive predictor to your imbalanced validation set, therefore, you obtain values for the confusion matrix categories . These values lead to the following performance scores .44. By reading these over-optimistic scores, then you will be very happy and will think that your machine learning algorithm is doing an excellent job. Obviously, you would be on the wrong track. On the contrary, to avoid these dangerous misleading illusions, there is another performance score that you can exploit the Matthews correlation coefficient (MCC). ). By considering the proportion of each class of the confusion matrix in its formula, its score is high only if your classifier is doing well on both the negative and the positive elements. In the example above, the MCC score would be undefined (since TN and FN would be 0, therefore the denominator of Equation 3 would be 0). By checking this value, instead of accuracy and score, you would then be able to notice that your classifier is going in the wrong direction, and you would become aware that there are issues you ought to solve before proceeding. Consider this other example. You ran a classification on the same dataset which led to the following values for the confusion matrix categories . In this example, the classifier has performed well in classifying positive instances, but was not able to correctly recognize negative data elements. Again, the resulting score and accuracy scores would be extremely high .24. Similarly to the previous case, if a researcher analyzed only these two score indicators, without considering the MCC, they would wrongly think the algorithm is performing quite well in its task, and would have the illusion of being successful. On the other hand, checking the Matthews correlation coefficient would be pivotal once again. In this example, the value of the MCC would be 0.14 (Equation 3), indicating that the algorithm is performing similarly to random guessing. Acting as an alarm, the MCC would be able to inform the data mining practitioner that the statistical model is performing poorly. For these reasons, we strongly encourage to evaluate each test performance through the Matthews correlation coefficient (MCC), instead of the accuracy and the score, for any binary classification problem. Chiccos passage might be read as endorsing the MCC score in cases with imbalanced data sets. This, however, is contested in particular, Zhu (2020) offers a strong rebuttal. Note that the score depends on which class is defined as the positive class. In the first example above, the score is high because the majority class is defined as the positive class. Inverting the positive and negative classes results in the following confusion matrix . The MCC doesnt depend on which class is the positive one, which has the advantage over the score to avoid incorrectly defining the positive class. See also Cohens kappa Contingency table Cram rs V, a similar measure of association between nominal variables. score Fowlkes Mallows index Polychoric correlation (subtype Tetrachoric correlation), when variables are seen as dichotomized versions of (latent) continuous variables Title Predictive learning URL https//en.wikipedia.org/wiki/Predictive_learning Content Predictive learning is a machine learning (ML) technique where an artificial intelligence model is fed new data to develop an understanding of its environment, capabilities, and limitations. This technique finds application in many areas, including neuroscience, business, robotics, and computer vision. This concept was developed and expanded by French computer scientist Yann LeCun in 1988 during his career at Bell Labs, where he trained models to detect handwriting so that financial companies could automate check processing. The mathematical foundation for predictive learning dates back to the 17th century, where British insurance company Lloyds used predictive analytics to make a profit. Starting out as a mathematical concept, this method expanded the possibilities of artificial intelligence. Predictive learning is an attempt to learn with a minimum of pre-existing mental structure. It was inspired by Jean Piagets account of children constructing knowledge of the world through interaction. Gary Dreschers book Made-up Minds was crucial to the development of this concept. The idea that predictions and unconscious inference are used by the brain to construct a model of the world, in which it can identify causes of percepts, goes back even further to Hermann von Helmholtzs iteration of this study. These ideas were further developed by the field of predictive coding. Another related predictive learning theory is Jeff Hawkins memory-prediction framework, which is laid out in his book On Intelligence. Mathematical procedures Training process Similar to ML, predictive learning aims to extrapolate the value of an unknown dependent variable Y displaystyle Y , given independent input data ) . A set of attributes can be classified into categorical data (discrete factors such as race, sex, or affiliation) or numerical data (continuous values such as temperature, annual income, or speed). Every set of input values is fed into a neural network to predict a value y displaystyle y . In order to predict the output accurately, the weights of the neural network (which represent how much each predictor variable affects the outcome) must be incrementally adjusted via backpropagation to produce estimates closer to the actual data. Once an ML model is given enough adjustments through training to predict values closer to the ground truth, it should be able to correctly predict outputs of new data with little error. Maximizing accuracy In order to ensure maximum accuracy for a predictive learning model, the predicted values y   F ( x ) displaystyle hat yF(x) must not exceed a certain error threshold when compared to actual values y displaystyle y by the risk formula R ( F )  E x y L ( y , F ( x ) ) displaystyle R(F)E_xyL(y,F(x)) , where L displaystyle L is the loss function, y displaystyle y is the ground truth, and F ( x ) displaystyle F(x) is the predicted data. This error function is used to make incremental adjustments to the models weights to eventually reach a well-trained prediction of F ( x )  argmin F ( x ) E x y displaystyle F(x)underset F(x)operatorname argmin ,E_xy L ( y , F ( x ) ) displaystyle L(y,F(x)) Once the error is negligible or considered small enough after training, the model is said to have converged. Ensemble learning In some cases, using a singular machine learning approach is not enough to create an accurate estimate for certain data. Ensemble learning is the combination of several ML algorithms to create a stronger model. Each model is represented by the function F ( x )  a 0  . An ensemble learning model is represented as a linear combination of the predictions from each constituent approach, a  . Applications Cognitive development Sensorimotor signals are neural impulses sent to the brain upon physical touch. Using predictive learning to detect sensorimotor signals plays a key role in early cognitive development, as the human brain represents sensorimotor signals in a predictive manner (it attempts to minimize prediction error between incoming sensory signals and top down prediction). In order to update an unadjusted predictor, it must be trained through sensorimotor experiences because it does not inherently have prediction ability. In a recent research paper, Dr. Yukie Nagai suggested a new architecture in predictive learning to predict sensorimotor signals based on a two-module approach a sensorimotor system which interacts with the environment and a predictor which simulates the sensorimotor system in the brain. Spatiotemporal memory Computers use predictive learning in spatiotemporal memory to completely create an image given constituent frames. This implementation uses predictive recurrent neural networks, which are neural networks designed to work with sequential data, such as a time series. Using predictive learning in conjunction with computer vision enables computers to create images of their own, which can be helpful when replicating sequential phenomena such as replicating DNA strands, face recognition, or even creating X-ray images. Social media consumer behavior In a recent study, data on consumer behavior was collected from various social media platforms such as Facebook, Twitter, LinkedIn, YouTube, Instagram, and Pinterest. The usage of predictive learning analytics led researchers to discover various trends in consumer behavior, such as determining how successful a campaign could be, estimating a fair price for a product to attract consumers, assessing how secure data is, and analyzing the specific audience of the consumers they could target for specific products. See also Reinforcement learning Predictive coding Title Predictive state representation URL https//en.wikipedia.org/wiki/Predictive_state_representation Content In computer science, a predictive state representation (PSR) is a way to model a state of controlled dynamical system from a history of actions taken and resulting observations. PSR captures the state of a system as a vector of predictions for future tests (experiments) that can be done on the system. A test is a sequence of action-observation pairs and its prediction is the probability of the tests observation-sequence happening if the tests action-sequence were to be executed on the system. One of the advantage of using PSR is that the predictions are directly related to observable quantities. This is in contrast to other models of dynamical systems, such as partially observable Markov decision processes (POMDPs) where the state of the system is represented as a probability distribution over unobserved nominal states. References Littman, Michael L. Richard S. Sutton Satinder Singh (2002). Predictive Representations of State (PDF). Advances in Neural Information Processing Systems 14 (NIPS). pp. 1555 1561. Singh, Satinder Michael R. James Matthew R. Rudary (2004). Predictive State Representations A New Theory for Modeling Dynamical Systems (PDF). Uncertainty in Artificial Intelligence Proceedings of the Twentieth Conference (UAI). pp. 512 519. Wiewiora, Eric Walter (2008), Modeling Probability Distributions with Predictive State Representations (PDF) Title Preference learning URL https//en.wikipedia.org/wiki/Preference_learning Content Preference learning is a subfield of machine learning that focuses on modeling and predicting preferences based on observed preference information. Preference learning typically involves supervised learning using datasets of pairwise preference comparisons, rankings, or other preference information. Tasks The main task in preference learning concerns problems in learning to rank. According to different types of preference information observed, the tasks are categorized as three main problems in the book Preference Learning Label ranking In label ranking, the model has an instance space ,! . The preference information is given in the form y i x y j displaystyle y_isucc _xy_j,! indicating instance x displaystyle x,! shows preference in y i displaystyle y_i,! rather than y j displaystyle y_j,! . A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance. It was observed that some conventional classification problems can be generalized in the framework of label ranking problem if a training instance x displaystyle x,! is labeled as class y i displaystyle y_i,! , it implies that j i , y i x y j displaystyle forall jneq i,y_isucc _xy_j,! . In the multi-label case, x displaystyle x,! is associated with a set of labels L Y displaystyle Lsubseteq Y,! and thus the model can extract a set of preference information  y i x y j  y i L , y j Y L  displaystyle y_isucc _xy_jy_iin L,y_jin Ybackslash L,! . Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label. Instance ranking Instance ranking also has the instance space X displaystyle X,! and label set Y displaystyle Y,! . In this task, labels are defined to have a fixed order y 1 y 2 y k displaystyle y_1succ y_2succ cdots succ y_k,! and each instance x l displaystyle x_l,! is associated with a label y l displaystyle y_l,! . Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances. Object ranking Object ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form x i x j displaystyle x_isucc x_j,! and the model should find out a ranking order among instances. Techniques There are two practical representations of the preference information A B displaystyle Asucc B,! . One is assigning A displaystyle A,! and B displaystyle B,! with two real numbers a displaystyle a,! and b displaystyle b,! respectively such that a  b displaystyle ab,! . Another one is assigning a binary value V ( A , B )  0 , 1  displaystyle V(A,B)in 0,1,! for all pairs ( A , B ) displaystyle (A,B),! denoting whether A B displaystyle Asucc B,! or B A displaystyle Bsucc A,! . Corresponding to these two different representations, there are two different techniques applied to the learning process. Utility function If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called utility function. For label ranking the mapping is a function f  X Y R displaystyle fXtimes Yrightarrow mathbb R ,! such that y i x y j f ( x , y i )  f ( x , y j ) displaystyle y_isucc _xy_jRightarrow f(x,y_i)f(x,y_j),! . For instance ranking and object ranking, the mapping is a function f  X R displaystyle fXrightarrow mathbb R ,! . Finding the utility function is a regression learning problem which is well developed in machine learning. Preference relations The binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervised learning approach. F rnkranz and H llermeier proposed this approach in label ranking problem. For object ranking, there is an early approach by Cohen et al. Using preference relations to predict the ranking will not be so intuitive. Since observed preference relations may not always be transitive due to inconsistencies in the data, finding a ranking that satisfies all the preference relations may not be possible or may result in multiple possible solutions. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification. Uses Preference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Lius survey paper. Another application of preference learning is recommender systems. Online store may analyze customers purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of users ratings to provide more user preferred contents. Title Prior knowledge for pattern recognition URL https//en.wikipedia.org/wiki/Prior_knowledge_for_pattern_recognition Content Pattern recognition is a very active field of research intimately bound to machine learning. Also known as classification or statistical classification, pattern recognition aims at building a classifier that can determine the class of an input pattern. This procedure, known as training, corresponds to learning an unknown decision function based only on a set of input-output pairs ( x i , y i ) displaystyle (boldsymbol x_i,y_i) that form the training data (or training set). Nonetheless, in real world applications such as character recognition, a certain amount of information on the problem is usually known beforehand. The incorporation of this prior knowledge into the training is the key element that will allow an increase of performance in many applications. Prior Knowledge Prior knowledge refers to all information about the problem available in addition to the training data. However, in this most general form, determining a model from a finite set of samples without prior knowledge is an ill-posed problem, in the sense that a unique model may not exist. Many classifiers incorporate the general smoothness assumption that a test pattern similar to one of the training samples tends to be assigned to the same class. The importance of prior knowledge in machine learning is suggested by its role in search and optimization. Loosely, the no free lunch theorem states that all search algorithms have the same average performance over all problems, and thus implies that to gain in performance on a certain application one must use a specialized algorithm that includes some prior knowledge about the problem. The different types of prior knowledge encountered in pattern recognition are now regrouped under two main categories class-invariance and knowledge on the data. Class-invariance A very common type of prior knowledge in pattern recognition is the invariance of the class (or the output of the classifier) to a transformation of the input pattern. This type of knowledge is referred to as transformation-invariance. The mostly used transformations used in image recognition are translation rotation skewing scaling. Incorporating the invariance to a transformation T  x T x displaystyle T_theta boldsymbol xmapsto T_theta boldsymbol x parametrized in displaystyle theta  into a classifier of output f ( x ) displaystyle f(boldsymbol x) for an input pattern x displaystyle boldsymbol x corresponds to enforcing the equality f ( x )  f ( T x ) , x , . displaystyle f(boldsymbol x)f(T_theta boldsymbol x),quad forall boldsymbol x,theta . Local invariance can also be considered for a transformation centered . displaystyle left.frac partial partial theta right_. The function f displaystyle f in these equations can be either the decision function of the classifier or its real-valued output. Another approach is to consider class-invariance with respect to a domain of the input space instead of a transformation. In this case, the problem becomes finding f displaystyle f so that f ( x )  y P , x P , displaystyle f(boldsymbol x)y_mathcal P, forall boldsymbol xin mathcal P, where y P displaystyle y_mathcal P is the membership class of the region P displaystyle mathcal P of the input space. A different type of class-invariance found in pattern recognition is permutation-invariance, i.e. invariance of the class to a permutation of elements in a structured input. A typical application of this type of prior knowledge is a classifier invariant to permutations of rows of the matrix inputs. Knowledge of the data Other forms of prior knowledge than class-invariance concern the data more specifically and are thus of particular interest for real-world applications. The three particular cases that most often occur when gathering data are Unlabeled samples are available with supposed class-memberships Imbalance of the training set due to a high proportion of samples of a class Quality of the data may vary from a sample to another. Prior knowledge of these can enhance the quality of the recognition if included in the learning. Moreover, not taking into account the poor quality of some data or a large imbalance between the classes can mislead the decision of a classifier. Notes References E. Krupka and N. Tishby, Incorporating Prior Knowledge on Features into Learning, Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS 07) Title Proactive learning URL https//en.wikipedia.org/wiki/Proactive_learning Content Proactive learning is a generalization of active learning designed to relax unrealistic assumptions and thereby reach practical applications. In real life, it is possible and more general to have multiple sources of information with differing reliabilities or areas of expertise. Active learning also assumes that the single oracle is perfect, always providing a correct answer when requested. In reality, though, an oracle (if we generalize the term to mean any source of expert information) may be incorrect (fallible) with a probability that should be a function of the difficulty of the question. Moreover, an oracle may be reluctant it may refuse to answer if it is too uncertain or too busy. Finally, active learning presumes the oracle is either free or charges uniform cost in label elicitation. Such an assumption is naive since cost is likely to be regulated by difficulty (amount of work required to formulate an answer) or other factors. Proactive learning relaxes all four of these assumptions, relying on a decision-theoretic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint. Title Proaftn URL https//en.wikipedia.org/wiki/Proaftn Content Proaftn is a fuzzy classification method that belongs to the class of supervised learning algorithms. The acronym Proaftn stands for (PROc dure dAffectation Floue pour la probl matique du Tri Nominal), which means in English Fuzzy Assignment Procedure for Nominal Sorting. The method enables to determine the fuzzy indifference relations by generalizing the indices (concordance and discordance) used in the ELECTRE III method. To determine the fuzzy indifference relations, PROAFTN uses the general scheme of the discretization technique described in, that establishes a set of pre-classified cases called a training set. To resolve the classification problems, Proaftn proceeds by the following stages Stage 1. Modeling of classes In this stage, the prototypes of the classes are conceived using the two following steps Step 1. Structuring The prototypes and their parameters (thresholds, weights, etc.) are established using the available knowledge given by the expert. Step 2. Validation We use one of the two following techniques in order to validate or adjust the parameters obtained in the first step through the assignment examples known as a training set. Direct technique It consists in adjusting the parameters through the training set and with the expert intervention. Indirect technique It consists in fitting the parameters without the expert intervention as used in machine learning approaches. In multicriteria classification problem, the indirect technique is known as preference disaggregation analysis. This technique requires less cognitive effort than the former technique it uses an automatic method to determine the optimal parameters, which minimize the classification errors. Furthermore, several heuristics and metaheuristics were used to learn the multicriteria classification method Proaftn. Stage 2. Assignment After conceiving the prototypes, Proaftn proceeds to assign the new objects to specific classes. References External links Site dedicated to the sorting problematic of MCDA Title Probabilistic numerics URL https//en.wikipedia.org/wiki/Probabilistic_numerics Content Probabilistic numerics is an active field of study at the intersection of applied mathematics, statistics, and machine learning centering on the concept of uncertainty in computation. In probabilistic numerics, tasks in numerical analysis such as finding numerical solutions for integration, linear algebra, optimization and simulation and differential equations are seen as problems of statistical, probabilistic, or Bayesian inference. Introduction A numerical method is an algorithm that approximates the solution to a mathematical problem (examples below include the solution to a linear system of equations, the value of an integral, the solution of a differential equation, the minimum of a multivariate function). In a probabilistic numerical algorithm, this process of approximation is thought of as a problem of estimation, inference or learning and realised in the framework of probabilistic inference (often, but not always, Bayesian inference). Formally, this means casting the setup of the computational problem in terms of a prior distribution, formulating the relationship between numbers computed by the computer (e.g. matrix-vector multiplications in linear algebra, gradients in optimization, values of the integrand or the vector field defining a differential equation) and the quantity in question (the solution of the linear problem, the minimum, the integral, the solution curve) in a likelihood function, and returning a posterior distribution as the output. In most cases, numerical algorithms also take internal adaptive decisions about which numbers to compute, which form an active learning problem. Many of the most popular classic numerical algorithms can be re-interpreted in the probabilistic framework. This includes the method of conjugate gradients, Nordsieck methods, Gaussian quadrature rules, and quasi-Newton methods. In all these cases, the classic method is based on a regularized least-squares estimate that can be associated with the posterior mean arising from a Gaussian prior and likelihood. In such cases, the variance of the Gaussian posterior is then associated with a worst-case estimate for the squared error. Probabilistic numerical methods promise several conceptual advantages over classic, point-estimate based approximation techniques They return structured error estimates (in particular, the ability to return joint posterior samples, i.e. multiple realistic hypotheses for the true unknown solution of the problem) Hierarchical Bayesian inference can be used to set and control internal hyperparameters in such methods in a generic fashion, rather than having to re-invent novel methods for each parameter Since they use and allow for an explicit likelihood describing the relationship between computed numbers and target quantity, probabilistic numerical methods can use the results of even highly imprecise, biased and stochastic computations. Conversely, probabilistic numerical methods can also provide a likelihood in computations often considered likelihood-free elsewhere Because all probabilistic numerical methods use essentially the same data type probability measures to quantify uncertainty over both inputs and outputs they can be chained together to propagate uncertainty across large-scale, composite computations Sources from multiple sources of information (e.g. algebraic, mechanistic knowledge about the form of a differential equation, and observations of the trajectory of the system collected in the physical world) can be combined naturally and inside the inner loop of the algorithm, removing otherwise necessary nested loops in computation, e.g. in inverse problems. These advantages are essentially the equivalent of similar functional advantages that Bayesian methods enjoy over point-estimates in machine learning, applied or transferred to the computational domain. Numerical tasks Integration Probabilistic numerical methods have been developed for the problem of numerical integration, with the most popular method called Bayesian quadrature. In numerical integration, function evaluations f ( x 1 ) , , f ( x n ) displaystyle f(x_1),ldots ,f(x_n) at a number of points x 1 , , x n displaystyle x_1,ldots ,x_n are used to estimate the integral f ( x ) ( d x ) displaystyle textstyle int f(x)nu (dx) of a function f displaystyle f against some measure displaystyle nu  . Bayesian quadrature consists of specifying a prior distribution over f displaystyle f and conditioning this prior on f ( x 1 ) , , f ( x n ) displaystyle f(x_1),ldots ,f(x_n) to obtain a posterior distribution over f displaystyle f , then computing the implied posterior distribution on f ( x ) ( d x ) displaystyle textstyle int f(x)nu (dx) . The most common choice of prior is a Gaussian process as this allows us to obtain a closed-form posterior distribution on the integral which is a univariate Gaussian distribution. Bayesian quadrature is particularly useful when the function f displaystyle f is expensive to evaluate and the dimension of the data is small to moderate. Optimization Probabilistic numerics have also been studied for mathematical optimization, which consist of finding the minimum or maximum of some objective function f displaystyle f given (possibly noisy or indirect) evaluations of that function at a set of points. Perhaps the most notable effort in this direction is Bayesian optimization, a general approach to optimization grounded in Bayesian inference. Bayesian optimization algorithms operate by maintaining a probabilistic belief about f displaystyle f throughout the optimization procedure this often takes the form of a Gaussian process prior conditioned on observations. This belief then guides the algorithm in obtaining observations that are likely to advance the optimization process. Bayesian optimization policies are usually realized by transforming the objective function posterior into an inexpensive, differentiable acquisition function that is maximized to select each successive observation location. One prominent approach is to model optimization via Bayesian sequential experimental design, seeking to obtain a sequence of observations yielding the most optimization progress as evaluated by an appropriate utility function. A welcome side effect from this approach is that uncertainty in the objective function, as measured by the underlying probabilistic belief, can guide an optimization policy in addressing the classic exploration vs. exploitation tradeoff. Local optimization Probabilistic numerical methods have been developed in the context of stochastic optimization for deep learning, in particular to address main issues such as learning rate tuning and line searches, batch-size selection, early stopping, pruning, and first- and second-order search directions. In this setting, the optimization objective is often an empirical risk of the form L ( )  1 N  . Epistemic uncertainty arises when the dataset size N displaystyle N is large and cannot be processed at once meaning that local quantities (given some displaystyle theta  ) such as the loss function L ( ) displaystyle L(theta ) itself or its gradient L ( ) displaystyle nabla L(theta ) cannot be computed in reasonable time. Hence, generally mini-batching is used to construct estimators of these quantities on a random subset of the data. Probabilistic numerical methods model this uncertainty explicitly and allow for automated decisions and parameter tuning. Linear algebra Probabilistic numerical methods for linear algebra have primarily focused on solving systems of linear equations of the form A  . A large class of methods are iterative in nature and collect information about the linear system to be solved via repeated matrix-vector multiplication v A v displaystyle vmapsto Av with the system matrix A displaystyle A with different vectors v displaystyle v . Such methods can be roughly split into a solution- and a matrix-based perspective, depending on whether belief is expressed over the solution x displaystyle x of the linear system or the (pseudo-)inverse of the matrix   . The belief update uses that the inferred object is linked to matrix multiplications  . Methods typically assume a Gaussian distribution, due to its closedness under linear observations of the problem. While conceptually different, these two views are computationally equivalent and inherently connected via the right-hand-side through  . Probabilistic numerical linear algebra routines have been successfully applied to scale Gaussian processes to large datasets. In particular, they enable exact propagation of the approximation error to a combined Gaussian process posterior, which quantifies the uncertainty arising from both the finite number of data observed and the finite amount of computation expended. Ordinary differential equations Probabilistic numerical methods for ordinary differential equations y ( t )  f ( t , y ( t ) ) displaystyle dot y(t)f(t,y(t)) , have been developed for initial and boundary value problems. Many different probabilistic numerical methods designed for ordinary differential equations have been proposed, and these can broadly be grouped into the two following categories Randomisation-based methods are defined through random perturbations of standard deterministic numerical methods for ordinary differential equations. For example, this has been achieved by adding Gaussian perturbations on the solution of one-step integrators or by perturbing randomly their time-step. This defines a probability measure on the solution of the differential equation that can be sampled. Gaussian process regression methods are based on posing the problem of solving the differential equation at hand as a Gaussian process regression problem, interpreting evaluations of the right-hand side as data on the derivative. These techniques resemble to Bayesian cubature, but employ different and often non-linear observation models. In its infancy, this class of methods was based on naive Gaussian process regression. This was later improved (in terms of efficient computation) in favor of Gauss Markov priors modeled by the stochastic differential equation d x ( t )  A x ( t ) d t  B d v ( t ) displaystyle mathrm d x(t)Ax(t),mathrm d tB,mathrm d v(t) , where x ( t ) displaystyle x(t) is a displaystyle nu  -dimensional vector modeling the first displaystyle nu  derivatives of y ( t ) displaystyle y(t) , and where v ( t ) displaystyle v(t) is a displaystyle nu  -dimensional Brownian motion. Inference can thus be implemented efficiently with Kalman filtering based methods. The boundary between these two categories is not sharp, indeed a Gaussian process regression approach based on randomised data was developed as well. These methods have been applied to problems in computational Riemannian geometry, inverse problems, latent force models, and to differential equations with a geometric structure such as symplecticity. Partial differential equations A number of probabilistic numerical methods have also been proposed for partial differential equations. As with ordinary differential equations, the approaches can broadly be divided into those based on randomisation, generally of some underlying finite-element mesh and those based on Gaussian process regression. Probabilistic numerical PDE solvers based on Gaussian process regression recover classical methods on linear PDEs for certain priors, in particular methods of mean weighted residuals, which include Galerkin methods, finite element methods, as well as spectral methods. History and related fields The interplay between numerical analysis and probability is touched upon by a number of other areas of mathematics, including average-case analysis of numerical methods, information-based complexity, game theory, and statistical decision theory. Precursors to what is now being called probabilistic numerics can be found as early as the late 19th and early 20th century. The origins of probabilistic numerics can be traced to a discussion of probabilistic approaches to polynomial interpolation by Henri Poincar in his Calcul des Probabilit s. In modern terminology, Poincar considered a Gaussian prior distribution on a function f  R R displaystyle fcolon mathbb R to mathbb R  , expressed as a formal power series with random coefficients, and asked for probable values of f ( x ) displaystyle f(x) given this prior and n N displaystyle nin mathbb N  observations f ( a i )  B i displaystyle f(a_i)B_i for  . A later seminal contribution to the interplay of numerical analysis and probability was provided by Albert Suldin in the context of univariate quadrature. The statistical problem considered by Suldin was the approximation of the definite integral u ( t ) d t displaystyle textstyle int u(t),mathrm d t of a function u   a , b  R displaystyle ucolon a,bto mathbb R  , under a Brownian motion prior on u displaystyle u , given access to pointwise evaluation of u displaystyle u at nodes t 1 , , t n  a , b  displaystyle t_1,dots ,t_nin a,b . Suldin showed that, for given quadrature nodes, the quadrature rule with minimal mean squared error is the trapezoidal rule furthermore, this minimal error is proportional to the sum of cubes of the inter-node spacings. As a result, one can see the trapezoidal rule with equally-spaced nodes as statistically optimal in some sense an early example of the average-case analysis of a numerical method. Suldins point of view was later extended by Mike Larkin. Note that Suldins Brownian motion prior on the integrand u displaystyle u is a Gaussian measure and that the operations of integration and of point wise evaluation of u displaystyle u are both linear maps. Thus, the definite integral u ( t ) d t displaystyle textstyle int u(t),mathrm d t is a real-valued Gaussian random variable. In particular, after conditioning on the observed pointwise values of u displaystyle u , it follows a normal distribution with mean equal to the trapezoidal rule and variance equal to 1 12  . This viewpoint is very close to that of Bayesian quadrature, seeing the output of a quadrature method not just as a point estimate but as a probability distribution in its own right. As noted by Houman Owhadi and collaborators, interplays between numerical approximation and statistical inference can also be traced back to Palasti and Renyi, Sard, Kimeldorf and Wahba (on the correspondence between Bayesian estimation and spline smoothing/interpolation) and Larkin (on the correspondence between Gaussian process regression and numerical approximation). Although the approach of modelling a perfectly known function as a sample from a random process may seem counterintuitive, a natural framework for understanding it can be found in information-based complexity (IBC), the branch of computational complexity founded on the observation that numerical implementation requires computation with partial information and limited resources. In IBC, the performance of an algorithm operating on incomplete information can be analyzed in the worst-case or the average-case (randomized) setting with respect to the missing information. Moreover, as Packel observed, the average case setting could be interpreted as a mixed strategy in an adversarial game obtained by lifting a (worst-case) minmax problem to a minmax problem over mixed (randomized) strategies. This observation leads to a natural connection between numerical approximation and Walds decision theory, evidently influenced by von Neumanns theory of games. To describe this connection consider the optimal recovery setting of Micchelli and Rivlin in which one tries to approximate an unknown function from a finite number of linear measurements on that function. Interpreting this optimal recovery problem as a zero-sum game where Player I selects the unknown function and Player II selects its approximation, and using relative errors in a quadratic norm to define losses, Gaussian priors emerge as optimal mixed strategies for such games, and the covariance operator of the optimal Gaussian prior is determined by the quadratic norm used to define the relative error of the recovery. Software ProbNum Probabilistic Numerics in Python. ProbNumDiffEq.jl Probabilistic numerical ODE solvers based on filtering implemented in Julia. Emukit Adaptable Python toolbox for decision-making under uncertainty. BackPACK Built on top of PyTorch. It efficiently computes quantities other than the gradient. See also Average-case analysis Information-based complexity Uncertainty quantification Title Probability matching URL https//en.wikipedia.org/wiki/Probability_matching Content Probability matching is a decision strategy in which predictions of class membership are proportional to the class base rates. Thus, if in the training set positive examples are observed 60 of the time, and negative examples are observed 40 of the time, then the observer using a probability-matching strategy will predict (for unlabeled examples) a class label of positive on 60 of instances, and a class label of negative on 40 of instances. The optimal Bayesian decision strategy (to maximize the number of correct predictions, see Duda, Hart  Stork (2001)) in such a case is to always predict positive (i.e., predict the majority category in the absence of other information), which has 60 chance of winning rather than matching which has 52 of winning (where p is the probability of positive realization, the result of matching would be p 2  ( 1 p ) 2 displaystyle p2(1-p)2 , here .6 .6  .4 .4 displaystyle .6times .6.4times .4 ). The probability-matching strategy is of psychological interest because it is frequently employed by human subjects in decision and classification studies (where it may be related to Thompson sampling). The only case when probability matching will yield same results as Bayesian decision strategy mentioned above is when all class base rates are the same. So, if in the training set positive examples are observed 50 of the time, then the Bayesian strategy would yield 50 accuracy (1 .5), just as probability matching (.5 .5  .5 .5). References Duda, Richard O. Hart, Peter E. Stork, David G. (2001), Pattern Classification (2nd ed.), New York John Wiley  Sons Shanks, D. R., Tunney, R. J.,  McCarthy, J. D. (2002). A re examination of probability matching and rational choice. Journal of Behavioral Decision Making, 15(3), 233-250. Title Product of experts URL https//en.wikipedia.org/wiki/Product_of_experts Content Product of experts (PoE) is a machine learning technique. It models a probability distribution by combining the output from several simpler distributions. It was proposed by Geoffrey Hinton in 1999, along with an algorithm for training the parameters of such a system. The core idea is to combine several probability distributions (experts) by multiplying their density functions making the PoE classification similar to an and operation. This allows each expert to make decisions on the basis of a few dimensions without having to cover the full dimensionality of a problem P ( y   x k  )  1 Z )). This is related to (but quite different from) a mixture model, where several probability distributions p j ( y   x j  ) displaystyle p_j(yx_j) are combined via an or operation, which is a weighted sum of their density functions P ( y   x k  )  . displaystyle sum _jalpha _j1. The experts may be understood as each being responsible for enforcing a constraint in a high-dimensional space. A data point is considered likely if and only if none of the experts say that the point violates a constraint. To optimize it, he proposed the contrastive divergence minimization algorithm. This algorithm is most often used for learning restricted Boltzmann machines. See also Mixture of experts Boltzmann machine References External links Product of experts article in Scholarpedia Geoffrey Hintons articles on PoE Title Programming by example URL https//en.wikipedia.org/wiki/Programming_by_example Content In computer science, programming by example (PbE), also termed programming by demonstration or more generally as demonstrational programming, is an end-user development technique for teaching a computer new behavior by demonstrating actions on concrete examples. The system records user actions and infers a generalized program that can be used on new examples. PbE is intended to be easier to do than traditional computer programming, which generally requires learning and using a programming language. Many PbE systems have been developed as research prototypes, but few have found widespread real-world application. More recently, PbE has proved to be a useful paradigm for creating scientific work-flows. PbE is used in two independent clients for the BioMOBY protocol Seahawk and Gbrowse moby. Also the programming by demonstration (PbD) term has been mostly adopted by robotics researchers for teaching new behaviors to the robot through a physical demonstration of the task. The usual distinction in literature between these terms is that in PbE the user gives a prototypical product of the computer execution, such as a row in the desired results of a query while in PbD the user performs a sequence of actions that the computer must repeat, generalizing it to be used in different data sets. For final users, to automate a workflow in a complex tool (e.g. Photoshop), the most simple case of PbD is the macro recorder. See also Query by Example Automated machine learning Example-based machine translation Inductive programming Lapis (text editor), which allows simultaneous editing of similar items in multiple selections created by example Programming by demonstration Test-driven development References External links Henry Liebermans page on Programming by Example Online copy of Watch What I Do, Allen Cyphers book on Programming by Demonstration Online copy of Your Wish is My Command, Henry Liebermans sequel to Watch What I Do A Visual Language for Data Mapping, John Carlsons description of an Integrated Development Environment (IDE) that used Programming by Example (desktop objects) for data mapping, and an iconic language for recording operations Title Prompt engineering URL https//en.wikipedia.org/wiki/Prompt_engineering Content Prompt engineering is the process of structuring or crafting an instruction in order to produce the best possible output from a generative artificial intelligence (AI) model. A prompt is natural language text describing the task that an AI should perform. A prompt for a text-to-text language model can be a query, a command, or a longer statement including context, instructions, and conversation history. Prompt engineering may involve phrasing a query, specifying a style, choice of words and grammar, providing relevant context, or describing a character for the AI to mimic. When communicating with a text-to-image or a text-to-audio model, a typical prompt is a description of a desired output such as a high-quality photo of an astronaut riding a horse or Lo-fi slow BPM electro chill with organic samples. Prompting a text-to-image model may involve adding, removing, emphasizing, and re-ordering words to achieve a desired subject, style, layout, lighting, and aesthetic. History In 2018, researchers first proposed that all previously separate tasks in natural language processing (NLP) could be cast as a question-answering problem over a context. In addition, they trained a first single, joint, multi-task model that would answer any task-related question like What is the sentiment or Translate this sentence to German or Who is the president? The AI boom saw an increase in the amount of prompting technique to get the model to output the desired outcome and avoid nonsensical output, a process characterized by trial-and-error. After the release of ChatGPT in 2022, prompt engineering was soon seen as an important business skill, albeit one with an uncertain economic future. A repository for prompts reported that over 2,000 public prompts for around 170 datasets were available in February 2022. In 2022, the chain-of-thought prompting technique was proposed by Google researchers. In 2023, several text-to-text and text-to-image prompt databases were made publicly available. The Personalized Image-Prompt (PIP) dataset, a generated image-text dataset that has been categorized by 3,115 users, has also been made available publicly in 2024. Text-to-text Multiple distinct prompt engineering techniques have been published. Chain-of-thought According to Google Research, chain-of-thought (CoT) prompting is a technique that allows large language models (LLMs) to solve a problem as a series of intermediate steps before giving a final answer. In 2022, Google Brain reported that chain-of-thought prompting improves reasoning ability by inducing the model to answer a multi-step problem with steps of reasoning that mimic a train of thought. Chain-of-thought techniques were developed to help LLMs handle multi-step reasoning tasks, such as arithmetic or commonsense reasoning questions. For example, given the question, Q The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?, Google claims that a CoT prompt might induce the LLM to answer A The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20  3. They bought 6 more apples, so they have 3  6  9. The answer is 9. When applied to PaLM, a 540 billion parameter language model, according to Google, CoT prompting significantly aided the model, allowing it to perform comparably with task-specific fine-tuned models on several tasks, achieving state-of-the-art results at the time on the GSM8K mathematical reasoning benchmark. It is possible to fine-tune models on CoT reasoning datasets to enhance this capability further and stimulate better interpretability. An example of a CoT prompting Q question A Lets think step by step. As originally proposed by Google, each CoT prompt included a few QA examples. This made it a few-shot prompting technique. However, according to researchers at Google and the University of Tokyo, simply appending the words Lets think step-by-step, has also proven effective, which makes CoT a zero-shot prompting technique. OpenAI claims that this prompt allows for better scaling as a user no longer needs to formulate many specific CoT QA examples. In-context learning In-context learning, refers to a models ability to temporarily learn from prompts. For example, a prompt may include a few examples for a model to learn from, such as asking the model to complete maison house, chat cat, chien  (the expected response being dog), an approach called few-shot learning. In-context learning is an emergent ability of large language models. It is an emergent property of model scale, meaning that breaks in downstream scaling laws occur, leading to its efficacy increasing at a different rate in larger models than in smaller models. Unlike training and fine-tuning, which produce lasting changes, in-context learning is temporary. Training models to perform in-context learning can be viewed as a form of meta-learning, or learning to learn. Self-consistency decoding Self-consistency decoding performs several chain-of-thought rollouts, then selects the most commonly reached conclusion out of all the rollouts. If the rollouts disagree by a lot, a human can be queried for the correct chain of thought. Tree-of-thought Tree-of-thought prompting generalizes chain-of-thought by prompting the model to generate one or more possible next steps, and then running the model on each of the possible next steps by breadth-first, beam, or some other method of tree search. The LLM has additional modules that can converse the history of the problem-solving process to the LLM, which allows the system to backtrack steps the problem-solving process. Prompting to disclose uncertainty By default, the output of language models may not contain estimates of uncertainty. The model may output text that appears confident, though the underlying token predictions have low likelihood scores. Large language models like GPT-4 can have accurately calibrated likelihood scores in their token predictions, and so the model output uncertainty can be directly estimated by reading out the token prediction likelihood scores. Prompting to estimate model sensitivity Research consistently demonstrates that LLMs are highly sensitive to subtle variations in prompt formatting, structure, and linguistic properties. Some studies have shown up to 76 accuracy points across formatting changes in few-shot settings. Linguistic features significantly influence prompt effectiveness such as morphology, syntax, and lexico-semantic changes which meaningfully enhance task performance across a variety of tasks. Clausal syntax, for example, improves consistency and reduces uncertainty in knowledge retrieval. This sensitivity persists even with larger model sizes, additional few-shot examples, or instruction tuning. To address sensitivity of models and make them more robust, several methods have been proposed. FormatSpread facilitates systematic analysis by evaluating a range of plausible prompt formats, offering a more comprehensive performance interval. Similarly, PromptEval estimates performance distributions across diverse prompts, enabling robust metrics such as performance quantiles and accurate evaluations under constrained budgets. Automatic prompt generation Retrieval-augmented generation Retrieval-augmented generation (RAG) is a technique that enables generative artificial intelligence (Gen AI) models to retrieve and incorporate new information. It modifies interactions with a large language model (LLM) so that the model responds to user queries with reference to a specified set of documents, using this information to supplement information from its pre-existing training data. This allows LLMs to use domain-specific and/or updated information. RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts. This method helps reduce AI hallucinations, which have led to real-world issues like chatbots inventing policies or lawyers citing nonexistent legal cases. By dynamically retrieving information, RAG enables AI to provide more accurate responses without frequent retraining. Graph retrieval-augmented generation GraphRAG (coined by Microsoft Research) is a technique that extends RAG with the use of a knowledge graph (usually, LLM-generated) to allow the model to connect disparate pieces of information, synthesize insights, and holistically understand summarized semantic concepts over large data collections. It was shown to be effective on datasets like the Violent Incident Information from News Articles (VIINA). Earlier work showed the effectiveness of using a knowledge graph for question answering using text-to-query generation. These techniques can be combined to search across both unstructured and structured data, providing expanded context, and improved ranking. Using language models to generate prompts Large language models (LLM) themselves can be used to compose prompts for large language models. The automatic prompt engineer algorithm uses one LLM to beam search over prompts for another LLM There are two LLMs. One is the target LLM, and another is the prompting LLM. Prompting LLM is presented with example input-output pairs, and asked to generate instructions that could have caused a model following the instructions to generate the outputs, given the inputs. Each of the generated instructions is used to prompt the target LLM, followed by each of the inputs. The log-probabilities of the outputs are computed and added. This is the score of the instruction. The highest-scored instructions are given to the prompting LLM for further variations. Repeat until some stopping criteria is reached, then output the highest-scored instructions. CoT examples can be generated by LLM themselves. In auto-CoT, a library of questions are converted to vectors by a model such as BERT. The question vectors are clustered. Questions nearest to the centroids of each cluster are selected. An LLM does zero-shot CoT on each question. The resulting CoT examples are added to the dataset. When prompted with a new question, CoT examples to the nearest questions can be retrieved and added to the prompt. Text-to-image In 2022, text-to-image models like DALL-E 2, Stable Diffusion, and Midjourney were released to the public. These models take text prompts as input and use them to generate AI-generated images. Text-to-image models typically do not understand grammar and sentence structure in the same way as large language models, thus may require a different set of prompting techniques. Text-to-image models do not natively understand negation. The prompt a party with no cake is likely to produce an image including a cake. As an alternative, negative prompts allow a user to indicate, in a separate prompt, which terms should not appear in the resulting image. Techniques such as framing the normal prompt into a sequence-to-sequence language modeling problem can be used to automatically generate an output for the negative prompt. Prompt formats A text-to-image prompt commonly includes a description of the subject of the art, the desired medium (such as digital painting or photography), style (such as hyperrealistic or pop-art), lighting (such as rim lighting or crepuscular rays), color, and texture. Word order also affects the output of a text-to-image prompt. Words closer to the start of a prompt may be emphasized more heavily. The Midjourney documentation encourages short, descriptive prompts instead of Show me a picture of lots of blooming California poppies, make them bright, vibrant orange, and draw them in an illustrated style with colored pencils, an effective prompt might be Bright orange California poppies drawn with colored pencils. Artist styles Some text-to-image models are capable of imitating the style of particular artists by name. For example, the phrase in the style of Greg Rutkowski has been used in Stable Diffusion and Midjourney prompts to generate images in the distinctive style of Polish digital artist Greg Rutkowski. Famous artists such as Vincent van Gogh and Salvador Dal have also been used for styling and testing. Non-text prompts Some approaches augment or replace natural language text prompts with non-text input. Textual inversion and embeddings For text-to-image models, textual inversion performs an optimization process to create a new word embedding based on a set of example images. This embedding vector acts as a pseudo-word which can be included in a prompt to express the content or style of the examples. Image prompting In 2023, Metas AI research released Segment Anything, a computer vision model that can perform image segmentation by prompting. As an alternative to text prompts, Segment Anything can accept bounding boxes, segmentation masks, and foreground/background points. Using gradient descent to search for prompts In prefix-tuning, prompt tuning, or soft prompting, floating-point-valued vectors are searched directly by gradient descent to maximize the log-likelihood on outputs. Formally, let . During training, the tunable embeddings, input, and output tokens are concatenated into a single sequence concat ( E  X  Y ) displaystyle textconcat(mathbf E mathbf X mathbf Y ) , and fed to the LLMs. The losses are computed over the Y displaystyle mathbf Y  tokens the gradients are backpropagated to prompt-specific parameters in prefix-tuning, they are parameters associated with the prompt tokens at each layer in prompt tuning, they are merely the soft tokens added to the vocabulary. More formally, this is prompt tuning. Let an LLM be written as L L M ( X )  F ( E ( X ) ) displaystyle LLM(X)F(E(X)) , where X displaystyle X is a sequence of linguistic tokens, E displaystyle E is the token-to-vector function, and F displaystyle F is the rest of the model. In prefix-tuning, one provides a set of input-output pairs  ( X i , Y i )  i displaystyle (Xi,Yi)_i , and then use gradient descent to search for arg max Z  i log P r  Y i  Z  E ( X i )  displaystyle arg max _tilde Zsum _ilog PrYitilde Zast E(Xi) . In words, log P r  Y i  Z  E ( X i )  displaystyle log PrYitilde Zast E(Xi) is the log-likelihood of outputting Y i displaystyle Yi , if the model first encodes the input X i displaystyle Xi into the vector E ( X i ) displaystyle E(Xi) , then prepend the vector with the prefix vector Z  displaystyle tilde Z , then apply F displaystyle F . For prefix tuning, it is similar, but the prefix vector Z  displaystyle tilde Z is pre-appended to the hidden states in every layer of the model. An earlier result uses the same idea of gradient descent search, but is designed for masked language models like BERT, and searches only over token sequences, rather than numerical vectors. Formally, it searches for arg max X  i log P r  Y i  X  X i  displaystyle arg max _tilde Xsum _ilog PrYitilde Xast Xi where X  displaystyle tilde X is ranges over token sequences of a specified length. Prompt injection Prompt injection is a cybersecurity exploit in which adversaries craft inputs that appear legitimate but are designed to cause unintended behavior in machine learning models, particularly large language models (LLMs). This attack takes advantage of the models inability to distinguish between developer-defined prompts and user inputs, allowing adversaries to bypass safeguards and influence model behaviour. While LLMs are designed to follow trusted instructions, they can be manipulated into carrying out unintended responses through carefully crafted inputs. See also Social engineering (security) Title Proximal gradient methods for learning URL https//en.wikipedia.org/wiki/Proximal_gradient_methods_for_learning Content Proximal gradient (forward backward splitting) methods for learning is an area of research in optimization and statistical learning theory which studies algorithms for a general class of convex regularization problems where the regularization penalty may not be differentiable. One such example is 1 displaystyle ell _1 regularization (also known as Lasso) of the form min w R d 1 n  . displaystyle min _win mathbb R dfrac 1nsum _ . Proximal gradient methods offer a general framework for solving regularization problems from statistical learning theory with penalties that are tailored to a specific problem application. Such customized penalties can help to induce certain structure in problem solutions, such as sparsity (in the case of lasso) or group structure (in the case of group lasso). Relevant background Proximal gradient methods are applicable in a wide variety of scenarios for solving convex optimization problems of the form min x H F ( x )  R ( x ) , displaystyle min _xin mathcal HF(x)R(x), where F displaystyle F is convex and differentiable with Lipschitz continuous gradient, R displaystyle R is a convex, lower semicontinuous function which is possibly nondifferentiable, and H displaystyle mathcal H is some set, typically a Hilbert space. The usual criterion of x displaystyle x minimizes F ( x )  R ( x ) displaystyle F(x)R(x) if and only if ( F  R ) ( x )  0 displaystyle nabla (FR)(x)0 in the convex, differentiable setting is now replaced by 0 ( F  R ) ( x ) , displaystyle 0in partial (FR)(x), where displaystyle partial varphi  denotes the subdifferential of a real-valued, convex function displaystyle varphi  . Given a convex function  H R displaystyle varphi mathcal Hto mathbb R  an important operator to consider is its proximal operator prox  H H displaystyle operatorname prox _varphi mathcal Hto mathcal H defined by prox ( u )  arg min x H ( x )  1 2 u x 2 2 , displaystyle operatorname prox _varphi (u)operatorname arg min _xin mathcal Hvarphi (x)frac 12u-x_22, which is well-defined because of the strict convexity of the 2 displaystyle ell _2 norm. The proximal operator can be seen as a generalization of a projection. We see that the proximity operator is important because x displaystyle x is a minimizer to the problem min x H F ( x )  R ( x ) displaystyle min _xin mathcal HF(x)R(x) if and only if . Moreau decomposition One important technique related to proximal gradient methods is the Moreau decomposition, which decomposes the identity operator as the sum of two proximity operators. Namely, let  X R displaystyle varphi mathcal Xto mathbb R  be a lower semicontinuous, convex function on a vector space X displaystyle mathcal X . We define its Fenchel conjugate  X R displaystyle varphi mathcal Xto mathbb R  to be the function ( u )  sup x X x , u ( x ) . displaystyle varphi (u)sup _xin mathcal Xlangle x,urangle -varphi (x). The general form of Moreaus decomposition states that for any x X displaystyle xin mathcal X and any  0 displaystyle gamma 0 that ) . The Moreau decomposition can be seen to be a generalization of the usual orthogonal decomposition of a vector space, analogous with the fact that proximity operators are generalizations of projections. In certain situations it may be easier to compute the proximity operator for the conjugate displaystyle varphi  instead of the function displaystyle varphi  , and therefore the Moreau decomposition can be applied. This is the case for group lasso. Lasso regularization Consider the regularized empirical risk minimization problem with square loss and with the 1 displaystyle ell _1 norm as the regularization penalty min w R d 1 n  . displaystyle x_iin mathbb R dtext and y_iin mathbb R . The 1 displaystyle ell _1 regularization problem is sometimes referred to as lasso (least absolute shrinkage and selection operator). Such 1 displaystyle ell _1 regularization problems are interesting because they induce sparse solutions, that is, solutions w displaystyle w to the minimization problem have relatively few nonzero components. Lasso can be seen to be a convex relaxation of the non-convex problem min w R d 1 n  . Sparse solutions are of particular interest in learning theory for interpretability of results a sparse solution can identify a small number of important factors. Solving for proximity operator For simplicity we restrict our attention to the problem  . To solve the problem min w R d 1 n  . Note that R displaystyle R is not strictly convex. Let us compute the proximity operator for R ( w ) displaystyle R(w) . First we find an alternative characterization of the proximity operator prox R ( x ) displaystyle operatorname prox _R(x) as follows  ) . displaystyle beginaligned).endaligned For R ( w )  w 1 displaystyle R(w)w_1 it is easy to compute R ( w ) displaystyle partial R(w)  the i displaystyle i th entry of R ( w ) displaystyle partial R(w) is precisely  w i    1 , w i  0 1 , w i  0  1 , 1  , w . displaystyle partial w_ibegincases1,w_i0-1,w_i0left-1,1right,w_i0.endcases Using the recharacterization of the proximity operator given above, for the choice of R ( w )  w 1 displaystyle R(w)w_1 and  0 displaystyle gamma 0 we have that prox R ( x ) displaystyle operatorname prox _gamma R(x) is defined entrywise by ( prox R ( x ) ) ) . Fixed point iterative schemes To finally solve the lasso problem we consider the fixed point equation shown earlier  ) ) . displaystyle xoperatorname prox _gamma Rleft(x-gamma nabla F(x)right). Given that we have computed the form of the proximity operator explicitly, then we can define a standard fixed point iteration procedure. Namely, fix some initial w 0 R d displaystyle w0in mathbb R d , and for  ) ) . displaystyle wk1S_gamma left(wk-gamma nabla Fleft(wkright)right). Note here the effective trade-off between the empirical error term F ( w ) displaystyle F(w) and the regularization penalty R ( w ) displaystyle R(w) . This fixed point method has decoupled the effect of the two different convex functions which comprise the objective function into a gradient descent step ( w k F ( w k ) displaystyle wk-gamma nabla Fleft(wkright) ) and a soft thresholding step (via S displaystyle S_gamma  ). Convergence of this fixed point scheme is well-studied in the literature and is guaranteed under appropriate choice of step size displaystyle gamma  and loss function (such as the square loss taken here). Accelerated methods were introduced by Nesterov in 1983 which improve the rate of convergence under certain regularity assumptions on F displaystyle F . Such methods have been studied extensively in previous years. For more general learning problems where the proximity operator cannot be computed explicitly for some regularization term R displaystyle R , such fixed point schemes can still be carried out using approximations to both the gradient and the proximity operator. Practical considerations There have been numerous developments within the past decade in convex optimization techniques which have influenced the application of proximal gradient methods in statistical learning theory. Here we survey a few important topics which can greatly improve practical algorithmic performance of these methods. Adaptive step size In the fixed point iteration scheme w k  1  prox R ( w k F ( w k ) ) , displaystyle wk1operatorname prox _gamma Rleft(wk-gamma nabla Fleft(wkright)right), one can allow variable step size k displaystyle gamma _k instead of a constant displaystyle gamma  . Numerous adaptive step size schemes have been proposed throughout the literature. Applications of these schemes suggest that these can offer substantial improvement in number of iterations required for fixed point convergence. Elastic net (mixed norm regularization) Elastic net regularization offers an alternative to pure 1 displaystyle ell _1 regularization. The problem of lasso ( 1 displaystyle ell _1 ) regularization involves the penalty term R ( w )  w 1 displaystyle R(w)w_1 , which is not strictly convex. Hence, solutions to min w F ( w )  R ( w ) , displaystyle min _wF(w)R(w), where F displaystyle F is some empirical loss function, need not be unique. This is often avoided by the inclusion of an additional strictly convex term, such as an 2 displaystyle ell _2 norm regularization penalty. For example, one can consider the problem min w R d 1 n  . displaystyle x_iin mathbb R dtext and y_iin mathbb R . For 0  1 displaystyle 0mu leq 1 the penalty term ( ( 1 ) w 1  w 2 2 ) displaystyle lambda left((1-mu )w_1mu w_22right) is now strictly convex, and hence the minimization problem now admits a unique solution. It has been observed that for sufficiently small  0 displaystyle mu 0 , the additional penalty term w 2 2 displaystyle mu w_22 acts as a preconditioner and can substantially improve convergence while not adversely affecting the sparsity of solutions. Exploiting group structure Proximal gradient methods provide a general framework which is applicable to a wide variety of problems in statistical learning theory. Certain problems in learning can often involve data which has additional structure that is known a priori. In the past several years there have been new developments which incorporate information about group structure to provide methods which are tailored to different applications. Here we survey a few such methods. Group lasso Group lasso is a generalization of the lasso method when features are grouped into disjoint blocks. Suppose the features are grouped into blocks  w 1 , , w G  displaystyle w_1,ldots ,w_G . Here we take as a regularization penalty R ( w )  . A similar proximity operator analysis as above can be used to compute the proximity operator for this penalty. Where the lasso penalty has a proximity operator which is soft thresholding on each individual component, the proximity operator for the group lasso is soft thresholding on each group. For the group w g displaystyle w_g we have that proximity operator of ( . In contrast to lasso, the derivation of the proximity operator for group lasso relies on the Moreau decomposition. Here the proximity operator of the conjugate of the group lasso penalty becomes a projection onto the ball of a dual norm. Other group structures In contrast to the group lasso problem, where features are grouped into disjoint blocks, it may be the case that grouped features are overlapping or have a nested structure. Such generalizations of group lasso have been considered in a variety of contexts. For overlapping groups one common approach is known as latent group lasso which introduces latent variables to account for overlap. Nested group structures are studied in hierarchical structure prediction and with directed acyclic graphs. See also Convex analysis Proximal gradient method Regularization Statistical learning theory Title Pythia (machine learning) URL https//en.wikipedia.org/wiki/Pythia_(machine_learning) Content Pythia is an ancient text restoration model that recovers missing characters from a damaged text input using deep neural networks. It was created by Yannis Assael, Thea Sommerschield, and Jonathan Prag, researchers from Google DeepMind and the University of Oxford. To study the society and the history of ancient civilisations, ancient history relies on disciplines such as epigraphy, the study of ancient inscribed texts. Hundreds of thousands of these texts, known as inscriptions, have survived to our day, but are often damaged over the centuries. Illegible parts of the text must then be restored by specialists, called epigraphists, in order to extract meaningful information from the text and use it to expand our knowledge of the context in which the text was written. Pythia takes as input the damaged text, and is trained to return hypothesised restorations of ancient Greek inscriptions, working as an assistive aid for ancient historians. Its neural network architecture works at both the character- and word-level, thereby effectively handling long-term context information, and dealing efficiently with incomplete word representations. Pythia is applicable to any discipline dealing with ancient texts (philology, papyrology, codicology) and can work in any language (ancient or modern). Title Quantification (machine learning) URL https//en.wikipedia.org/wiki/Quantification_(machine_learning) Content In machine learning and data mining, quantification (variously called learning to quantify, or supervised prevalence estimation, or class prior estimation) is the task of using supervised learning in order to train models (quantifiers) that estimate the relative frequencies (also known as prevalence values) of the classes of interest in a sample of unlabelled data items. For instance, in a sample of 100,000 unlabelled tweets known to express opinions about a certain political candidate, a quantifier may be used to estimate the percentage of these tweets which belong to class Positive (i.e., which manifest a positive stance towards this candidate), and to do the same for classes Neutral and Negative. Quantification may also be viewed as the task of training predictors that estimate a (discrete) probability distribution, i.e., that generate a predicted distribution that approximates the unknown true distribution of the items across the classes of interest. Quantification is different from classification, since the goal of classification is to predict the class labels of individual data items, while the goal of quantification it to predict the class prevalence values of sets of data items. Quantification is also different from regression, since in regression the training data items have real-valued labels, while in quantification the training data items have class labels. It has been shown in multiple research works that performing quantification by classifying all unlabelled instances and then counting the instances that have been attributed to each class (the classify and count method) usually leads to suboptimal quantification accuracy. This suboptimality may be seen as a direct consequence of Vapniks principle, which states If you possess a restricted amount of information for solving some problem, try to solve the problem directly and never solve a more general problem as an intermediate step. It is possible that the available information is sufficient for a direct solution but is insufficient for solving a more general intermediate problem. In our case, the problem to be solved directly is quantification, while the more general intermediate problem is classification. As a result of the suboptimality of the classify and count method, quantification has evolved as a task in its own right, different (in goals, methods, techniques, and evaluation measures) from classification. Quantification tasks The main variants of quantification, according to the characteristics of the set of classes used, are Binary quantification, corresponding to the case in which there are only . Regression quantification, a task which stands to standard quantification as regression stands to classification. Strictly speaking, this task is not a quantification task as defined above (since the individual items do not have class labels but are labelled by real values), but has enough commonalities with other quantification tasks to be considered one of them. Most known quantification methods address the binary case or the single-label multiclass case, and only few of them address the multi-label, ordinal, and regression cases. Binary-only methods include the Mixture Model (MM) method, the HDy method, SVM(KLD), and SVM(Q). Methods that can deal with both the binary case and the single-label multiclass case include probabilistic classify and count (PCC), adjusted classify and count (ACC), probabilistic adjusted classify and count (PACC), and the Saerens-Latinne-Decaestecker EM-based method (SLD). Methods for multi-label quantification include regression-based quantification (RQ) and label powerset-based quantification (LPQ). Methods for the ordinal case include Ordinal Quantification Tree (OQT), and ordinal versions of the above-mentioned ACC, PACC, and SLD methods. Methods for the regression case include Regress and splice and Adjusted regress and sum. Evaluation measures for quantification Several evaluation measures can be used for evaluating the error of a quantification method. Since quantification consists of generating a predicted probability distribution that estimates a true probability distribution, these evaluation measures are ones that compare two probability distributions. Most evaluation measures for quantification belong to the class of divergences. Evaluation measures for binary quantification and single-label multiclass quantification are Absolute Error Squared Error Relative Absolute Error Kullback-Leibler divergence Pearson Divergence Evaluation measures for ordinal quantification are Normalized Match Distance (a particular case of the Earth Movers Distance) Root Normalized Order-Aware Distance Applications Quantification is of special interest in fields such as the social sciences, epidemiology, market research, and ecological modelling, since these fields are inherently concerned with aggregate data. However, quantification is also useful as a building block for solving other downstream tasks, such as improving the accuracy of classifiers on out-of-distribution data, allocating resources, measuring classifier bias, and estimating the accuracy of classifiers on out-of-distribution data. Resources LQ 2021 the 1st International Workshop on Learning to Quantify LQ 2022 the 2nd International Workshop on Learning to Quantify LQ 2023 the 3rd International Workshop on Learning to Quantify LQ 2024 the 4th International Workshop on Learning to Quantify LeQua 2022 the 1st Data Challenge on Learning to Quantify LeQua 2024 the 2nd Data Challenge on Learning to Quantify QuaPy An open-source Python-based software library for quantification QuantificationLib A Python library for quantification and prevalence estimation Title Quantum machine learning URL https//en.wikipedia.org/wiki/Quantum_machine_learning Content Quantum machine learning is the integration of quantum algorithms within machine learning programs. The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer, i.e. quantum-enhanced machine learning. While machine learning algorithms are used to compute immense quantities of data, quantum machine learning utilizes qubits and quantum operations or specialized quantum systems to improve computational speed and data storage done by algorithms in a program. This includes hybrid methods that involve both classical and quantum processing, where computationally difficult subroutines are outsourced to a quantum device. These routines can be more complex in nature and executed faster on a quantum computer. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. Beyond quantum computing, the term quantum machine learning is also associated with classical machine learning methods applied to data generated from quantum experiments (i.e. machine learning of quantum systems), such as learning the phase transitions of a quantum system or creating new quantum experiments. Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics are applicable to classical deep learning and vice versa. Furthermore, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as quantum learning theory. Machine learning with quantum computers Quantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving and often expediting classical machine learning techniques. Such algorithms typically require one to encode the given classical data set into a quantum computer to make it accessible for quantum information processing. Subsequently, quantum information processing routines are applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit reveals the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices. Quantum associative memories and quantum pattern recognition Associative (or content-addressable memories) are able to recognize stored content on the basis of a similarity measure, rather than fixed addresses, like in random access memories. As such they must be able to retrieve both incomplete and corrupted patterns, the essential machine learning task of pattern recognition. Typical classical associative memories store p patterns in the O ( n 2 ) displaystyle O(n2) interactions (synapses) of a real, symmetric energy matrix over a network of n artificial neurons. The encoding is such that the desired patterns are local minima of the energy functional and retrieval is done by minimizing the total energy, starting from an initial configuration. Unfortunately, classical associative memories are severely limited by the phenomenon of cross-talk. When too many patterns are stored, spurious memories appear which quickly proliferate, so that the energy landscape becomes disordered and no retrieval is anymore possible. The number of storable patterns is typically limited by a linear function of the number of neurons, p O ( n ) displaystyle pleq O(n) . Quantum associative memories (in their simplest realization) store patterns in a unitary matrix U acting on the Hilbert space of n qubits. Retrieval is realized by the unitary evolution of a fixed initial state to a quantum superposition of the desired patterns with probability distribution peaked on the most similar pattern to an input. By its very quantum nature, the retrieval process is thus probabilistic. Because quantum associative memories are free from cross-talk, however, spurious memories are never generated. Correspondingly, they have a superior capacity than classical ones. The number of parameters in the unitary matrix U is O ( p n ) displaystyle O(pn) . One can thus have efficient, spurious-memory-free quantum associative memories for any polynomial number of patterns. Linear algebra simulation with quantum amplitudes A number of quantum algorithms for machine learning are based on the idea of amplitude encoding, that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations. Since a state of n displaystyle n qubits is described by 2 n displaystyle 2n complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits n displaystyle n , which amounts to a logarithmic time complexity in the number of amplitudes and thereby the dimension of the input. Many quantum machine learning algorithms in this category are based on variations of the quantum algorithm for linear systems of equations (colloquially called HHL, after the papers authors) which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entry wise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse or low rank. For reference, any known classical algorithm for matrix inversion requires a number of operations that grows more than quadratically in the dimension of the matrix (e.g. O ( n 2.373 ) displaystyle Omathord left(n2.373right) ), but they are not restricted to sparse matrices. Quantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression, the least-squares version of support vector machines, and Gaussian processes. A crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases, this step easily hides the complexity of the task. Variational Quantum Algorithms (VQAs) VQAs are one of the most studied classes of quantum algorithms, as modern research demonstrates their applicability to the vast majority of known major applications of the quantum computer, and they appear to be a leading hope for gaining quantum supremacy. VQAs are a mixed quantum-classical approach where the quantum processor prepares quantum states and measurement is made and the optimization is done by a classical computer. VQAs are considered best for NISQ as VQAs are noise tolerant compared to other algorithms and give quantum superiority with only a few hundred qubits. Researchers have studied circuit-based algorithms to solve optimization problems and find the ground state energy of complex systems, which were difficult to solve or required a large time to perform the computation using a classical computer. Variational quantum circuits (VQCs) Variational Quantum Circuits also known as Parametrized Quantum Circuits (PQCs) are based on Variational Quantum Algorithms (VQAs). VQCs consist of three parts preparation of initial states, quantum circuit, and measurement. Researchers are extensively studying VQCs, as it uses the power of quantum computation to learn in a short time and also use fewer parameters than its classical counterparts. It is theoretically and numerically proven that we can approximate non-linear functions, like those used in neural networks, on quantum circuits. Due to VQCs superiority, neural network has been replaced by VQCs in Reinforcement Learning tasks and Generative Algorithms. The intrinsic nature of quantum devices towards decoherence, random gate error and measurement errors caused to have high potential to limit the training of the variation circuits. Training the VQCs on the classical devices before employing them on quantum devices helps to overcome the problem of decoherence noise that came through the number of repetitions for training. Quantum binary classifier Pattern reorganization is one of the important tasks of machine learning, binary classification is one of the tools or algorithms to find patterns. Binary classification is used in supervised learning and in unsupervised learning. In quantum machine learning, classical bits are converted to qubits and they are mapped to Hilbert space complex value data are used in a quantum binary classifier to use the advantage of Hilbert space. By exploiting the quantum mechanic properties such as superposition, entanglement, interference the quantum binary classifier produces the accurate result in short period of time. Quantum machine learning algorithms based on Grover search Another approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grovers search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians and the k-nearest neighbors algorithms. Other applications include quadratic speedups in the training of perceptron and the computation of attention. An example of amplitude amplification being used in a machine learning algorithm is Grovers search algorithm minimization. In which a subroutine uses Grovers search algorithm to find an element less than some previously defined element. This can be done with an oracle that determines whether or not a state with a corresponding element is less than the predefined one. Grovers algorithm can then find an element such that our condition is met. The minimization is initialized by some random element in our data set, and iteratively does this subroutine to find the minimum element in the data set. This minimization is notably used in quantum k-medians, and it has a speed up of at least O ( n k ) displaystyle mathcal Oleft(sqrt frac nkright) compared to classical versions of k-medians, where n displaystyle n is the number of data points and k displaystyle k is the number of clusters. Amplitude amplification is often combined with quantum walks to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Googles PageRank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework. Quantum-enhanced reinforcement learning Reinforcement learning is a branch of machine learning distinct from supervised and unsupervised learning, which also admits quantum enhancements. In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical or quantum environment and occasionally receives rewards for its actions, which allows the agent to adapt its behavior in other words, to learn what to do in order to gain more rewards. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Implementations of these kinds of protocols have been proposed for systems of trapped ions and superconducting circuits. A quantum speedup of the agents internal decision-making time has been experimentally demonstrated in trapped ions, while a quantum speedup of the learning time in a fully coherent (quantum) interaction between agent and environment has been experimentally realized in a photonic setup. Quantum annealing Quantum annealing is an optimization technique used to determine the local minima and maxima of a function over a given set of candidate functions. This is a method of discretizing a function with many local minima or maxima in order to determine the observables of the function. The process can be distinguished from Simulated annealing by the Quantum tunneling process, by which particles tunnel through kinetic or potential barriers from a high state to a low state. Quantum annealing starts from a superposition of all possible states of a system, weighted equally. Then the time-dependent Schr dinger equation guides the time evolution of the system, serving to affect the amplitude of each state as time increases. Eventually, the ground state can be reached to yield the instantaneous Hamiltonian of the system. NISQ Circuit as Quantum Model As the depth of the quantum circuit advances on NISQ devices, the noise level rises, posing a significant challenge to accurately computing costs and gradients on training models. The noise tolerance will be improved by using the quantum perceptron and the quantum algorithm on the currently accessible quantum hardware. A regular connection of similar components known as neurons forms the basis of even the most complex brain networks. Typically, a neuron has two operations the inner product and an activation function. As opposed to the activation function, which is typically nonlinear, the inner product is a linear process. With quantum computing, linear processes may be easily accomplished additionally, due to the simplicity of implementation, the threshold function is preferred by the majority of quantum neurons for activation functions. Quantum sampling techniques Sampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications. A computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution. Sampling from generic probabilistic models is hard algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects. Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks. The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset. The D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks. The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward. Reverse annealing has been used as well to solve a fully connected quantum restricted Boltzmann machine. Inspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed. Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines. Quantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template. This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware. Quantum neural networks Quantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models. Quantum neural networks are often defined as an expansion on Deutschs model of a quantum computational network. Within this model, nonlinear and irreversible gates, dissimilar to the Hamiltonian operator, are deployed to speculate the given data set. Such gates make certain phases unable to be observed and generate specific oscillations. Quantum neural networks apply the principals quantum information and quantum computation to classical neurocomputing. Current research shows that QNN can exponentially increase the amount of computing power and the degrees of freedom for a computer, which is limited for a classical computer to its size. A quantum neural network has computational capabilities to decrease the number of steps, qubits used, and computation time. The wave function to quantum mechanics is the neuron for Neural networks. To test quantum applications in a neural network, quantum dot molecules are deposited on a substrate of GaAs or similar to record how they communicate with one another. Each quantum dot can be referred as an island of electric activity, and when such dots are close enough (approximately 10 - 20 nm) electrons can tunnel underneath the islands. An even distribution across the substrate in sets of two create dipoles and ultimately two spin states, up or down. These states are commonly known as qubits with corresponding states of  0 displaystyle 0rangle  and  1 displaystyle 1rangle  in Dirac notation. Quantum Convolution Neural Network A novel design for multi-dimensional vectors that uses circuits as convolution filters is QCNN. It was inspired by the advantages of CNNs and the power of QML. It is made using a combination of a variational quantum circuit(VQC) and a deep neural network(DNN), fully utilizing the power of extremely parallel processing on a superposition of a quantum state with a finite number of qubits. The main strategy is to carry out an iterative optimization process in the NISQ devices, without the negative impact of noise, which is possibly incorporated into the circuit parameter, and without the need for quantum error correction. The quantum circuit must effectively handle spatial information in order for QCNN to function as CNN. The convolution filter is the most basic technique for making use of spatial information. One or more quantum convolutional filters make up a quantum convolutional neural network (QCNN), and each of these filters transforms input data using a quantum circuit that can be created in an organized or randomized way. Three parts that make up the quantum convolutional filter are the encoder, the parameterized quantum circuit (PQC), and the measurement. The quantum convolutional filter can be seen as an extension of the filter in the traditional CNN because it was designed with trainable parameters. Quantum neural networks take advantage of the hierarchical structures, and for each subsequent layer, the number of qubits from the preceding layer is decreased by a factor of two. For n input qubits, these structure have O(log(n)) layers, allowing for shallow circuit depth. Additionally, they are able to avoid barren plateau, one of the most significant issues with PQC-based algorithms, ensuring trainability. Despite the fact that the QCNN model does not include the corresponding quantum operation, the fundamental idea of the pooling layer is also offered to assure validity. In QCNN architecture, the pooling layer is typically placed between succeeding convolutional layers. Its function is to shrink the representations spatial size while preserving crucial features, which allows it to reduce the number of parameters, streamline network computing, and manage over-fitting. Such process can be accomplished applying full Tomography on the state to reduce it all the way down to one qubit and then processed it in subway. The most frequently used unit type in the pooling layer is max pooling, although there are other types as well. Similar to conventional feed-forward neural networks, the last module is a fully connected layer with full connections to all activations in the preceding layer. Translational invariance, which requires identical blocks of parameterized quantum gates within a layer, is a distinctive feature of the QCNN architecture. Dissipative Quantum Neural Network Dissipative QNNs (DQNNs) are constructed from layers of qubits coupled by perceptron called building blocks, which have an arbitrary unitary design. Each node in the network layer of a DQNN is given a distinct collection of qubits, and each qubit is also given a unique quantum perceptron unitary to characterize it. The input states information are transported through the network in a feed-forward fashion, layer-to-layer transition mapping on the qubits of the two adjacent layers, as the name implies. Dissipative term also refers to the fact that the output layer is formed by the ancillary qubits while the input layers are dropped while tracing out the final layer. When performing a broad supervised learning task, DQNN are used to learn a unitary matrix connecting the input and output quantum states. The training data for this task consists of the quantum state and the corresponding classical labels. Inspired by the extremely successful classical Generative adversarial network(GAN), dissipative quantum generative adversarial network (DQGAN) is introduced for unsupervised learning of the unlabeled training data . The generator and the discriminator are the two DQNNs that make up a single DQGAN. The generators goal is to create false training states that the discriminator cannot differentiate from the genuine ones, while the discriminators objective is to separate the real training states from the fake states created by the generator. The relevant features of the training set are learned by the generator by alternate and adversarial training of the networks that aid in the production of sets that extend the training set. DQGAN has a fully quantum architecture and is trained in quantum data. Hidden quantum Markov models Entangled Hidden Markov Models An Entangled Hidden Markov Model (EHMM) is a quantum extension of the classical Hidden Markov Model (HMM), introduced by Abdessatar Souissi and El Gheteb Souedidi. EHMMs establish a bridge between classical probability and quantum entanglement, providing a more profound understanding of quantum systems using observational data. Mathematical Formulation Let ( d_H, d_O ) be two positive integers representing the dimensions of the hidden and observable states, respectively. Define - ( mathcalM_d_H ) as the ( C )-algebra of ( d_H times d_H ) matrices. - ( mathcalM_d_O ) as the ( C )-algebra of ( d_O times d_O ) matrices. - The identity element in ( mathcalM_d_H ) is denoted by ( mathbbI_d_H ). - The Schur (Hadamard) product for two matrices ( A, B in mathcalM_d_H ) is defined as  A diamond .  Define the hidden and observable sample algebras  mathcalA_H  bigotimes_mathbbN mathcalM_d_H, quad mathcalA_O  bigotimes_mathbbN mathcalM_d_O,  with the full sample algebra  mathcalA_H,O  bigotimes_mathbbN (mathcalM_d_H otimes mathcalM_d_O).  Hidden Quantum Markov Models Hidden Quantum Markov Models (HQMMs) are a quantum-enhanced version of classical Hidden Markov Models (HMMs), which are typically used to model sequential data in various fields like robotics and natural language processing. Unlike other quantum-enhanced machine learning algorithms, HQMMs can be viewed as models inspired by quantum mechanics that can be run on classical computers as well. Where classical HMMs use probability vectors to represent hidden belief states, HQMMs use the quantum analogue density matrices. Recent work has extended HQMMs through the introduction of Entangled Hidden Markov Models (EHMMs), which incorporate quantum entanglement into their structure. The EHMM framework builds upon classical HQMMs by defining entangled transition expectations, which allow for enhanced modeling of quantum systems. Additionally, EHMMs have been linked to Matrix Product States (MPS) and provide a new perspective on probabilistic graphical models in quantum settings. Since classical HMMs are a particular kind of Bayes net, HQMMs and EHMMs provide insights into quantum-analogous Bayesian inference, offering new pathways for modeling quantum probability and non-classical correlations in quantum information processing. Furthermore, empirical studies suggest that EHMMs improve the ability to model sequential data when compared to their classical counterparts, though further research is required to fully understand these benefits. Transition Expectation and Emission Operators A linear map ( mathcalE_H  mathcalM_d_H otimes mathcalM_d_H to mathcalM_d_H ) is called a transition expectation if it is completely positive and identity-preserving cite. Similarly, a linear map ( mathcalE_H,O  mathcalM_d_H otimes mathcalM_d_O to mathcalM_d_H ) is called an emission operator if it is completely positive and identity-preserving. Fully quantum machine learning In the most general case of quantum machine learning, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic. One class of problem that can benefit from the fully quantum approach is that of learning unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case. (This also relates to work on quantum pattern matching.) The problem of learning unitary transformations can be approached in a similar way. Going beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum. Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in, where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning. Such a speedup in the reinforcement-learning paradigm has been experimentally demonstrated in a photonic setup. Explainable quantum machine learning The need for models that can be understood by humans emerges in quantum machine learning in analogy to classical machine learning and drives the research field of explainable quantum machine learning (or XQML in analogy to XAI/XML). These efforts are often also referred to as Interpretable Machine Learning (IML, and by extension IQML). XQML/IQML can be considered as an alternative research direction instead of finding a quantum advantage. For example, XQML has been used in the context of mobile malware detection and classification. Quantum Shapley values have also been proposed to interpret gates within a circuit based on a game-theoretic approach. For this purpose, gates instead of features act as players in a coalitional game with a value function that depends on measurements of the quantum circuit of interest. Additionally, a quantum version of the classical technique known as LIME (Linear Interpretable Model-Agnostic Explanations) has also been proposed, known as Q-LIME. Classical learning applied to quantum problems The term quantum machine learning sometimes refers to classical machine learning performed on data from quantum systems. A basic example of this is quantum state tomography, where a quantum state is learned from measurement. Other applications include learning Hamiltonians and automatically generating quantum experiments. Quantum learning theory Quantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider specific problems and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained. The starting point in learning theory is typically a concept class, a set of possible concepts. Usually a concept is a function on some domain, such as  0 , 1  n displaystyle 0,1n . For example, the concept class could be the set of disjunctive normal form (DNF) formulas on n bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn (exactly or approximately) an unknown target concept from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it. In active learning, a learner can make membership queries to the target concept c, asking for its value c(x) on inputs x chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of quantum exact learning, the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more. If complexity is measured by the amount of time the learner uses, then there are concept classes that can be learned efficiently by quantum learners but not by classical learners (under plausible complexity-theoretic assumptions). A natural model of passive learning is Valiants probably approximately correct (PAC) learning. Here the learner receives random examples (x,c(x)), where x is distributed according to some unknown distribution D. The learners goal is to output a hypothesis function h such that h(x)c(x) with high probability when x is drawn according to D. The learner has to be able to produce such an approximately correct h for every D and every target concept c in its concept class. We can consider replacing the random examples by potentially more powerful quantum examples x D ( x )  x , c ( x ) displaystyle sum _xsqrt D(x)x,c(x)rangle  . In the PAC model (and the related agnostic model), this doesnt significantly reduce the number of examples needed for every concept class, classical and quantum sample complexity are the same up to constant factors. However, for learning under some fixed distribution D, quantum examples can be very helpful, for example for learning DNF under the uniform distribution. When considering time complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions). This passive learning type is also the most common scheme in supervised learning a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis h is a step of induction. Classically, an inductive model splits into a training and an application phase the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources. Implementations and experiments The earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009. Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer. A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits. Using a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number 6 and 9 on a liquid-state quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal. Photonic implementations are attracting more attention, not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013. Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule. A core building block in many learning algorithms is to calculate the distance between two vectors this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015. Recently, based on a neuromimetic approach, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor. This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed, and an experiment with quantum dots performed. A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network. Since 2016, IBM has launched an online cloud-based platform for quantum software developers, called the IBM Q Experience. This platform consists of several fully operational quantum processors accessible via the IBM Web API. In doing so, the company is encouraging software developers to pursue new algorithms through a development environment with quantum capabilities. New architectures are being explored on an experimental basis, up to 32 qubits, using both trapped-ion and superconductive quantum computing methods. In October 2019, it was noted that the introduction of Quantum Random Number Generators (QRNGs) to machine learning models including Neural Networks and Convolutional Neural Networks for random initial weight distribution and Random Forests for splitting processes had a profound effect on their ability when compared to the classical method of Pseudorandom Number Generators (PRNGs). However, in a more recent publication from 2021, these claims could not be reproduced for Neural Network weight initialization and no significant advantage of using QRNGs over PRNGs was found. The work also demonstrated that the generation of fair random numbers with a gate quantum computer is a non-trivial task on NISQ devices, and QRNGs are therefore typically much more difficult to use in practice than PRNGs. A paper published in December 2018 reported on an experiment using a trapped-ion system demonstrating a quantum speedup of the deliberation time of reinforcement learning agents employing internal quantum hardware. In March 2021, a team of researchers from Austria, The Netherlands, the US and Germany reported the experimental demonstration of a quantum speedup of the learning time of reinforcement learning agents interacting fully quantumly with the environment. The relevant degrees of freedom of both agent and environment were realized on a compact and fully tunable integrated nanophotonic processor. Skepticism While machine learning itself is now not only a research field but an economically significant and fast growing industry and quantum computing is a well established field of both theoretical and experimental research, quantum machine learning remains a purely theoretical field of studies. Attempts to experimentally demonstrate concepts of quantum machine learning remain insufficient. Further, another obstacle exists at the prediction stage because the outputs of quantum learning models are inherently random. This creates an often considerable overhead, as many executions of a quantum learning model have to be aggregated to obtain an actual prediction. Many of the leading scientists that extensively publish in the field of quantum machine learning warn about the extensive hype around the topic and are very restrained if asked about its practical uses in the foreseeable future. Sophia Chen collected some of the statements made by well known scientists in the field I think we havent done our homework yet. This is an extremely new scientific field, - physicist Maria Schuld of Canada-based quantum computing startup Xanadu. When mixing machine learning with quantum, you catalyse a hype-condensate. - Jacob Biamonte a contributor to the theory of quantum computation. There is a lot more work that needs to be done before claiming quantum machine learning will actually work, - computer scientist Iordanis Kerenidis, the head of quantum algorithms at the Silicon Valley-based quantum computing startup QC Ware. I have not seen a single piece of evidence that there exists a meaningful machine learning task for which it would make sense to use a quantum computer and not a classical computer, - physicist Ryan Sweke of the Free University of Berlin in Germany. Dont fall for the hype! - Frank Zickert, who is the author of probably the most practical book related to the subject beware that quantum computers are far away from advancing machine learning for their representation ability , and even speaking about evaluation and optimization for any kind of useful task quantum supremacy is not yet achieved. Furthermore, nobody among the active researchers in the field make any forecasts about when it could possibly become practical. See also Differentiable programming Quantum computing Quantum algorithm for linear systems of equations Quantum annealing Quantum neural network Quantum image Title Rabbit r1 URL https//en.wikipedia.org/wiki/Rabbit_r1 Content The Rabbit is an Android-powered, ChatGPT-based personal assistant device developed by tech startup Rabbit Inc and co-designed by Teenage Engineering. It is designed to perform various functions, including web searches and media control, using voice commands and touch interaction, allowing AI to be used to provide services commonly associated with smartphones, such as ordering food delivery. Rabbit Inc was started by Jesse Lyu Cheng. Hardware Display A 2.88-inch touchscreen for interactive user input. Input push-to-talk button to activate voice commands scroll wheel Gyroscope Magnetometer Accelerometer GPS. Camera 8 MP single camera, with a resolution of 3264x2448, allowing for the connected external AI to use computer vision Audio Equipped with a speaker and dual microphones for audio interaction. Connectivity Supports Wi-Fi and cellular connections via a SIM card slot to access internet services. Processor Runs on a 2.3GHz MediaTek Helio processor. Memory Contains 4GB of RAM for operational tasks. Storage Offers 128GB of internal storage for data. Ports Utilizes a USB-C port for charging and data connections. Software The Rabbit runs on Rabbit OS, based on the Android Open Source Project (AOSP), specifically version 13. Lyu has claimed that Rabbit OS runs with a very bespoke AOSP. The device employs a large action model (LAM) designed to perform actions and assist with tasks like web searches, streaming music, and transportation services. Perplexity.ai, an AI search engine, is one of the Large Language Models (LLM) used to respond to user queries and execute commands. The personal assistant is also capable of various actions such as ordering a cab or playing music from Spotify. This is through the connections system on the account management site, which the assistant calls rabbits. Rabbit issued 15 software updates within the first four months after releasing . On July 11, 2024, Rabbit launched the beta rabbit advanced search and conversation assistant to give more thoughtful responses to complex questions that require multiple steps of research and deeper reasoning. Reception Funding Rabbit raised 10 million in funding in December 2023. Sales Following its announcement at the 2024 Consumer Electronics Show, 130,000 units were sold. On August 13, 2024, Rabbit announced that sales of had expanded to the entire European Union (except Malta) and United Kingdom. On August 21, 2024, sales of expanded to Singapore. Reviews The was met with strong criticism immediately after Rabbit began shipping the device. Some reviews questioned what the device was able to do that a smartphone could not, while comparing it to the similar Humane Ai Pin. YouTuber Marques Brownlee called the device barely reviewable. Android Authoritys Mishaal Rahman managed to install Rabbit s software on a Pixel 6a smartphone, after a tipster shared an APK file. The Verge echoed the claims made by Rahman. In response, Lyu published statements confirming its use of Android, but denying that the is an Android app. Mashable called its Vision features impressive, but said that these praise-worthy features are overshadowed by buggy performance. Ars Technica wrote a blog post claiming the company is blocking access from bootleg APKs. TechCrunch gave a slightly more positive review, calling the device a fun peep at a possible future, but could not advise anyone to buy one now. Shortly after the launch of , Rabbit began a weekly cadence of software updates to address much of the criticism from the early reviews, including battery and GPS performance, time zone selection, and more. Digital Trends said the Magic Camera feature takes the most mundane, ordinary, and badly composed photos and makes something fun and eye-catching from them. Mashable said the beta rabbit feature makes Rabbit more conversational and intelligent. Controversies GAMA project Rabbit Inc has garnered attention due to allegations surrounding its funding and the companys past projects. The company came under scrutiny when Stephen Findeisen, known as Coffeezilla on YouTube, published a video in May 2024, alleging that Rabbit Incorporation was built on a scam. Rabbit Incorporation, initially named Cyber Manufacturing Co, rebranded just two months before launching the Rabbit . The company, under its former name, raised 6 million in November 2021 for a project called GAMA, described as a Next Generation NFT Project. Jesse Lyu, the CEO of Rabbit Incorporation, referred to GAMA as a fun little project. Coffeezilla, who investigates influencer scams, highlighted old Clubhouse recordings of Jesse Lyu discussing the GAMA project. In these recordings, Lyu emphasized the substantial funding behind GAMA and its potential to be a revolutionary, carbon-negative cryptocurrency. Coffeezilla questioned the whereabouts of the funds raised for GAMA, estimating that approximately 1 million in refunds to investors remained unresolved. He suggested that the rebranding to Rabbit Incorporation and the shift to developing the Rabbit were attempts to divert from the GAMA projects issues. In response to Coffeezillas inquiries, Rabbit Incorporation stated that the 6 million raised was indeed used for the GAMA project. The company said that NFTs cannot be refunded unless the owner agrees to burn them on the blockchain. Rabbit Incorporation also said that the GAMA project was open-sourced and returned to the community, aligning with community feedback. They also mentioned that efforts to buy back NFTs were made to counteract malicious trading and maintain market stability. Security In June 2024, Engadget reported that the Rabbitude team, a community reverse engineering project, had gained access to the s codebase revealing that s software contained several hardcoded API keys in its code for ElevenLabs, Microsoft Azure, Yelp, and Google Maps, potentially allowing unauthorized access to responses, including those containing the users personal information. For a short time, Rabbit immediately began revoking and rotating those secrets and confirmed that the code was leaked by an employee who had been terminated and remains under investigation. Additionally, on June 26, 2024, a YouTuber known as Bringus Studios found that the security of the was so bad, he was able to hack it to get stock Android running on it. In July 2024, the company revealed that all user chats and device pairing data were logged on the with no ability to delete them. This meant that lost or stolen devices could be used to extract user data. The company stated that it addressed the issue by introducing a factory reset option and limited the data stored on the , as well as preventing paired devices from reading data. Title Rademacher complexity URL https//en.wikipedia.org/wiki/Rademacher_complexity Content In computational learning theory (machine learning and theory of computation), Rademacher complexity, named after Hans Rademacher, measures richness of a class of sets with respect to a probability distribution. The concept can also be extended to real valued functions. Definitions Rademacher complexity of a set Given a set A R m displaystyle Asubseteq mathbb R m , the Rademacher complexity of A is defined as follows 326 Rad ( A )  1 m E  sup a A .e. Pr ( ) . Some authors take the absolute value of the sum before taking the supremum, but if A displaystyle A is symmetric this makes no difference. Rademacher complexity of a function class Let  . Then, the empirical Rademacher complexity of F displaystyle mathcal F given S displaystyle S is defined as Rad S ( F )  1 m E  sup f F  .e. F S   ( f ( z 1 ) , , f ( z m ) ) f F  displaystyle mathcal Fcirc S(f(z_1),ldots ,f(z_m))mid fin mathcal F The worst case empirical Rademacher complexity is Rad m ( F )  sup  . The Rademacher complexity of the function class F displaystyle mathcal F with respect to P displaystyle P for sample size m displaystyle m is Rad P , m ( F )  E S P m  Rad S ( F )  displaystyle operatorname Rad _P,m(mathcal F)mathbb E _Ssim Pmleftoperatorname Rad _S(mathcal F)right where the above expectation is taken over an identically independently distributed (i.i.d.) sample  . Intuition The Rademacher complexity is typically applied on a function class of models that are used for classification, with the goal of measuring their ability to classify points drawn from a probability space under arbitrary labellings. When the function class is rich enough, it contains functions that can appropriately adapt for each arrangement of labels, simulated by the random draw of i displaystyle sigma _i under the expectation, so that this quantity in the sum is maximised. Examples 1. A displaystyle A contains a single vector, e.g.,  . Then Rad ( A )  1 2 ( 1 4 ( a  b )  1 4 ( a b )  1 4 ( a  b )  1 4 ( a b ) )  0 displaystyle operatorname Rad (A)1 over 2cdot left(1 over 4cdot (ab)1 over 4cdot (a-b)1 over 4cdot (-ab)1 over 4cdot (-a-b)right)0 The same is true for every singleton hypothesis class. 56 2. A displaystyle A contains two vectors, e.g.,  . Then Rad ( A )  1 2 ( 1 4 max ( 1  1 , 1  2 )  1 4 max ( 1 1 , 1 2 )  1 4 max ( 1  1 , 1  2 )  1 4 max ( 1 1 , 1 2 ) )  1 8 ( 3  0  1 2 )  1 4 displaystyle beginalignedoperatorname Rad (A)1 over 2cdot left(1 over 4cdot max(11,12)1 over 4cdot max(1-1,1-2)1 over 4cdot max(-11,-12)1 over 4cdot max(-1-1,-1-2)right)5pt1 over 8(301-2)1 over 4endaligned Using the Rademacher complexity The Rademacher complexity can be used to derive data-dependent upper-bounds on the learnability of function classes. Intuitively, a function-class with smaller Rademacher complexity is easier to learn. Bounding the representativeness In machine learning, it is desired to have a training set that represents the true distribution of some sample data S displaystyle S . This can be quantified using the notion of representativeness. Denote by P displaystyle P the probability distribution from which the samples are drawn. Denote by H displaystyle H the set of hypotheses (potential classifiers) and denote by F displaystyle mathcal F the corresponding set of error functions, i.e., for every hypothesis h H displaystyle hin H , there is a function f h F displaystyle f_hin F , that maps each training sample (features,label) to the error of the classifier h displaystyle h (note in this case hypothesis and classifier are used interchangeably). For example, in the case that h displaystyle h represents a binary classifier, the error function is a 0 1 loss function, i.e. the error function f h displaystyle f_h returns 0 if h displaystyle h correctly classifies a sample and 1 else. We omit the index and write f displaystyle f instead of f h displaystyle f_h when the underlying hypothesis is irrelevant. Define L P ( f )  E z P  f ( z )  displaystyle L_P(f)mathbb E _zsim Pf(z) the expected error of some error function f F displaystyle fin mathcal F on the real distribution P displaystyle P  L S ( f )  1 m  . The representativeness of the sample S displaystyle S , with respect to P displaystyle P and F displaystyle mathcal F , is defined as Rep P ( F , S )  sup f F ( L P ( f ) L S ( f ) ) displaystyle operatorname Rep _P(mathcal F,S)sup _fin F(L_P(f)-L_S(f)) Smaller representativeness is better, since it provides a way to avoid overfitting it means that the true error of a classifier is not much higher than its estimated error, and so selecting a classifier that has low estimated error will ensure that the true error is also low. Note however that the concept of representativeness is relative and hence can not be compared across distinct samples. The expected representativeness of a sample can be bounded above by the Rademacher complexity of the function class If F displaystyle mathcal F is a set of functions with range within  0 , 1  displaystyle 0,1 , then 326 Rad P , m ( F ) ln 2 2 m E S P m  Rep P ( F , S )  2 Rad P , m ( F ) displaystyle operatorname Rad _P,m(mathcal F)-sqrt frac ln 22mleq mathbb E _Ssim Pmoperatorname Rep _P(mathcal F,S)leq 2operatorname Rad _P,m(mathcal F) Furthermore, the representativeness is concentrated around its expectation For any displaystyle epsilon  , with probability 1 2 e 2 2 m displaystyle geq 1-2e-2epsilon 2m , Rep P ( F , S ) E S P m  Rep P ( F , S )  displaystyle operatorname Rep _P(mathcal F,S)in mathbb E _Ssim Pmoperatorname Rep _P(mathcal F,S)pm epsilon  Bounding the generalization error The Rademacher complexity is a theoretical justification for empirical risk minimization. When the error function is binary (0-1 loss), for every  0 displaystyle delta 0 , sup f F ( L P ( f ) L S ( f ) ) 2 Rad S ( F )  4 2 ln ( 4 / ) m displaystyle sup _fin mathcal F(L_P(f)-L_S(f))leq 2operatorname Rad _S(mathcal F)4sqrt 2ln(4/delta ) over m with probability at least 1 displaystyle 1-delta  . 328 There exists a constant c  0 displaystyle c0 , such that when the error function is squared ( y  , y )  ( y  y ) 2 displaystyle ell (hat y,y)(hat y-y)2 , and the function class F displaystyle mathcal F consists of functions with range within  1 ,  1  displaystyle -1,1 , then for any  0 displaystyle delta 0 L P ( f ) L S ( f ) c  L S ( f )  ( ln m ) 4 Rad m ( F ) 2  ln ( 1 / ) m  , f F displaystyle L_P(f)-L_S(f)leq cleftL_S(f)(ln m)4overline operatorname Rad _m(mathcal F)2frac ln(1/delta )mright,quad forall fin mathcal F with probability at least 1 displaystyle 1-delta  . Theorem 2.2 Oracle inequalities Let the Bayes risk . Let the function class F displaystyle mathcal F be split into complexity classes F r displaystyle mathcal F_r , where r R displaystyle rin mathbb R  are levels of complexity. Let p r displaystyle p_r be real numbers. Let the complexity measure function p displaystyle p be defined such that p ( f )  min  p r  f F r  displaystyle p(f)minp_rfin mathcal F_r . For any dataset S displaystyle S , let f  displaystyle hat f be a minimizer of L S ( f )  p ( f ) displaystyle L_S(f)p(f) . If sup f F r  L P ( f ) L S ( f )  p r , r displaystyle sup _fin mathcal F_rL_P(f)-L_S(f)leq p_r,quad forall r then we have the oracle inequality L ( f  ) L inf r ( inf f F r L ( f ) L  2 p r ) displaystyle L(hat f)-Lleq inf _rleft(inf _fin mathcal F_rL(f)-L2p_rright) Define f r arg min f F r L ( f ) displaystyle f_rin arg min _fin mathcal F_rL(f) . If we further assume r s implies F r F s and p r p s displaystyle rleq stext implies mathcal F_rsubseteq mathcal F_stext and p_rleq p_s and r , sup f F r ( L P ( f ) L P ( f r ) 2 ( L S ( f ) L S ( f r ) ) ) 2 p r / 7 sup f F r ( L S ( f ) L S ( f r ) 2 ( L P ( f ) L P ( f r ) ) ) 2 p r / 7 displaystyle beginalignedforall r,sup _fin mathcal F_rleft(L_P(f)-L_Pleft(f_rright)-2left(L_S(f)-L_Sleft(f_rright)right)right)leq 2p_r/7sup _fin mathcal F_rleft(L_S(f)-L_Sleft(f_rright)-2left(L_P(f)-L_Pleft(f_rright)right)right)leq 2p_r/7endaligned then we have the oracle inequality L P ( f  ) L inf r ( inf f F r L P ( f ) L  3 p r ) displaystyle L_P(widehat f)-Lleq inf _rleft(inf _fin mathcal F_rL_P(f)-L3p_rright)  Theorem 2.3 Bounding the Rademacher complexity Since smaller Rademacher complexity is better, it is useful to have upper bounds on the Rademacher complexity of various function sets. The following rules can be used to upper-bound the Rademacher complexity of a set A R m displaystyle Asubset mathbb R m . 329 330 1. If all vectors in A displaystyle A are translated by a constant vector a 0 R m displaystyle a_0in mathbb R m , then Rad(A) does not change. 2. If all vectors in A displaystyle A are multiplied by a scalar c R displaystyle cin mathbb R  , then Rad(A) is multiplied by  c  displaystyle c . 3. Rad ( A  B )  Rad ( A )  Rad ( B ) displaystyle operatorname Rad (AB)operatorname Rad (A)operatorname Rad (B) . 56 4. (Kakade  Tewari Lemma) If all vectors in A displaystyle A are operated by a Lipschitz function, then Rad(A) is (at most) multiplied by the Lipschitz constant of the function. In particular, if all vectors in A displaystyle A are operated by a contraction mapping, then Rad(A) strictly decreases. 5. The Rademacher complexity of the convex hull of A displaystyle A equals Rad(A). 6. (Massart Lemma) The Rademacher complexity of a finite set grows logarithmically with the set size. Formally, let A displaystyle A be a set of N displaystyle N vectors in R m displaystyle mathbb R m , and let a displaystyle bar a be the mean of the vectors in A displaystyle A . Then Rad ( A ) max a A a a 2 log N m displaystyle operatorname Rad (A)leq max _ain Aa-bar acdot sqrt 2log N over m In particular, if A displaystyle A is a set of binary vectors, the norm is at most m displaystyle sqrt m , so Rad ( A ) 2 log N m displaystyle operatorname Rad (A)leq sqrt 2log N over m Bounds related to the VC dimension Let H displaystyle H be a set family whose VC dimension is d displaystyle d . It is known that the growth function of H displaystyle H is bounded as for all m  d  1 displaystyle md1  Growth ( H , m ) ( e m / d ) d displaystyle operatorname Growth (H,m)leq (em/d)d This means that, for every set h displaystyle h with at most m displaystyle m elements,  H h  ( e m / d ) d displaystyle Hcap hleq (em/d)d . The set-family H h displaystyle Hcap h can be considered as a set of binary vectors over R m displaystyle mathbb R m . Substituting this in Massarts lemma gives Rad ( H h ) 2 d log ( e m / d ) m displaystyle operatorname Rad (Hcap h)leq sqrt 2dlog(em/d) over m With more advanced techniques (Dudleys entropy bound and Hausslers upper bound) one can show, for example, that there exists a constant C displaystyle C , such that any class of  0 , 1  displaystyle 0,1 -indicator functions with Vapnik Chervonenkis dimension d displaystyle d has Rademacher complexity upper-bounded by C d m displaystyle Csqrt frac dm . Bounds related to linear classes The following bounds are related to linear operations on S displaystyle S a constant set of m displaystyle m vectors in R n displaystyle mathbb R n . 332 333 1. Define A 2   ( w x 1 , , w x m ) w 2 1   displaystyle A_2(wcdot x_1,ldots ,wcdot x_m)mid w_2leq 1 the set of dot-products of the vectors in S displaystyle S with vectors in the unit ball. Then Rad ( A 2 ) max i x i 2 m displaystyle operatorname Rad (A_2)leq max _ix_i_2 over sqrt m 2. Define A 1   ( w x 1 , , w x m ) w 1 1   displaystyle A_1(wcdot x_1,ldots ,wcdot x_m)mid w_1leq 1 the set of dot-products of the vectors in S displaystyle S with vectors in the unit ball of the 1-norm. Then Rad ( A 1 ) max i x i 2 log ( 2 n ) m displaystyle operatorname Rad (A_1)leq max _ix_i_infty cdot sqrt 2log(2n) over m Bounds related to covering numbers The following bound relates the Rademacher complexity of a set A displaystyle A to its external covering number the number of balls of a given radius r displaystyle r whose union contains A displaystyle A . The bound is attributed to Dudley. 338 Suppose A R m displaystyle Asubset mathbb R m is a set of vectors whose length (norm) is at most c displaystyle c . Then, for every integer M  0 displaystyle M0  Rad ( A ) c 2 M m  6 c m .i.d. random variables with zero-mean and variance 1, i.e. g i N ( 0 , 1 ) displaystyle g_isim mathcal N(0,1) . Gaussian and Rademacher complexities are known to be equivalent up to logarithmic factors. Equivalence of Rademacher and Gaussian complexity Given a set A R n displaystyle Asubseteq mathbb R n then it holds that G ( A ) 2 log n Rad ( A ) 2 G ( A ) displaystyle frac G(A)2sqrt log nleq textRad(A)leq sqrt frac pi 2G(A) Where G ( A ) displaystyle G(A) is the Gaussian Complexity of A. As an example, consider the rademacher and gaussian complexities of the ball. The Rademacher complexity is given by exactly 1, whereas the Gaussian complexity is on the order of log d displaystyle sqrt log d (which can be shown by applying known properties of suprema of a set of subgaussian random variables). References Peter L. Bartlett, Shahar Mendelson (2002) Rademacher and Gaussian Complexities Risk Bounds and Structural Results. Journal of Machine Learning Research 3 463 482 Giorgio Gnecco, Marcello Sanguineti (2008) Approximation Error Bounds via Rademachers Complexity. Applied Mathematical Sciences, Vol. 2, 2008, no. 4, 153 176 Title Random feature URL https//en.wikipedia.org/wiki/Random_feature Content Random features (RF) are a technique used in machine learning to approximate kernel methods, introduced by Ali Rahimi and Ben Recht in their 2007 paper Random Features for Large-Scale Kernel Machines, and extended by. RF uses a Monte Carlo approximation to kernel functions by randomly sampled feature maps. It is used for datasets that are too large for traditional kernel methods like support vector machine, kernel ridge regression, and gaussian process. Mathematics Kernel method Given a feature map  R d V textstyle phi mathbb R dto V , where V textstyle V is a Hilbert space (more specifically, a reproducing kernel Hilbert space), the kernel trick replaces inner products in feature space ( x i ) , ( x j ) V displaystyle langle phi (x_i),phi (x_j)rangle _V by a kernel function k ( x i , x j )  R d R d R displaystyle k(x_i,x_j)mathbb R dtimes mathbb R dto mathbb R  Kernel methods replaces linear operations in high-dimensional space by operations on the kernel matrix K X   k ( x i , x j )  i , j 1  N displaystyle K_Xk(x_i,x_j)_i,jin 1N where N textstyle N is the number of data points. Random kernel method The problem with kernel methods is that the kernel matrix K X textstyle K_X has size N N textstyle Ntimes N . This becomes computationally infeasible when N textstyle N reaches the order of a million. The random kernel method replaces the kernel function k textstyle k by an inner product in low-dimensional feature space R D textstyle mathbb R D  k ( x , y ) z ( x ) , z ( y ) displaystyle k(x,y)approx langle z(x),z(y)rangle  where z textstyle z is a randomly sampled feature map z  R d R D textstyle zmathbb R dto mathbb R D . This converts kernel linear regression into linear regression in feature space, kernel SVM into SVM in feature space, etc. Since we have K X Z X T Z X displaystyle K_Xapprox Z_XTZ_X where Z ) . Random Fourier feature Radial basis function kernel The radial basis function (RBF) kernel on two samples x i , x j R d displaystyle x_i,x_jin mathbb R d is defined as k ( x i , x j )  exp ( x i x j 2 2 2 ) displaystyle k(x_i,x_j)exp left(-frac x_i-x_j22sigma 2right) where x i x j 2 displaystyle x_i-x_j2 is the squared Euclidean distance and displaystyle sigma  is a free parameter defining the shape of the kernel. It can be approximated by a random Fourier feature map z  R d R 2 D displaystyle zmathbb R dto mathbb R 2D  z ( x )  1 D  cos 1 , x , sin 1 , x , , cos D , x , sin D , x  T displaystyle z(x)frac 1sqrt Dcos langle omega _1,xrangle ,sin langle omega _1,xrangle ,ldots ,cos langle omega _D,xrangle ,sin langle omega _D,xrangle T where 1 , . . . , D displaystyle omega _1,...,omega _D are IID samples from the multidimensional normal distribution N ( 0 , 2 I ) displaystyle N(0,sigma -2I) . Since cos , sin displaystyle cos ,sin  are bounded, there is a stronger convergence guarantee by Hoeffdings inequality. Claim 1 Random Fourier features By Bochners theorem, the above construction can be generalized to arbitrary positive definite shift-invariant kernel k ( x , y )  k ( x y ) displaystyle k(x,y)k(x-y) . Define its Fourier transform p ( )  1 2 R d e j , k ( ) d displaystyle p(omega )frac 12pi int _mathbb R de-jlangle omega ,Delta rangle k(Delta )dDelta  then 1 , . . . , D displaystyle omega _1,...,omega _D are sampled IID from the probability distribution with probability density p displaystyle p . This applies for other kernels like the Laplace kernel and the Cauchy kernel. Neural network interpretation Given a random Fourier feature map z displaystyle z , training the feature on a dataset by featurized linear regression is equivalent to fitting complex parameters 1 , , D C displaystyle theta _1,dots ,theta _Din mathbb C  such that f ( x )  R e ( k k e i k , x ) displaystyle f_theta (x)mathrm Re left(sum _ktheta _keilangle omega _k,xrangle right) which is a neural network with a single hidden layer, with activation function t e i t displaystyle tmapsto eit , zero bias, and the parameters in the first layer frozen. In the overparameterized case, when 2 D N displaystyle 2Dgeq N , the network linearly interpolates the dataset  ( x i , y i )  i 1  N displaystyle (x_i,y_i)_iin 1N , and the network parameters is the least-norm solution   arg min C D , f ( x k )  y k k 1  N displaystyle hat theta arg min _theta in mathbb C D,f_theta (x_k)y_kforall kin 1Ntheta  At the limit of D displaystyle Dto infty  , the norm  f K H displaystyle hat theta to f_K_H where f K displaystyle f_K is the interpolating function obtained by the kernel regression with the original kernel, and H displaystyle cdot _H is the norm in the reproducing kernel Hilbert space for the kernel. Other examples Random binning features A random binning features map partitions the input space using randomly shifted grids at randomly chosen resolutions and assigns to an input point a binary bit string that corresponds to the bins in which it falls. The grids are constructed so that the probability that two points x i , x j R d displaystyle x_i,x_jin mathbb R d are assigned to the same bin is proportional to K ( x i , x j ) displaystyle K(x_i,x_j) . The inner product between a pair of transformed points is proportional to the number of times the two points are binned together, and is therefore an unbiased estimate of K ( x i , x j ) displaystyle K(x_i,x_j) . Since this mapping is not smooth and uses the proximity between input points, Random Binning Features works well for approximating kernels that depend only on the L 1 displaystyle L_1 distance between datapoints. Orthogonal random features Orthogonal random features uses a random orthogonal matrix instead of a random Fourier matrix. Historical context In NIPS 2006, deep learning had just become competitive with linear models like PCA and linear SVMs for large datasets, and people speculated about whether it could compete with kernel SVMs. However, there was no way to train kernel SVM on large datasets. The two authors developed the random feature method to train those. It was then found that the O ( 1 / D ) displaystyle O(1/D) variance bound did not match practice the variance bound predicts that approximation to within 0.01 displaystyle 0.01 requires D 10 4 displaystyle Dsim 104 , but in practice required only 10 2 displaystyle sim 102 . Attempting to discover what caused this led to the subsequent two papers. See also Kernel method Support vector machine Fourier transform Monte Carlo method References External links Random Walks - Random Fourier features Title Reasoning language model URL https//en.wikipedia.org/wiki/Reasoning_language_model Content Reasoning language models are artificial intelligence systems that combine natural language processing with structured reasoning capabilities. These models are usually constructed by prompting, supervised finetuning (SFT), and reinforcement learning (RL) initialized with pretrained language models. Prompting A language model is a generative model of a training dataset of texts. Prompting means constructing a text prompt, such that, conditional on the text prompt, the language model generates a solution to the task. Prompting can be applied to a pretrained model (base model), a base model that has undergone SFT, or RL, or both. Chain of thought Chain of Thought prompting (CoT) prompts the model to answer a question by first generating a chain of thought, i.e. steps of reasoning that mimic a train of thought. It was published in 2022 by the Brain team of Google on the PaLM-540B model. In CoT prompting, the prompt is of the form Input Lets think step by step, and the model would respond with a chain of reasoning steps, ended with an answer Input Step 1 Step 2 Step n Reasoning chain Answer displaystyle textInputrightarrow underbrace textStep_1rightarrow textStep_2rightarrow cdots rightarrow textStep_n _textReasoning chainrightarrow textAnswer Similarly, Tree of Thought prompting generalizes CoT by prompting the model to generate one or more possible next steps, and then running the model on each of the possible next steps by breadth-first, beam, or some other method of tree search. Similarly, Graph of Thought generalizes CoT so that the reasoning steps form a directed acyclic graph. Self-consistency decoding performs several chain-of-thought rollouts, then selects the most commonly reached conclusion out of all the rollouts. If the rollouts disagree by a lot, a human can be queried for the correct chain of thought. Retrieval-augmented generation A language model may answer a query by first querying a database of documents using the query. The document retrieval can be via a vector database, summary index, tree index, or keyword table index. Following document retrieval, the LLM generates an output that incorporates information from both the query and the retrieved documents. Tool use Language models can perform long reasoning steps by calling external methods, such as numerical recipes, program interpreters, API calls, and so on. This can be prompt-engineered by describing the external methods in-context (an example of in-context learning) or finetuned into the model. Supervised finetuning A base model can be finetuned on a dataset of reasoning tasks with example solutions and reasoning traces. The finetuned model would then be able to generate reasoning traces for a given problem. As it is expensive to get humans to write reasoning traces for a SFT dataset, researchers have proposed ways to automatically construct SFT datasets. In rejection sampling finetuning (RFT), new reasoning traces are collected via a loop Sample a task prompt Generate many reasoning traces for the prompt. Use a verifier to remove reasoning traces with the wrong final answer. For each remaining trace, extract the set of equations appearing in it. Deduplicate the traces so that each one has a different set of equations. Add those to the dataset. Reinforcement learning A pretrained language model can be further trained by RL. In the RL formalism, a generative language model is a policy displaystyle pi  . A prompt specifying a task to solve is an environmental state x displaystyle x , and the response of the language model to the prompt is an action y displaystyle y . The probability that the language model responds x displaystyle x with y displaystyle y is ( y  x ) displaystyle pi (yx) . Training a reasoning language model by RL then consists of constructing a reward model r ( x , y ) displaystyle r(x,y) to guide the RL process. Intuitively, a reward model describes how desirable/appropriate/good the response is for the prompt. For reasoning language model, the prompt describes a reasoning task, and the reward would be high if the response solves the task, and low if the response fails to solve the task. For reasoning language models, the models response y displaystyle y may be broken down into multiple steps, in which case it is written as y 1 , y 2 , , y n displaystyle y_1,y_2,dots ,y_n . Outcome Reward Model Outcome reward model, or outcome-supervised RM (ORM), is a reward model that computes the reward of a step r ( x , y 1 , , y i ) displaystyle r(x,y_1,dots ,y_i) determined by the final answer r ( x , y 1 , , y i )  r ( x , y n ) displaystyle r(x,y_1,dots ,y_i)r(x,y_n) . They are also called verifiers. For tasks with an answer that is easy to verify, such as word problems in math, the outcome reward can simply be binary 1 if the final answer is correct, and 0 otherwise. If the answer is not easy to verify programmatically, humans can manually label the answers as correct or not, then the labels can be used to finetune a base model that predicts the human label. For other kinds of tasks, such as creative writing, where task performance is not binary true/false, one can train a reward model by finetuning a base model on human ranked preference data, such as used in reinforcement learning from human feedback. A base model can also be finetuned to predict, given a partial thinking trace x , y 1 , , y m displaystyle x,y_1,dots ,y_m , whether the final answer would be correct or not. This can then be used as a binary reward signal. The ORM is usually trained via logistic regression, i.e. minimizing cross-entropy loss. Given a PRM, an ORM can be constructed by multiplying the total process reward during the reasoning trace, or by taking the minimum, or some other method to aggregate the process rewards. Process Reward Model Process reward model, or process-supervised RM (PRM), is a reward model that computes the reward of a step r ( x , y 1 , , y i ) displaystyle r(x,y_1,dots ,y_i) determined by the steps so far ( x , y 1 , , y i ) displaystyle (x,y_1,dots ,y_i) . Given a partial thinking trace x , y 1 , , y m displaystyle x,y_1,dots ,y_m , a human can be queried as to whether the steps so far are correct, regardless of whether the ultimate answer would be correct. This can then be used as a binary reward signal. As human labels are expensive, a base model can be finetuned to predict the human labels. The PRM is usually trained via logistic regression, i.e. minimizing cross-entropy loss. As an example, in a 2023 OpenAI paper, 800K process labels were collected for 75K solution traces. A labeler would be presented with a solution trace, and keep labelling positive if the step progresses towards the solution, neutral if it is not wrong, but does not progress towards solution, and negative if it is a mistake. As soon as a negative label is entered, the labeler stops labeling that thinking trace, and begins labeling another one. The idea was that, while labelling subsequent reasoning steps can provide even richer supervision signals, simply labeling up to the first error was sufficient for training a competent PRM. As human labels are expensive, researchers have proposed methods to create PRM without human labels on the processes. Inspired by Monte Carlo tree search (MCTS), the Math-Shepherd method samples multiple continuations until the end, starting at each reasoning step y i displaystyle y_i , and set the reward at that step to be either  (correct answers)  (total answers) displaystyle frac text(correct answers)text(total answers) in the case of soft estimation, or  1 if one of the answers is correct 0 else displaystyle begincases1textif one of the answers is correct0textelseendcases in the case of hard estimation. This creates process reward using only an ORM, which is usually easier or cheaper to construct. After creating these process reward labels, a PRM can be trained on them. Some have tried a fully MCTS approach. One can also use an ORM to implicitly construct a PRM, similar to direct preference optimization. Guided sampling A trained ORM can be used to select the best response. The policy would rollout multiple responses, and a trained ORM would select the best response. This allows a simple form of test time compute scaling (best-of-N). A trained PRM can also be used to guide reasoning by greedy tree search. That is, the policy model generates several possible next reasoning steps, and the PRM selects the best one, and the process repeats. This is similar to how a trained ORM can be used to select the best response. Beam search perform better than greedy search. Lookahead search is another tree search method, where the policy model generates several possible next reasoning steps, then make a (partial) rollout for each. If a solution endpoint is reached during the forward simulation, the process halts early. Otherwise, the PRM is used to calculate the total reward for each rollout. The step with the highest rollout is selected. Self-consistency can be combined with an ORM. The model would be used to generate multiple answers, and the answers would be clustered, so that each cluster has the same answer. The ORM is used to compute the reward for each answer, and the rewards within each cluster is summed. The answer corresponding to the cluster with the highest summed reward is output. Applications Prompt engineering was discovered in GPT-3 as few-shot learning, which began a period of research into eliciting capacities of pretrained language models. It was then found that a model could be prompted to perform CoT reasoning, which improves its performance on reasoning tasks. Benchmark The reasoning ability of language models are usually tested on problems with unambiguous solutions that can be cheaply checked, and requires reasoning when solved by a human. Such problems are usually in mathematics and competitive programming. The answer is usually an array of integers, a multiple choice letter, or a program that passes unit tests within a limited runtime. Some common ones include GSM8K (Grade School Math) 8.5K linguistically diverse elementary school math word problems that require 2 to 8 basic arithmetic operations to solve. MMLU (Measuring Massive Multitask Language Understanding) 16,000 multiple-choice questions spanning 57 academic subjects including mathematics, philosophy, law, and medicine. GPQA (Google-Proof QA) 448 multiple-choice questions written by domain experts in biology, physics, and chemistry, and requires PhD-level experts to solve. HumanEval Programming problems where the solution is always a python function, often just a few lines long. The benchmark scores are of the following kinds passn The model is given n displaystyle n attempts to solve each problem. If any attempt is correct, the model earns a point. The passn score is the models average score over all problems. consn The model is given n displaystyle n attempts to solve each problem. If the most common answer is correct, the model earns a point. The consn score is the models average score over all problems. Here cons stands for consensus or majority voting. The passn score can be estimated more accurately by making N  n displaystyle Nn attempts, and use the unbiased estimator 1 ( N c n ) ( N n ) displaystyle 1-frac binom N-cnbinom Nn , where c displaystyle c is the number of correct attempts. See also Generative pre-trained transformer Neuro-symbolic AI Automated theorem proving Automated reasoning Reflection (artificial intelligence) Large language model References External links Fortes, Armando (2025-01-27), atfortes/Awesome-LLM-Reasoning, retrieved 2025-01-27 Huang, Jie Chang, Kevin Chen-Chuan (2023-05-26), Towards Reasoning in Large Language Models A Survey, arXiv2212.10403 Besta, Maciej Barth, Julia Schreiber, Eric Kubicek, Ales Catarino, Afonso Gerstenberger, Robert Nyczyk, Piotr Iff, Patrick Li, Yueling (2025-01-23), Reasoning Language Models A Blueprint, arXiv2501.11223